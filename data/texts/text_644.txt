In this paper, we present GRecX, an open-source TensorFlow framework for benchmarking GNN-based recommendation models in an eî€œcient and uniî€›ed way. GRecX consists of core libraries for building GNN-based recommendation benchmarks, as well as the implementations of popuplar GNN-based recommendation models. The core libraries provide essential components for building eî€œcient and uniî€›ed benchmarks, including FastMetrics (eî€œcient metrics computation libraries), VectorSearch (eî€œcient similarity search libraries for dense vectors), BatchEval (eî€œcient mini-batch evaluation libraries), and DataManager (uniî€›ed dataset management libraries). Especially, to provide a uniî€›ed benchmark for the fair comparison of diî€erent complex GNN-based recommendation models, we design a new metric GRMF-X and integrate it into the FastMetrics component. Based on a TensorFlow GNN library tf_geometric, GRecX carefully implements a variety of popular GNN-based recommendation models. We carefully implement these baseline models to reproduce the performance reported in the literature, and our implementations are usually more eî€œcient and friendly. In conclusion, GRecX enables uses to train and benchmark GNN-based recommendation baselines in an eî€œcient and uniî€›ed way. We conduct experiments with GRecX, and the experimental results show that GRecX allows us to train and benchmark GNN-based recommendation baselines in an eî€œcient and uniî€›ed way. The source code of GRecX is available at https://github.com/maenzhier/GRecX. Personalized recommendation is an important yet challenging task, which has attracted substantial attention in the past decade. Most traditional approaches consider recommendation as a matching task [16], and can be solved by estimating the matching score based upon semantic representations of users and items [5â€“7]. Recently, graph representation learning approaches are emerging tools to pursue a meaningful vector representation for each node in graphs, which can eî€ectively model users, items and their corresponding relationships. Graph Neural Networks (GNNs), such as GCN [10], GraphSAGE [3] and GAT [13], have shown impressive performance in aggregating feature information of neighboring nodes. In recommender systems, the interactions between users and items can be represented as a bipartite graph and the goal is to predict new potential edges (i.e., which items could a user be interested in), which 2,32,3 Table 1: An Example of Experimental Settings of Diî€erent Baselines. NSS denotes negative sampling strategies, where "single" and "multiple" represent the number of negative samples. The comparison is unfair due to diî€erent settings. Negtive Sampling Strategy single(1) single(1) multiple(500+) can be achieved with GNNs, which are called GNN-based recommendation methods. GNN-based recommendation techniques has attracted researchers and engineers from a variety of î€›elds, and they have been utilized to build various real-world applications such as medicine recommendation [8], micro-video recommendation [2, 15], and social recommendation [1, 17, 18]. Although many existing approaches provide oî€œcial implementations, it is still diî€œcult to build eî€œcient and uniî€›ed benchmarks due to the following limitations: (1) The evaluation of GNN-based recommendation approaches may involve many computationally expensive operations, which should be optimized to perform eî€œcient benchmarking. However, most oî€œcial implementations usually ignore the problem, and some of the computationally expensive operations are still widely used by these implementations. For example, these implementations usually rely on the matrix multiplication between the user representation matrix and the item representation matrix to perform the topK retrieval of items for users, which may result in large time and space complexity. (2) It is diî€œcult to build a uniî€›ed benchmark since diî€erent baselines may adopt diî€erent loss functions, diî€erent negative sampling strategies, etc. For example, UltraGCN [12] adopts NGCF [14] and LightGCN [4] as baselines. However, the negative sampling strategy of UltraGCN [12] is diî€erent from that of NGCF [14] and LightGCN [4]. As shown in Table 1, NGCF and LightGCN adopt a negative sampling strategy that uses only one negative sample, while UltraGCNâ€™s negative sampling strategy samples more than 500 negative samples. It is well known that GNN-based recommendation models can be improved by simply increasing the number of negative samples [8]. As a result, the experimental results reported by UltraGCN [12] cannot verify the eî€ectiveness of the model. In this paper, we present GRecX, an open-source TensorFlow framework for benchmarking GNN-based recommendation models in an eî€œcient and uniî€›ed way. To address the eî€œciency problem, we develop core libraries to provide essential components for build eî€œcient benchmarks, including FastMetrics (eî€œcient metrics computation libraries), VectorSearch (eî€œcient similarity search libraries for dense vectors), BatchEval (eî€œcient mini-batch evaluation libraries), and DataManager (uniî€›ed dataset management libraries). To build a uniî€›ed benchmark for the fair comparison of diî€erent complex GNN-based recommendation models, we design a new metric named GRMF-X and integrate it into the FastMetrics component. In addition, we also provide eî€œcient implementations of a variety of popular GNN-based recommendation models, which enable us to build a more comprehensive benchmark. We conduct experiments with GRecX, and the experimental results show that GRecX allows us to train and benchmark GNN-based recommendation baselines in an eî€œcient and uniî€›ed way. All features of GRecX and a collection of examples is provided with the source code, which is available at https://github.com/maenzhier/GRecX. Figure 1 shows the overall framework of GRecX, which mainly consists of core libraries and implementations of popuplar GNN-based recommendation models. In this section, we provide an overview of the framework of GRecX. The core libraries provide essential components including FastMetrics (eî€œcient metrics computation libraries), VectorSearch (eî€œcient similarity search libraries for dense vectors), BatchEval (eî€œcient mini-batch evaluation libraries), and DataManager (uniî€›ed dataset management libraries). There components enable us to build efî€›cient and uniî€›ed benchmarks In this section, we will introduce each component of GRecXâ€™s core libraries in detail. 2.1.1 FastMetrics. FastMetrics provides eî€œcient implementations for various recommendation metrics, such as NDCG@N, Precision, and Recall. It is non-trivial to implement eî€œcient metrics computation libraries for recommendation due to the complexity of real-world data. Moreover, for fair comparison, we design a new metric named GRMF-X and integrate it into FastMetrics. GRMF-X is the abbreviation forGainRelative toMFin terms of the metrics X, and it is deî€›ned as follows: ğºğ‘…ğ‘€ ğ¹ âˆ’ğ‘‹ (ğ‘€ğ‘‚ğ·ğ¸ğ¿, ğ¶ğ‘‡ ğ‘‹ ) =ğ‘‹ _ğ‘†ğ¶ğ‘‚ğ‘…ğ¸ (ğ‘€ğ‘‚ğ·ğ¸ğ¿, ğ¶ğ‘‡ ğ‘‹ )ğ‘‹ _ğ‘†ğ¶ğ‘‚ğ‘…ğ¸ (ğ‘€ğ¹, ğ¶ğ‘‡ ğ‘‹ )âˆ’1.0 (1) whereğ‘‹ _ğ‘†ğ¶ğ‘‚ğ‘…ğ¸ (ğ‘€ğ‘‚ğ·ğ¸ğ¿, ğ¶ğ‘‡ ğ‘‹ )is the evaluated score of model ğ‘€ğ‘‚ğ·ğ¸ğ¿under the metricğ‘‹and contextğ¶ğ‘‡ ğ‘‹, andğ‘€ğ¹denotes a tuned Matrix Factorization (MF) [11] model. We introduce this naive evaluation metric since it can eî€ectively help us to verify the superiority of GNN-based recommendation models. There are mainly two reasons why our uniî€›ed benchmark can beneî€›t from GRMF-X: (1) We observe that although some researchâ€™s experimental results show that their proposed model outperforms all the baselines, the reported performance of the baselines or the proposed models may not be competitive with the simple welltuned MF model. Note that although some research provide the performance of MF, they may employ a MF model that is not well tuned, which usually show poor performance. This shows that the authors do not conduct experiments with well-implemented baselines, and thus the experimental results are not convincing. (2) We also observe that some research do not conduct experiments of diî€erent baselines under the same context. Here we use contextğ¶ğ‘‡ ğ‘‹to denote the some important settings beyond the GNN architectures, such as the negative sampling strategies. For example, as shown in Table 1, NGCF and LightGCN employ a negative sampling strategy that uses only one negative sample, while UltraGCNâ€™s negative sampling strategy samples more than 500 negative samples. It is well known that GNN-based recommendation models can be improved by simply increasing the number of negative samples [8]; therefore, directly adopting the performance of the oî€œcial implementations may result in unfair comparison. In Equation 1, we implicitly constrain that diî€erent models are compared under the same contextğ¶ğ‘‡ ğ‘‹. Note some hyper-parameters such as the learning rate and L2 coeî€œcient are not considered as the context. For these hyper-parameters, diî€erent models may rely on diî€erent parameter settings to achieve their best performance, and we should carefully tune these hyper-parameters to obtain the best performance. 2.1.2 VectorSearch. Many GNN-based recommendation approaches perform recommendation by similarity based searching, which ranks items based on the similarity scores between the dense vector representation of users and items. Most evaluation metrics require a global scan over all the items, which may result in large time and space complexity. We leverage industrial solutions such as Faiss [9] to build eî€œcient similarity search libraries for dense vectors named VectorSearch, which can perform dense vector searching eî€œciently with millions of candidate items. 2.1.3 BatchEval. The evaluation of GNN-based recommendation models can beneî€›t from mini-batch techniques, which can take advantage of GPUâ€™s parallel processing ability to improve the eî€œciency of the evaluation. It requires a lot of tricks to design minibatch implementations on irregular real-world recommendation data. To provide friendly and handy mini-batch solutions, we design BatchEval. The users only need to provide the learned representations or the similarity computation function, and BatchEval can automatically perform eî€œcient mini-batch based evaluation with the provided information. 2.1.4 DataManager. DataManager provide abstract Dataset class as interfaces for users to custom handy dataset APIs. The abstract Dataset class can automatically handle the whole lifecycle of data processing, such as data downloading, data preprocessing, data caching, etc. Usually, users can easily custom their Dataset class by subclassing the abstract Dataset class, providing the download urls of raw dataset, and overriding the preprocessing process. Then, the abstract Dataset class will handle the rest of data processing process. In addition, we already implement several widely-used datasets as Dataset classes, which can be directly used to load these datasets. In this paper, we implement the state-of-the-art GNN-based recommendation algorithms (e.g. NGCF [14], LightGCN [4], UltraGCN [12]) and a basic MF model [11] as baselines. Especially, we carefully implement these baseline models to reproduce the performance reported in the literature, and our implementations are usually more eî€œcient and friendly. We conduct experiments on several benchmark datasets to provide a fair comparison of some popular baselines with GRecX. The complete experimental results are at https://github.com/maenzhier/GRecX. We are still working on some baselines to provide more comprehensive benchmark. Here, we only list partial experimental results on two benchmark datasets to demonstrate the performance of models implemented by GRecX and our new evaluation metrics GRMF-X. We use two datasets: yelp2018 and gowalla. Note that some dataset may have diî€erent versions, and we use the version used by LightGCN [4]. The statistics of the two datasets is listed in Table 2. In terms of the baselines, we choose a basic model MF [11], and two state-of-the-art GNN-based recommendation models UltraGCN [12] and LightGCN [4]. Here we employ a widely-used metrics NDCG@20 and our new metrics GRMF-X (GRMF-NDCG@20) for evaluation. As mentioned in Section 2.1.1, for other hyper-parameters, we carefully tune these hyper-parameters and report the performance with them. We use BCE and BPR as the ranking losses respectively and experimental results are shown in Table 3 and Table 4. For all comparable models, we set diî€erent parameters including number of negative samples and dimensionality of representation. In terms of number of negative samples, we set it to 1 and 800 to align with the original experimental settings of NGCF (one negative sample), LightGCN (one negative sample) and UltraGCN (800 negative samples). Note that UltraGCNmeans the UltraGCN model with using one negative sample in the training phase. For the dimensionality of representation setting, we set the dimensionality of learned user/item representations to 64 for all the models. Speciî€›cally, we î€›nd that NGCF model concatenates the output embedding of each graph convolution layer including input layer to construct the î€›nal users/itemsâ€™ representations, which are then combined with the CF mechanism for recommendations. Taking NGCF model with three layers as an example, its dimensionality of î€›nal output representations is equal to 256 (3âˆ—64+64), which may be unfair to other models. So, we design MLP+MF model which just replaces all graph convolution layer of NGCF model with an MLP layer. And we also show results of MF model with 64- and 256-dimensional representations. In this paper, we implement a simple well-tuned MF model as a important baseline. And we introduce a naive evaluation metric GRMF-X for verifying the superiority of GNN-based recommendation models eî€ectively. The results for recommendations on two datasets in terms of the BCE loss and BPR loss are reported in Table 4 and 3, respectively. Note that the experimental results are preliminary and will be updated continuously. In addition, compared with results of UltraGCNin table 4, results of LightGCN with a same negative sampling startegy in table 3 shows better performances. Interestingly, compared with results of NGCF in table 3 and table 4, MF model (64- and 256-dimensional representations) and MLP+MF model achieve better results on two datasets in most cases. In this paper, we present GRecX, an open-source TensorFlow framework for benchmarking GNN-based recommendation models in an eî€œcient and uniî€›ed way. GRecX consists of core libraries for building GNN-based recommendation benchmarks, as well as the implementations of popuplar GNN-based recommendation models. With GRecX, we can eî€œciently perform fair comparison between diî€erent GNN-based recommendation models in a uniî€›ed benchmark. In the future, we will integrate more baselines into GRecX and further improve the performance of both the core libraries and implementations.