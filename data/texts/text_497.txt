A long-standing challenge in Recommender Systems (RCs) is the data sparsity problem that often arises when users rate very few items. Multi-Target Multi-Domain Recommender Systems (MTMDR) aim to improve the recommendation performance in multiple domains simultaneously. The existing works assume that the data of diî€erent domains can be fully shared, and the computation can be performed in a centralized manner. However, in many realistic scenarios, separate recommender systems are operated by diî€erent organizations, which do not allow the sharing of private data, models, and recommendation tasks. This work proposes an MTMDR based on Assisted AutoEncoders (AAE) and Multi-Target Assisted Learning (MTAL) to help organizational learners improve their recommendation performance simultaneously without sharing sensitive assets. Moreover, AAE has a broad application scope since it allows explicit or implicit feedback, user- or item-based alignment, and with or without side information. Extensive experiments demonstrate that our method signiî€›cantly outperforms the case where each domain is locally trained, and it performs competitively with the centralized training where all data are shared. As a result, AAE can eî€ectively integrate organizations from diî€erent domains to form a community of shared interest. Recommender Systems, Multi-Organization Learning, Distributed Machine Learning ACM Reference Format: Enmao Diao, Vahid Tarokh, and Jie Ding. 2021. Privacy-Preserving MultiTarget Multi-Domain Recommender Systems with Assisted AutoEncoders. In Proceedings of ACM Conference (Conferenceâ€™17). ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn In recent years, Recommender Systems (RSs) have become one of the most popular techniques in web applications that involve big data. This is because they can eî€ectively extract helpful information for relevant users, e.g., in recommending restaurants, videos, and e-commerce products [1,18]. However, a long-standing challenge in recommender systems is the data sparsity problem because the users usually rate very few items. To address this issue, the research direction of Cross-Domain Recommendation (CDR) has been developed to leverage ratings from a source domain where users may have relatively more information to improve the performance of a target domain [2]. A system that simultaneously improves the performance of both the source and target domains is referred to as Dual-Target CDR [33]. Since then, Multi-Target Multi-Domain Recommendation (MTMDR) has become a popular solution to improve the recommendation performance of multiple domains simultaneously. However, most existing works on this topic require that the data of diî€erent domains are fully shared, and the computation must be performed in a centralized manner [3,28,31,34]. Since most recommender systems are built upon usersâ€™ sensitive data, e.g., user proî€›les and usage history, and the model and task information are also proprietary to organizational learners [25], collaborations among organizations from diî€erent domains are often restricted by ethical, regulatory, and commercial constraints [27,33]. Therefore, a privacy-preserving MTMDR, as shown in Figure 1, is the keystone for leveraging isolated data held by various domains. This work proposes an MTMDR based on Assisted AutoEncoders (AAE) and Multi-Target Assisted Learning (MTAL), which help different organizations improve their recommendation performance simultaneously while preserving their local data, models, and task labels. In particular, each organization will calculate a set of â€˜residualsâ€™ and broadcast these to other organizations. These residuals approximate the fastest direction of reducing the training loss in hindsight. Subsequently, other organizations will î€›t the residuals using their local data, models, and objective functions and broadcast the î€›tted values back to each other. Each learner will then assign weights to its peers to approximate the fastest direction of learning. The prediction will be aggregated from the î€›tted values. The above procedure is repeated until all organizations accomplish a suî€œcient level of learning. Moreover, our approach can handle explicit or implicit feedback [9], user- or item-based alignment [33], and with or without side information [24]. We perform extensive experiments to demonstrate that our method signiî€›cantly outperforms the case where each domain is locally trained. It performs competitively with centralized training where all data are shared. As a result, our method can eî€ectively integrate organizations from diî€erent domains to form a community of shared interest, as shown in Figure 1. Our main contributions are summarized below. â€¢We present a new paradigm of privacy-preserving Multi- Target Multi-Domain Recommendation (MTMDR), which can eî€ectively improve the recommendation performance of diî€erent domains simultaneously, without sharing their local data, models, or task labels. Figure 1: (a) User aligned (b) Item aligned Muti-Target Multi-Domain Recommender Systems. Organizations from diî€erent domains form a community of shared interest. Assisted Learning as a systematic solution to the proposed MTMDR scenario. It exchanges domain-speciî€›c information through pseudo-residuals î€›tted with local data and models. Our method covers broad application scenarios, including explicit or implicit feedback, user- or item-based alignment, and with or without side information. â€¢We conduct extensive experiments to demonstrate that our method can signiî€›cantly outperform the case where each domain is locally trained and perform competitively with the centralized training where all data are shared. As a result, AAE can eî€ectively integrate organizations from diî€erent domains to form a community of shared interest. Recommender Systems (RCs) predict usersâ€™ preference on items and provide personalized recommendations for users [1,18]. Recommendation approaches are mainly classiî€›ed into three categories [1,10], namely collaborative î€›ltering, content-based recommendation, and hybrid systems. Speciî€›cally, collaborative î€›ltering learns from user-item interactions, while the content-based recommendation is primarily based on side information. Hybrid systems leverage both user-item interactions and side information. Our proposed method is a hybrid recommender system that can integrate 1) user-item interactions, 2) either explicit feedback (e.g., userâ€™s previous ratings) or implicit feedback (e.g., browsing history), and 3) side information. Cross-Domain Recommender SystemsIn most realistic scenarios, users usually provide very few ratings among many items [19]. It results in a highly sparse interaction matrix which hinders the recommendation performance. To address the data sparsity problem, cross-domain recommendation (CDR) [2] has been proposed to utilize relatively richer information from the source domain to improve the recommendation performance in the target domain. Recent CDR methods use the latent factors obtained from a source domain for a target domain [6,16,29,30,32]. Furthermore, DualTarget CDR was proposed to improve the performance of both source and target domains [33]. A natural extension of Dual-Target CDR is Multi-Target Multi-Domain Recommendation (MTMDR), which aims to improve the recommendation performance of multiple domains simultaneously [28,34]. In this research direction, the existing works assume that diî€erent data domains can be trained in a centralized manner. However, in many practical scenarios, separate recommender systems are operated by various organizations, prohibiting the sharing of private data and model parameters. To address this challenge, we will propose a method to enhance the performance of multiple domains simultaneously without transmitting local data and models. Autoencoder-based Recommender SystemsAutoRec [20] applies AutoEncoders (AE) to RCs and has found many successful applications. It takes user vectors or item vectors as input and reconstructs them in the output layer. Several recent studies improve the performance of AutoRec by using denoising AE [13,22], variational AE [14,15], and Dropout [5,12,21]. Side information can also be leveraged with autoencoders to tackle the cold start problem [23]. AutoEncoder-based recommender systems have two variants, namely user-based and item-based. Our method can leverage user-based and item-based AutoEncoders to handle user-based and item-based alignment in MTMDR, respectively. Assisted Learning (AL) [26] is a collaborative learning framework where organizations being assisted or assisting others do not share private local data and models. The recently proposed Gradient Assisted Learning (GAL) [4] generalizes AL from a sequential protocol to parallel aggregation across multiple organizations. This work develops and generalizes the GAL algorithm for privacy-preserving MTMDR based on a novel design of autoencoder-based recommender systems. Recommender SystemsLetU = {ğ‘¢, . . . , ğ‘¢}andV = {ğ‘£, . . . , ğ‘£} denote respectively the set of users and items, whereğ‘šis the number of users andğ‘›is the number of items. We have a user-item interaction or rating matrixR âˆˆ R, whereğ‘Ÿâˆˆ Rdenotes the rating that userğ‘¢gives to itemğ‘£. A recommender system ğ¹ (Â·)predicts the ratingË†ğ‘Ÿgiven a pair of user and item(ğ‘¢, ğ‘£), i.e.Ë†ğ‘Ÿ= ğ¹ (ğ‘¢, ğ‘£). The recommender system can also incorporate side information such as user proî€›leğ‘ âˆˆ Sand item attributes ğ‘ âˆˆ Sthat are associated with the userğ‘¢and itemğ‘£, so that Ë†ğ‘Ÿ= ğ¹ (ğ‘¢, ğ‘£, ğ‘ , ğ‘ ). We train a recommender system by minimizing an average of loss values in the form of ğ¿(Ë†ğ‘Ÿ, ğ‘Ÿ). Multi-Target Multi-Domain Recommender Systems (MTMDR)Suppose that there areğ¾observed domains including theî€ˆî€‰î€ˆî€‰ user setsU, . . . , Uand the item setsV, . . . , V. We haveî€ˆî€‰ a set of rating matricesR, . . . , R, where domainğ‘˜hasğ‘šusers andğ‘›items. Domainğ‘˜can train a separate recommender system Ëœğ¹(Â·). However, in order to resolve the data sparsity problem, a multi-domain recommender systemğ¹(Â·)aims to improve the performance of the locally trained recommender system by leveraging the common usersUor the common itemsVthat appear in other sets of users or items. The common users or items are those shared between a pair of domains, described by As shown in Figure 1, multi-domain recommender systems can be categorized based on their alignment. Depending on the application scenario, a user-aligned multi-domain recommender system leverages the common users, while an item-aligned one leverages the common items. The general goal is to develop a multi-domain recommender system that signiî€›cantly outperforms each locally trained recommender system, and that performs competitively with the recommender systemğ¹ (Â·)jointly trained from all user and item sets. In other words, where the expectationEis over test data. To achieve the above objective for all data domains whose data are distributed in practice, we develop a multi-target multi-domain recommender system that does not share the rating matrixR, user proî€›leS, item attribute S, model ğ¹(Â·), and objective function ğ¿(Â·). AutoEncodersAn autoencoder is a network consisting of an encoderğ‘§ = ğ¸ (ğ‘¥):ğ‘…â†’ ğ‘…and a decoderË†ğ‘¥ = ğ· (ğ‘¥):ğ‘…â†’ ğ‘…, whereğ‘‘andğ‘‘are dimensions of the input vectorğ‘¥and output vectorË†ğ‘¥, respectively. Unlike Collaborative Filtering, the input of an autoencoder-based recommender system is not a pair of user ğ‘¢and itemğ‘£. Instead, the input of a user-based autoencoder is a partially observed vectorr= (ğ‘Ÿ, . . . , ğ‘Ÿ) âˆˆ Rwhich represents the ratings of userğ‘¢giving to all itemsğ‘£. . . ğ‘£. We represent the partially observed vectorras a sparse vector, where the unknown ratings are zeros. Similarly, the input of an itembased autoencoder isr= (ğ‘Ÿ, . . . , ğ‘Ÿ) âˆˆ R. The encoder transforms the observed vector into a dense lower-dimensional code, namelyz= ğ¸ (r)orz= ğ¸ (r). The decoder produces the output vectorË†r= ğ· (ğ¸ (r)) âˆˆ RandË†r= ğ· (ğ¸ (r)) âˆˆ R, whereğ‘›is the number of items, andğ‘šis the number of users. The dimension of the input vector is the same as that of the output vector for an autoencoder-based reocommender system. We train an autoencoder-based recommender system by minimizing the average loss between the input and output vectors. When computing the loss function, we mask out the unobserved output ratings. Assisted AutoEncodersOur model is inspired by a series of autoencoder-based recommender systems [5,12,20], but it has an unusual architecture. Suppose that there areğ¾observed domains. Each domain can train its own Assisted AutoEncoder (AAE). The input of a user-based AAE is a partially observed vectorr= (ğ‘Ÿ, . . . , ğ‘Ÿ) âˆˆ Rthat represents the ratings of userğ‘¢giving to all itemsğ‘£. . . ğ‘£in domainğ‘˜, whereğ‘›is the number of items in domainğ‘˜. We represent the partially observed vectorras a sparse vector, where the unknown ratings are zeros. Similarly, the input of an item-based AAE isr= (ğ‘Ÿ, . . . , ğ‘Ÿ) âˆˆ R, where ğ‘šis the number of users in domainğ‘˜. The key diî€erence between previous works and ours is that the output vector of a user-based and item-based AAE has the dimension of total number of items or users, i.e.Ë†r= ğ· (ğ¸(r)) âˆˆ RandË†r= ğ· (ğ¸(r)) âˆˆ R, where ğ‘›is the total number of items, andğ‘šis the total number of users, across all domains. Figure 2: Assisted AutoEncoders (AAE) incorporate side information by summing up the codes encoded with two additional encoders corresponding to the user proî€›le and item attribute. For each domain ğ‘˜, the input dimension is the number of users or items of that domain, while the output dimension is the total number of users or items of all domains. It uses tanh(Â·) for nonlinear activation and Dropout for regularization. Figure 2 demonstrates an example of the AAE network. Both encoder and decoder consist of fully connected (FC) layers. We usetanh(Â·)as our nonlinear activation function because it is important for hidden layers to contain negative parts [5,12]. We adopt Dropout [21] at the encoded space as suggested by [12]. We also consider side information of domainğ‘˜such as user proî€›le Sâˆˆ Rand item attributesSâˆˆ R, whereğ‘‘and ğ‘‘denote the feature dimension of user proî€›le and item attribute at domainğ‘˜, respectively. We can transform non-structural side information such as text, image, and video into dense feature representations [24]. AAE can train two additional encoders for the user proî€›le and item attribute. For a user-based AAE, the input of the encoder for user proî€›le issâˆˆ R, and the input of the Figure 3: Learning and Prediction stages for Multi-Target Assisted Learning (MTAL). Organizations from diî€erent domains construct the set of common users or items. The domain ğ‘˜ learns local models ğ‘“ predicted outputsË†ğ‘Ÿfrom all the domains 1 to ğ¾. encoder for item attribute is the summation of the observed itemÃ attributes, namelysâˆˆ R, whereVrepresents the set of observed items for userğ‘¢. When considering side information, we sum up the codes encoded from ratings and side information. We cannot train AAE directly because, in our privacy-preserving setting, an isolated domain does not have access to the ratings from other domains. To address this issue, we apply the assisted learning framework and develop a multi-target algorithm for MTMDR. We propose Multi-Target Assisted Learning (MTAL) as demonstrated in Algorithm 1, so that each domain can operate on its own local data, model, and objective function. Our algorithm considers both user- and item-aligned multi-domain recommender systems. The item-aligned case can be regarded as a transposed version of the user-aligned case. Figure 3 gives a î€ow chart of our algorithm. Next, we describe the learning and prediction procedures in detail. At the beginning, each organization, or domainğ‘˜, coordinates with others to construct the set of common usersUor itemsV, depending on whether the system is user-aligned or item-aligned. During the Learning stage, each domain initializes with an unbiased base modelğ¹(R) = E(R)= ğµÃrâˆˆ R. For the explicit feedback, the base model is the average ratings, whereğµ is the number of ratings of each item at domainğ‘˜. For the implicit feedback, the base model is the popularity estimates, whereğµis the number of users at domainğ‘˜. At every assistance roundğ‘¡, each domain computes its own â€˜pseudo residualsâ€™ğ‘Ÿand broadcast its common residuals ğ‘Ÿto another domain ğ‘™, where ğ‘Ÿ= âˆ’ğœ•ğ¿ğ¹(R), R, ğ‘Ÿ= {ğ‘Ÿ, ğ‘– âˆˆ Uâˆ© U}, (5) ğ¿(Â·)is the overarching loss function used by domainğ‘˜, andğ¹(R) is the output from the previous assistance round. Here, the superscript ofğ‘Ÿmeans the domainğ‘–transmits the residuals to domain ğ‘—, or the domainğ‘—receives from domainğ‘–. In Figure 3, we letğ‘Ÿ denote all the received residuals of domainğ‘˜from domains 1 toğ¾. Then, each domain aggregates its own residuals together with the received common residuals from other domains into â€˜pseudo targetsâ€™Ë†R= {ğ‘Ÿ, . . . , ğ‘Ÿ} âˆˆ R. Note thatË†Ris a sparse matrix of sizeğ‘šÃ— ğ‘›. The dimension of items increases fromğ‘› toğ‘›, because the received common residuals from other domains introduce unobserved targets of items. Next, each domain will î€›t a local AAEğ‘“with the pseudo targetsË†Rand the local loss functionâ„“(Â·). Each domain will then broadcast the common predicted outputsË†ğ‘Ÿto domains 1 to ğ¾. Subsequently, each domain can train suitable gradient assistance weightsğ‘¤to aggregate received outputs, and gradient assisted learning rate ğœ‚to minimize the overarching loss. whereğ‘ƒ= {ğ‘¤ âˆˆ R:Ãğ‘¤=1, ğ‘¤â‰¥0}denotes the probability simplex. We note that optimizingğ‘¤andğœ‚is optional. We have conducted ablation studies ofğ‘¤andğœ‚in Section 5. Finally, the output at round ğ‘¡ is The above procedure can be iterated forğ‘‡assistance rounds until we obtain a satisfactory performance (e.g., on validation data). In the Prediction stage, each domain will predict outputs from their local modelsğ‘“for all assistance rounds from 1 toğ‘‡. The Algorithm 1: MTAL: Multi-Target Assisted Learning for Recommender Systems. Input: ğ¾ decentralized data domains, domain ğ‘˜ holding rating matrix R âˆˆ R Learning Stage: Alignment: Prediction Stage: Gather predictionsË†ğ‘Ÿ= ğ‘“(R), ğ‘¡ = 1, . . . , ğ‘‡ from each domain ğ‘—, ğ‘— = 1, . . . , ğ¾ Return ğ¹(R) predicted results will be broadcast to other domains, which will aggregate them with gradient assistance weightsğ‘¤and gradient assisted learning rateğœ‚, to eventually generate a î€›nal prediction ğ¹(ğ‘¥)that is implicitly operated onR. Compared with the Learning stage, the Prediction stage does not need synchronization of each assistance round because we can operate all local AAEs acrossğ‘‡ rounds before broadcasting the outputs. Our framework can help participants to form a shared community of interest. In particular, every data domain can provide its own task and seek assistance from others. The end-to-end assistance provided for another organization does not require the sharing of anyoneâ€™s proprietary data, models, and objective functions. In practice, the participating organizations may receive î€›nancial rewards from the assisting ones. Consequently, all participants can become mutually beneî€›cial to each other. Why AutoEncoders?We choose autoencoders as our local backbone model because AE is naturally compatible with our proposed MTAL algorithm. In particular, autoencoders can treat the rating matrix as tabular data. The rows and columns of the rating matrix are considered as data sample and feature space, respectively. A multi-domain system can be viewed as each domain holding a subset of the feature space. We can adopt user-based autoencoders for Ë†R= {ğ‘Ÿ, . . . , ğ‘Ÿ} âˆˆ R Ë†ğ‘Ÿto other domains , ğ‘– âˆˆ Uâˆ© U} user-aligned multi-domain recommender systems, and item-based for item-aligned systems. On the contrary, collaborative î€›ltering takes user-item pairs as the input, which do not contain a feature space. Thus, it is not suitable to integrate collaborative î€›ltering with our MTAL algorithm. It is worth mentioning that the proposed MTAL algorithm is not restricted to autoencoders. A possible future work is to discover better local models which can treat the rating matrix as tabular data for MTAL. Loss functionOur algorithm involves two kinds of loss functions, namely a local loss functionâ„“(Â·)for î€›tting the pseudo-targets Ë†R, and an overarching loss functionğ¿(Â·)for î€›tting the ratings Rin hindsight. Since î€›tting the pseudo-targets is a regression problem, it is standard to letâ„“(Â·)beâ„“- orâ„“-norm. Depending on the ratings are explicit or implicit feedback, we choose diî€erent overarching loss functions. We use regression loss functions such as theâ„“-norm for explicit feedback and classiî€›cation loss functions such as binary cross-entropy for implicit feedback. Technical noveltiesOur proposed AAE absorbs many merits of previous autoencoder-based recommender systems, such astanh(Â·)activation function [5,12], encoders for side information [23], and Dropout [5,12]. AAE extends the scope of standard autoencoders for multi-domain recommender systems by increasing the output dimension. Moreover, the proposed MTAL algorithm develops AL in two ways. First, we generalize AL from a singletarget to a multi-target learning framework. In particular, GAL [4] assumes multiple participants assist one organization. Our MTAL method î€›ts multiple targets of all domains with a single local model. Therefore, MTAL can simultaneously improve the performance of all participants. Meanwhile, we also perform ablation studies of gradient assistance weights and gradient assisted learning rate. Our results show that 1) a constant gradient assisted learning rate suits the explicit feedback, while an optimized gradient assisted learning rate suits the implicit feedback, and 2) optimized gradient assistance weights tend to improve the result if the domain partition is Non-IID. PrivacyOur proposed algorithm allows diî€erent domains to improve their recommendation performance without sharing their local data, models, and objective functions. We consider this requirement as a bottom line for privacy-preserving MTMDR. Nevertheless, we are aware that it is possible to apply further privacy enhancement techniques such as Diî€erential Privacy (DP) by adding noises to the transmitted residuals [4]. DataWe conduct experiments with MovieLens100K (ML100K), MovieLens1M (ML1M), and MovieLens10M (ML10M) datasets [7] under various circumstances. We have following control settings. 1) Explicit vs. implicit feedback. The explicit feedback is the default rating(1âˆ’5), while the implicit feedback is the binarization of default ratings (positive if greater than 3.5). 2) User- vs. item-alignment. We introduce two types of data partition for multi-domain recommender systems. The â€˜Uniformâ€™ data partition splits items (respectively users) for user-aligned (respectively item-aligned) multi-domain recommender systems intoğ¾ =8 domains. Each domain has roughly the same number of items or users. The â€˜Genreâ€™ data partition splits items according toğ¾ =18 of movies for user-aligned multi-domain recommender systems. 3) With vs. without side information. The user proî€›les like gender and occupation are available for ML100K and ML1M datasets. The item attributes are one-hot indicators of genres. 4) Ablation studies. We conduct ablation studies for gradient assisted learning rate, gradient assistance weights, and partial alignment. In addition, the summary statistics of each dataset are elaborated in Table 7 in the Appendix. For all datasets, we train on 90% of the available data and test on the remaining. We conduct four random experiments for all datasets with diî€erent seeds. The standard errors of all our results are smaller than 1ğ¸ âˆ’ 3. ModelWe compare the results of AAE with the unbiased Base model described in Section 4.2, and collaborative î€›ltering methods such as Matrix Factorization (MF) [17], Multi-Layer Perceptron (MLP) [8], and Neural Collaborative Filtering (NCF) [8]. We also use a subscriptğ‘ to represent models incorporating side information, namely MFand AAE. Our proposed method is a novel integration of Assisted Autoencoders (AAE) and Multi-Target Assisted Learning (MTAL) algorithm. Although we have used the same model architecture for every domain in the experiments, our algorithm does not require every domain to use the same local models. An organization can choose the size of autoencoders based on its local computational capabilities. We use theâ„“-norm as the local loss functionâ„“throughout experiments. We use the Adam optimizer with a learning rate of 10[11]. We set the number of assistance roundsğ‘‡ =10 throughout our experiments. The number of local training epochs at each round is 20. To optimize the gradient assistance weights and gradient assisted learning rate, we use the Limited-Memory BFGS (L-BFGS) optimizer with a learning rate of 1. Details of model architecture and learning hyper-parameters are included in Tables 5 and 6 in the Appendix. BaselineWe compare the proposed method with two baselines, â€˜Jointâ€™ and â€˜Aloneâ€™. The â€˜Jointâ€™ denotes the centralized case where all the data are held by one organization. The â€˜Aloneâ€™ denotes the case where every domain trains and tests on its local data. As mentioned in Equations 3 and 4, the goal of our method is to signiî€›cantly outperform the â€˜Aloneâ€™ case and perform competitively with the â€˜Jointâ€™ case. We note that in â€˜Aloneâ€™ and â€˜Jointâ€™ cases when AAE is not equipped with MTAL, the input and output dimensions of AAE are the same. In that case, the AAE reduces to a conventional autoencoder-based recommender system. We tabulate experimental results in Tables 1, 2, and 4. We illustrate evaluations across all assistance rounds in Figures 4, 5, and 6. We also provide ablation studies of gradient assisted learning rate, gradient assistance weights, and partial alignment in Tables 3, 8, and 9, respectively. We provide detailed discussions below. Explicit vs. Implicit feedbackAs shown in Tables 1, 2, and 4, AAE is able to eî€ectively handle explicit and implicit feedback by using diî€erent objective functions. We consider explicit feedback as a regression task. For explicit feedback, the objective function is â„“-norm, and the metric is Root Mean Squared Error (RMSE). For implicit feedback, it was shown that we could treat the problem as a binary classiî€›cation task [17] and use a ranking criterion Mean Average Precision (MAP) to evaluate the performance. Therefore, we use binary cross-entropy as our objective function for implicit feedback. As shown in Tables 1, 2, and 4, AAE performs competitively with the baseline recommender systems in â€˜Jointâ€™ scenario with explicit feedback. However, compared with baseline models, AAE does not perform well in the â€˜Aloneâ€™ scenario with explicit feedback. Moreover, as shown in the results of ML10M, AAE sometimes performs worse in â€˜Jointâ€™ than it does in â€˜Aloneâ€™. It shows that the performance of autoencoder-based recommender systems may be sensitive to the input dimension, namely the number of local items ğ‘›or the number of local usersğ‘šin a user- or item-based recommender. Speciî€›cally, when an organization has explicit feedback for a small number of items or users, it should use collaborative î€›lter methods instead of autoencoder-based methods. Moreover, when the organization has explicit feedback for a very large number of items or users, the width and depth of hidden layers of AAE may also increase to alleviate the curse of dimensionality. On the other hand, AAE outperforms baseline models in both â€˜Jointâ€™ and â€˜Aloneâ€™ cases with implicit feedback. Our proposed method AAE equipped with MTAL signiî€›cantly outperforms all â€˜Aloneâ€™ cases with diî€erent backbone models for both explicit and implicit feedback. Our proposed method also performs competitively with the â€˜Jointâ€™ cases with explicit feedback. Furthermore, our method can moderately outperform the â€˜Jointâ€™ case with implicit feedback and â€˜Genreâ€™ data partition. It was demonstrated in [4] that local models could specialize on the Non-IID partitioned data and outperforms the â€˜Jointâ€™ case. User- vs. Item-alignmentIt needs to point out that that userand item-alignment have no impact on baseline models because the input of the base model and collaboratively î€›lter models can be a pair of user and item. However, we use diî€erent training procedures for user- and item-aligned recommender systems to compare with autoencoder-based systems. Recall that user- and item-based autoencoder-based recommender systems handle userand item-alignment in a tabular fashion, respectively. Therefore, each data sample of a user- or item-aligned task corresponds to the ratings of one user giving to associated items or one item received from associated users. The results of baseline models for explicit feedback are similar for both user- and item-alignment. As for implicit feedback, the results of baseline models are diî€erent because of the ranking criterion MAP. We point out that the item-based AAE outperforms baseline models in the â€˜Jointâ€™ case with both types of feedback, and in the â€˜Aloneâ€™ case with implicit feedback. AAE equipped with MATL signiî€›cantly outperforms all â€˜Aloneâ€™ cases for both types of alignment, and also performs close to the â€˜Joint caseâ€™ for useralignment. Furthermore, our method also moderately outperforms the â€˜Jointâ€™ case for item-alignment. With vs. Without side informationRecall that we use a subscriptğ‘ to denote models incorporating side information. The side information contains user proî€›les such as occupation and gender, and item attributes such as genre. The results show that all models can beneî€›t from the side information. However, autoencoder-based recommender systems do not leverage the side information as much as collaborative î€›lter methods. Some potential future works include incorporating raw side information such as text, image, and video, better modeling techniques of side information, and using side information from domains without ratings. Gradient assisted learning rateAs shown in Tables 3, 8, and 9, we perform an ablation study of the gradient assisted learning rate ğœ‚. We have three control groups ofğœ‚, including â€˜ğœ‚=0.1â€™, â€˜ğœ‚= 0.1â€™, and â€˜Optimizeğœ‚â€™. Here, â€˜ğœ‚=0.1â€™ and â€˜ğœ‚=0.3â€™ represent a constant gradient assisted learning rate for all domains. In all three cases, the optimization of gradient assistance weights is disabled. In particular, the weighted average becomes a direct average of outputs. As demonstrated in Figures 4, 5, and 6, the optimizationğœ‚ for explicit feedback may result in gradient explosion due to a very largeğœ‚. However, compared withğœ‚=0.1, a moderately large ğœ‚=0.3 can improve the convergence rate and result. On the other hand, the optimization ofğœ‚for implicit feedback greatly improves the performance, because the binary cross-entropy function may require a largeğœ‚at the early assistance rounds. The results show thatğœ‚has a large impact on the success of our method. A more reliable solution to choose ğœ‚is to use a validation set. Gradient assistance weightsWe setğœ‚=0.1 for the ablation study of gradient assistance weightsğ‘¤. As shown in (b,d) of Figures 4, 5, and 6, the optimization ofğ‘¤labeled as â€˜AAE (Optimize ğ‘¤)â€™ moderately improves the performance for â€˜Genreâ€™ data partition. Although the impact ofğ‘¤is not obvious in other scenarios, previous works show that usingğ‘¤is robust against noisy residuals when we apply privacy enhancement techniques. A potential future work is to develop better techniques to aggregate outputs from each domain. Partial AlignmentTo compare with baseline models, we assume all users or items are common for user-aligned or item-aligned recommender systems. As mentioned in Equations 1 and 2, the common users or items are shared between a pair of domains. Speciî€›cally, two domains can share a subset of their users or items. Therefore, we conduct an ablation study of partial alignment. We assume 50% of users or items are shared among all domains, labeled as â€˜50% Alignmentâ€™, while the other half is unique for each domain. We setğœ‚=0.1 and disable the optimization ofğ‘¤. As demonstrated in Figures 4, 5, and 6, 50% alignment performs worse than full alignment with explicit feedback and close to the full alignment with implicit feedback. It indicates that our proposed solution is robust enough to improve the performance of partially aligned domains. Table 1: Results of the ML1M dataset for explicit and implicit feedback measured with RMSE and MAP, user- and itemalignment, with and without side information, and Uniform (ğ¾ = 8) and Genre (ğ¾ = 18) data partition. In this work, we present a new framework of privacy-preserving Multi-Target Multi-Modal Recommender system (MTMDR) based on Assisted AutoEncoders (AAE) and Multi-Target Assisted Learning (MTAL). Our proposed solution allows diî€erent organizations to improve their recommendation performance simultaneously while preserving their local data, models, and task labels. Our method covers broad application scenarios, including user- or item-based Figure 4: Results of the ML1M dataset across all assistance rounds. MTAL signiî€›cantly outperforms â€˜Aloneâ€™ and p erforms close to â€˜Joint.â€™ (a-d) Explicit feedback and User-Alignment. (e-h) Implicit feedback and User-Alignment. (i-q) Item Alignment. Table 2: Results of the ML10M dataset for explicit and implicit feedback measured with RMSE and MAP, user- and item alignment, with and without side information, and Uniform (ğ¾ = 8) and Genre (ğ¾ = 18) data partition. alignment, explicit or implicit feedback, and with or without side information. Extensive experiments demonstrate that our method Table 3: Ablation study of the ML1M dataset for gradient assistance weights, gradient assisted learning, and partial alignment. signiî€›cantly outperforms the case where each domain is locally trained, and it performs competitively with the centralized training where all data are shared. Consequently, our approach can eî€ectively integrate organizations from diî€erent domains to form a community of shared interest. TheAppendixincludes further experimental details and results.