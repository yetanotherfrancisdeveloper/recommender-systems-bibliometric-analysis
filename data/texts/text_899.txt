The issue of fairness in recommendation is becoming increasingly essential as Recommender Systems (RS) touch and inî€uence more and more people in their daily lives. In fairness-aware recommendation, most of the existing algorithmic approaches mainly aim at solving a constrained optimization problem by imposing a constraint on the level of fairness while optimizing the main recommendation objective, e.g., click through rate (CTR). While this alleviates the impact of unfair recommendations, the expected return of an approach may signiî€›cantly compromise the recommendation accuracy due to the inherent trade-oî€ between fairness and utility. This motivates us to deal with these conî€icting objectives and explore the optimal trade-oî€ between them in recommendation. One conspicuous approach is to seek a Pareto eî€œcient/optimal solution to guarantee optimal compromises between utility and fairness. Moreover, considering the needs of real-world e-commerce platforms, it would be more desirable if we can generalize the whole Pareto Frontier, so that the decision-makers can specify any preference of one objective over another based on their current business needs. Therefore, in this work, we propose a fairness-aware recommendation framework using multi-objective reinforcement learning (MORL), called MoFIR (pronounced â€œmore fairâ€), which is able to learn a single parametric representation for optimal recommendation policies over the space of all possible preferences. Specially, we modify traditional Deep Deterministic Policy Gradient (DDPG) by introducing conditioned network (CN) into it, which conditions the networks directly on these preferences and outputs Q-value-vectors. Experiments on several real-world recommendation datasets verify the superiority of our framework on both fairness metrics and recommendation measures when compared with all other baselines. We also extract the approximate Pareto Frontier on real-world datasets generated by MoFIR and compare to state-of-the-art fairness methods. â€¢ Information systems â†’ Recommender systems;â€¢ Computing methodologies â†’ Sequential decision making. Etsy Inc. Recommender System; Multi-Objective Reinforcement Learning; Pareto Eî€œcient Fairness; Unbiased Recommendation ACM Reference Format: Yingqiang Ge, Xiaoting Zhao, Lucia Yu, Saurabh Paul, Diane Hu, Chu-Cheng Hsieh, Yongfeng Zhang. 2022. Toward Pareto Eî€œcient Fairness-Utility Tradeoî€ in Recommendation through Reinforcement Learning. In Proceedings of the Fifteenth ACM International Conference on Web Search and Data Mining (WSDM â€™22), February 21â€“25, 2022, Tempe, AZ, USA. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3488560.3498487 Personalized recommender systems (RS), which are extensively employed in e-commerce platforms, have been acknowledged for their capacity to deliver high-quality services that bridge the gap between products and customers [7,17,44,51]. Despite these huge advantages, several recent studies also raised concerns that RS may be vulnerable to algorithmic bias in several aspects, which may result in detrimental consequences for underrepresented or disadvantaged groups [19,29,43,59]. For example, the â€œMatthew Eî€ectâ€ becomes increasingly evident in RS, which creates a huge disparity in the exposure of the producers/products in real-world recommendation systems [16,18,33]. Fortunately, these concerns about algorithmic fairness have resulted in a resurgence of interest to develop fairness-aware recommendation models to ensure such models do not become a source of unfair discrimination in recommendation [13, 15, 26, 28]. In the area of fairness-aware recommendation, the methods can be roughly divided into three categories: pre-processing, inprocessing and post-processing algorithms [14,29]. Pre-processing methods usually aim to remove bias in data, e.g., sampling from data to cover items of all groups or balancing data to increase coverage of minority groups. In-processing methods aim at encoding fairness as part of the objective function, while post-processing methods tend to modify the presentations of the results. Even though all of them could successfully alleviate the impact of unfair recommendations to some extent, the expected return of an approach may signiî€›cantly compromise the recommendation accuracy due to the inherent trade-oî€ between fairness and utility, which has been demonstrated by several recent work both empirically and theoretically [22, 23, 32, 55]. In light of the above, one fundamental research questions is asked,RQ1: Can we learn a recommendation model that allows for higher fairness without signiî€›cantly compromising recommendation accuracy? And a more challenging one is,RQ2: Can we learn a single recommendation model that is able to produce optimal recommendation policies under diî€erent levels of fairness-utility trade-oî€ so that it would be more desirable for decision-makers of e-commerce platforms to specify any preference of one objective over another based on their current business needs? To deal withRQ1, one conspicuous approach is to seek a Pareto optimal solution to guarantee optimal compromises between utility and fairness, where a Pareto eî€œcient/optimal solution means no single objective can be further improved without hurting the others. To î€›nd solutions with diî€erent levels of trade-oî€ between utility and fairness (RQ2), we need to generalize their Pareto frontier in the objective space, where Pareto frontier denotes a set, whose elements are all Pareto optimal. Unfortunately, state-of-the-art approaches of fairness-aware recommendation are limited in understanding the fairness-utility trade-oî€. Therefore, in this work, we aim to address the above problems and propose a fairness-aware recommendation framework using multi-objective reinforcement learning (MORL) with linear preferences, called MoFIR, which aims to learn a single parametric representation for optimal recommendation policies over the space of all possible preferences. Technically, we î€›rst formulate the fairnessaware recommendation task as a Multi-Objective Markov Decision Process (MOMDP), with one recommendation objective, e.g., CTR, and one fairness objective, e.g., item exposure fairness (our method is able to generalize to more recommendation objectives as well as more fairness objectives). Second, we modify classic and commonly-used RL algorithmâ€”DDPG [42] by introducing conditioned networks [3] into it, which is a representative method to deal with multi-objective reinforcement learning. Specially, we condition the policy network and the value network directly on the preferences by augmenting them to the feature space. Finally, we utilize the vectorized Q-value functions together with modiî€›ed loss function to update the parameters. The contributions of this work can be summarized as follows: â€¢We study the problem of Pareto optimal/eî€œcient fairness-utility trade-oî€ in recommendation and extensively explore their Pareto frontier to better satisfy real-world needs; â€¢We formulate the problem into a MOMDP and solve it through a MORL framework, MoFIR, which is optimized over the entire space of preferences in a domain, and allows the trained model to produce the optimal policy for any speciî€›ed preferences; â€¢Unlike prior methods for fairness-aware recommendation, the proposed framework does not employ any relaxation for objectives in the optimization problem, hence it could achieve stateof-the-art results; â€¢Experiments on several real-world recommendation datasets verify the superiority of our framework on both fairness measures and recommendation performance when compared with all other baselines. There have been growing concerns on fairness in recommendation as recommender systems touch and inî€uence more and more people in their daily lives. Several recent works have found various types of bias in recommendations, such as gender and race [2,8], item popularity [15,16,59], user feedback [13,25,27] and opinion polarity [54]. There are two primary paradigms adopted in recent studies on algorithmic discrimination: individual fairness and group fairness. Individual fairness requires that each similar individual should be treated similarly, while group fairness requires that the protected groups should be treated similarly to the advantaged group or the populations as a whole. Our work focuses on the item popularity fairness from a group level, yet it can be used to solve multiple types of fairness simultaneously by properly deî€›ning and adding them as additional objectives. The relevant methods related to fairness in ranking and recommendation can be roughly divided into three categories: preprocessing, in-processing and post-processing algorithms [14,28, 29]. First of all, pre-processing methods usually aim to minimize the bias in data as bias may arise from the data source. This includes fairness-aware sampling methodologies in the data collection process to cover items of all groups, or balancing methodologies to increase coverage of minority groups, or repairing methodologies to ensure label correctness, remove disparate impact [14]. However, most of the time, we do not have access to the data collection process, but are given the dataset. Secondly, in-processing methods aim at encoding fairness as part of the objective function, typically as a regularizer [1,4]. Finally, post-processing methods tend to modify the presentations of the results, e.g., re-ranking through linear programming [25,43,53] or multi-armed bandit [5]. However, there is no free lunch, imposing fairness constraints to the main learning task introduces a trade-oî€ between these objectives, which have been asserted in several studies [22,23,32,55], e.g., Dutta et al.[12] showed that because of noise on the underrepresented groups the trade-oî€ between accuracy and equality of opportunity exists. Unfortunately, there is very few work of fairness-aware recommendation that can be found to study the fairness-utility trade-oî€. The closest one to our work is [47], which mainly focused on the trade-oî€ between two-sided fairness in e-commerce recommendation. [47] used a traditional multiple gradient descent algorithm to solve multi-objective optimization problem, meaning that they need to train one network per point on the Pareto frontier, while our MoFIR generates the full Pareto frontier of solutions in a single optimization run. Besides, the authors relaxed all their objectives to get their diî€erentiable approximations, which, to some extent, hurt its performance, as is shown in the experiment part, Fig. 2. Recommendation with multiple objectives is a signiî€›cant but challenging problem, with the core diî€œculty stemming from the potential conî€icts between objectives. In most real-world recommendation systems, recommendation accuracy (e.g., CTR-oriented objectives) is the dominating factor, while some studies believed that other characteristics, such as usability, proî€›tability, usefulness, or diversity should be considered at the same time [20,21,36]. When multiple objectives are concerned, it is expected to get a Pareto optimal/eî€œcient recommendation [31, 39, 50]. The approaches on recommendation with multiple objectives to achieve Pareto eî€œciency can be categorized into two groups: evolutionary algorithm [60] and scalarization [31]. Ribeiro et al.[39, 40] jointly considered multiple trained recommendation algorithms with a Pareto-eî€œcient manner, and conducted an evolutionary algorithm to î€›nd the appropriate parameters for weighted model combination. Besides, Lin et al.[31] optimized GMV and CTR in e-commerce simultaneously based on multiple-gradient descent algorithm, which combines scalarization with Pareto-eî€œcient SGD, and used a relaxed KKT condition. Our proposed method, MoFIR, belongs to scalarization, however, compared with earlier attempts in multi-objective recommendation [31,47], our method learns to adapt a single network for all the trade-oî€ combinations of the inputted preference vectors, therefore it is able to approximate all solutions of the Pareto frontier after a single optimization run. RL-based recommenders have recently become an important and attractive topic, as it is natural to model the recommendation process as a Markov Decision Process (MDP) and use RL agents to capture the dynamics in recommendation scenarios [34,35,41,48,49,58]. Generally speaking, RL-based recommendation systems can be further classiî€›ed into two categories: policy-based [6,9,11] or valuebased [37,56,58] methods. On one hand, policy-based methods aim to learn strategies that generate actions based on state (such as recommending items). These methods are optimized by policy gradient, which can be deterministic approaches [11,30,42] or stochastic approaches [6,9]. On the other hand, value-based methods aims to model the quality (e.g. Q-value) of actions so that the best action corresponds to the one with the highest Q-value. Apart from using RL in general recommendation task, there also existed several works focusing on using RL in explainable recommendation through knowledge graphs [48, 49]. Currently, there are very few studies using MORL in recommendation. Xie et al.[50] studied multi-objective recommendation to capture usersâ€™ objective-level preferences. However, unlike our proposed MoFIR, which learns a single parametric representation for optimal recommendation policies, they conducted a Pareto-oriented RL to generate the personalized objective weights in scalarization for each user, which is a totally diî€erent problem formulation. In reinforcement learning, agents aim at learning to act in an environment in order to maximize their cumulative reward. A popular model for such problems is Markov Decision Processes (MDP), which is a tupleğ‘€ = (S, A, P, R, ğœ‡, ğ›¾), whereğ‘†is a set ofğ‘›states, Ais a set ofğ‘šactions,P:S Ã— A Ã— S â†’ [0,1]denotes the transition probability function,R:SÃ—A Ã—S â†’ Ris the reward function, ğœ‡:S â†’ [0,1]is the starting state distribution, andğ›¾ âˆˆ [0,1)is the discount factor. We denote the set of all stationary policies byÎ , where a stationary policyğœ‹ âˆˆ Î :S â†’ ğ‘ƒ (A)is a map from states to probability distributions over actions, withğœ‹ (ğ‘|ğ‘ )denoting the probability of selecting actionğ‘in stateğ‘ . We aim to learn a policy ğœ‹ âˆˆ Î , able to maximize a performance measure,ğ½ (ğœ‹), which is typically taken to be the inî€›nite horizon discounted total return, whereğœdenotes a trajectory, e.g.,ğœ = (ğ‘ , ğ‘, ğ‘ , ğ‘, . . . ), andğœ âˆ¼ ğœ‹indicates that the distribution over trajectories depends onğœ‹ :ğ‘ âˆ¼ ğœ‡, ğ‘âˆ¼ ğœ‹(Â·|ğ‘ ), ğ‘ âˆ¼ ğ‘ƒ(Â·|ğ‘ , ğ‘). We denoteğ‘…(ğœ)as the discounted rewards of a trajectory, the on-policy value function as ğ‘‰(ğ‘ ) î€‘E[ğ‘…(ğœ)|ğ‘ = ğ‘ ], the on-policy action-value function as ğ‘„(ğ‘ , ğ‘) î€‘E[ğ‘…(ğœ)|ğ‘ = ğ‘ , ğ‘= ğ‘], and the advantage function as ğ´(ğ‘ , ğ‘) î€‘ ğ‘„(ğ‘ , ğ‘) âˆ’ ğ‘‰(ğ‘ ). Multi-Objective Markov Decision Processes (MOMDP) are MDPs with a vector-valued reward functionr= R(ğ‘ , ğ‘), where each component ofrcorresponds to one certain objective. A scalarization function f maps the multi-objective value of a policyğœ‹to a scalar value. In this work, we consider the commonly-used class of MOMDPs with linear preference functions, e.g.,f(R(ğ‘ , ğ‘)) = ğ Â· R(ğ‘ , ğ‘). It is worth noting that ifğis î€›xed to a single value, this MOMDP collapses into a standard MDP. An optimal solution for an MOMDP under linear f is a convex coverage set (CCS), e.g., a set of undominated policies containing at least one optimal policy for any linear scalarization. Abels et al.[3] studied multi-objective reinforcement learning with linear preferences and proposed a novel algorithm for learning a single Q-network that is optimized over the entire space of preferences in a domain. The main idea is called Conditioned Network (CN), in which a Q-Network is augmented to output weight-dependent multi-objective Q-value-vectors, as is shown in the right side of Fig. 1 (Conditioned Critic Network, where action and state representations together with weight vector are inputed to the network). Besides, to promote quick convergence on the new weight vectorâ€™s policy and to maintain previously learned policies, the authors updated each experience tuple in a mini-batch with respect to the current weight vector and a random previously encountered weight vector. Specially, given a mini-batch of trajectories, they computed the loss for a given trajectory(ğ‘ , ğ‘, r, ğ‘ )as the sum of the loss on the active weight vectorğand onğrandomly sampled from the set of encountered weights. y= r+ ğ›¾QargmaxQî€€ğ‘, ğ‘ ; ğî€Â· ğ, ğ‘ ; ğ(3) whereQ(ğ‘, ğ‘ ; ğ)is the networkâ€™s Q-value-vector for actionğ‘ in stateğ‘ and with weight vectorğ. They claimed that training the same sample on two diî€erent weight vectors has the added advantage of forcing the network to identify that diî€erent weight vectors can have diî€erent Q-values for the same state. A more comprehensive review of MOMDPs and CN can be seen in [3]. In the original paper, the authors only proposed an algorithm based on Double DQN with discrete action space, which is not suitable for recommendation scenarios as the action space of recommendation is very large. Therefore, we modify the traditional DDPG [42] by introducing conditioned network into its policy network as well as critic network, and more importantly, we modify the original loss functions for both of them. We choose DDPG as it is a commonly adopted methods in RL, while our modiî€›cation can be generalized to other reinforcement learning methods, such as trust region ploicy optimization. More details about our modiî€›cation will be introduced in Section 5. The recommendation agent will take the feature representation of the current user and item candidatesIas input, and generate a list of itemsğ¿ âˆˆ Ito recommend, whereğ¾ â‰¥1 after a user sends a request to it at timestampğ‘¡ âˆˆ(ğ‘¡,ğ‘¡,ğ‘¡,ğ‘¡,ğ‘¡,. . .). User ğ‘¢who has received the list of recommended itemsğ¿will give feedbackğµvia clicking on this set of items, which can be used to measure the recommendation performance. Besides, based on the recommendation results, we will acquire the total number of exposure for each item groupğº, which can later be used to measure fairness. Thus, the stateğ‘ can be represented by user features (e.g., userâ€™s recent click history), actionğ‘is represented by items in ğ¿, rewardris the immediate reward vector after taking actionğ‘, with each component ofrcorresponds to one certain objective (e.g., whether user clicks on an item inğ¿for utility objective or whether an item comes from predeî€›ned disadvantageous group for fairness objective). The problem formulation is formally presented as follows: â€¢ State S:A stateğ‘ is the representation of userâ€™s most recent positive interaction historyğ»with the recommendation system, together with his/her demographic information (if exists). â€¢ Action A:An actionğ‘= {ğ‘, . . . , ğ‘}is a recommendation list with ğ¾ items to a user ğ‘¢ at time ğ‘¡ with current state ğ‘ . â€¢ Vector Reward Function ğ’“ :A vector-valued reward function r= R(ğ‘ , ğ‘), where each component ofrcorresponds to one certain objective. In this work, the reward vector includes two elements: utility objective and fairness objective. The details of the deî€›nition of our task-speciî€›c objectives will be introduced in the following section. â€¢ Scalarization function f :In this paper, we consider the class of MOMDPs with linear preferences functionsf, which is a commonly-used scalarization function. Under this setting, each objective is given a weightğœ”, such that the scalarization functionÃ becomes f(R) = ğ Â· R, where each ğœ”âˆˆ [0, 1] andğœ”= 1. â€¢ Discount rate ğ›¾: ğ›¾ âˆˆ [0,1]is a discount factor measuring the present value of long-term rewards. We aim to learn a policyğœ‹, mapping from states to actions, to generate recommendations that achieve the Pareto eî€œcient tradeoî€ between fairness and utility. The reward vector is designed to measure the recommendation systemâ€™s gain regarding utility and fairness. While our method is capable of dealing with multiple objectives simultaneously, for simplicity we deliberately select click through rate and item (group) exposure fairness as our two objectives recommendation utility and item exposure fairness respectively. 4.2.1 Utility Objective. On one hand, given the recommendation based on the actionğ‘and the user stateğ‘ , the user will provide feedback, e.g. click or purchase, etc. The recommender receives immediate rewardğ‘…(ğ‘ , ğ‘)according to the userâ€™s positive feedback. We also normalize the reward value by dividingğ¾, which is the length of the recommendation list. ğ‘…(ğ‘ , ğ‘, ğ‘ ) =ğŸ™(ğ‘gets positive feedback)ğ¾(4) 4.2.2 Fairness Objective. On the other hand, based on the recommendation listğ‘, the total number of exposure of each item group will be counted and used to measure exposure fairness. Here, we calculate the ratio of items from sensitive group to the total number of recommended items, and use a hinge loss with marginğ›½to punish the abuse of fairness. Usually, we setğ›½to be the ratio of the number of items in sensitive group to the total number of items. ğ‘…(ğ‘ , ğ‘, ğ‘ ) = maxğŸ™(ğ‘ğ‘–ğ‘  ğ‘–ğ‘› ğ‘ ğ‘’ğ‘›ğ‘ ğ‘–ğ‘¡ğ‘–ğ‘£ğ‘’ ğ‘”ğ‘Ÿğ‘œğ‘¢ğ‘)ğ¾, ğ›½(5) The conditioned actor is almost the same as traditional actor except that we condition the predictions of the policy network to the preference vectors. Practically, we concatenate the state representationğ‘ with the vectorğand train a neural network on this joint feature space, which is depicted in Fig. 1 (Conditioned Actor Network). The conditioned actorğœ‹parameterized byğœƒserves as a stochastic policy that samples an actionğ‘âˆˆ Igiven the current state ğ‘ âˆˆ Rof a user and the preference vector ğ. First of all, we deî€›neğ‘ as the concatenation of the user embedding eâˆˆ Rand their recent history embedding h: where the recent history embeddingh= GRU(ğ»)is acquired by encodingğ‘item embeddings via Gated Recurrent Units (GRU) [10], andğ»= {ğ», ğ», . . . , ğ»}denotes the most recentğ‘items from userğ‘¢â€™s interaction history. We deî€›ne the userâ€™s recent history is organized as a queue with î€›xed length, and update it only if the recommended itemğ‘âˆˆ ğ‘receives a positive feedback, which ensures that the state can always represent the userâ€™s most recent interests.î€š ğ»={ğ», . . . , ğ», ğ‘} ğ‘gets positive feedbackğ»Otherwise(7) Secondly, we assume that the probability of actions conditioned on states and preferences follows a continuous high-dimensional Gaussian distribution. We also assume it has meanğœ‡ âˆˆ Rand covariance matrixÎ£ âˆˆ R(only elements at diagonal are nonzeros and there are actuallyğ¾ğ‘‘parameters). In order to achieve better representation ability, we approximate the distribution via a deep neural network, which maps the encoded stateğ‘ and preferencesğtoğœ‡andÎ£. Speciî€›cally, we adopt a Multi Layer Perceptron (MLP) with tanh(Â·) as the non-linear activation function, Once receivedğœ‡andÎ£, we sample a vector from the acquired Gaussian distributionN (ğœ‡, Î£)and convert it into a proposal matrixğ‘Š âˆ¼ N (ğœ‡, Î£) âˆˆ R, whoseğ‘˜-th row, denoted byğ‘Šâˆˆ R, represents an â€œidealâ€ embedding of a virtual item. Finally, the probability matrixğ‘ƒ âˆˆ Rof selecting theğ‘˜-th candidate item is given byğ‘ƒ= soî‚‰max(ğ‘ŠV), ğ‘˜ =1, . . . , ğ¾, whereV âˆˆ Ris the embedding matrix of all candidate items. This is equivalent to using dot product to determine similarity betweenğ‘Šand any item. As the result of taking the action at step ğ‘¡, the actor recommends the ğ‘˜-th item as follows: whereğ‘ƒdenotes the probability of taking theğ‘–-th item at rankğ‘˜. The conditioned criticğœ‡also diî€ers from the traditional critic in that we concatenate the state representationğ‘ with the vectorğas well as the embedding of actionğ‘, and require the output to be a Qvalue-vector with the size equal to the number of objectives, which is depicted in Fig. 1 (Conditioned Critic Network). The conditioned criticğœ‡is parameterized withğœƒand is constructed to approximate the true state-action value vector functionQ(ğ‘ , ğ‘, ğ )and is used in the optimization of the actor. Following Eq. 2 introduced in conditioned network [3], the conditioned critic network is updated according to temporal-diî€erence learning that minimizes the following loss function: where y= r+ ğ›¾Q(ğ‘ , ğ‘, ğ ; ğœƒ). We present the detailed training procedure of our proposed model, MoFIR, in Algorithm 1 and the model architecture in Fig. 1. As mentioned before, we modify traditional single-objective DDPG into multi-objective DDPG by introducing the conditioned networks to both its actor network and critic network. In each episode, there are two phases â€” the trajectory generation phase (line 15-20) and model updating phase (line 22-32). In the trajectory generation phase, we sample one linear preferenceğand î€›x it to generate user-item interaction trajectories. Then in the model updating phase, we sample anotherNpreferences together withğto update the conditioned actor network and the conditioned critic network. Here, we do not follow the original setting in [3], which only uses one more random sampled preference vector, as Yang et al.[52] observed that increasing the number of sampled preference vectors can further improve the coverage ratio of RL agent and diminish the adaptation error in their experiments. In this section, we î€›rst introduce the datasets, the comparison baselines, then discuss and analyse the experimental results. To evaluate the models under diî€erent data scales, data sparsity and application scenarios, we perform experiments on three realworld datasets. Some basic statistics of the experimental datasets are shown in Table 1. â€¢ Movielens:We choose Movielens100K, which includes about one hundred thousand user transactions, respectively (user id, item id, rating, timestamp, etc.). â€¢ Ciao:Ciao was collected by Tang et al.[45] from a popular product review site, Epinions, in the month of May, 2011. For each user, they collected user proî€›les, user ratings and user trust relations. For each rating, they collected the product name and its category, the rating score, the time point when the rating is created, and the helpfulness of this rating. â€¢ Etsy: We collect a few weeks of user-item interaction data on a famous e-commerce platform, Etsy. For each record, we collect user id, item id and timestamp. Since the original data is sparse, we î€›lter out users and items with fewer than twenty interactions. For each dataset, we î€›rst sort the records of each user based on the timestamp, and then split the records into training and testing sets chronologically by 4:1. The last item in the training set of each user is put into the validation set. Since we focus on item exposure fairness, we need to split items into two groupsğº andğºbased on item popularity. It would be desirable if we have the item impression/listing information and use it to group items, however, since Movielens and Ciao are public dataset and only have Table 1: Basic statistics of the experimental datasets. interaction data, we use the number of interaction to group items in them. Speciî€›cally, for Movielens and Ciao, the top 20% items in terms of number of interactions belong to the popular groupğº, and the remaining 80% belong to the long-tail groupğº, while for Etsy data, we additionally collect the listing impressions per month for each item and group items based on this. Moreover, for RL-based methods, we set the initial state for each user during training as the î€›rst î€›ve clicked items in the training set, and the initial state during testing as the last î€›ve clicked items in the training set. We also set the RL agent recommend ten items to a user each time. 6.2.1Baselines:We compare our proposed method with the following baselines, including both traditional and reinforcement learning based recommendation models. â€¢ MF: Collaborative Filtering based on matrix factorization [24] is a representative method for rating prediction. However, since not all datasets contain rating scores, we turn the rating prediction task into ranking prediction. Speciî€›cally, the user and item interaction vectors are considered as the representation vector for each user and item. â€¢ BPR-MF: Bayesian Personalized Ranking [38] is one of the most widely used ranking methods for top-K recommendation, which models recommendation as a pair-wise ranking problem. In the implementation, we conduct balanced negative sampling on nonpurchased items for model learning. â€¢ NGCF: Neural Graph Collaborative Filtering [46] is a neural network-based recommendation algorithm, which integrates the user-item interactions into the embedding learning process and exploits the graph structure by propagating embeddings on it to model the high-order connectivity. â€¢ LIRD: The original paper for List-wise recommendation based on deep reinforcement learning (LIRD) [57] utilized the concatenation of item embeddings to represent the user state, and the actor will provide a list of K items as an action. We also include two state-of-the-art fairness frameworks to show the fairness performance of our proposed method. â€¢ FOE: Fairness of Exposure in Ranking (FOE) [43] is a type of postprocessing algorithm incorporating a standard linear program and the Birkhoî€-von Neumann decomposition. It is originally designed for searching problems, so we follow the same modiî€›cation method mentioned in [16,47], and use ranking prediction model such as MF, BPR, and NGCF as the base ranker, where the raw utility is given by the predicted probability of userğ‘– clicking itemğ‘—. In our experiment, we haveMF-FOE,BPR-FOE andNGCF-FOEas our fairness baselines. Since FOE assumes independence of items in the list, it cannot be applied to LIRD, which is a sequential model and the order in its recommendation makes a diî€erence. â€¢ MFR: Multi-FR (MFR) [47] is a generic fairness-aware recommendation framework with multi-objective optimization, which jointly optimizes fairness and utility for two-sided recommendation. In our experiment, we only choose its item popularity fairness. We also modify it as the original fairness considers position bias as well, which is not the same setting as ours. Finally, we haveMF-MFR,BPR-MFRandNGCF-MFR. For same reason as FOE, we do not include LIRD as well. Table 2: Summary of the performance on three datasets. We evaluate for ranking (ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™, ğ¹ % symbol is omitted in the table for clarity) and fairness (ğ¾ğ¿ ğ·ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’ and ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ ğ‘…ğ‘ğ‘¡ğ‘’, also in % values), while ğ¾ is the length of recommendation list. Bold scores are used when MoFIR is the best, while underlined scores indicate the strongest baselines. When MoFIR is the best, its improvements against the best baseline are signiî€›cant at p < 0.01. We implement MF, BPR-MF, NGCF, MF-FOE, BPR-FOE, NGCFFOE, MF-MFR BPR-MFR and NGCF-MFR using Pytorch with Adam optimizer. For all of them, we consider latent dimensionsğ‘‘from {16, 32, 64, 128, 256}, learning rateğ‘™ğ‘Ÿfrom {1e-1, 5e-2, 1e-2, . . . , 5e-4, 1e-4}, and the L2 penalty is chosen from {0.01, 0.1, 1}. We tune the hyper-parameters using the validation set and terminate training when the performance on the validation set does not change within 5 epochs. Further, since the FOE-based methods needs to solve a linear programming with size|I| Ã— |I|for each consumer, which brings huge computational costs, we rerank the top-200 items from the base model then select the new top-K (K<100) as the î€›nal recommendation. Similarly, we implementMoFIRwithğ‘ƒğ‘¦ğ‘¡ğ‘œğ‘Ÿğ‘â„. We î€›rst perform basic MF to pretrain 16-dimensional user and item embeddings, and î€›x them through training and test. We set|ğ»| =5, and use two GRU layers to get the state representationğ‘ . For the actor network and the critic network, we use two hidden layer MLP with tanh(Â·) as activation function. Finally, we î€›ne-tune MoFIRâ€™s hyper-parameters on our validation set. In order to examine the trade-oî€ between performance and fairness, we use diî€erent level of preference vectors in test. Since MoFIR is able to approximate all possible solutions of the Pareto frontier, we simply input diî€erent Figure 2: Approximate Pareto frontier in three datasets generated by MoFIR and NGCF-MFR, where ğ‘¥-axis represents the ğ¿ğ‘œğ‘›ğ‘”ğ‘¡ğ‘ğ‘–ğ‘™ ğ‘…ğ‘ğ‘¡ğ‘’@20 (ğ¿ğ‘œğ‘›ğ‘”ğ‘¡ğ‘ğ‘–ğ‘™ ğ‘…ğ‘ğ‘¡ğ‘’ equals to one minus ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ ğ‘…ğ‘ğ‘¡ğ‘’) and ğ‘¦-axis represents the value of ğ‘ ğ·ğ¶ğº@20. preference vetorsğinto the trained model to get variants of MoFIR and denote the resulting alternatives asMoFIR-1.0,MoFIR-0.5, andMoFIR-0.1, where the scalar is the weight on the recommendation utility objective. 6.2.2Evaluation Metrics:We select several most commonly used top-K ranking metrics to evaluate each modelâ€™s recommendation performance, includingRecall,F1 Score, andNDCG. For fairness evaluation, we deî€›nePopularity Rate, which simply refers to the ratio of the number of popular items in the recommendation list to the total number of items in the list. We also employ KL-divergence(KL) to compute the expectation of the diî€erence between protected group membership at top-K vs. in the over-allÃ population, whereğ‘‘(ğ·||ğ·)=ğ·( ğ‘—) lnwithğ·represents the true group distribution betweenğºandğºin top-K recommendation list, andğ·= [,]represents their ideal distribution of the overall population. The major experimental results are shown in Table 2, besides, we also plot the approximate Pareto frontier between NDCG and Longtail Rate (namely, 1-Popularity Rate) in Fig. 2. We analyze and discuss the results in terms of the following perspectives. 6.3.1 Recommendation Performance. For recommendation performance, we compare MoFIR-1.0 with MF, BPR, NGCF, and LIRD based onğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ‘˜,ğ¹1@ğ‘˜andğ‘ ğ·ğ¶ğº@ğ‘˜and provide the these results of the recommendation performance in Table 2. Among all the baseline models, we can see that all sequential recommendation methods (LIRD, MoFIR-1.0) are much better than the traditional method, which demonstrates the superiority of sequential recommendation on top-K ranking tasks. Speciî€›cally, LIRD is the strongest baseline in all three datasets on all performance metrics: when averaging across recommendation lengths LIRD achieves 41.28% improvement than MF, 27.08% improvement than BPR-MF, and 8.97% improvement than NGCF. Our MoFIR approach achieves the best top-K recommendation performance against all baselines on all datasets: when averaging across three recommendation lengths on all performance metrics, MoFIR gets 41.40% improvement than the best baseline on Movielens100K; MoFIR gets 46.45% improvement than LIRD on Ciao; and MoFIR gets 18.98% improvement than LIRD on Etsy. These above observations imply that the proposed method does have the ability to capture the dynamic nature in user-item interactions, which results in better recommendation results. Besides, unlike LIRD, which only concatenates user and item embeddings together, MoFIR uses several GRU layers to better capture the sequential information in user history, which beneî€›ts the model performance. 6.3.2 Fairness Performance. For fairness performance, we compare MoFIRs with FOE-based methods and MFR-based methods based on ğ¾ğ¿ ğ·ğ‘–ğ‘£ğ‘’ğ‘Ÿğ‘”ğ‘’ğ‘›ğ‘ğ‘’@ğ‘˜andğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ ğ‘…ğ‘ğ‘¡ğ‘’@ğ‘˜, which are also shown in Table 2. It is easy to î€›nd that there does exist a trade-oî€ between the recommendation performance and the fairness performance, which is understandable, as most of the long-tail items have relatively fewer interactions with users. When comparing the baselines, we can easily î€›nd that MFR is able to achieve better trade-oî€ than FOE as it is also a multi-objective optimization method. From Table 2, MoFIR is able to adjust the degree of trade-oî€ between utility and fairness through simply modifying the weight of recommendation utility objective. It is worth noting that MoFIR0.1 can always closely achieve the ideal distribution as its ğ¾ğ¿s are close to zero. In Table 2, we can î€›nd that even MoFIR has the similar performance of fairness with other baselines, it can still achieve much better recommendation performance (for example, BPR-FOE and MoFIR-0.5 in Movielens100k or NGCF-FOE and MoFIR-0.5 in Ciao or MF-MFR and MoFIR-0.5 in Etsy), which indicates its capability of î€›nding better trade-oî€. 6.3.3 Fairness-Utility Trade-oî€›. We only compare MoFIR with MFR, since FOE is a post-processing method, which doesnâ€™t optimize the fairness-utility trade-oî€. In order to better illustrate the trade-oî€ between utility and fairness, we î€›x the length of the recommendation list at 20 and plotğ‘ ğ·ğ¶ğº@20 againstğ¿ğ‘œğ‘›ğ‘”ğ‘¡ğ‘ğ‘–ğ‘™ ğ‘…ğ‘ğ‘¡ğ‘’ in Fig. 2 for all datasets, whereğ¿ğ‘œğ‘›ğ‘”ğ‘¡ğ‘ğ‘–ğ‘™ ğ‘…ğ‘ğ‘¡ğ‘’equals to one minus ğ‘ƒğ‘œğ‘ğ‘¢ğ‘™ğ‘ğ‘Ÿğ‘–ğ‘¡ğ‘¦ ğ‘…ğ‘ğ‘¡ğ‘’. Each blue point is generated by simply changing the input weights to the î€›ne-tuned MoFIR, while each orange point is generated by running the entire MFR optimization. The clear margin distance between the blue pointsâ€™ curve (Approximate Pareto frontier) and the orange pointsâ€™ curve demonstrates the great eî€ectiveness of MORL compared with traditional multi-objective optimization method in recommendation. In this work, we achieve the approximate Pareto eî€œcient tradeoî€ between fairness and utility in recommendation systems and characterize their Pareto Frontier in the objective space in order to î€›nd solutions with diî€erent level of trade-oî€. We accomplish the task by proposing a fairness-aware recommendation framework using multi-objective reinforcement learning (MORL) with linear preferences, called MoFIR, which aims to learn a single parametric representation for optimal recommendation policies over the space of all possible preferences. Experiments across three diî€erent datasets demonstrate the eî€ectiveness of our approach in both fairness measures and recommendation performance. We gratefully acknowledge the valuable cooperation of Runzhe Yang from Princeton University and Shuchang Liu from Rutgers University.