With the growing number of Location-Based Social Networks, privacy preserving location prediction has become a primary task for helping users discover new points-of-interest (POIs). Traditional systems consider a centralized approach that requires the transmission and collection of usersâ€™ private data. In this work, we present FedPOIRec, a privacy preserving federated learning approach enhanced with features from usersâ€™ social circles for top-ğ‘ POI recommendations. First, the FedPOIRec framework is built on the principle that local data never leave the ownerâ€™s device, while the local updates are blindly aggregated by a parameter server. Second, the local recommenders get personalized by allowing users to exchange their learned parameters, enabling knowledge transfer among friends. To this end, we propose a privacy preserving protocol for integrating the preferences of a userâ€™s friends after the federated computation, by exploiting the properties of the CKKS fully homomorphic encryption scheme. To evaluate FedPOIRec, we apply our approach into ï¬ve real-world datasets using two recommendation models. Extensive experiments demonstrate that FedPOIRec achieves comparable recommendation quality to centralized approaches, while the social integration protocol incurs low computation and communication overhead on the user side. Keywords: Federated Learning, Privacy, POI Recommendation, Fully Homomorphic Encryption, Social Network 1. Introduction social circles and sharing common interests. In particular, LBSNs provide explicit usersâ€™ information, such as visits to venues [1]. One of the main tasks in LBSNs services is to capture users preferences for providing useful POI recommendations [2]. interactions and train high-quality recommendation models. On the other hand, centralized approaches tend to collect not only a userâ€™s preferences, but also individual-speciï¬c parameters such as the timestamp of a check-in and social relationships. Besides data dissemination and collection, a modelâ€™s release may leak sensitive attributes. Common recommendation services mainly rely on latent factor models, i.e., matrix factorization techniques [3]. Although these approaches can provide high quality recommendations, its output may lead to information leakage in both centralized [4] and collaborative learning [5]. setting, stochastic gradient descent (SGD) iterations for a modelâ€™s update are performed locally, in the data ownerâ€™s device. The updates from each participating client step to form the new global parameters and the process continues until model convergence. Location-Based Social Networks (LBSNs), such as Foursquare, have become an everyday tool for building online On the one hand, traditional recommenders collect users check-in lists in an eï¬€ort to beneï¬t from multiple Privacy concerns related to usersâ€™ data can be minimized by leveraging a federated learning approach [6]. In this leakage from the model itself [5]. A passive parameter server, who follows the protocol but attempts to infer additional information, can deduce a userâ€™s interactions by simply observing the diï¬€erences in item updates [5], in the case of latent factors models. To avoid such a leakage, common privacy preserving techniques include the utilization of secure multiparty computation (SMC) [7] or homomorphic encryption (HE) [8] approaches for enabling a blind aggregation. In the former setting, users participated in the training process, mask their parameters with random values and enable the aggregator to cancel their masks when added together. In the latter, the aggregator can only operate on encrypted data and thus, it cannot deduce any information for the learned parameters. However, HE comes with heavy communication and computation overhead on the usersâ€™ side. Hence, in this work, we utilize a SMC approach that allows the parameter server to only learn the aggregated result. framework. Although latent factor models can lead to high recommendation quality, their accuracy can be further inï¬‚uenced by social features [2]. For instance, the preferences of a userâ€™s friends can lead to better recommendations [9]. That is, homophily suggests that individuals with common interests tend to share a friendship relationship [10]. In federated learning, friendships are not directly available, as local data is never transmitted to an external entity. transfer among users in terms of local vectors learned by the latent factor model. For instance, matrix factorization captures the preference between a userâ€™s vector and the corresponding interacted item vectors, in an eï¬€ort to provide an estimation for unobserved actions [3]. In this work, we exploit the similarity of the learned vectors in a social circle after the federated computation to beneï¬t from direct neighbors. Speciï¬cally, the transmission of a userâ€™s vector within the ï¬rst neighborhood is intended to create a weighted mean vector that captures the preferences from friends for providing higher quality recommendations. However, transmitting these parameters in plain format can lead to the reconstruction of a particular userâ€™s preferences. A passive user could collect the underlying user vectors from their social circle to determine the preferences of an individual to speciï¬c items (POIs). To overcome this challenge, we propose a privacy preserving approach based on a fully homomorphic encryption (FHE) scheme, that allows the computation of the weighted mean vector by retaining data conï¬dentiality. Our approach is generic and can also be applied to diï¬€erent tasks when computations with privacy concerns minimization are required. collaborative ï¬ltering and fully homomorphic encryption. Section 3 discusses related work in federated POI recommendation. Section 4 presents a generic privacy preserving federated collaborative ï¬ltering algorithm which can be adapted to any top-ğ‘ recommender. Section 5 discusses the method for integrating social features and details the operations on the FHE scheme for aggregating individual-level learned parameters in a privacy preserving manner. The experimental results are shown in Section 6. Finally, Section 7 presents the conclusions of this study. Although federated learning is a privacy-by-design solution, it does not guarantee the prevention of information Combining federated learning with a secure aggregation scheme can lead to a privacy preserving recommendation A straightforward way to enhance personalization, and as a result the recommendation quality, is knowledge The main contributions of this paper are summarized as follows: â€¢ We present FedPOIRec, a privacy preserving federated learning framework that can be integrated with any collaborative ï¬ltering algorithm for providing top-ğ‘ POI recommendations. â€¢ We propose an eï¬ƒcient privacy preserving approach for data aggregation based on FHE computations. We beneï¬t from social relationships and aggregate individual-level parameters over encrypted data to enhance the personalization and recommendation quality, while minimizing data exposure. â€¢ We adapt two recommendation models in the federated setting: a conventional matrix factorization model optimized with Bayesian Personalized Ranking (BPR) [11] and a sequential recommender based on a convolutional neural network (CASER) [12]. To our knowledge, this is the ï¬rst attempt for recommendation based on sequential patterns in federated learning. â€¢ We conduct extensive experiments on ï¬ve real-world datasets. Our results show that the integration of social features enhances the recommendation quality, while FedPOIRec maintains high eï¬ƒciency and minimizes privacy concerns. The remainder of this paper is organized as follows. Section 2 describes the preliminaries of federated learning, 2. Preliminaries the concept of FHE, which we consider in our privacy preserving social integration protocol, is introduced. 2.1. Federated Learning third party. However, information processing and dissemination are minimized by regulations, such as the GDPR [13] and HIPAA [14]. Recently, a distributed training paradigm, federated learning [6], emerged which enables users to collaborate within a training computation without local data transmission. In this setting, each user locally trains a model shared from a parameter server, by utilizing the local data. Then, unlike traditional machine learning, users only share their calculated parameters, enabling privacy-by-design data utilization. The parameter server summarizes the updates received by participating clients and distributes the aggregated model to a number of randomly selected users in the next round. challenges arise. Among limitations, data heterogeneity can lead to inconsistent updates, which in turn may bias the global model towards certain clients or lead to decreased convergence [15]. In our learning task, users holding many observations may bias the model towards the POIs they visit. As a result, the generated model may fail to be representative of the global distribution and, consequently, it may provide low-quality recommendations to users with limited local observations. To minimize the impact of data heterogeneity, we employ a personalization and a social features fusion step, after the learning process. In naive federated learning, the parameter server collects several clientsâ€™ updates in plain format. Therefore, private information, such as the local dataset may be deduced [16]. In the case of latent factor models which are considered in this work, the parameter server can directly deduce the interacted items by observing the diï¬€erences between a clientâ€™s update and the previous global model [5]. To achieve privacy, cryptographic techniques, such as SMC and HE, are commonly used in the aggregation phase. In this work, we choose to integrate a SMC approach because HE introduces heavy computation and communication costs, especially when integrated with latent factor models [17]. 2.2. Collaborative Filtering userâ€™s feedback [18]. Usersâ€™ responses fall into two main categories: rating feedback (e.g., 1-10 stars) and unary feedback, i.e., the observed instances only capture interactions without explicitly deï¬ning the userâ€™s preference [19]. Although explicit feedback formulation have been widely used for predicting unobserved ratings [3], privacy aware users may refrain from providing their response. Hence, unary feedback is easier to collect and without interrupting usersâ€™ actions. In this work, we consider implicit feedback, i.e., local devices only collect user visits, without asking for a userâ€™s response. algorithms [19]. In memory-based algorithms, the prediction is made by considering users (neighbors) whose local observations are correlated to those of the target user. Model-based approaches, on the other hand, capture latent representations of user-item interactions using a learning procedure and predict new content for the target user. However, the former approaches suï¬€er from selection uncertainty, while model-based methods, in many cases, fail to correctly learn from latent representations due to high sparsity or limited users interactions [20]. In this work, we ï¬rst employ a model-based approach and after the training process, we beneï¬t from direct neighborsâ€™ preferences to enhance personalization and the recommendation performance. More precisely, we make use of the latent vector that independently calculated in each userâ€™s side. Then, these vectors are used to calculate similarities among friends. Finally, the similarities are used as weights to fuse the learned parameters, thus transforming the model into a social recommender [21]. (model-based) and our approach. Figure 1a presents an illustration of the recommended POI to a user in a memorybased approach, taking into account the cosine similarities, which are equivalent to the Otsuka-Ochiai coeï¬ƒcient in the case of binary vectors, between the target user visits and the corresponding visits by the rest of the users. In In this section, we ï¬rst present the preliminaries of federated learning and collaborative ï¬ltering methods. Then, Training machine learning models is a task that traditionally requires users to transfer their data to a (trusted) Although federated learning facilitates machine learning without requiring local observations transmission, several Another major challenge is how the aggregation step should be performed to provide suï¬ƒcient privacy guarantees. The main goal of a recommendation service is to predict a list of top-ğ‘ unobserved interactions based on past Two common methods for the generation of a recommender are by leveraging memory-based or model-based Figure 1 shows the recommendation generation process in a memory-based method, a simple matrix factorization Figure 1b, the score for each unobserved visit is calculated by the dot product between the learned user vector and the corresponding POI vector. Our approach combines the two methods in the inference stage (Figure 1c). First, similarities between learned user vectors among friends are calculated and then, the target user vector is transformed considering a weighted mean of the vectors. Finally, the generated social vector is used to generate the recommendations for the target user. 2.3. Fully Homomorphic Encryption a userâ€™s preferences. Consequently, the social features integration, i.e., calculating the similarities between users and the corresponding aggregation, should be performed in a privacy preserving manner. In this paper, we make use of a FHE scheme, which does not require a synchronous setting. The computation can be performed by a third party as soon as encrypted data is available, without requiring the online presence of users. [22], are partially homomorphic, supporting either addition or multiplication on a ciphertext. Gentry [23] proposed the construction of a FHE scheme, allowing the evaluation of arbitrary functions on encrypted data. Although the advances in the construction of FHE schemes, they still remain ineï¬ƒcient [24]. For this reason, practical applications rely on their somewhat or leveled HE variants, which allow the evaluation of a limited or predeï¬ned number of functions for a ciphertext, respectively [25]. (CKKS) scheme [26] to generate the desired weighted mean based on a userâ€™s friends. The CKKS scheme is based on the Ring Learning with Errors (RLWE) problem [27] and allows the evaluation of a function on real numbers. Due to the approximate arithmetic nature of RLWE, noise is added to the least signiï¬cant bits during encryption and hence, the decryption of a ciphertext corresponds to the original plaintext packed with extra noise. encrypted data. Then, the encrypted similarities are employed as weights for calculating the weighted mean vector. The resulting ciphertext can be only decrypted in the querierâ€™s side and therefore, the privacy of users participated in the computation is maintained. Speciï¬c details for our privacy preserving protocol are given in Section 5. BPR learning algorithm [11] to the federated setting, which we also consider in this paper. The authors proposed a plain update transmission to the parameter server by allowing users to share the local calculated vectors with a certain probability, while the learning procedure is performed considering sequential learning or selecting all clients in a training round. The authors in [29] followed a standard federated setting [6], except that multiple parameter servers are introduced to cover geographic regions, which also collect and aggregate clientsâ€™ updates in plain format. Nevertheless, both works do not study social features in the context of recommendations and the aggregation step is not performed using a secure strategy. In both centralized and federated model-based collaborative ï¬ltering, the learned feature vectors directly expose A HE scheme allows the evaluation of a function over encrypted data. Traditional HE systems, such as Paillier Since the parameters that need to be aggregated are ï¬‚oating point numbers, we utilize the Cheon-Kim-Kim-Song In this work, we exploit the homomorphic properties of CKKS to calculate cosine similarities between users over In this section, we summarize the existing research in federated POI recommendation. Directly related to our work are the federated systems presented in [28, 29]. Ferarra et al. [28] extended the distributed system, each user locally calculates the gradients of the visited venues and then, each computed parameter is propagated in plain format to remote neighbors using a random walk approach. Consequently, the remote users update their corresponding local venue vectors based on the received gradients. Although the system is fully decentralized, local gradients sharing directly exposes the preferences of a user. user is updated by considering the computed local parameters from the k-nearest neighbors, who are found based on the geographical distance. The global part is owned by the parameter server, who collects perturbed user-venue interactions. The vector for each POI in the global proï¬le is updated only from users who have visited this speciï¬c venue using a secure aggregation strategy [7]. Finally, the prediction is made by considering the local and global learned parameters. The main bottleneck of the system is the communication overhead that is introduced, as users need to i) identify their k-nearest neighbors, ii) communicate with them and exchange parameters for updating the local models and iii) communicate with the parameter server for updating the global part. Moreover, the collection of user-venue interactions, although perturbed, may lead to preference leakage. procedure, which can be integrated with any model-based collaborative ï¬ltering algorithm. In FedPOIRec, users do not outsource any of their local observations (i.e., visits to POIs) and a secure aggregation strategy is integrated to minimize the local updates exposure to the parameter server. Besides, we consider a standard federated learning scenario, without requiring all users to be available at any given time. Finally, we explore the social inï¬‚uence in the inference stage after the federated learning procedure. In a real-world scenario, the federated process occurs when only a fraction of users is available. Hence, the formulation of parameters transmission with privacy guarantees in the training stage incurs heavy communication and computation overhead. To avoid huge costs on the client side, we fuse learned parameters between friends after the global modelâ€™s generation, using a FHE construction. Our privacy preserving protocol allows the computation of the weighted mean vector without local parameters exposure and can also be adapted to tasks besides recommendation systems. 4. FedPOIRec System technique are described without considering the social features integration part. 4.1. Overview model-based collaborative algorithm. At the end of the training process, the generated model is transmitted to every participant, who performs additional training steps to achieve personalization. resulting local model. To this end, we utilize a federated learning procedure enhanced with a SMC approach to keep local data and models conï¬dential. More speciï¬cally, our learning system consists of two main entities: A ï¬rst attempt that formulates on device training with neighborhood inï¬‚uence was presented in [30]. In that In [31], the collaborative ï¬ltering model is divided into a local and a global part. The local model for each Unlike previous work on federated POI recommendation, our study presents a generic privacy preserving learning In this section, the federated collaborative ï¬ltering process and the privacy preserving modelâ€™s aggregation We consider a setting where ğ‘€ parties, each locally holds private observations who wish to jointly train a The parties involved in the training process are interested in preserving the privacy of their local data and the â€¢ Data owners: We consider a set U consisting of ğ‘€ data owners. Each user owns a device with suï¬ƒcient storage, computing capabilities and access to a network. Each client ğ‘¢ âˆˆ {1, ..., ğ‘€} holds a local dataset ğ· consisting of a set of tuples in the form of (ğ‘¢, ğ‘–, ğ‘Ÿ), where ğ‘– âˆˆ {1, ..., |I|} is a venueâ€™s id from the global POI proï¬le I and ğ‘Ÿdenotes the preference of user ğ‘¢ to the venue ğ‘–. Since, we formulate unary-response preferences, an observed interaction is deï¬ned as ğ‘Ÿ= 1. In the training phase, each local dataset is expanded with negative feedback by sampling unobserved interactions, thus formulating a number of non-visited POIs as ğ‘Ÿ= 0. In addition, the size for each local dataset |ğ·| can vary, while the local observations are possibly drawn from diï¬€erent distributions. â€¢ Parameter Server: A parameter server is responsible for random usersâ€™ selection in each global round as well as for summarizing the received updates. ğ· = The main objective is to enable a privacy preserving training method without local data transmission. More speciï¬cally, during local training on user ğ‘¢, no other party should learn the preference of ğ‘¢ to a speciï¬c POI ğ‘–. described. Table 1 summarizes the notations and the description of parameters in the learning process of FedPOIRec. 4.2. Local Training some global parameters ğ‘Š. In each training round ğ‘¡ âˆˆ ğ‘‡, the parameter server distributes the current global variables global parameters and performs ğ¸ gradient descent steps using the local dataset ğ· training phase is transferred on the client side and thus, users never outsource their local data. which is not biased towards some clients or venues. In general, in matrix factorization, the preference of a user ğ‘¢ to an unobserved item ğ‘– is predicted using the dot product between the corresponding user and item vectors, i.e., training instances. More precisely, the learning objective can be solved using SGD by sampling a random batch of samples ğ· where ğœ‚ is the learning rate and âˆ‡ is the gradient of the loss with respect to model parameters. Note that in a federated setting, the local user vector ğ‘ˆ Consequently, the user vectors are not outsourced to any entity in the system due to privacy concerns [29]. randomly selecting instances from the local dataset ğ· is to further sample unobserved interactions [32]. In our scenario, the local datasets only contain visits to POIs and hence, leveraging only positive interactions, would result in a biased model towards the most popular venues. Hence, the local datasets in each training iteration are expanded by selecting unobserved interactions uniformly at random. calculating the new global model. The updated modelâ€™s variables ğ‘Š to the parameter server. Upon collecting the updates to generate the global parameters for the next round. The communication between participating entities is synchronous and the process repeats until a desired level of modelâ€™s quality is achieved. Diï¬€erent from traditional machine learning, where an entity collects observations from multiple users, i.e.,Ã ğ·, and employs a collaborative ï¬ltering algorithm, FedPOIRec utilizes a federated model-based approach. In the following subsections, the main steps of the federated training for a recommendation model generation are = {ğ‘– âˆˆ I : ğ‘– âˆ‰ ğ·} Unobserved interactions for user ğ‘¢, ğ‘– âˆˆ {1, 2, ..., |I|}. âŠ† ğ‘ˆSet of selected participants in step ğ‘¡. Federated learning is deï¬ned over a number of global rounds ğ‘‡ and local epochs ğ¸ by an iterative process over to a uniformly random subsetËœğ‘ˆâŠ† ğ‘ˆâŠ† U of available participants. Each selected participant downloads the The main challenge in federated collaborative ï¬ltering is how users can jointly build a recommendation model, = ğ‘ˆÃ— ğ¼[3]. The goal in the iterative learning process is to minimize a loss function L considering the userâ€™s In FedPOIRec, the iterative process of SGD is performed independently in each participating client Ëœğ‘¢ âˆˆËœğ‘ˆby After the local SGD iterations, each client should distribute the updated model parameters to the server for 4.3. Model Aggregation and generates the new global model by summarizing the received updates. In this case, the server observes all the intermediate local models, which may reveal sensitive attributes [16]. Therefore, secure aggregation strategies should be utilized to minimize the potential information leakage from the transmitted updates. are based on the SecAggâ€™s construction for providing better eï¬ƒciency or robustness. In this work, we utilize the SecAgg protocol to achieve a privacy preserving aggregation. Below, we summarize the main operations involved in the protocol. Proposing new ways for aggregation is beyond the scope of this paper. each pair of users agree on random seeds and each user locally computes the masks based on the random seed using a pseudo random generator (PRG). Finally, the calculated local parameters on a user Ëœğ‘¢ after applying SGD, are transformed using the random variables by: introduced masks. To ensure fault tolerance, additional masks are randomly generated in each userâ€™s side, which are then secret shared with other participants using a t-out-of-N secret sharing technique, thus further transforming the masked parameters calculated in equation 2 by: SecAgg scheme can eï¬€ectively handle up to (|ğ‘ˆ tasks and storage requirements are not required compared to HE approaches. For instance, in the simplest form of a latent factor model, users should encrypt the whole item proï¬le, i.e., a ğ‘‘ Â·|I| matrix, where ğ‘‘ is the latent dimension and |I| is the number of items in the proï¬le [17]. On the other hand, utilizing a SMC approach, users only communicate to exchange a small parameter and perform a simple transformation on the latent matrix. Therefore, HE approaches are not suitable for recommendation systems and thus, SMC is selected as the building block of the secure aggregation strategy. 4.4. FedPOIRec Learning Process of global and local rounds ğ‘‡ and ğ¸, respectively, the global parameters in the ï¬rst round ğ‘Š fraction of online participants ğ¶ to consider in a global round and the number of unobserved interactions ğ‘›ğ‘¢ğ‘šğ‘ğ‘’ğ‘”, which is a special parameter in the case of unary-feedback for collaborative ï¬ltering, are given as input. Then, the parameter server samples the available participating clients at time step ğ‘¡, selects uniformly at random at least three online users for local training and transmits them the current global parameters (lines 2 - 5). The constraint of three available clients is introduced to prevent a passive entity from deducing information in the next round. For instance, consider a scenario with two participants, one of whom is a passive entity and selected in consecutive rounds. The diï¬€erence between the aggregated model in the next round and the local model calculated in the precious step, discloses the local weights of the honest participant. gradient descent steps are performed using the local observations expanded with unobserved items, which are selected uniformly at random (lines 7 - 11). Finally, the updated local model is blurred using one-time masks and is transmitted to the parameter server for aggregation. In the last step of the learning procedure, the parameter server summarizes the updates and generates the new global model ğ‘Š rative ï¬ltering model may provide suï¬ƒcient recommendations to each user. Due to the nature of federated learning, In the naive approach of the federated computation, the parameter server collects the intermediate local parameters The most widely employed SMC aggregation is the SecAgg scheme [7]. Other approaches, such as [33, 34, 35], The main idea behind SecAgg [7] is that users agree on one-time masks for each calculated value. More precisely, [ğ‘Š] := ğ‘Š+ğ‘ƒğ‘…ğº ( Ëœğ‘¢, ğ‘–) âˆ’ The server collects the blurred parameters and correctly calculates the sum of the updates by canceling the When a user drops, the server communicates with t participants to cancel the associated masks. Note that the Although, SMC approaches introduce some additional communication cost between users, heavy computation Putting it all together, the privacy preserving learning process of FedPOIRec is given in Algorithm 1. The number Each selected client receives the current global model and calculates the set of unobserved items ğ·. Then, ğ¸ After convergence, the training phase ends and the global model is ï¬nalized. At this stage, the federated collabo- Algorithm 1 FedPOIRec inevitably, some users were not selected in recent global updates. For instance, some clients may were oï¬„ine or the random process did not select them. Hence, the learned parameters may fail to provide high quality recommendations to those users. To overcome this limitation, the participating clients run a number of local epochs to further minimize the local objective. 4.5. Privacy Analysis honest in terms of correctly performing SGD iterations and the corresponding aggregation, respectively, but they try to infer additional information for a participating user ğ‘¢. In addition, we assume that the parameter server and the participating clients do not collude. userâ€™s update. Although in each training round the sum of the plain updates is correctly calculated, the intermediate results do not leak a speciï¬c userâ€™s interactions. In a similar manner, the participating clients in the next round cannot deduce information regarding the clients who updated the global model in the previous round. A special case, where data leakage can be caused by a passive user is when the parameter server selects only two clients in two consecutive rounds, as previously discussed. This limitation is almost unlikely to happen in federated settings given that there is a huge number of participants [36] and is further alleviated by adding the constraint of selecting at least three clients (Algorithm 1). It is worth noting that an attackerâ€™s advantage for deducing information from an aggregated global model is almost negligible and decreasing by the number of clients who participate in a global round [37]. minimizes privacy concerns regarding the identiï¬cation of a userâ€™s preferences as it enables an aggregation of seemingly random parameters, without requiring users to transmit any of their local data. 5. Privacy Preserving Social Features Integration average of user vectors in a social circle. First, we present an overview of our approach on plaintext data and then, we proceed to describe our privacy preserving protocol leveraging the CKKS FHE scheme. Ëœğ‘ˆ= random(ğ‘ˆ, totalParticipants) In our learning scenario, we assume that participating entities, i.e., the data owners and the parameter server, are The adopted secure aggregation strategy prevents the server from directly deducing information regarding a single To summarize, the utilization of a federated setting enhanced with a secure aggregation strategy based on SMC, In this section, we describe our protocol for enhancing the recommendation quality by computing a weighted 5.1. Overview friends) in the training stage. In the centralized setting, the learning algorithm has access to model parameters, including user vectors and adapts to clients with similar behavior. In contrast, in the considered scenario, the local user vectors are never transmitted from clients and consequently, the learned model may fail to correctly capture the similarities between users. Hence, we argue that transferring knowledge between learned parameters in a social circle in terms of user vectors, after the federated learning process, leads to recommendation quality enhancement. To this end, the local vectors are fused using a weighted averaging aggregation. More speciï¬cally, similarities among friends are calculated using the usersâ€™ latent representations, which reï¬‚ect the preferences to POIs. The similarity between two users is calculated from the cosine measure, which is widely employed in memory-based collaborative ï¬ltering methods [19]. Formally, the cosine similarity between two vectors ğ‘ and ğ‘ is deï¬ned as: with ğ‘‘ the vectorâ€™s dimension and kğ‘k, kğ‘k, the euclidean norm of vector ğ‘ and ğ‘, respectively. latent representation, i.e., social features are integrated into the local model. More speciï¬cally, the local learned user vector is transformed by: where F and ğ‘£ with respect to the local user vectors. In our scenario, there are two master entities, the parameter server who coordinated the federated learning procedure and is not aware of social connections and the social network service which only knows the friendships set F leaked to any other party. Our goal, is to enable a privacy preserving preference fusion by calculating similarities between friends without revealing any information regarding the local parameters. network. Given that participating users in federated learning may be millions [36], computing similarities among all users introduce heavy communication and computation overhead. adapts social information on top of BPR [11], leverages friendships by further sampling unobserved items that are interacted by the friends of the target user. However, the approach of modeling social connections in the training stage of federated learning is not straightforward. First, exchanging local observation violates the principle of data locality. Second, the availability of devices varies and thus, real-time parameters distribution even if local observations are secured under a privacy preserving mechanism, cannot be ensured. Besides, the communication and computation overhead may be restrictive and therefore, we leave the adaptation of social features integration in the training stage for future work. 5.2. Problem Deï¬nition relationships on recommendations by generating a weighted mean user vector, which combines the personal taste of a user with the preferences of their friends, regarding the POIs in the global item proï¬le. By incorporating social features, we aim at providing higher quality recommendations. of the direct neighbors. The main objective is to provide privacy guarantees to both queriers and friends, i.e., the local user vectors should not be disclosed to any party except for the corresponding owner. The computed local vector directly reï¬‚ects the preferences of a user and thus, it should not revealed. Intuitively, a model-based collaborative ï¬ltering algorithm integrates the correlation between users (and as a result, cos(ğ‘, ğ‘) =ğ‘ Ã— ğ‘kğ‘k kğ‘k=ğ‘ğ‘î±Ãî±Ã(4) The similarity between users is used as weight to generate a weighted mean vector, which replaces the local user WeightedVector=Ãğ‘¤(ğ‘¢, ğ‘£) is the set of friends for a user ğ‘¢ with ğ‘¢ âˆˆ Fand ğ‘¤(ğ‘¢, ğ‘£)denotes the cosine similarity between users ğ‘¢ We choose to only integrate the friendsâ€™ vectors as a way for approximating a userâ€™s nearest neighbors in the whole The integration of social features may be performed directly in the training phase. For instance, SBPR [38], which After the federated computation, each user owns a local user vector ğ‘ˆ. We study the inï¬‚uence of social The goal for each user ğ‘¢ is to compute a weighted average by incorporating the local user vector and the vectors computation based on the CKKS scheme. From the deï¬nition of cosine similarity (equation 4), the number of multiplications and additions required for the calculation is ï¬xed. Hence, we use the leveled variant of CKKS for better eï¬ƒciency. Our protocol is practical since direct connections and synchronization between users are not required, while the operations for computing the desired weighted mean vector are performed over encrypted data. 5.3. System Model cryptoEvaluator may be equivalent to the parameter server from the federated computation. Below, we summarize the entities involved in a weighted mean vector computation: local vector and the vectors from friends are outsourced in an encrypted form using the querierâ€™s public key to cryptoEvaluator. After computing the similarities over encrypted data, the resulting ciphertexts are utilized from cryptoEvaluator as weights for calculating a weighted mean of the outsourced vectors. Note that the operations are always performed on encrypted data and therefore, cryptoEvaluator cannot deduce any information. Below, we present the complete protocol, which is secure against (non-colluding) passive parties. 5.4. The Privacy Preserving Social Integration Protocol emphasize that at the end of the protocol the input vectors should not be leaked to any party (except for the vector owner), the identities of the querierâ€™s friends should not be disclosed to any party (except for the querier) and the output should be decryptable only under the secret key of the user who initiated the weighted mean vector computation. Nearest Neighbors Search problem [39] as well as adapted to a network without friendships. Inevitably, HE introduces additional communication and computation overhead. Hence, social information is intended to approximate a userâ€™s k-nearest neighbors in the whole network. Table 2 summarizes the operations needed for calculating the weighted averaged vector in our protocol. Note that the opetations on FHE schemes require evaluation keys that need to be transferred to the entity which performs computations on encrypted data [40]. To overcome this challenge and maintain both high eï¬ƒciency and privacy, we propose a privacy-preserving The solution is based on a single server, called cryptoEvaluator, that operates on encrypted data. Note that â€¢ Querier: A user who participated in the federated learning process and owns a local user vector. The querier initiates a weighted mean vector computation by outsourcing the local user vector in an encrypted form to cryptoEvaluator. â€¢ Friends: Users who own local vectors (participants in the federated computation) and are direct neighbors of a querier. After receiving a weighted mean computation plan, they outsource their local vectors to cryptoEvaluator, encrypted under the querierâ€™s public key. â€¢ cryptoEvaluator: A third party who collects encrypted data and performs evaluation operations on ciphertexts. On a higher level of the protocol, each user (querier) initiates a weighted mean vector computation plan. The In this section, we describe our privacy preserving mean vector computation protocol from a userâ€™s friends. We In the following subsections, we summarize the four main phases of our protocol. The scheme can be extended to the 5.4.1. Phase 1 â€“ Key Generation this phase, users generate key pairs as well as evaluation keys, which will be later transmitted to cryptoEvaluator for operating on encrypted data. Figure 2 summarizes the operations and the communication on the cryptoEvaluator and user side. More precisely, the key initialization phase consists of the following steps: 5.4.2. Phase 2 â€“ Mean Vector Computation Announcement and Encrypted Data Transmission an encrypted vector ğ¶ğ‘ˆ included in the encrypted local user vector. In this work, we employ the cosine measure (equation 4) which calculates the similarity by the inner product between two vectors and the euclidean norm of each vector. be performed: i) dot product ii) multiplication and iii) division. The latter operation requires inferring whether a ciphertext is invertible and then, in our case, ï¬nding the inverse of the euclidean norm based on the received encrypted vector. by transferring the calculation of the inverse euclidean norm on the userâ€™s side. This way, each user calculates the inverse euclidean norm using the (plain) local vector and then encrypts the generated value, enabling cryptoEvaluator to correctly calculate cosine similarities by performing multiplication. Hence, we ask each user who participates in a weighted vector computation to locally calculate the encryption of the inverse euclidean norm as ğ¶ğ‘ˆ Encrypt(ğ‘ƒğ¾ old ğ‘Šğ‘‡ ğ¸ğ‘£ğ‘ğ‘™ğ¾ e.g., through a social network service. The service inspects the plan and aborts it when the number of friendships is In the key generation phase, cryptoEvaluator and clients agree on a context, i.e., public parameters. At the end of 1. CryptoEvaluator runs a setup process by ContextGen(ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘ ) to create a context which includes the public parameters. The context is transmitted to users who participated in the federated learning process. 2. Each user, after receiving the public parameters, generates a public-secret key pair (ğ‘ƒğ¾, ğ‘†ğ¾), using the Keygen(ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡) algorithm. In addition, a tuple of evaluation keys (ğ¸ğ‘£ğ‘ğ‘™ğ¾, ğ¸ğ‘£ğ‘ğ‘™ğ¾, ğ¸ğ‘£ğ‘ğ‘™ğ¾) are independently generated by EvalKeyGen(ğ‘ğ‘œğ‘›ğ‘¡ğ‘’ğ‘¥ğ‘¡, ğ‘†ğ¾) for enabling cryptoEvaluator to operate on encrypted data; ğ¸ğ‘£ğ‘ğ‘™ğ¾is used for performing multiplications, ğ¸ ğ‘£ğ‘ğ‘™ğ¾for calculating the sum of a vector and ğ¸ğ‘£ğ‘ğ‘™ğ¾for rotating on encrypted data, which is discussed in a later part of this section. 3. Each client transmits a tuple of keys (ğ‘ƒğ¾, ğ¸ğ‘£ğ‘ğ‘™ğ¾, ğ¸ğ‘£ğ‘ğ‘™ğ¾, ğ¸ğ‘£ğ‘ğ‘™ğ¾) to cryptoEvaluator. Note that these keys should not reveal anything for the underlying secret key ğ‘†ğ¾nor the plaintext values by observing data encrypted under ğ‘ƒğ¾. Users may initialize a mean vector computation plan after their key generation process. First, the querier ğ‘¢ prepares In a naive plaintext communication scenario for calculating the cosine similarity, three main operations should The sub-tasks for performing division over encrypted data incur signiï¬cant cost and consequently, are replaced The resulting ciphertext tuple (ğ¶ğ‘ˆ, ğ¶ğ‘ˆ), a random unique identiï¬er ğ‘ˆğ‘ˆ ğ¼ ğ·and a waiting time threshare transmitted to cryptoEvaluator, who associates the parameters with the key tuple (ğ‘ƒğ¾, ğ¸ğ‘£ğ‘ğ‘™ğ¾, , ğ¸ğ‘£ğ‘ğ‘™ğ¾). The computation plan, consisting of the triplet (ğ‘ƒğ¾, ğ‘ˆğ‘ˆ ğ¼ğ·, ğ‘Šğ‘‡) is announced to the friends of the querier ğ‘¢, less than two because at the end of the computation, a single friendship will lead to the leakage of the friendâ€™s vector to the querier. The waiting time parameter determines the period in which users can send encrypted data under ğ‘ƒğ¾ associated with a speciï¬c ğ‘ˆğ‘ˆğ¼ ğ· of their encrypted local vectors. For instance, a user may decide not to participate due limited resources. local vector and the inverse euclidean norm are initiated using the ğ‘ƒğ¾ in compliance with federated learning job schedules [36], i.e., when the local device is in charge mode, idle and connected to a network. mous connections, e.g., using Torâ€™s onion routing [41]. The anonymous communications are intended to prevent cryptoEvaluator from reconstructing the social graph from clients, e.g., by inspecting client IP addresses. The received encrypted data are appended into a ğ‘ encrypted similarities and the ï¬nal weighted mean vector. Figure 3 summarizes the encryption operations on the querier ğ‘¢ and their friends ğ‘£ âˆˆ F 5.4.3. Phase 3 â€“ Similarity Computation similar to the previous inspection regarding the number of friends, cryptoEvaluator reviews the number of received encrypted parameters and continues the process only when more than two friends has responded to the computation plan. The ï¬rst step in the computation is the calculation of cosine similarities between the querierâ€™s vector and the friendsâ€™ vectors. The received encrypted vectors and the corresponding euclidean norms are sequentially processed by: When a querierâ€™s friend has been informed and accepted the computation plan, the encryption processes of the The resulting ciphertexts along with the corresponding ğ‘ˆğ‘ˆ ğ¼ ğ·are transmitted to cryptoEvaluator through anony- Once the waiting time ğ‘Šğ‘‡has passed, the process continues for calculating the desired weighted average. First, Edot(ğ‘ˆ, ğ‘ˆ)= EvalDotî€€ğ¸ğ‘£ğ‘ğ‘™ğ¾, ğ¸ğ‘£ğ‘ğ‘™ğ¾, ğ¶ğ‘ˆ, ğ¶ğ‘ˆî€ Multiplying (6) by (7), cryptoEvaluator generates the cosine similarity between the querier ğ‘¢ and the friend ğ‘£: weighted average using negative values may result in breaking the learned parameters for the querier and subsequently, in low quality recommendations. Hence, a normalization step should be performed. The most common approach for normalization is to transform the resulting output to the range [0, 1]. In our scenario, the computed similarities are only used as weights and therefore, only an addition is performed to the encrypted outputs by a scalar of 1, thus transforming the resulting values to the range [0, 2]. 5.4.4. Phase 4 â€“ Weighted Mean Computation ciphertexts are used as weights for generating a weighted average. In a plaintext scenario, i) each received vector is multiplied with the calculated cosine similarity (i.e., a scalar), ii) the resulting vectors as well as the weights are added together and ï¬nally, iii) a division operation between the sum of the weighted vectors and the sum of weights is performed. style operations are commonly used [42]. Another strategy is to independently encrypt each value in a vector. However, this method will result in a huge ciphertext expansion, which further leads to huge storage and communication costs. Hence, we use a SIMD-like structure to transform a vector to a single ciphertext, which enables computations to be performed in parallel. vectors. However, SIMD operations over ciphertexts are performed element-wise. The cosine similarity is calculated on encrypted vectors (equation 8) and therefore, the resulting output is an encrypted SIMD-like structure, which contains the desired cosine similarity in the ï¬rst slot (index) of the vector, while the rest of the slots contains values close to zero. Subsequently, by multiplying the encrypted cosine with the corresponding encrypted vector, leads the resulting output with a correct value in the ï¬rst slot and values close to zero in the rest slots, thus breaking correctness. introduced on the encrypted cosine similarity. More precisely, cryptoEvaluator shifts the ï¬rst slot to the right to clone the encrypted similarity to every other slot in the dimension ğ‘‘ using the rotation key, ğ¸ğ‘£ğ‘ğ‘™ğ¾ is almost as expensive as multiplication and is the main bottleneck in our approach. Algorithm 2 Weighted Vector Computation in Phase 3 is described in Algorithm 2. To achieve the cloning operation, a rotation procedure is introduced for each The computation of the cosine similarity for two vectors results in a value to the range [âˆ’1, 1]. Performing the After calculating the cosine similarities between the querierâ€™s vector and their friendsâ€™ vectors, the resulting For eï¬ƒcient computations over ciphertexts, especially in a vector format, Single Instruction Multiple Data (SIMD) The ï¬rst step for computing a weighted mean, is to multiply the encrypted cosine similarities with the corresponding To overcome the above limitation, rotation, i.e., shifting the value from a slot to other slots in a vector [43], is Input: vector h Ciphertext i eVector, vector h Ciphertext i eCos, ğ‘‘ Output: vector h Ciphertext i eResult The process for calculating a weighted vector on encrypted data using the resulting cosine similarity calculated latent dimension ğ‘‘. For each rotation, a SIMD addition is performed between the rotated vector and the previous rotation (lines 4 - 8). Note that the addition operation cancels the noise introduced from the CKKS encoding. The resulting ciphertext contains the value of the cosine similarity in each slot. Finally, a SIMD multiplication between the calculated vector after rotations and the corresponding encrypted user vector (line 9) is performed. vectors as well as the resulting cosine similarities are added together. Note that a weighted average computation requires a division (equation 5), which is an expensive operation as already discussed. Hence, both the numerator and denominator are transferred to the querier, who performs division on plaintext values after decryption. Figure 4 shows the computations involved in the cryptoEvaluatorâ€™s side as well as the decryption operation on the querierâ€™s ğ‘¢ side for generating a weighted mean user vector based on a single friendship. 5.5. Privacy Analysis the participants and cryptoEvaluator are honest in terms of their computations, but they may try to infer additional information regarding the local learned parameters of a user ğ‘¢. The learned parameters leak the preferences of the target user and consequently, a local vector exposure leads to local dataset deduction. key ğ‘ƒğ¾ arises. Although cryptoEvaluator cannot deduce any information from the outsourced ciphertexts due to the CPAsecurity guarantee of the CKKS scheme [44], it can still learn the friendships graph by collecting the underlying IP addresses in a weighted mean vector computation plan. This way, it can reconstruct the social graph for each querier and by merging the collected sub-graphs, the whole social graph is leaked in plain format. Consequently, a linking attack that may identify the individuals can be performed. To alleviate this threat, ï¬rst, the friends of a querier control the dissemination of their local vectors and second, the encrypted vectors distribution to cryptoEvaluator is performed through anonymous communications. This ensures the anonymity of the participating clients and therefore, cryptoEvaluator cannot execute the identiï¬ed linking attack. that queriers are aware of the their local vectors and the ï¬nal output after the weighted mean vector computation. Although, in the general case, the querier is not able to deduce the friendsâ€™ local vectors, there is a direct privacy leakage when the computation plan only involves a single friend. In this case, the querier can easily compute their friendâ€™s local vector based on the resulting output transmitted from cryptoEvaluator and consequently, to determine the friendâ€™s exact preferences. To prevent such a leakage, we have introduced inspection operations before the announcement of After performing the rotation and multiplication operations for each received vector, the generated weighted In our privacy-preserving mean vector computation protocol, similar to the federated setting, we assume that The transmitted parameters for a computation plan are always in an encrypted form under the querierâ€™s public . Hence, cryptoEvaluator cannot decrypt any intermediate result. However, a diï¬€erent privacy challenge Except for privacy guarantees against cryptoEvaluator, privacy against passive users should be also ensured. Recall the computation plan and when the computation over encrypted data is about to be utilized. First, the social network service aborts the computation when a querier has less than two friends and second, cryptoEvaluator aborts it when only a single encrypted vector is received for a computation plan. This way, the computation requires the transmission of at least two friendsâ€™ vectors to be utilized. The ï¬nal aggregated output blurs the preferences of a single friend and therefore, the leakage of preferences is minimized. i.e., no party can deduce any information about the plaintext data by observing the corresponding ciphertexts, which are encrypted under a ğ‘ƒğ¾ attack is alleviated using anonymous communications and an information deducing attack is minimized by checking operations before execution. Hence, our protocol provides high privacy guarantees against non colluding passive entities. 6. Experiments perimentally show that the privacy preserving social features integration protocol incurs low communication and computation cost along with low decryption error. In Section 6.1, we introduce the datasets, the evaluation metrics and the collaborative ï¬ltering algorithms. Then, Section 6.2 compares the recommendation quality of FedPOIRec against the equivalent centralized approaches. Section 6.3 shows the recommendation enhancement by integrating the social features. Finally, Section 6.4 discusses the communication and computation cost that is introduced by integrating social inï¬‚uence over encrypted data. serving social features integration is implemented in PALISADE v1.11.4 [46], which is an open-source homomorphic encryption library. The experiments were conducted on a desktop with AMD Ryzen 9 CPU with 12-cores at 3.8 GHz and GPU support for faster training time on the collaborative ï¬ltering algorithms. The security level in CKKS is set to 128 bits according to the recommendation of the homomorphic encryption standard [40]. 6.1. Experimental Setup Datasets. We employ the publicly available dataset from Foursquare collected in [47], which contains user check-ins and social relationships (friendships) from Twitter. The dataset contains 22,809,624 global-scale check-ins by 114,324 users on 3,820,891 POIs with 363,704 friendships. Without loss of generality, we select ï¬ve urban areas for top-ğ‘ POI predictions: Athens, New York, Sao Paolo, Tokyo and Istanbul. practice in the literature for POI recommendations [48, 49], we remove users and venues having less than ï¬ve interactions as cold-start recommendation is treated as a diï¬€erent problem. We then keep users from the largest connected component of the social network for each urban area, following [47, 50], to evaluate the social impact on recommendations. After that, we group actions by users and sort the interactions by timestamps. We remove duplicate check-ins and keep the last interaction in a venue for each user as recent actions tend to contain more information for future recommendations [12]. and utilize the remaining 80% as training data. The statistics of the preprocessed training datasets are summarized in Table 3. Note that the coverage entry shows the average fraction of interacted POIs that are also visited by the friends of a user. Evaluation Metrics. The recommendation quality is measured for each user in the test dataset. Let ğ‘…ğ‘ğ‘›ğ‘˜ be the test set for a user ğ‘¢ âˆˆ U and dataset for a user ğ‘¢. The predicted ranked list is evaluated by exploiting Precision@K, Recall@K and Mean Average Precision (MAP). In this work, we report precision and recall with ğ¾ = {1, 5, 10}. We also report the F1 score, which is the harmonic mean between precision and recall. included in ğ‘…ğ‘ğ‘›ğ‘˜ against the number of recommended venues by ğ‘ƒ@ğ¾ = ğ‘…ğ‘ğ‘›ğ‘˜ âˆ© Summarizing, the outsourced local vectors are protected under the CPA-security of the underlying FHE scheme, In this section, we evaluate the recommendation quality of FedPOIRec on ï¬ve real-world datasets and we ex- We implement the learning process in both centralized and federated setting using PyTorch [45]. The privacy pre- For dataset preprocessing, we treat the presence of a check-in as a positive interaction. Following the common To evaluate the recommendation models, we hold the last 20% of interactions in each userâ€™s check-in data for testing Precision measures the recommendation quality by identifying the number of recommended venues that are in the test set by ğ‘…@ğ¾ = ğ‘…ğ‘ğ‘›ğ‘˜ âˆ© of the training dataset for a user by ğ´ğ‘ƒ = ğ¾ venue is included in ğ‘…ğ‘ğ‘›ğ‘˜. Implementation Details. To verify the eï¬€ectiveness of FedPOIRec, we compare it with random recommendations without any learning procedure and the corresponding centralized approaches. In more detail, we consider the following models: (Algorithm 1) by modifying the local dataset representation and replacing the objective function. Note that both models formulate unobserved instances to minimize their corresponding loss functions. and the ğ‘™ Adaptive Moment Estimation (Adam) [51] optimizer with the learning rate ğœ‚ from {10 Recall is calculated by the number of recommended venues that are included in ğ‘…ğ‘ğ‘›ğ‘˜ against the number of venues MAP measures the average precision (AP) by exploiting the ğ‘ƒ@ğ¾ against the number of venues that are not partÃ â€¢ Null: It is the simplest baseline that generates random recommendations (Null-RR) or rank venues according to random embeddings (Null-RE). â€¢ BPR-MF: It is a state-of-the-art optimization method based for unary feedback [11], which uses a pair of an observed and an unobserved interaction in the training stage and is integrated into matrix factorization. In the training stage, the model learns to provide higher ranks to observed instances by minimizing the following objective function: L= âˆ’ğ‘™ğ‘›ğœ( Ë†ğ‘¥) where ğœ is the logistic function and Ë†ğ‘¥= Ë†ğ‘¥âˆ’ Ë†ğ‘¥with {ğ‘–, ğ‘— } a pair of observed and unobserved interaction, respectively. â€¢ CASER: It is a convolutional neural network that formulates sequences of interactions in chronological order [12]. Given the sequence of interactions for a user, the objective function captures the probability that an item is the next in the pattern by minimizing the binary cross-entropy loss: L= âˆ’ğ‘™ğ‘œğ‘”ğœ( Ë†ğ‘¥) +ğ‘™ğ‘œğ‘”(1 âˆ’ ğœ( Ë†ğ‘¥)) where ğ‘  denotes a sequence of interactions, ğ‘ âˆˆ ğ‘ƒ the next interacted item in the training sequence and ğ‘ an unobserved sequence of interactions. â€¢ FedBPR-MF: It is the BPR-MF algorithm adapted to the federated setting. A similar setting is also presented in [28]. â€¢ FedCASER: It is the CASER sequential approach adapted to the federated setting. The two considered models, i.e., BPR-MF and CASER, can be easily adjusted in the generic FedPOIRec system For common hyperparameters, we consider the dimension of embeddings ğ‘‘ from {4, 8, 16, 32, 50, 64, 100, 128} regularizer from {1, 10, ..., 10}. For faster convergence in the centralized setting, we consider the federated setting, we consider the SGD optimization method with the learning rate ğœ‚ from {10 other hyperparameters in the CASER model, e.g., the number of convolutional ï¬lters, we follow the observations from [12]. with respect to three considered metrics. The latent dimension ğ‘‘ is ï¬xed to 128, the learning rate ğœ‚ to 10 centralized approaches and 10 interaction for each visit and triplets of random sequences for each training sequence in BPR and CASER, respectively. The learning algorithms are trained from scratch by repeating the experiments 10 times using diï¬€erent initialization seeds and we report the averaged results. In federated variants, we select the 10% of the users in each global round, following [6]. The subset of users that participate in each round is selected randomly such that all groups of participants are equal probable and each participant conducts 5 epochs of local training. At the end of training, we keep the modelâ€™s parameters at the iteration that achieved the highest quality with respect to the considered metrics. 6.2. Performance Comparison Quality Comparison. The aim of this experimental comparison is to evaluate whether the federated models can achieve comparable recommendation quality to their corresponding centralized approaches as well as to assess whether they outperform random models. Due to space limitations, we present the averaged results for each dataset using MAP and F1 score at {1, 5, 10} (Figure 5). The complete results for MAP, precision, recall and F1 score are given in Appendix A, Table A.1. Note that there is no training phase in Null models. After grid search, we ï¬xed the models with the hyperparameters that achieved the highest recommendation quality We conduct 150 training rounds for both the centralized and federated setting and we sample one unobserved federated learning signiï¬cantly outperform the Null models. To verify the signiï¬cance of the results we have assessed Studentâ€™s paired test and we observed p-values close to zero (e.g., 10 on the experimental results, we observe that the bigger the dataset with respect to the number of users and venues, the greater the reduction in recommendation quality. For instance, for the smaller dataset (Athens), the federated models achieve almost equivalent quality with the centralized approaches, i.e., 0.09475 vs 0.08181 and 0.09671 vs 0.08517 in BPR and CASER for the MAP metric, respectively. On the other hand, the recommendation quality of the federated approaches on the dataset with the greater number of users (Istanbul) is almost half the corresponding centralized models, i.e., 0.08976 vs 0.05633 and 0.09219 vs 0.05361, respectively. compared to the centralized setting, federated models can provide satisfactory recommendations, while oï¬€ering high privacy guarantees, thus optimizing the privacy-utility trade-oï¬€. A way to further enhance the recommendation quality, taking into account the observation that the quality may increase with the drop of the number of users that participate in a global model computation, is to employ a geographical clustering approach to create multiple regional models. We plan to explore such an approach in the future. Eï¬ƒciency Comparison. In the general case, the CASER sequential model provides higher recommendation quality. Although, CASER outperforms the simple BPR model, the communication cost may be prohibitive. In our setting, each client needs to download the global model, operate on it using the local data and then forward the updates to the parameter server. The BPR approach is a simple matrix factorization, i.e., it requires the distribution of a ğ‘‘ Â· |ğ‘ | matrix, while CASER includes additional convolutional ï¬lters and fully connected layers. In particular, the communication cost increases linearly with the number of POIs in the proï¬le. On the larger dataset with respect to the number of venues, i.e., Tokyo which contains 32,515 POIs, the BPR and CASER models, require the transmission of 15 MB and 50 MB of information, respectively. Note that the communication cost can be reduced by adopting a compression strategy, such as the Sparse Ternary Compression method [15]. Hence, the adoption of complex models in the federated setting is not trivial as the communication cost may be prohibitive. On a higher level, both models can provide high quality recommendations and therefore, in a real world scenario, a simple matrix factorization is preferable due to its simplicity. 6.3. Performance Comparison with Social Features Integration in inconsistent local objectives with the global minimum [52]. Since the selection of clients in each training round is performed uniformly at random, a ï¬nalized global model may not be representative for each userâ€™s distribution. Consequently, additional training iterations on the generalized model, minimize local objectives and can lead to quality enhancement. In addition, collaborative ï¬ltering algorithms try to identify patterns between users, which cannot be directly achieved in the federated setting, as local data never leaves the ownerâ€™s device. The former case can be mitigated using diï¬€erent optimization techniques, e.g., [53], and personalization steps, while for the latter, we argue that fusing learned parameters between clients, can lead to higher quality recommendations. which is used as a baseline ii) FedPOIRec with global model inference, iii) FedPOIRec with personalization, i.e., each client locally performs additional training iterations to the ï¬nalized global models, iv) FedPOIRec with personalization and social inï¬‚uence, i.e., fusion of learned parameters among 1-hop friends and v) FedPOIRec with personalization and network inï¬‚uence, i.e., fusion of learned parameters among all participating users. of the global model, slightly increase the recommendation quality in both considered algorithms (Figures 6a and 6b). In the standard setting, the model is iteratively generated by considering the intermediate local updates, which are computed based on local observations. of each user to POIs is reï¬‚ected in terms of local learned user vectors. From Table 3, it is evident that on average, the 25% of the usersâ€™ interacted venues are also visited by their friends. The strongest correlation between a userâ€™s visits and direct neighborsâ€™ visits in terms of coverage is observed in Athens, while the weakest correlation in New From the experimental results, it is observed that the collaborative ï¬ltering algorithms in both centralized and Compared to centralized approaches, the federated models tend to achieve lower recommendation quality. Based In any case, the federated setting signiï¬cantly outperforms the Null models. Although the quality decrease The decrease in the quality of the federated models is attributed to data heterogeneity among clients, which results Figure 6 reports the averaged MAP in ï¬ve settings for the two considered models: i) the centralized approach, Compared to the standard federated learning, i.e., global model training, locally trained models after the generation In the real world, the preferences of friends directly inï¬‚uence an individualâ€™s actions. In our setting, the preferences York. Consequently, by fusing the learned parameters, we observed similar behavior in the recommendation quality: in the general case, the quality increases, while the strongest inï¬‚uence is observed in Athens. Going further, with the fusion of the parameters with every user in the dataset, the recommendation quality further increases. However, the increase on the latter case has high computational cost as every local vector needs to be included in the computation. Hence, friendships can approximate strong correlations between users and therefore social information can be utilized to enhance the recommendation quality. In addition, the diï¬€erence in the quality increase for the ï¬ve datasets, after the integration of social features, can be attributed to the homogeneity of links in the network as well as to speciï¬c characteristics of the population. Hence, the extraction of preference indicators among users from the social network needs to be further investigated. equivalent recommendation quality to the corresponding centralized approaches (Section 6.2), at least for small datasets regarding the fraction of users - venues. For instance, in Athens, the quality loss of the federated models compared to the corresponding centralized approaches, after integrating the social information, is less than 0.01 in terms of MAP (0.00793 and 0.00924, respectively). Accordingly, the investigation of new aggregation algorithms during the training stage as well as personalization methods after the global model generation, in combination with the integration of social features, may lead the federated setting to generate equivalent recommendations with the conventional machine learning. The combination of the above methods in federated collaborative ï¬ltering remains an open issue and we intend to explore it in future work. features integration, it is observed that quality in the BPR case is almost equivalent the corresponding CASER model. This is because in the former case, the predicted ranked list for each user is generated by directly considering the learned latent user vectors, i.e., a dot product calculation between the userâ€™s vector and the venues vectors. On the other hand, in the CASER case, the latent user vectors are fed to convolutional ï¬lters which are further transformed using fully connected layers, thus decreasing their inï¬‚uence to the generation of recommendations. Considering the fact that CASER also introduces further communication cost due to its complex architecture, BPR or simple matrix factorization techniques may be more eï¬€ective, at least in the federated setting for collaborative ï¬ltering. The introduction of social features enhances the observation that the federated models tend to achieve almost Comparing the two considered collaborative ï¬ltering algorithms in terms of quality increase by introducing social Finally, we emphasize that the social features integration should be performed using a privacy preserving approach. The local vectors directly expose the preferences of a particular user and therefore, a passive entity may deduce the local data, thus violating the whole privacy concept. In the next section, we evaluate our privacy preserving computation solution which is based on the CKKS FHE scheme. 6.4. Privacy Preserving Mean Vector Computation Performance preserving computation protocol for a weighted mean vector generation. Our results show that our protocol introduces negligible cost on the userâ€™s side with low decryption error. computations on encrypted data. The ï¬rst operation is to calculate the cosine similarity, which requires the computation of the dot product between the querierâ€™s and a friendâ€™s vector. Recall that we modiï¬ed the computation of the cosine measure to avoid the division operation. Instead of division, users distribute the inverse of the euclidean norm of their vectors and cryptoEvalutor performs multiplication between the computed dot product and the corresponding euclidean norms. Hence, the operation for calculating the cosine similarity consumes a depth of 2 multiplications. Then, rotation operations are introduced to clone the ï¬rst slot of the SIMD vector that contains the encrypted result of the cosine similarity to every other slot, depending on the dimension ğ‘‘ of the latent factors. Finally, the resulting ciphertext after rotations is multiplied with the friendâ€™s vectors, thus consuming another level of multiplication depth. Consequently, our protocol requires a multiplication depth of 3 to correctly perform the introduced computations and output the desired mean vector. Note that the computation overhead and storage requirements increase with the number of the multiplication depth. of the computations modulus [26], and the ciphertext dimension to 16384 to achieve 128-bits security. The ciphertext dimension determines the number of real values that can be encoded into a single vector, which is half the corresponding dimension. calculation. Our results showed that the introduced decryption error is imperceptible, as we observed a maximum error of 3.5 Â· 10 plaintext counterparts, respectively. Therefore, the FHE scheme provides equivalent results with the corresponding plaintext computation, while preserving the privacy of the participants. Hence, we report the computation overhead that it is introduced to each participating entity in a mean vector computation. Table 4 shows the storage requirements and average computation time needed per entity, sorted by operation, beginning with the context generation. Storage Requirements and Communication Cost. We ï¬rst focus on the userâ€™s side, where the operations involve the public-private key pair and evaluation keys generation, the encryption process and ï¬nally the decryption operation. Note that when a user only participates as a friend in a computation, the operations only involve the encryption process. In this section, we provide the computation and communication cost that is introduced by leveraging our privacy On a higher level of our scheme, after receiving the encrypted vectors, a server (cryptoEvaluator) performs For other common parameters on HE schemes, we set the plaintext modulus to 50, which determines the precision Since CKKS is not an exact arithmetic scheme, the result of a computation approximates the equivalent plaintext For a computation plan generation, a querier needs 32.23 MB of additional storage for caching and transmitting the required parameters. It is easily observed that the evaluation keys consume most of the storage capacity required for a computation, i.e., 28.4 MB, in total. More precisely, the multiplication and rotation keys consume 3.2 MB of additional storage, respectively, while the sum key needed for a dot product computation consume 22.0 MB. This is because a dot product operation performs a multiplication between two encrypted vectors and then rotations are introduced to calculate the sum of the resulting ciphertext. The public key that needs to be outsourced to each entity that is involved in a computation plan requires 1.1 MB and ï¬nally, the encrypted parameters that are transmitted to cryptoEvaluator, i.e., the local vector and the inverse of the euclidean norm require 1.1 MB, respectively. As a ï¬nal step in the computation, cryptoEvaluator transmits the sum of the weighted vectors and the sum of weights to the querier and therefore 1.1 MB of additional storage is introduced. Note that queriers only need a single transmission of their keys and encrypted data and therefore, heavy communication overhead is avoided. In addition, for the users that only participate as friends in a computation, only the public key of the corresponding querier should be cached and 2.2 MB of information should be shared with cryptoEvaluator. public and evaluation keys as well as encrypted vectors should be cached. Hence, cryptoEvaluator should store the parameters received from a querier as well as the data received from the friends of the querier. Clearly, the storage requirements linearly increase by the number of users who participate in a mean vector computation. Figure 7b shows the increase in storage requirements per number of friends for caching the required keys and the encrypted vectors transmitted from users. Note that additional storage is also required for the intermediate computations, i.e., each resulting cosine similarity and weighted vector require 791.9 KB, respectively. As a ï¬nal step, the cached weighted vectors and cosine similarities are added together and produce 1.1 MB of data, in total. Although storage requirements are high for caching every outsourced data and intermediate results, cryptoEvaluator is a powerful server that can provide adequate storage. Computation Cost. Each operation in our protocol is performed in terms of milliseconds (ms). From Table 4, it is observed that all usersâ€™ operations can be executed under a single second. Note however, that our evaluation is performed on a desktop much powerful than devices with limited resources. On the other hand, the computation time that is introduced can be almost negligible considering the advances in CPU constructions of mobile devices. The mean vector computation operations are performed on the cryptoEvaluatorâ€™s side. Every required data, i.e., Similar to storage requirements, the heaviest task is the generation of evaluation keys on the querierâ€™s side, which requires 85.74 ms of execution. The operations for generating the public-secret key pair as well as the encryption and decryption processes require as little as 16 ms of execution, respectively, and hence, our protocol remains practical on the userâ€™s side. of clients that participate in a computation plan as well as the number of values packed in a single encrypted vector, i.e., the latent dimension in our case. The ï¬rst operation requires the computation of the cosine similarities between the querierâ€™s and the users vectors that responded to the computation plan. In a similar manner, Algorithm 2 for calculating weighted vectors should be executed for each received data. The ï¬nal step involves addition operations between the resulting ciphertexts. Figure 7a reports the increase on execution time (in seconds) for a mean vector computation on the serverâ€™s side per number of users involved in a computation. Again, the heaviest task is the computation of a weighted mean vector, which involves as many rotations as the number of values packed in the encrypted vectors. A single rotation is almost as expensive as multiplication and therefore, a weighted vector computation is the main bottleneck of our approach. On the other hand, even in extreme cases, i.e., when many users respond to a computation plan, the whole process can be executed within a few minutes. For instance, the execution time for a computation plan with 100 users requires about 90 seconds of operations with 128 values packed into a single ciphertext and hence, the protocol remains eï¬ƒcient. needs a single transfer of the required data for a computation to cryptoEvaluator and after calculations only two encrypted vectors are transmitted back to the querier for decryption, i.e., 2 communication rounds are needed. In a similar manner, the users that participate in a computation, receive the corresponding public key and only transmit their encrypted vectors to cryptoEvaluator, i.e., 2 communication rounds are required. Hence, only 2 communication rounds are introduced on each userâ€™s side and thus, heavy communication costs are avoided. In a similar manner, the heaviest computational tasks (similarity and mean vector computations) are transferred to the serverâ€™s side and therefore, additional overhead is not introduced on usersâ€™ devices. Figure 7 summarizes the computation overhead and the storage requirements that are introduced at cryptoEvaluator per number of users that participate in a computation plan. Even when the number of users that participate in a computation plan is high, a complete run can be executed fairly quick and the protocol remains eï¬ƒcient. Hence, our privacy preserving protocol is practical, communication eï¬ƒcient as well as it does not require heavy storage requirements on the usersâ€™ side. 7. Conclusion enhanced with knowledge transfer between friends by operating on encrypted data. The federated learning procedure complies with data dissemination and minimization regulations, as local data is maintained in the ownerâ€™s device. The intermediate computations are summarized using a secure aggregation strategy based on secure multiparty computation to prevent passive entities from deducing additional information. The social features integration incurs after the federated process, where a server operates over encrypted data and fuses learned user vectors using the properties of a fully homomorphic encryption scheme. Our privacy preserving protocol for social fusion can also be adapted to other cases and tasks, when a privacy preserving computation based on local data is required. traditional machine learning approaches, it is a promising solution with high privacy guarantees. The privacy preserving social features integration protocol enhances the recommendation quality, while incurring low communication and computation overhead on the usersâ€™ side, as it requires only two rounds of communication between a querier and the server, with low decryption error. further analysis and sophisticated algorithms on personalization should be investigated, evaluation of the optimization algorithms which can lead to the generation of more generalized and high quality models should be performed, and ways for formulating social features integration during the training stage with communication eï¬ƒciency should be proposed. Regarding privacy, it is crucial to carry out a formal security analysis on the security and privacy guarantees oï¬€ered by the existing secure aggregation schemes, the adaptation of the proposed privacy preserving protocol for a weighted mean vector computation in other learning tasks, such as clustering, should be evaluated as well as ways Diï¬€erent from the operations on the users side, at cryptoEvaluator, the execution time is dependent with the number Summarizing, high storage requirements and communication costs are transferred to a server. A querier only In this paper, we presented FedPOIRec, a generic practical privacy preserving solution for POI recommendations The experimental results demonstrate that although federated learning leads to quality decrease compared to Possible future directions are twofold: federated learning and privacy preserving based. For the federated setting, for further reduction on the computation and communication overhead using fully homomorphic schemes should be explored. Competitiveness, Entrepreneurship and Innovation, under the call RESEARCH â€“ CREATE â€“ INNOVATE (project code: T1EDK-02474, grant no.: MIS 5030446). This work has been co-ï¬nanced by the European Union and Greek national funds through the Operational Program