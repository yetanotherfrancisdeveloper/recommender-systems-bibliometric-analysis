Contrastive learning (CL) recently has received considerable attention in the î€›eld of recommendation, since it can greatly alleviate the data sparsity issue and improve recommendation performance in a self-supervised manner. A typical way to apply CL to recommendation is conducting edge/node dropout on the user-item bipartite graph to augment the graph data and then maximizing the correspondence between representations of the same user/item augmentations under a joint optimization setting. Despite the encouraging results brought by CL, however, what underlies the performance gains still remains unclear. In this paper, we î€›rst experimentally demystify that the uniformity of the learned user/item representation distributions on the unit hypersphere is closely related to the recommendation performance. Based on the experimental î€›ndings, we propose a graph augmentation-free CL method to simply adjust the uniformity by adding uniform noises to the original representations for data augmentations, and enhance recommendation from a geometric view. Speciî€›cally, the constant graph perturbation during training is not required in our method and hence the positive and negative samples for CL can be generated on-the-î€y. The experimental results on three benchmark datasets demonstrate that the proposed method has distinct advantages over its graph augmentation-based counterparts in terms of both the ability to improve recommendation performance and the running/convergence speed. The code is released at https://github.com/Coder-Yu/QRec. Self-Supervised Learning, Recommender Systems, Contrastive Learning, Graph Augmentation, Graph Learning Recently, a resurgence of contrastive learning (CL) [13,14,18] has been witnessed in deep representation learning. Due to the ability to extract the general features from massive unlabeled data and regularize representations in a self-supervised manner, CL has led to major advances in multiple research î€›elds [5,8,29,36]. As the data annotation is not required in CL, it is a natural antidote to the data sparsity issue in recommender systems [22]. An increasing number of very recent studies [29,33,39,41,44,45] have sought to harness CL for improving recommendation performance Figure 1: Graph contrastive learning with edge dropout for recommendation. and have demonstrated signiî€›cant gains. A typical way [29] to apply CL to recommendation is perturbing the original user-item bipartite graph with random edge/node dropout at î€›rst, and then maximizing the consistency between representations of the same user/item learned from diî€erent augmentations via graph encoders (e.g., graph convolutional network [12,15]). In this situation, the CL task acts as the auxiliary task, and is optimized with the recommendation task under a joint learning setting (see Fig. 1). Despite the encouraging results provided by CL, however, what underlies the performance gains is still a mystery. Intuitively, we expect that contrasting diî€erent graph augmentations can capture the essential information existing in the original user-item interaction, by randomly removing the redundancy and impurity with the edge/node dropout. Unexpectedly, a few latest works [16,39,46] have reported that the dropout rate only has a trivial impact on the performance, and changing it will not cause distinct performance î€uctuation. To be more speciî€›c, even an extremely sparse graph augmentation (with dropout rate 0.9) in CL can bring decent recommendation performance gains. Such a î€›nding is quite elusive and counter-intuitive because a large dropout rate will result in a huge loss of the raw information and a highly skewed graph structure. It naturally raises a meaningful question that Do we really need graph augmentations when integrating CL with recommendation? To answer this question, we î€›rst conduct experiments with and without the graph augmentation for a performance comparison. The results show that the performance merely slightly drops when the graph augmentation is absent, which is in line with the î€›ndings in the aforementioned works. We then investigate the diî€erences between the representations learned by non-CL and CL-based recommendation methods. By visualizing the distributions of the representations on the unit hypersphere and associating them with the performance, we î€›nd that what really matters for the recommendation performance is the uniformity of the distributions, rather than the graph augmentation, and the contrastive loss InfoNCE [20] plays a pivot role in regulating the uniformity. Despite not as eî€ective as expected, the graph augmentation is not utterly useless. [1,5] demonstrate that data augmentation can provide more negative samples and data variances for learning representations invariant to the disturbance factors. However, the dropout-based graph augmentation is time-consuming because it requires constant graph adjacency matrix reconstruction during training and the extra memory for the storage of the augmented graphs. A follow-up question then arises. That is - Are there more eî€ective and eî€œcient data augmentation strategies? In this paper, we give a positive response to the question. On top of our î€›nding that the uniformity of the representation distribution is the key point, we innovatively design a graph augmentation-free CL method in which the uniformity is more adjustable. Technically, we follow the graph contrastive learning framework presented in Fig. 1, but we discard the dropout-based graph augmentation and instead directly add the random noises generated by a uniform distribution to the original representations for data augmentation. The added noise vector should be scaled to rotate the original representation by a small angle (see Fig. 3). Then the perturbed representations are propagated through the graph encoders to amplify the deviation. Imposing diî€erent random noises leads to diî€erent data augmentations, and we then contrast them to reî€›ne the original representations. The principle behind this easy-to-implement way is that, for each item/user representation, the rotation caused by the added noise vector corresponds to an oî€set. As the noises are uniformly distributed, by tuning the scale of the added noises, we can easily control how far the representation deviates from its original position and adjust the uniformity of the representation distribution in a straightforward way. Additionally, since the graph augmentation is not required in our method, the positive and negative samples for CL can be generated on-the-î€y so that the running time is signiî€›cantly reduced. The major contributions of this paper are summarized as follows: â€¢We experimentally investigate why CL can improve recommendation and reveal that the uniformity of the learned representation distributions, rather than the graph augmentation, is the decisive factor. â€¢We propose a graph augmentation-free CL method to regulate the uniformity by adding simple random noises to the representations, and enhance recommendation from a geometric view. â€¢We conduct extensive experiments on multiple benchmark datasets, and the experimental results show that the proposed method has distinct advantages over its graph augmentation-based counterparts in terms of both the ability to improve recommendation performance and the running/convergence speed. CL is often applied to recommendation with various forms of data augmentations [29,34,41,45]. In this paper, we focus on the most commonly used dropout-based data augmentation on graphs [29, 36], and launch an investigation into the recent CL-based recommendation method, SGL [29], which performs node and edge dropout for data augmentation and adopts InfoNCE [20] for CL. Formally, the join learning scheme in SGL is deî€›ned as: which consists of two losses: recommendation lossLand CL lossLcontrolled by the hyperparameterğœ†. The formulation of the CL loss InfoNCE in SGL is deî€›ned as: whereğ‘–, ğ‘—are users/items in the node setN,z(z) areğ¿normalizedğ‘‘-dimensional node representations learned from two diî€erent dropout-based graph augmentations, andğœ >0 (e.g., 0.2) is the temperature. The CL loss encourages consistency betweenzandz which are the augmented representations of the same nodeğ‘–and are the positive sample of each other, while minimizing the agreement betweenğ‘§andz, which are the negative sample of each other. To learn the representations from the user-item graph, SGL employs a popular graph encoder LightGCN [12] as its base, whose message passing process is deî€›ned as: whereEâˆˆ Ris the randomly initialized node embeddings,|ğ‘ |is the number of nodes,ğ¿is the number of layers, and Â¯A âˆˆ Ris the normalized undirected adjacency matrix. By replacingÂ¯Awith the adjacency matrices of the corrupted graph augmentations,z(z) can be learned via Eq. (3). Note that,z= andeis the corrupted version ofeinE. For conciseness, here we just abstract the core ingredients of SGL and LightGCN. For more technical details, we refer readers to the original papers [12, 29]. To demystify how CL-based recommendation methods work, we î€›rst investigate the necessity of the graph augmentation in SGL. In this case, we construct a new variant of SGL, termedSGL-WA (WA stands for â€˜without augmentationâ€™), in which the CL loss is in the following form: Because we only learn representations from the original user-item graph, then we havez= z= z. The experiments for the performance comparison are conducted on two large benchmark datasets: Yelp2018 and Amazon-Book [12,25]. A 2-layer setting is used and the hyperparameters are tuned according to the original paper of SGL (see more experimental details in Section 4.1). The results are Figure 2: Distributions of item representations learned by diî€erent methods on the unit hypersphere S plot feature distributions with Gaussian kernel density estimation (KDE) in R point (x,y) âˆˆ S). The rightmost plot visualizes item feature distributions learned by our proposed model GACL. shown in Table 1. Three variants of SGL proposed in the paper are evaluated (-ND denotes node dropout, -ED is short for edge dropout, and -RW means random walk. CL Only means that only the CL loss in SGL-ED is minimized). Table 1: Performance comparison of diî€erent SGL variants. As can be observed, all the variants of SGL outperform LightGCN by a large margin, which demonstrates the eî€ectiveness of CL in improving recommendation performance. However, to our surprise, when the graph augmentation is absent, the improvements are still so remarkable that SGL-WA even exhibits superiority over SGL-ND and SGL-RW. Although SGL-ED maintains a small advantage, it requires additional expenses for the reconstruction and storage of the perturbed graph adjacency matrices (see Section 4.2.3), which makes it less valuable. Therefore, we need to rethink the necessity of graph augmentations and reveal the primary cause that may explain the results. Wang and Isola [24] have identiî€›ed that the contrastive loss optimizes two properties in the visual representation learning: alignment of features from positive pairs, and uniformity of the normalized feature distribution on the unit hypersphere. It is unclear if the CL-based recommendation methods will exhibit similar patterns that can explain the results in Section 2.2. Since top-N recommendation is a one-class problem, we only investigate the uniformity by following the visualization method in [24]. We î€›rst map the learned representations of items (randomly sample 1,000 items for each dataset) to 2-dimensional normalized vectors on the unit hypersphereS(i.e., circle with radius 1) by using t-SNE [23]. Then we plot the feature distributions with the nonparametric Gaussian kernel density estimation [3] inR(shown in Fig. 2). For a clearer presentation, the density estimations on angles for each point onSare also visualized. According to Fig. 2, we can observe notably diî€erent feature/density distributions. In the leftmost column, LightGCN shows highly clustered features that mainly reside on some narrow arcs. While in the second and the third columns, the distributions become more uniform, and the density estimation curves are less sharp, no matter if the graph augmentations are applied. In the forth column, we plot the features learned only by the contrastive loss in Eq. (2). The distributions are almost completely uniform and form closed circles, and meanwhile the density estimation curves î€atten out. Two issues may account for the highly clustered feature distributions in the leftmost column. The î€›rst one is the over-smoothing of node embeddings [4] caused by the message passing in LightGCN. The second one could be the popularity bias in the recommendation data. Recall the pairwise BPR loss [21] used in LightGCN: which is with a triplet input(ğ‘¢, ğ‘–, ğ‘—). By optimizing the BPR loss with the stochastic gradient descent (SGD), we can get: whereğœ‚is the learning rate,ğœis the sigmoid function,ğ‘  = ğœ (eeâˆ’ ee),eis the user embedding, andeandedenote the positive and negative item embeddings, respectively. Since the recommendation data usually follows a long-tail distribution, whenğ‘¢is a popular user with a large number of interactions, the positive item embedding will be optimized towardsğ‘¢â€™s direction while the randomly sampled negative is optimized towards the reverse direction. The two issues are coupled with each other (i.e.,eandeaggregate information from each other in the graph convolution) so that exacerbate the clustering problem and cause representation degeneration [7]. This may explain why opposite clusters on the unit hypersphere Sare observed in the leftmost plots. As for the distributions in other columns, the contrastive losses in Eq. (2) and (4) can give a clue. By rewriting Eq. (4), we can derive L=Â©î‚­âˆ’1/ğœ + log(exp(1/ğœ) +exp(zz/ğœ))ÂªÂ®. (7) Because 1/ğœis a constant, optimizing the CL loss is actually minimizing the cosine similarity between diî€erent nodes embeddings eande, which will push connected nodes away from the highdegree hubs in the representation space and lead to a more uniform distribution even under the inî€uence of the recommendation loss. By associating the results in Table 1 with the distributions in Fig. 2, we can easily draw a conclusion that the uniformity of the distribution has close ties with the recommendation performance. Optimizing the CL loss can be seen as an implicit way to debias. As a result, a more uniform representation distribution can preserve the intrinsic characteristics of nodes and improve the generalization ability. It also should be noted that, by only minimizing the CL loss in Eq. (2), a poor performance will be obtained, which means that a positive correlation between the uniformity and the performance only holds in a limited scope. The excessive pursuit to the uniformity will overlook the closeness of interacted pairs and similar users/items, and impairs recommendation performance. Besides, although the impact of the data augmentation is not as large as expected, [5,11] have evidenced that a good augmentation can provide more informative transformed negatives for the CL loss, helping to learn representations invariant to noise factors. This could be the reason that SGL-ED still maintains a small advantage over SGL-WA on the recommendation performance. Based on the î€›ndings in Section 2, we speculate that by adjusting the uniformity of the learned representation in a certain scope, the balance between the recommendation loss and CL loss can be reached, where the method will show the optimal performance. In this section, we target developing an eî€ective and eî€œcient Graph Augmentation-freeCLmethod (GACL) to freely regulate the uniformity and provide informative data variance. Since manipulating the graph structure for the uniform representation distribution is intractable and time-consuming, we shift our attention to the node representation. Inspired by the adversarial examples [9] which are constructed by adding imperceptibly small perturbation to the input images, we directly add scaled random noises to the representation for an easy-to-implement but eî€ective and eî€œcient data augmentation. Formally, given a nodeğ‘–and its representationein theğ‘‘dimensional embedding space, we can fulî€›ll the data augmentation by following: where the added noise vectorsÎ”andÎ”are subject toâˆ¥Î”âˆ¥= ğœ– andÎ” = |Î”| âŠ™ sign(e). The î€›rst constraint controls the scale of the added noises, andÎ”andÎ”are numerically equivalent to points on a hypersphere with the radiusğœ–. The second constraint requires that e,Î”andÎ”should be in the same hyperoctant, so that adding the noises will not cause a large deviation ofe. In Fig. 3, we illustrate Eq. (8) inR. From a geometric perspective, we can understand Eq. (8) more straightforwardly. As shown in Fig. 3, by adding the scaled noise vectors to the original representation, we rotateeby two small angles (ğœƒandğœƒ). Each rotation corresponds to a deviation ofe, and leads to an augmented representation (eande). Since the rotation is small enough, the augmented representation retains most information of the original representation and meanwhile also keeps some subtlety. Note that, for each node representation, the added random noises are diî€erent. Following SGL, we adopt LightGCN as the graph encoder to propagate node information and amplify the impact of the deviation. At each layer, diî€erent scaled random noises are imposed on the current node embeddings. The î€›nal perturbed node representations are learned by: It should mentioned that we skip the input embeddingEin all the three encoders when calculating the î€›nal representations, because Figure 3: An illustration of the proposed random noisebased data augmentation in R. we experimentally î€›nd that skipping it can lead to more stable and better performance in our situation. However, without the CL task, this operation will result in a performance drop. In our method, we also unify the recommendation loss and the CL loss, and then use Adam to optimize the joint loss presented in Eq. (1). The recommendation loss and the CL loss used are as the same as those in Eq. (5) and Eq. (2). Besides the temperatureğœ, another two hyperparametersğœ†and ğœ–can control the eî€ect of CL. Butğœ–can provide a î€›ner-grained regulation beyond that provided only by tuningğœandğœ†. By freely adjusting the value ofğœ–, we can directly control how far the augmented representations deviate from the original. Intuitively, a largerğœ–will lead to a more roughly uniform distribution of the learned representation, because the original representation will oscillate in the sector between the two deviated augmented representations. When they are enough far away from the original, the information lying in their representations is also heavily decided by the noises. As the noises are initially sampled from a uniform distributionğ‘ˆ (0,1), a more uniform distribution of the learned representation is led. In the rightmost column in Fig. 2, we plot the distributions of the î€›nal representations learned by GACL with ğœ– =0.1 on two datasets. The setting ofğœ†is the same as that used in SGL variants. We can clearly observe that the distributions in the rightmost column are evidently more uniform than those learned by SGL variants and LightGCN, especially on the dataset of Yelp2018. Meanwhile, the better performance is also reported in Table 4. In this section, we analyze the time complexity of GACL, and compare it with that of LightGCN and its graph-augmentation based counterpart SGL-ED. Here we only discuss the time complexity in a batch. Let|ğ¸|be the edge number in the graph,ğ‘‘be the embedding size,ğµbe the batch size,ğ‘€be the node number in a batch, andğœŒ denotes the edge keep rate in SGL-ED. We can easily get: â€¢For LightGCN and GACL, no graph augmentations are required, so they just need to normalize the original adjacency matrix which has 2|ğ¸|non-zero elements. For SGL-ED, two graph augmentations are used and each has 2ğœŒ |ğ¸| non-zero elements. â€¢In the graph convolution stage, a three-encoder architecture (see Fig. 1) is employed in both SGL-ED and GACL to learn augmented node representations. So, the time costs of SGL-ED and GACL are almost three times that of LightGCN. â€¢As for the recommendation loss, three methods all use the BPR loss and each batch containsğµinteractions, so they have the same time cost in this component. â€¢When calculating the CL loss, the computation costs between the positive/negative samples areO(ğµğ‘‘)andO(ğµğ‘€ğ‘‘), respectively, because each node only considers itself as the positive, while the other nodes all are negatives. Comparing GACL with SGL-ED, we can clearly see that SGL-ED theoretically spends less time in the stage of graph convolution, and this bonus may possibly oî€set GACLâ€™s advantage in the stage of the adjacency matrix construction. However, when putting them into practice, we actually observe that GACL is much more time-eî€œcient (See Table 5). That is because, the computation for graph convolution is mostly î€›nished on GPUs, while the graph perturbation and reconstruction are performed on CPUs. Besides, in each epoch, the graph augmentations in SGL-ED have to be reconstructed, while in GACL, the adjacency matrix of the original graph only needs to be generated once at the start of the training. Therefore, most computation of GACL can be î€›nished on GPUs, and the positive and negative samples for CL can be created on-the-î€y. In a nutshell, GACL is far more eî€œcient than SGL-ED and its other variants, beyond what we can observe from the theoretical analysis. Datasets.Three public benchmark datasets: Douban-Book, Yelp2018 [12], and Amazon-Book [29] are used in our experiments to evaluate GACL. Because we focus on the Top-N recommendation, by following the convention in the previous research [40,41], we discard ratings less than 4 in Douban-Book, which is with a 1-5 rating scale, and reset the rest to 1. The statistics of the datasets is shown 3-Layer in Table 3. We split the datasets into three parts (training set, validation set, and test set) with a ratio of 7:1:2. Two common metrics: Recall@ğ¾and NDCG@ğ¾are used and we setğ¾=20. For a rigorous and unbiased evaluation, each experiment in this section is conducted 5 times and we then report the average result by ranking all the items in the test set. Baselines.Besides LightGCN and the SGL variants which have been introduced in Section 2, we also compare GACL with the following recent self-supervised or CL-based recommendation methods. â€¢ Mult-VAE[17] is a variational autoencoder-based recommendation model. It can be seen as a special self-supervised recommendation model because it has a reconstruction objective. â€¢ DNN+SSL[35] is a recent DNN-based recommendation method which adopts the similar architecture and idea in Fig. 1 for contrastive learning. â€¢ BUIR[16] has a two-branch architecture which consists of a target network and an online network, and only uses positive examples for self-supervised learning. Hyperparameters.For a fair comparison, we refer to the best hyperparameter settings reported in the original papers of the baselines and then î€›ne-tune all the hyperparameters of the baselines with the grid search. As for the general settings of all the baselines, the Xavier initialization is used on all the embeddings. The embedding size is 64, the parameter forğ¿regularization is 10and the batch size is 2048. We use Adam with the learning rate 0.001 to optimize all the models. In GACL and SGL, we empirically let the temperatureğœ =0.2, and this value is also reported as the best in the original paper of SGL. As one of the core ideas of this paper is that graph augmentations are not indispensable and ineî€œcient in CL-based recommendation, in this part, we conduct a comprehensive comparison between SGL and GACL in terms of the recommendation performance, convergence speed, and running time. 4.2.1 Performance Comparison. We î€›rst further compare SGL with GACL on three diî€erent datasets under diî€erent parameter settings. The hyperparameterğœ†, which controls the magnitude of CL in GACL, is set to 0.2 on Douban-Book, 0.5 on Yelp2018, and 2 on Amazon-Book, where the best performances are reached. The magnitude for the added noiseğœ–in GACL is 0.1 on all the datasets. We thicken the î€›gures representing the best performance and underline the second best. The improvements are calculated by using LightGCN as the baseline. According to Table 4, we can draw the following observations and conclusions: â€¢All the SGL variants and GACL are eî€ective in improving Light- GCN under diî€erent settings. The largest improvements are observed on Douban-Book. When the layer number is 1, GACL can remarkably improve LightGCN by 33.5% on Recall, and 40.5% on NDCG. â€¢SGL-ED is the most eî€ective variant of SGL while SGL-ND is the least eî€ective one. When a 2-layer or 3-layer setting is used, SGLWA outperforms SGL-ND in most cases and shows advantages over SGL-RW in a few cases. These results demonstrate that the CL loss is the main driving force of the performance improvement while intuitive graph augmentations may not be as eî€ective as expected, and some of them may even lower the performance. â€¢GACL shows the best performance in all the cases, which proves the eî€ectiveness of the random noised-based data augmentation. Particularly, on the two larger datasets: Yelp2018 and AmazonBook, GACL signiî€›cantly outperforms the SGL variants by large margins. 4.2.2 Convergence Speed Comparison. In the original paper of SGL (Section 4.3.2), the authors discover that SGL converges much faster than LightGCN does. In this part, we manage to reproduce their results and further show that GACL has a much faster convergence speed than what SGL shows. A 2-layer setting is used in this part and the other parameter values are not changed. According to Fig. 4 and Fig. 5, we can see that, GACL reaches its best performance on the test set at the 25epoch on DoubanBook, the 11epoch on Yelp2018, and the 10epoch on AmazonBook. By contrast, SGL-ED reaches its best performance at the 38epoch on Douban-Book, the 17epoch on Yelp2018, and the 14epoch on Amazon-Book. GACL only needs 2/3 epochs that SGL variants need to reach its peak. Besides, the curve of SGL-WA almost overlaps that of SGL-ED on Yelp2018 and Amazon-Book, and exhibits the same tendency to convergence. It seems that the dropout-based graph augmentations cannot speed up the model to reach its convergence. Despite that, all the CL-based methods show advantages over LightGCN at the convergence speed. When the other three methods begin to get overî€›tted, LightGCN is still far away from getting converged. In the paper of SGL, the authors guess that the multiple negatives in the CL loss may contribute to the fast convergence. However, with almost inî€›nite negative samples created by dropout, SGL-ED is basically on par with SGL-WA in speeding up the training, though the latter only has a certain number of negative samples. As for GACL, we consider that the remarkable convergence speed stems from the noises. They averagely provide a constant increment to the gradient values so that the method can be ahead throughout the convergence speed comparison. In addition to the results in Fig. 4 and 5, we also î€›nd that a largerğœ–leads to faster convergence. But when it is too large (e.g., greater than 1), despite the rapid decrease of BPR loss, GACL requires more time to reach its peak performance. A largeğœ–acts like a large learning rate, causing the progressive zigzag optimization that will overshoot the minimum. Table 5: Running time for per epoch (x in the brackets represents times). 4.2.3 Running Time Comparison. In the paper of SGL (section 4.3.2), the authors use the performance and loss curves to prove that SGL can greatly reduce the training time. However, they neglect Figure 4: The performance curves in the î€›rst 50 epo chs. Figure 5: The loss curves in the î€›rst 50 epochs. that SGL may require much longer time in practice for the graph augmentation. In this part, we report the real running time of the compared methods for one epoch. The results in Table 5 are obtained on an Intel(R) Xeon(R) Gold 5122 CPU and a GeForce RTX 2080Ti GPU. As shown in Table 5, we calculate how many times slower the other methods are when compared with LightGCN. Because there is no graph augmentation in SGL-WA, we can see its running speed is very close to that of LightGCN. For SGL-ED, two graph augmentations are required and the computation in this part is mostly î€›nished on CPUs, so that it is even 5.7 times slower than LightGCN on Amazon-Book. The running time increases with the scale of the datasets. By contrast, despite not as fast as SGL-WA, GACL is only 2.4 times slower than LightGCN on Amazon-Book, and the slope of the speed increase is much smaller than that of SGL-ED as well. Considering that GACL only needs half of the epochs that SGL-ED requires to reach the best performance, it outperforms SGL in all aspects. In this part, we investigate the impact of the two important hyperparameters in GACL. Here we adopt the experimental settings used in section 4.2.2. 4.3.1 Impact ofğœ†. By î€›xingğœ–at 0.1, we changeğœ†to a set of predetermined representative values presented in Fig. 6. As can be observed, with the increase ofğœ†, the performance of GACL starts to increase at the beginning, and it gradually reaches its peak when ğœ†is 0.2 on Douban-Book, 0.5 on Yelp2018, and 2 on Amazon-Book. Afterwards, it starts to decline. Besides, in contrast to Fig. 7, more dramatic changes are observed in Fig. 6 thoughğœ–andğœ†are tuned in the same scope, which demonstrates thatğœ–can provide a î€›nergrained regulation beyond that provided only by tuning ğœ†. 4.3.2 Impact ofğœ–. We think changingğœ–can regulate the uniformity of the learned representation distribution. A largerğœ–leads to a more Figure 6: Inî€uence of the magnitude ğœ† for CL. uniform distribution that can help to debias. However, when it is too large, the recommendation task will be hindered because the high similarity between connected nodes can not be reî€ected by an over-uniform distribution. We î€›xğœ†at the best values on the three datasets as reported in Fig. 6, and then adjustğœ–to see the performance change. As can be seen in Fig. 7, the shapes of the curves are as expected. On all the datasets, whenğœ–is near 0.1, GACL achieves the best performance. Three other recent self-supervised learning or CL-based methods also claim themselves as the SOTA methods. However, in our comparison shown in Table 6, they are all uncompetitive, even outperformed by LightGCN in most cases. We think the main reasons are that: (1). LightGCN and GACL have a more powerful graph convolution structure compared with Mult-VAE. (2). DNNs are proved eî€ective when user/item features are available. In our datasets, no features are provided and we mask embeddings learned by DNN to conduct self-supervised learning. Only the weak self-supervision signals are extracted, so that it cannot fulî€›ll itself in this situation. (3). In the paper of BUIR, it removes long-tail nodes to achieve a good performance, but we use all the users and items. Besides, its siamese network may also collapse to a trivial solution on some long-tail nodes because it does not use negative examples, which may account for the performance drop. In GACL, we use the random noises sampled from a uniform distribution to obtain data augmentation. However, there are other types of noises including the Gaussian noises and adversarial noises. Here we also test diî€erent noises and report the results in Table 7 (denotes uniform noises,denotes Gaussian noises generated by the standard Gaussian distribution, anddenotes adversarial Table 6: Performance comparison with other SOTA models. Method noises generated by FGSM [9]). The experimental settings in section 4.2.2 are also used here. According to Table 7, GACLshows comparable performance while GACLis less eî€ective. We think it is because we applyğ¿normalization to the noises generated by the standard Gaussian distribution and then multiplyğœ–to meet the î€›rst constraint. The normalized noises can î€›t a much î€atter Gaussian distribution (can be easily proved) which approximates a uniform distribution. So, the comparable results are observed. As for GACL, the adversarial noises are generated by only targeting maximizing the CL loss while the recommendation loss has a dominant status that impacts the performance more during optimization. Table 7: Performance comparison between diî€erent GACL variants. Graph Neural Networks (GNNs) [6,10,31] now have become widely acknowledged powerful architectures for modeling recommendation data. This new neural network paradigm ends the regime of MLP-based recommendation models in the academia, and boosts the neural recommender systems to a new level. A large number of recommendation models, which adopt GNNs as their bases, claim that they have achieved state-of-the-art performance [12,26,27,41] in diî€erent subî€›elds. Particularly, GCN [15], as the most prevalent variant of GNNs, further fuels the development of the graph neural recommendation models like GCMC [2], NGCF [25], LightGCN [12], and LCF [42]. Despite the diî€erent implementations in details, these GCN-driven models share a common idea that is to acquire the information from the neighbors in the user-item graph layer by layer to reî€›ne the target nodeâ€™s embeddings and fulî€›ll graph reasoning [30]. Among these methods, LightGCN is the most popular one due to its simple structure and decent performance. Following [28], it removes the redundant operations such as the transformation matrices and the nonlinear activation functions. Such a design is proved eî€œcient and eî€ective, and inspires a lot of follow-up CL-based recommendation models like SGL [29] and MHCN [39]. As CL works in a self-supervised manner, it is inherently a possible solution to the data sparsity issue [37,38] in recommender systems. Inspired by the achievements of CL in other î€›elds, there has been a wave of new research that integrates CL with recommendation [19,29,33,39,41,45]. Zhou et al. [45] adopted random masking on attributes and items to create sequence augmentations for sequential model pretraining with mutual information maximization. Similar ideas are also used in [35], where a two-tower DNN architecture is developed for recommendation, and the other two towers contrast augmented item features for the self-supervised task. SGL [29] uniî€›es the CL task and the recommendation task under a joint learning framework where the data augmentation is based on the stochastic perturbations on graphs including the node/edge dropout and random walk. SEPT [39] and COTREC [32] further proposes to mine multiple positive samples with semi-supervised learning on the perturbed graph for social/session-based recommendation. In addition to the dropout, CL4Rec [34] proposes to reorder and crop item segments for sequential data augmentation. Yu et al. [41], Zhang et al. [43] and Xia et al. [33] leveraged hypergraph to model recommendation data, and proposed to contrast diî€erent hypergraph structures for representation regularization. In addition to the data sparsity problem, Zhou et al. [44] theoretically proved that CL can also mitigate the exposure bias in recommendation, and developed a method named CLRec to improve deep match in terms of fairness and eî€œciency. In this paper, we revisit the dropout-based graph contrastive learning in recommendation, and investigate how it improves recommendation performance. We reveal that, in CL-based recommendation models, the CL loss is the core and the graph augmentation only plays a secondary role. Optimizing the CL loss leads to a more uniform representation distribution, which helps to debias in the scenario of recommendation. We then develop a graph augmentationfree CL method to regulate the uniformity of the representation distribution in a more straightforward way. By adding directed random noises to the representation for diî€erent data augmentations and contrast, the proposed method can signiî€›cantly enhance recommendation. The extensive experiments demonstrate that the proposed method outperforms its graph augmentation-based counterparts and meanwhile the running time is dramatically reduced.