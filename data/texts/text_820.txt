huaizepeng2020@ia.ac.cn,jhtao@nlpr.ia.ac.cn,chefeihu2017@ia.ac.cn,dawei.zhang@nlpr.ia.ac.cn,guohua.yang@nlpr.ia.ac.cn (Since KDD requires the papers that have been submitted to arXiv at least one month prior to the deadline need a diî€erent title and abstract, here we give a brief version of title and abstract.) Knowledge Graphs (KGs) have shown great success in recommendation. This is attributed to the rich attribute information contained in KG to improve item and user representations as side information. However, existing knowledge-aware methods leverage attribute information at a coarse-grained level both in item and user side. In this paper, we proposed a novel attentive knowledge graph attribute network(AKGAN) to learn item attributes and user interests via attribute information in KG. Technically, AKGAN adopts a heterogeneous graph neural network framework, which has a diî€erent design between the î€›rst layer and the latter layer. With one attribute placed in the corresponding range of element-wise positions, AKGAN employs a novel interest-aware attention network, which releases the limitation that the sum of attention weight is 1, to model the complexity and personality of user interests towards attributes. Experimental results on three benchmark datasets show the eî€ectiveness and explainability of AKGAN. â€¢ Information systems â†’ Recommender systems. Recommender systems have shown great success in e-commerce, online advertisement, and social medial platforms. The core task of recommender systems is to solve the overload information problem [31,32,40] and suggest items that users are potentially interested in. Traditional methods to achieve information î€›ltering are content-based and collaborative î€›ltering (CF)-based recommender systems[9,20,34], which utilize itemsâ€™ content features and the similarity of users or items from interaction data respectively[7]. However, both of them donâ€™t introduce much side information. In recent years, introducing knowledge graphs (KGs) into recommender systems as side information has been eî€ective for improving recommendation performance. KGs represent real-world entities and illustrate the relationship between them with graph data structure, which can reveal multiple attributes of items and explore the potential reason for user-item interactions. Generally, a KG contains item nodes and attribute nodes, and attribute nodes can not only describe itemsâ€™ attributes directly but also represent other attribute nodesâ€™ attributes. For example, a movie knowledge graph (as shown in î€›gure 1) has six types of nodes, where movie node (i.e., ğ‘’) is item node and the others (i.e., ğ‘’) are attribute nodes.ğ‘’represent the actor attribute of movieğ‘’, which means actorsğ‘’stared in movieğ‘’. Andğ‘’represent the singer attribute of songğ‘’, which means singerğ‘’sang the songğ‘’. To integrate attribute information into recommender system, earlier works focus on embedding-based methods [1,3,12,45,46] and path-based methods [19,26,41â€“43]. The former exploits KGs with knowledge graph embedding (KGE) algorithm (i.e., TransE [2] and TransH [38]) to learn entity embeddings and then feed them into a recommender framework. The latter usually predeî€›nes a path scheme (i.e., meta-path) and leverages path-level semantic similarities of entities to reî€›ne the representations of users and items. However, both of them donâ€™t capture high-order connectivities and fail to exploit both the rich semantics and topology of KGs. More recently, the propagation-based methods, a.k.a. graph neural network (GNN)-based methods such as KGAT[33], KGNN-LS[30], KNI[23], AKGE[25], KGIN[35], have attracted considerable interest of researchers. With the attribute information iteratively propagating in KGs, GNN-based methods integrate multi-hop neighbors into representations and have achieved the state-of-the-art recommendation results. Despite the success of GNN in exploiting multi-hop attribute information, we argue that there are still three shortcomings: (1)The pollution caused by weighted sum operation in merging different attribute information. Diî€erent attributes are independent in terms of semantics and user preference. Take the actor attributeğ‘’and the singer attributeğ‘’of movieğ‘’in î€›gure 1 as an example, semantics independence meansğ‘’doesnâ€™t co-occur withğ‘’in each movie, since an actor and a singer are invited to work for a movie respectively. Preference independence means whether a user prefers actorğ‘’is independent with whether he likes singerğ‘’. However, existing GNN-based methods pool embeddings from diî€erent attribute nodes with weighted sum (i.e., sum, mean, attention) operation, which brings about semantic pollution Figure 1: An example of how KG contains multiple attributes information. Best viewed in color. and the diî€œculty to distill user interested attribute information. (2)The nonlinearity between the distance and importance of attribute node relative to item node. Usually, high-order neighbors are less relative to the center node in graph data, but this is not absolute. For example, singer attributeğ‘’is more valued than director attributeğ‘’by some movie viewers who like music, whileğ‘’, a 2-hop neighbor, is further thanğ‘’, a 1-hop neighbor. This problem is caused by inherent graph topology, which means some signiî€›cant attribute nodes donâ€™t connect to item nodes directly, such that it requires multiple passes to integrate these high-order neighbors to the center node. However, existing GNN-based algorithms neglect this issue and decrease the weight of signiî€›cant high-order neighbors coupled with the increase of propagation times. (3)The complexity and personality of user interests towards diî€erent attributes. User interests show the following pattern: a user just gets interested in a part of rather than all attributes of an item, and diî€erent users prefer diî€erent attributes even towards the same item. For example, both userğ‘¢andğ‘¢watch movieğ‘’because ğ‘¢likesğ‘’â€™s actorğ‘’rather than directorğ‘’whileğ‘¢prefers the theme songğ‘’rather than actorğ‘’. Therefore, we should integrate actorğ‘’rather than directorğ‘’attribute embedding intoğ‘¢â€™s representation and integrate songğ‘’rather than actorğ‘’attribute embedding intoğ‘¢â€™s representation. In other words, the item representation learned by GNN propagation contains noisy signals and we should distill part attributes interested by user personally. However, existing GNN-based algorithms recognize this pattern insuî€œciently, like [33] doesnâ€™t consider noisy attributes and [35] doesnâ€™t consider personal extraction. To address the foregoing problems, we propose a novel attentive knowledge graph attribute network (AKGAN), which consists of two components: (1)knowledge graph attribute network (KGAN). The core task of KGAN is to learn informative item representations without semantic pollution and weight decrease of signiî€›cant attribute nodes. Technologically, KGAN has a diî€erent design between the î€›rst layer and the latter layer under GNN framework. The î€›rst layer, called attribute modeling layer, aims to generate initial item representations without semantic pollution. It regards each relation in KG as an attribute and has two key designs: embedding each entity in diî€erent attribute spaces and using concatenation operation to merge diî€erent attribute information. The latter layer, called attribute propagation layer, aims to remain semantics unpolluted and avoids weight decrease of signiî€›cant neighbors after GNN propagation. Finally, KGAN generates item representations where one attribute is represented within a speciî€›c range of element-wise positions independently. (2)user interest-aware attention network. With diî€erent attributes placed in corresponding element-wise positions, we design an interest-aware attention layer to distill user interested attributes. Speciî€›cally, for a particular user, we pool the representations of his interacted items and extract each attribute representation according to corresponding element-wise positions. Then each attribute representation will be fed into an attention module to calculate a personal interest score, which describes how much he prefers this attribute. We introduce a novel activation unit to release the limitation that the sum of attention weight is 1, which aims to reserve the intensity of user interests[50]. Finally all interest scores and item representations are further combined to infer user representations. To summarize, the main contributions of this work are as follows: â€¢On the item side, we propose a novel knowledge graph attribute network, which employs a heterogeneous design in diî€erent layers under GNN framework, embeds each entity in diî€erent attribute spaces, and combines diî€erent attributes via concatenation operation, to avoid pollution caused by weighted sum operation and weight decrease of signiî€›cant neighbors. â€¢On the user side, we develop an interest-aware attention network, which introduces a novel activation unit and releases the limitation that the sum of attention weight is 1 , to model user interests towards attributes personally. â€¢We conduct extensive experiments on three public benchmarks, and the results demonstrate the superior performance of AKGAN over state-of-the-art baselines. In-depth analyses are provided to illustrate the interpretability of AKGAN for user personal interests. We begin by introducing some related notations and then deî€›ning the KG enhanced recommendation problem. LetU = {ğ‘¢}and I = {ğ‘–}separately denote the user and item sets. A typical recommender system usually has historical user-item interactions, which is deî€›ned asO= {(ğ‘¢, ğ‘–)|ğ‘¢ âˆˆ U, ğ‘– âˆˆ I}. Each(ğ‘¢, ğ‘–)pair indicates userğ‘¢has interacted with itemğ‘–before, such as clicking, review, or purchasing. We also have a knowledge graph which stores the structured semantic information of real-world facts. We denote the entity and relation sets in KG asR = {ğ‘Ÿ }andE = {ğ‘’}. KG is presented asG = {(â„, ğ‘Ÿ, ğ‘¡)|â„ âˆˆ E, ğ‘Ÿ âˆˆ R, ğ‘¡ âˆˆ E}}, where each tripile means there is a relationğ‘Ÿfrom head entityâ„to tail entityğ‘¡. For example,(ğ½ ğ‘ğ‘šğ‘’ğ‘  ğ¶ğ‘ğ‘šğ‘’ğ‘Ÿğ‘œğ‘›, ğ·ğ‘–ğ‘Ÿğ‘’ğ‘ğ‘¡, ğ´ğ‘£ğ‘ğ‘‘ğ‘’ğ‘Ÿ )describes the fact that James Cameron is the director of the movie Avatar. Note thatRcontains relations in both canonical direction (e.g., Direct) and inverse direction (e.g., DirectedBy). The bridge of KG and recommender system is that items are contained in entities. Speciî€›clly,Econsists of item nodeI(I âŠ† E) as well as attribute nodeE \ I, which can futher reî€›ne user representation eand item representation e. We now formulate the KG enhanced recommendation task to be addressed in this paper: â€¢ Input: a knowledge graphG, which contains rich sturcture senmeantic information of item, and the user-item interaction data â€¢ Output: a scoring function that denotes the probability that user u will interact with item ğ‘–. In this subsection, we introduce the framework of AKGAN. With a widely used two-tower structure in recommender model[8,21, 47], AKGAN consists of two main modules: (1) knowledge graph attribute network, which is illustrated in Figure 2, and (2) user interest-aware attention module, which is illustrated in Figure 3. The core task of KGAN is to learn KG enhanced item representations that contain multiple attribute information. KGAN adopts GNN framework and has a heterogeneous design in the î€›rst layer and the latter layer, which are named attribute modeling layer and attribute propagation layer respectively. Attribute modeling layer(AML) is to construct initial entity representations by merging one-hop neiborhoodsâ€™ attribute information in knowledge graph, as follows: Attribute propagation layer(APL) recursively propagates attribute information to acquire more informative item representations as After performingğ¿layers, we obtain multiple item representations, namely{e, ..., e}. As the output ofğ‘™layer represents ğ‘™âˆ’hop attribute information, we conduct sum opration to pool them and infer î€›nal item representations as When item representations have been learned,interest-aware attention(IAA) module is to learn user representations via interaction data with a novel relation-aware attention mechanism which represents user interests towards relations, a.k.a. attributes. Formally, user representations are obtained by IAA as When user representations and item representations are learned, a scoring function is used to predict their matching score and here we adopt inner product operation as In KGs, one entity has diî€erent types of relations with neighbors. For example, in î€›gure 2,ğ‘’has diî€erent relations withğ‘’andğ‘’. To model a pure initial representation without semantic pollution, we regard each relation as an attribute and embeds each entity in all attribute embedding spaces, which is similar to the design of FFM[14] that embeds each feature in diî€erent î€›elds. Therefore each entity has sevecal embeddings and we denote embedding set in diî€erent attribute spaces as whereeâˆˆ Rdenotes the embedding of entityğ‘–in attributeğ‘Ÿ space and ğ‘‘is the embedding dimension of attribute ğ‘Ÿspace. Then we construct initial entity representations by aggregating attribute embeddings of one-hop neighbors. We can see that each neighbor has several embeddings and just one relation-aware embedding will be used. Taking î€›gure 2 as an example, there are two links(ğ‘’, ğ‘Ÿ, ğ‘’)and(ğ‘’, ğ‘Ÿ, ğ‘’), we use embeddingseto represents the singer attribute of songğ‘’and embeddingeto represents the friend attribute of actorğ‘’, respectively. After we prepared the relation-aware embeddings, we adopt average operation to pool the same relation-aware neighbors to acquire the main semantics of this attribute as whereğ‘— âˆˆ NandN= {ğ‘— |ğ‘— âˆˆ (ğ‘—, ğ‘Ÿ, ğ‘–), (ğ‘—, ğ‘Ÿ, ğ‘–) âˆˆ G}denotes the set of head entities that belong to the triplet whereğ‘–is tail entity andğ‘Ÿis relation. For example, ifğ‘’are both comedy actors and ğ‘’is an action actor, such thatğ‘šğ‘’ğ‘ğ‘›(e)represents movieğ‘’is likely to show much funny performance. Note that not all attributes occur in one-hop range, like movieğ‘’doesnâ€™t has friend attribute ğ‘Ÿ. To obtain a î€›xed-length representation, we provide zero vector to the absent attribute. Finally, all attribute embeddings will be integrated into one representation by concatenation operation as Note that we adopt concatenation rather than weighted sum operation (i.e., sum, mean, attention), which aims to solve the problem of semantic pollution. More specî€›cally, equation 8 shows that one attribute is represented within a speciî€›c range of element-wise positions. In addition, one type of attribute is placed in the same element-wise postions for all entities. For example, two represenmean movieğ‘’and movieğ‘’have the same director and diî€erent actors. In a word, a concatented representation avoids the interaction and preserves semantic independence of diî€erent attributes, which will be further advantageous to maintain the weight of siginiî€›cant high-order neighbors in subsection 3.3 and distill user interested attributes in subsection 3.4. When we obtain initial entity representations via one-hop neighbors, an intuitive idea is to propagate them via GNN framework, so as to aggregate high-order neighbors into the center node and acquire more informative item representations. Here we regard a KG as a heterogeneous graph and adopt a widely used two-step scheme[5,36,44] to aggregate the representations of neighbors: (1) same relation-aware neighbors aggregation; (2) relations combination. The same relation-aware neighbors contains the similiar attribute types. For example, in î€›gure 2, bothğ‘’andğ‘’are actors and contain friend attribute whileğ‘’is a song and contains singer attribute. Therefore, we î€›rstly aggregate representations of the same relation-aware neighbors. Secondly, to learn a more comprehensive representation, we need to fuse multiple attributes hold in diî€erent relation-aware neighbors. The pooling methods in the above two steps are average and sum operation, respectively. And we leave the further exploration of other pooling methods like attention as the future work. More formally, in theğ‘™âˆ’th layer, we recursively formulate the representation of an entity as: Now the î€›nal item representationehas been learned by equation 3, letâ€™s check how KGAN avoids pollution and weight decrease of signiî€›cant high-order neighbors. We re-examineefrom the perspective of element-wise position. Without loss of generality, eis contructed as whereeâˆˆ Ris a truncated vector ineâˆˆ Randğ· = Ãğ‘‘, respectively. Reviewing the generation process ofe, we can see thateis learned byğ‘–â€™s neighborsâ€™ embeddings in attribute ğ‘Ÿspace and doesnâ€™t contain any embeddings from other attribute spaces. For example, in î€›gure 2,eis learned bye,e, ande. This meansemaintains the independence of attributeğ‘Ÿand KGAN remains semantics unpolluted after multi-layer propagations. Furthermore, we check sepeciî€›c neighbors which are aggregated intoe. Takeeas an exapmle, after 1-order propagation (one AML),eis a zero vector since attributeğ‘Ÿdoesnâ€™t occur in onehop neighbors of movieğ‘’. Then after 2-order propagation (one AML and one APL),e= e, which seems as if there were a link (ğ‘’, ğ‘Ÿ, ğ‘’)andğ‘’connected toğ‘’directly. In general, when one relation, a.k.a attribute (i.e.,ğ‘Ÿ), î€›rstly appear in the receptive î€›eld of center node (i.e.,ğ‘’) atğ‘™âˆ’hop (i.e.,2âˆ’hop) position, the weight of its corresponding node (i.e.,ğ‘’) will not been decreased, which proves that KGAN maintains the weight of signiî€›cant high-order neighbors. After item representations have been obtained by KGAN, a typical idea in recommender system is to enhance user representations by clicked items, like [16,29,39]. We take avearage pooling as an example asîƒ• whereN= {ğ‘–|(ğ‘¢, ğ‘–) âˆˆ O}anderepresents user embedding for collaborative î€›ltering. However, avearage pooling doesnâ€™t consider user preferences personally, thereby recent works aims to model user interests via attention mechanism, such as [17,22,33,35,49]. Here we propose a novel attention module to model user interests towards diî€erent attributes. We assign an interest scoreğ‘“(ğ‘¢, ğ‘Ÿ) to each pair of attributeğ‘Ÿand userğ‘¢, and personally generate user representation by combining interest scores and interacted items. We î€›rstly introduce how to calculateğ‘“(ğ‘¢, ğ‘Ÿ)and then illustrate how to combine ğ‘“(ğ‘¢, ğ‘Ÿ) with interacted items. With diî€erent attributes placed in corresponding element-wise positions as shown in equation 10, we truncate user vectorewith the same strategy as whereeâˆˆ Ris a truncated vector ineâˆˆ R. The start and ending index ofeineareÃğ‘‘andÃğ‘‘, respectively. Then the interest score of user ğ‘¢ towards attribute ğ‘Ÿis calculated by whereğœis temperature coeî€œcient as a hyperparameter. In equa-Ã tion 13,eedenotes the interest-degree of userğ‘¢ for his interacted items in attributeğ‘Ÿ, whileÃee denotes the interest-degree of userğ‘¢for all items in attributeğ‘Ÿ. If userğ‘¢attachs great importance to attributeğ‘Ÿwhen choosing items, the corresponding truncated representationeof his interacted items will be radically diî€erent from that of other uninterestedÃ items, such thateewill be much greater than Ãee, and vice versa. Therefore, the ratio between the above two expressions denotes the interest-degree of userğ‘¢ for attributeğ‘Ÿ. Since a negative value of ratio is meaningless, we î€›rst feed this ratio toReluand then selecttanhas the nonlinear activation function, therefore ğœ (ğ‘¥) = tanh(Relu(ğ‘¥)). Afterğ‘“(ğ‘¢, ğ‘Ÿ)has been prepared, we need to combine it with interacted items to learn user representations. We î€›rstly re-express equation 11 as where||is the concatenation operation. Therefore an intuitive idea is to useğ‘“(ğ‘¢, ğ‘Ÿ)to control how much attribute information,Ã a.k.a.e, will be passed to user. Consider the limit case, whenğ‘“(ğ‘¢, ğ‘Ÿ) = 0which means userğ‘¢doesnâ€™t pay attention to attributeğ‘Ÿat all, any information of attributeğ‘Ÿshould not be contained in user representaion. Formally, user representation is obtained by Note that here we relax the constraint that the sum of attentionÃ weights towards all attributes is 1, a.k.a.ğ‘“(ğ‘¢, ğ‘Ÿ) â‰  1. The reason is as follows: when the number of members participating in the attention calculation is large and the limitation that the sum of attention weights is 1 is still reserved at this time, it will cause the attention weight of each member to be dispersed, making it diî€œcult to learn the coeî€œcients of important nodes. This problem has also appeared in Graphair [11], which shows estimating O(| N |)coeî€œcients exposes the risk of overî€›tting. Therefore, we introduce the above novel activation unit to release the limitation that the sum of attention weight is 1, and the same idea is adopted by DIN [50]. With the user representationeand the item representatione ready, equation 5 is adopted to calculate the prediction score of each pair of user and item. Then we employ the BPR loss [24] to encourage that the observed interactions should be assigned higher prediction values than unobserved ones. The objective function is formulated as L =âˆ’ln ğœ^ğ‘¦(ğ‘¢, ğ‘–) âˆ’^ğ‘¦(ğ‘¢, ğ‘–)+ğœ†âˆ¥Î˜âˆ¥(16) whereO = {(ğ‘¢, ğ‘–, ğ‘–)|(ğ‘¢, ğ‘–) âˆˆ O, (ğ‘¢, ğ‘–) âˆˆ O}denotes the training set, which contains the observed interactions Oand the unobserved interactionsO;ğœ (Â·)is the sigmoid function.Î˜ = {e, e|ğ‘– âˆˆ I, ğ‘¢ âˆˆ U, ğ‘Ÿâˆˆ R}is the model parameter set.ğ¿ regularization parameterized byğœ†onÎ˜is conducted to prevent overî€›tting. We employ the Adam [15] optimizer and use it in a mini-batch manner. We evaluate our proposed AKGAN method on three benchmark datasets to answer the following research questions: Table 1: Statistics of the datasets. â€™Int./userâ€™ indicates the average number of interactions per user. â€¢ RQ1: Does our proposed AKGAN outperform the state-of-the-art recommendation methods? â€¢ RQ2: How do diî€erent components (i.e., attribute modeling layer, attribute propagation layer, and interest-aware attention layer) aî€ect AKGAN? â€¢ RQ3: Can AKGAN provide potential explanations about user preferences towards attributes? Dataset Description. We choose three benchmark datasets to evaluate our method: Amazon-Book, Last-FM, and Alibaba-iFashion. The former two datasets are released in [33] and the last one is released in [35], and all of them are publicly available. Each dataset consists of two parts: user-item interactions and a corresponding knowledge graph. The basic statistics of the three datasets are presented in Table 1. We follow the same data partition used in [33,37] to split the datasets into training and testing sets. For each observed user-item interaction, we randomly sample one negative item that the user has not interacted with before, and pair it with the user as a negative instance. Evaluation Metrics. We evaluate our method in the task of top-K recommendation. For each user, we treat all the items that the user has not interacted with as negative and the observed items in the testing set as positive. Then we rank all these items and adopt two widely-used evaluation protocols: Recall@ğ¾and NDCG@ğ¾, where ğ¾is set as 20 by default. We report the average metrics for all users in the testing set. Baselines. To demonstrate the eî€ectiveness, we compare AKGAN with KG-free (MF), embedding-based (CKE), path-based (RippleNet), and GNN-based (KGAT, KGNN-LS, KGIN) methods: â€¢ MF[24]: This is matrix factorization optimized by the Bayesian personalized ranking (BPR) loss, which only considers the useritem interactions and leaves KG untouched. â€¢ CKE[45]: This method uses TransR [18], a typical knowledge graph embedding algorithm, to regularize the representations of items, which are fed into MF framework for recommendation. â€¢ RippleNet[28]: This model combines embedding-based methods and path-based methods to propagate usersâ€™ preferences on the KG for recommendation. RippleNet î€›rst assigns entities in the KG with initial embeddings using TransE and represents a user via entities related to his historically clicking items. â€¢ KGAT[33]: This method encodes user behaviors and item knowledge as an uniî€›ed knowledge graph to exploit high-order connectivity. KGAT applies an attentive neighborhood aggregation mechanism on a holistic graph and introduces TransR to regularize the representations. â€¢ KGNN-LS[31]: It uses a user-speciî€›c relation scoring function to transform a heterogeneous KG into a user-personalized weighted graph and employs label smoothness regularization to avoid overî€›tting of edge weights. â€¢ KGIN[35]: KGIN is the state-of-the-art GNN-based recommender. It models each intent as an attentive combination of KG relations to explore intents behind user-item interactions and adopts a novel relational path-aware aggregation scheme. Parameter Settings. We implement our AGKAN model in Pytorch and Deep Graph Library (DGL), which is a Python package for deep learning on graphs. We released all implementations (code, datasets, parameter settings, and training logs) to facilitate reproducibility. The embedding size of one attribute space in AGKAN varies from 4 to 64, and the detailed design will be introduced in Appendix A.1. For a fair comparison, we î€›x the size of ID embeddings as 64, which equals the max embedding size of AKGAN, for all baselines, except RippleNet 32 due to its high computational cost. We adopt Adam [15] as the optimizer and the batch size is î€›xed at 1024 for all methods. We use the Xavier initializer [6] to initialize model parameters. We apply a grid search for hyper-parameters: the learning rate is searched in{10, 10, 10, 10}, the coeî€œcient of L2 normalization is tuned in{10, 10, Â·Â·Â· , 10}, the number of GNN layersğ¿is searched in{1, 2, 3}for GNN-based methods, and the dropout ratio is tuned in{0.0, 0.1,Â· Â· Â·, 0.9}. Besides, we use the node dropout technique for KGAT, KGIN, and AKGAN, where the ratio is searched in{0.0, 0.1,Â· Â· Â·, 0.9}. For RippleNet, we set the number of hops as 2, and the memory size as 5, 15, 8 for Alibaba-iFashion, Last-FM, and Amazon-book respectively to obtain the best performance. Since RippleNet is a model for CTR prediction, we generate top-K items with top-K scores in all items, which are compared with the test set to compute Recall@ğ¾and NDCG@ğ¾. For KGAT, we use the pre-trained ID embeddings of MF as the initialization, which is also adopted by KGIN. Moreover, early stopping strategy is performed, i.e., premature stopping if Recall@20 on the test set does not increase for 10 successive epochs. We report the empirical results in Table 2 where we highlight the results of the best baselines (starred) and our AKGAN (boldfaced). And we also use%Imp. to denote the percentage of relative improvement on each metric. The observations are as followed: â€¢AKGAN consistently achieves the best performance on three datasets in terms of all measures. Speciî€›cally, it achieves signiî€›cant improvements over the strongest baselines w.r.t. NDCG@20 Table 2: Overall Performance Comparison. The best performance is boldfaced; the runner up is labeled with â€™*â€™. â€™%Imp.â€™ indicates the improvements. Alibaba-iFashion by 8.63%, 25.71%, and 11.87% in Amazon-Book, Last-FM, and Alibaba-iFashion, respectively. These improvements are attributed to the following reasons: (1) By combining all attributes using concatenation operation, AKGAN avoids semantic pollution caused by weighted sum operation and learns more high-quality item representations for recommendation. (2) Compared to GNNbased baselines (i.e., KGAT, KGNN-LS, KGIN), AKGAN maintains the weight of signiî€›cant high-order neighbors by placing different attributes in corresponding element-wise positions. (3) Beneî€›ting from our novel interest-aware attention module that assigns an interest score to each pair of user and attribute, AKGAN can recognize the pattern of user interest at a î€›ne-grained level to conduct a better personal recommendation. â€¢Jointly analyzing AKGAN across the three datasets, we î€›nd that the improvement on Last-FM is more signiî€›cant than that on Alibaba-iFashion and Amazon-Book. The main reason is that the interaction number per user of Last-FM (128.78) is much larger than that of the other two datasets (11.99, 15.52). Therefore, there exists richer interaction information on the Last-FM dataset for AKGAN to reî€›ne user and item representation by collaborative signals. This indicates that AKGAN will fully realize its potential in recommendation scenarios with dense interaction data. â€¢KG-free method (MF) underperforms knowledge-aware methods (i.e., CKE, RippleNet, AKGAN). A clear reason is MF doesnâ€™t leverage the rich attribute information in KG. â€¢GNN-based methods (i.e., KGAT, KGNN-LS, KGIN, AKGAN) achieve better performance than path-based (RippleNet) and embedding-based (CKE) methods. A possible reason is that these three kinds of recommenders adopt diî€erent usages of attributes. GNN-based methods aggregate neighborsâ€™ attribute information into item nodes to learn more informative representations. The other two methods have a common limitation that both of them donâ€™t break loose from employing knowledge graph embedding algorithms to model attribute information and regularize node representations. â€¢In four GNN-based methods, AKGAN performs best, KGIN is the second-best, while KGAT and KGNN-LS are at the same level and achieve the worst results. The decreasing performance is because that the level of how a recommender learns item attributes and user interests is in descending order, from î€›negrained to coarse-grained. AKGAN uses concatenation operation to avoid attribute interaction while other models adopt weighted sum (attentive combination) operation. AKGAN maintains the weight of signiî€›cant high-order neighbors while other models Table 3: GNN-based models Comparison: (1) CM - Combination methods of diî€erent attributes (Con - concatenation, WS - weighted sum); (2) WD - whether to avoid weight decrease of signiî€›cant high-order neighbors; (3) GF - the GNN framework that a recommender adopts (Het - heterogeneous design in diî€erent layers, Hom - identical network in each layer; (4) IA - whether to learn user interests towards attributes; (5) level - the level of how a recommender learns item attributes and user interests (FG - î€›ne-grained, CG coarse-grained). ModelCMWDGFIA level neglect this issue. To achieve the above two advantages, AKGAN adopts a heterogeneous design in diî€erent layers while the others use a homogeneous GNN framework. AKGAN, KGIN, and KGNNLS all learn user interests towards attributes explicitly while KGAT doesnâ€™t. The above comparison is listed in Table 3. In this section, we î€›rst conduct an ablation study to investigate the eî€ect of cancatenation operation and interest-aware attention layer. Towards the further analysis, we study the inî€uence of layer numbers. In what follows, we explore how the hyperparameter, a.k.a. temperature coeî€œcient, aî€ects the performance. Impact of cancatenation operation & interest score. To demonstrate the necessity of cancatenation oepration and interest score, we compare the performance of AKGAN with the following three variants: (1) combing diî€erent attributes with average operation and discarding interest score, termed AKGAN-mean, (2) combing different attributes with sum operation and discarding interest score, termed AKGAN-sum, (3) only discarding interest score, termed AKGAN-noatt. Discarding interest score means we use equation 11 to learn user representations. Note that we donâ€™t adopt weighted sum operation and interest-aware attention layer simultaneously. The reason is that cancatenation operation is the prerequisite of interest-aware attention layer, therefore we have to discard interest Table 4: Impact of cancatenation operation and interest score. AKGAN-mean 0.1504 0.0799 0.0849 0.0713 0.1064 0.0660 AKGAN-noatt 0.1768 0.0969 0.1141 0.1012 0.1173 0.0739 AKGAN-1 0.1737 0.0974 0.1192 0.1060 0.1245 0.0791 AKGAN-2 0.1752 0.0977 0.1205 0.1069 0.1253 0.0801 AKGAN-3 0.1783 0.0994 0.1209 0.1066 0.1221 0.0776 score when testing weighted sum operation. The results are shown in Table 4 and we summarize the major î€›ndings as below: â€¢Comparing AKGAN-mean/AKGAN-sum with AKGAN-noatt, we can clearly see that replacing concatenation operation with weighted sum operation dramatically degrades performance of recommendation, which indicates the superiority of cancatenation operation. â€¢The improvement from AKGAN-noatt to AKGAN veriî€›es the necessity of interest score. â€¢Jointly comparing AKGAN-noatt and AKGAN across the three datasets, we î€›nd that the improvement on Last-FM is more signiî€›cant than that on Alibaba-iFashion and Amazon-Book, which is consistent with the aforesaid conclusion in subsection 4.2. The main reason is that Last-FM has more dense interaction data than the other two datasets, such that AKGAN can better learn user interest in Last-FM. Impact of model depth. We investigate the inî€uence of depth of receptive î€›eld in AKGAN by searchingğ¿in the range of{1, 2, 3}. Particularly,ğ¿ = 1means AKGAN has only one attribute modeling layer. The results are reported in Table 5. In Amazon-Book and Last-FM, we can see that increasing propagation times of attributes can boost the performance, because more attribute information is aggregated into the center node to learn more informative user and item representations. While in Alibaba-iFashion, AKGAN-2 is the best and AKGAN-3 is the worst, which is caused by its inherent topology. Alibaba-iFashion just has two kinds of triplets: the î€›rstorder connectivity of an item is its components, a.k.a. (fashion outî€›t, including, fashion staî€), and the second-order connectivity is the category of staî€, a.k.a. (staî€, having-category-?, ?). Therefore, all signiî€›cant attribute information has been captured in the 2-hop range, leading to the best performance of AKGAN-2. Impact of temperature coeî€œcient. We vary the temperature coeî€œcient in the range ofğœ = {0.01, 0 .1, 0.2, ..., 0.9, 1}to study its inî€uence on the performance of AKGAN. The experiment is ongoing. In this section, we visualize the interest score to show how AKGAN learns user preferences towards attributes. We choose AlibabaiFashion as the example dataset since it helps to provide an intuitive explanation. We begin with introducing this dataset brieî€y for further better understanding. Alibaba-iFashion is an E-commerce dataset, which contains user-outî€›t click history for recommendation. Each outî€›t consists of several fashion staî€s (e.g., tops, bottoms, shoes) and each staî€ is assigned with diî€erent categories. For example, trench coat, T-shirt and sweater are three kinds of tops, pants and long skirt are two kinds of bottoms. Alibaba-iFashion regards these categories as relations in KG and there is one more relation between outî€›t and staî€, called including. Therefore, AlibabaiFashion KG has two kinds of triples: (outî€›t, including, staî€) and (staî€, having-category-?, ?), where ? is the speciî€›c category like T-shirt. According to Table 1, Alibaba-iFashion KG has 51 relations, where we number relation including as 0 and number 50 staî€ categories from 1 to 50. In î€›gure 4, we î€›rst exhibit the training set and testing set of a speciî€›c user (user0). Then we visualize the interest score learned from the training set by AKGAN and use the testing set to validate the eî€ectiveness of these scores. From î€›gure 4 we can see that: â€¢The interest score accurately models the user preferences towards attributes. Among 11 outî€›ts clicked by user0, the tops contain four main kinds: T-shirt, shirt, trench coat, and sweater, which means user0 prefers the above four types of tops. The corresponding interest score are 0.3254, 0.4379, 0.6793, 0.7256, respectively. The most clicked bottoms are womenâ€™s pants which are assigned with a high score 0.8223, while the jeans and skirt occur less frequently and their scores, 0.0601 and 0.0671, are particularly small. As for shoes, user0 prefers womenâ€™s shoes and boots only appear once, their scores, 0.7623 and 0.0431, also reî€ect this diî€erence. The earrings and hat are two main accessories and both of them are assigned with high scores, 0.5506 and 0.3506. â€¢All outî€›ts clicked by user0 have only one kind of handbag and the interest scoreğ‘“(ğ‘¢, ğ‘Ÿ) = 0.1506, which is a small value. A possible reason is that outî€›ts are manually created by Taobaoâ€™s fashion experts who prefer to include handbags for the completion of the outî€›t [4]. Therefore user0 judges whether to click an outî€›t based on other staî€s rather than the handbag. Another explanation is that handbag is not as frequently changed as other staî€s like tops in daily dressing, such that user0 pays less attention to the handbag when browsing through outî€›ts. â€¢Testing set proves the eî€ectiveness of interest score. For example, ğ‘“(ğ‘¢, ğ‘Ÿ) = 0.6793is proved to be reansonable since outî€›t12 which contains a trench coat appears in the testing set. And ğ‘“(ğ‘¢, ğ‘Ÿ) = 0.8233is validated by outî€›t13 and outî€›t14 with same reason. â€¢In one staî€, the interest degree varies according to category. Take accessories as an example, user0 prefers earrings than hat and ğ‘“(ğ‘¢, ğ‘Ÿ) = 0.5506 > ğ‘“(ğ‘¢, ğ‘Ÿ) = 0.3506. This result is also proved by the testing set, where two outî€›ts contain earrings while only one outî€›t has a hat. â€¢Unconcerned attribute is assigned with a zero score, likeğ‘“(ğ‘¢, ğ‘Ÿ) = 0. The reason is that user0 perhaps is female since her click histories are all womenâ€™s clothing and she isnâ€™t interested in menâ€™s pants. â€¢Relation including has the highest score 1. The reason is that this relation (outî€›t, including, staî€) is the necessary bridge to acquire what staî€s an outî€›t consists of. And the same reason has been reî€ected in the impact of model depth. Our work is highly related with the knowledge-aware recommendation, which can be grouped into three categories. Embedding-based Methods[1,3,12,29,45,46] hire KG embedding algorithm (i.e., TransE [2] and TransH [38]) to model prior representations of item, which are used to guide the recommender model. For example, DKN [29] learns knowledge-level embedding of entities in news content via TransD [13] for news recommendation. KSR [12] utilizes knowledge base information learned with TransE as attribute-level preference to enhance the sequential recommendation. Path-based Methods[19,26,41â€“43] usually predeî€›nes a path scheme (i.e., meta-path) and leverages path-level semantic similarities of entities to reî€›ne the representations of users and items. For example, MCRec [10] learns the explicit representations of metapaths to depict the interaction context of user-item pairs. RKGE [27] mines the path relation between user and item automatically and encodes the entire path using a recurrent network to predict user preference towards this item. GNN-based Methods[23,25,30,33,35,48] utilizes the messagepassing mechanism in graph to aggregate high-order attribute informations into item representation for enhanced and explainable recommendation. KGAT[33] regards user-item interaction as a new relation added to KG and then employees attentive mechanism to propagate attribute information. IntentGC [48] reconstructs userto-user relationships and item-to-item relationships based on KG and proposes a novel graph convolutional network to aggregate the attribute information from neighbors. KGIN [35] learn user interest via an attentive combination of attributes and integrates relational information from multi-hop paths to reî€›ne the representations. In this paper, we study the attribute information in knowledge graphs intending to improve the recommendation performance. On the item side, the proposed AKGAN can learn more high-quality item representation by remaining the independency of attributes and maintaining the weight of high-order signiî€›cant attribute nodes. On the user side, AKGAN mines user interests towards attributes and provides a personal recommendation. Extensive experiments demonstrate the eî€ectiveness and explainability of AKGAN. For future work, we plan to investigate how to model the evolving process of user interests towards attributes based on the longterm behavior sequence. Another direction is to explore whether attribute interaction will beneî€›t recommendation since feature interaction has shown great success in click-through rate prediction. The corresponding author Jianhua Tao thanks the support of National Natural Science Foundation of China.