The explanation of a recommendation list plays an increasingly important role in the interaction of a user with a recommender system: the pervasiveness of economic interest and the inscrutability of most Artiî€›cial Intelligence systems make users ask for some form of accountability in the behavior of the systems they interact with. Given the explanation that a system can provide to a user we identify at least two characteristics that the explanation part should enforce [1, 2, 3]: â€¢Adherence to reality: the explanation should mention only features that really pertain to the recommended item. For instance, if the system recommends the movie â€œTitanicâ€, it should not explain this recommendation by saying â€œbecause it is a War Movieâ€ since it is by no means an adherent description of that movie; â€¢Constancy in the behavior: when the explanation is generated based on some sample, and such a sample is drawn with a probability distribution, the entire process should not exhibit a random behavior to the user. For instance, if the explanation for recommending the movie â€œThe Matrixâ€ to the same user is î€›rst â€œbecause it is a Dystopian Science Fictionâ€, and then â€œbecause it is an Acrobatic Duels Movieâ€, for the same user, this behavior would be perceived as nondeterministic, and thus reducing its trustworthiness. Among several ways of generating explanations, we study here the application of LIME [4] to the recommendation process. LIME is an algorithm that can explain the predictions of any classiî€›er or regressor in a faithful way, by approximating it locally with an interpretable model. LIME belongs to the category of post-hoc algorithms and it sees the prediction system as a black box by ignoring its underlying operations and algorithms. Since we can consider the recommendation task as a particular Machine Learning task, the LIME approach can also be applied to recommendation. LIME-RS [5] is an adaptation of the general algorithm to the recommendation task and can be considered in all respects as a black-box explainer. This means that it generates an explanation by drawing a huge number of (random) calls to the system, collecting the answers, building a model of behavior of the system, and then constructing the explanation for the particular recommended item. While the fact of adopting a black-box approach lets LIME-RS to be applicable for every recommender system, the way of building a model by drawing a huge random sample of system behaviors makes it lose both adherence and constancy, as our experiments show later on this paper. This suggests that the direct application of LIME-RS to recommender systems is not advisable, and that further research is needed to assess the usefulness of LIME-RS in explaining recommendations. The paper is organized as follows: Section 2 reviews the state of the art on explanation in recommendation; Section 3 details LIME to make the paper self-contained. Section 4 shows the results of experiments with two mainstream recommendation models: Attribute Item-kNN and Vector Space Model. We discuss the outcomes of the experiments in Section 5, and conclude with Section 6. In recent years, the theme of Explanation in Artiî€›cial Intelligence has come to the foreground, capturing the attention not only of the Machine Learning and related communities â€“ that deal more speciî€›cally with the algorithmic part â€“ but also of î€›elds closer to Social Sciences, such as Sociology or Cognitivism, which look with great interest to this area of research [1]. The growing interest in this area is also dictated by new regulations of both Europe [6] and US [7] with respect to sensitive issues in the î€›eld of personal data processing, and legal responsibility. This trend has also touched the research î€›eld of recommender systems [8,9,10,11]. However, topics such as explanation are by no means new to this î€›eld. In fact, we can date back to 2014 the introduction of the term â€œexplainable recommendationâ€ [12], although the need to provide an explanation that accompanies the recommendation is a need that emerged as early as 1999 by Schafer et al.[13], when people began trying to explain a recommendation with other similar items familiar to the user who received that recommendation. Catalyzation of interest around the topic of explanation of recommendations coincides also with the awareness achieved in considering metrics beyond accuracy as fundamental in evaluating a recommendation system [14,15]. Indeed, all of the well-known metrics of novelty, diversity, and serendipity are intended to improve the user experience, and in this respect, a key role is played by explanation [3,16]. â€œWhy are you recommending that?"â€”this is the question that usually accompanies the user when a suggestion is provided. Tintarev and Masthoî€˜[2]detailed in a scrupulous way the aspects involved in the process of explanation when we talk about recommendation. They identiî€›ed 7 aspects: userâ€™s trust, satisfaction, persuasiveness, eî€ciency, eî€˜ectiveness, scrutability, and transparency. This is the starting point to deî€›ne Explainable Recommendation as a task that aims to provide suggestions to the users and make them aware of the recommendation process, explaining also why that speciî€›c object has been suggested. Gedikli et al.[3]evaluated diî€˜erent types of explanations and drew a set of guidelines to decide what the best explanation that should equip a recommendation system is. This is due to the fact that popular recommendation systems are based on Matrix Factorization (MF) [17]; for this type of model, trying to provide an explanation opens the way to new challenges [1, 18, 19, 20]. There are two diî€˜erent approaches to address this type of issue. â€¢On the one hand, the model-intrinsic explanation strategy aims to create a user-friendly recommendation model or encapsulates an explaining mechanism. However, as Lipton[21]points out, this strategy will weigh in on the trade-oî€˜ between the transparency and accuracy of the model. Indeed, if the goal becomes to justify recommendations, the purpose of the system is no longer to provide only personalized recommendations, resulting in a distortion of the recommendation process. â€¢On the other hand, we have a model-agnostic [22] approach, also known as post-hoc [23], which does not require to intervene on the internal mechanisms of the recommendation model and therefore does not aî€˜ect its performance in terms of accuracy. Most recommendation algorithms take an MF-approach, and thus the entire recommendation process is based on the interaction of latent factors that bring out the level of liking for an item with respect to a user. Many post-hoc explanation methods have been proposed for precisely these types of recommendation models. It seems evident that the most diî€cult challenge for this type of approach lies in making these latent factors explicit and understandable for the user [9]. Peake and Wang[23] generate an explanation by exploiting the association rules between features; Tao et al.[24]in their work, î€›nd beneî€›t from regression trees to drive learning, and then explain the latent space; instead, Gao et al.[25]try a deep model based on attention mechanisms to make relevant features emerge. Along the same lines are Pan et al.[11], who present a feature mapping approach that maps the uninterpretable general features onto the interpretable aspect features. Among other approaches to consider, [12] proposes an explicit factor model that builds a mapping between the interpretable features and the latent space. On the same line we also î€›nd the work by Fusco et al. [26]. In their work, they provide an approach to identify, in a neural model, which features contribute most to the recommendation. However, these post-hoc explanation approaches turn out to be built for very speciî€›c models. Purely model-agnostic approaches include the recent work of Tsang et al.[27], who present GLIDER, an approach to estimate interactions between features rather than on the signiî€›cance of features as in the original LIME [4] algorithm. This type of solution is constructed regardless of the recommendation model. Our paper focuses on the operation of LIME, a modelagnostic method for a surrogate-based local explanation. When a user-item pair is provided, this model returns as an outcome of the explanation a set of feature weights, for any recommender system. However, the recommendation task is very speciî€›c, so there is a version called LIME-RS [5] that applies the explanation model technique to the recommendation domain. In this way, any recommender is seen as a black box, so LIME-RS plays the role of a model-agnostic explainer whose result is a set of interpretable features and their relative importance. The goal of LIME-RS is to exploit the predictive power of the recommendation (black box) model to generate an explanation about the suggestion of a particular item for a user. In this respect, it exploits a neighborhood drawn according to a generic distribution compared to the candidate item for the explanation. It seems obvious that the choice of the neighborhood plays a crucial role within the process of explanation generation by LIME-RS. We can compare this sample extraction action to a perturbation of the user-item pair we are using to generate the explanation. In the case of LIME-RS this perturbation must generate consistent samples with respect to the source dataset. We see that this choice represents a critical issue for all the post-hoc models which base their expressiveness on the locality of the instance to explain. This trend is conî€›rmed in several papers addressing this issue of surrogate-based explanation systems such as LIME and SHAP [28]. In two recent papers, Alvarez-Melis and Jaakkola[29]have shown how the explanations generated with LIME are not very robust: their contribution aims to bring out how small variations or perturbations in the input data cause signiî€›cant variations in the explanation of that speciî€›c input [30]. In their paper, a new strategy is introduced to strengthen these methods by exploiting local Lipschitz continuity. By deeply investigating this drawback, they introduced self-explaining models in stages, progressively generalizing linear classiî€›ers to complex yet architecturally explicit models. Saito et al.[31]also explored this issue by turning their gaze to diî€˜erent types of sampling to make the result of an explanation generated through LIME more robust. In particular, in their work, they introduce the possibility of generating realistic samples produced with a Generative Adversarial Network. Finally, Slack et al.[32]adopt a similar solution in order to control the perturbation generating neighborhood data points by attempting to mitigate the generation of unreliable explanations while maintaining a stable black-box model of prediction. From a formal point of view, we can deî€›ne a LIMEgenerated explanation for a generic instanceğ‘¥ âˆˆ ğ’³ produced by a model ğ‘“ as: whereâ„’represents the î€›delity of the surrogate model to the originalğ‘“, andğ‘’represents a particular instance of the classğ¸of all possible explainable models. Among all the possible models, the one most frequently chosen is based on a linear prediction. In this case, an explanation refers to the weights of the most important interpretable features, which, when combined, minimize the divergence from the black-box model. The function ğœ‹measures the distance between the instance to be explainedğ‘¥ âˆˆ ğ’³, and the samplesğ‘¥âˆˆ ğ’³extracted from the training set to train the modelğ‘’. Finally,â„¦(ğ‘’) represents the complexity of the explanation model. Two pieces of evidence make the application of LIME possible: (i) the existence of a feature spaceğ’µon which to train the surrogate model ofğ‘“, (ii) and the presence of a surjective function that maps the space mentioned above (ğ’µ) to the original space of instances (ğ’³). Going into more detail, we consider the î€›delity functionâ„’as the mean square deviation between the prediction for a generic instanceğ‘¥âˆˆ ğ’³of the black-box model and that generated for the counterpartğ‘§âˆˆ ğ’µby the surrogate model. Starting from these considerations we can express â„’ with the following formula: In the formula aboveğœ‹plays a fundamental role as it expresses the distance between the instance to be explained and the sampled instance used to build the surrogate model. From a generic perspective, we can express this function as a kernel function likeğœ‹= ğ‘’ğ‘¥ğ‘(âˆ’ğ·(ğ‘¥, ğ‘¥)/ğœ), whereğ·is any measure of distance. The full impact of this distance is captured when the î€›delity function also considers the transformation of the surrogate sample in the original space. As mentioned earlier, we consider a surjective functionğ‘that maps the original space into the feature spaceğ‘ : ğ’³ â†’ ğ’µ. We can also consider the function that allows us to move in the opposite directionğ‘: ğ’³ â†’ ğ’µ. At this point, Equation (2) becomes: From this last equation, we can grasp the criticality of the surjective mapping function. Indeed, the neighborhood inğ’µ-space cannot be guaranteed with the transformation inğ’³-space. Thus, some samples selected to train the surrogate model could not satisfy the neighborhood criterion for which they were chosen. We must therefore stress on the centrality of the sampling function: how do we extract the neighborhood of our instance to be explained? If we look at the application of LIME to the recommendation domain, we can compare this sampling action to a local perturbation around our instanceğ‘¥; however, this perturbation aims to generateğ‘›samplesğ‘¥, which might contain inconsistencies: as an example, suppose we want to explain Jamesâ€™s feeling about the movie The Matrix. The original triple of the instance to be explained associates to the user-item pair the genre of the movie (representing the explainable space) and in this case it is of the type âŸ¨ğ½ğ‘ğ‘šğ‘’ğ‘ , ğ‘‡ â„ğ‘’ğ‘€ğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥, ğ‘†ğ‘ğ‘–-ğ¹ ğ‘–âŸ©. A perturbation around this instance could generate inconsistencies of the type âŸ¨ğ½ğ‘ğ‘šğ‘’ğ‘ , ğ‘‡ â„ğ‘’ğ‘€ğ‘ğ‘¡ğ‘Ÿğ‘–ğ‘¥, ğ‘Š ğ‘’ğ‘ ğ‘¡ğ‘’ğ‘Ÿğ‘›âŸ©. For this reason, in LIME-RS the perturbation considers only real and not synthetic data. This choice is dictated by the avoidance of the out-of-sample (OOS) process phenomenon. Closely related to this problem predicted by OOS is that the interpretation examples selected in LIME-RS represent the ability to capture the locality through disturbance mechanisms eî€˜ectively. One of the disadvantages of LIME-like methods is that they sometimes fail to estimate an appropriate local replacement model but instead generate a model that focuses on explaining the examples and is also aî€˜ected by more general trends in the data. This issue is central to our work, and it involves two aspects: (i) the î€›rst one concerns the sampling function of the samples precisely. In the LIME-RS implementation, this function is driven by the popularity distribution of the items within the dataset. (ii) The second critical issue concerns the modelâ€™s ability to wittily discriminate the userâ€™s taste from the neighborhood extracted to build the surrogate model. A model that squashes too much on bias or is inaccurate cannot bring out the peculiarities of user taste that are critical in building the explainable model which are, in turn, useful in generating the explanation for the instance of interest. These observations dictate the two research questions that motivated our work: RQ1Can we consider the surrogate-based model on which LIME-RS is built to generate always the same explanations, or does the extraction of a different neighborhood severely impact the systemâ€™s constancy? RQ2Are LIME-RS explanations adherent to item content, despite the fact that the sampling function is uncritical and based only on popularity? This section is devoted to illustrating how the experimental campaign was conducted. The datasets used for this phase of experimentation are Movielens 1M [33], Movielens Small [33], and Yahoo! Movies. Their characteristics are shown in Table 1. Table 1 Characteristics of the datasets involved in the experiments. As for the choice of the models to be used in this work is concerned, we selected two well-known recommendation models that are able to exploit the information content of the items to produce a recommendation: Attribute Item kNN (Att-Item-kNN) and Vector Space Model (VSM). The two chosen models represent the simplest solution to address the recommendation problem by exploiting the content associated with the items in the catalog. Att-Item-kNN exploits the characteristics of neighborhood-based models but expresses the representation of the items in terms of their content and, based on this representation, it computes a similarity between users. Starting from this similarity and exploiting the collaborative contribution in terms of interactions between users and items, Att-Item-kNN tries to estimate the level of liking of the items in the catalog. VSM represents both users and items in a new space to link users and items to the considered information content. Once obtained this new representation, with an appropriate function of similarity, VSM estimates which are the most appealing items for a speciî€›c user. The implementation of both models are available in the ELLIOT [34] evaluation framework. This benchmarking framework was used to select the best conî€›guration for the two recommendation models by exploiting the corresponding conî€›guration î€›le. Our experiments start by selecting the best conî€›gurations based on nDCG [35,36] for the two models on the considered datasets. Then, we generate the top-10 list of recommendations for each user, and we take into account the î€›rst itemğ‘–on these lists for each userğ‘¢. Finally, each recommendation pair(ğ‘¢, ğ‘–)is explained with LIME-RS. The explanation consists of a weighted vector(ğ‘”, ğ‘¤)whereğ‘”is the genre of the movies in the dataset â€“ i.e., the features â€“ andğ‘¤is the weight associated toğ‘”by LIME-RS within the explanation. Then, this vector is sorted by descending weights. In this way, the genres of the movies which play a key role within the recommendation, as explained by LIME-RS, are highlighted at the î€›rst positions of the vector. These operations are then repeatedğ‘› = 10times and changing the seed each time, as10is likely to be a good choice to detect a general pattern in the behavior of LIME-RS. At this point, for each pair(ğ‘¢, ğ‘–), we have a group of10explanations ordered by descending values ofğ‘¤, which we exploit to answer our two research questions. RQ1.Empirically, since in a real scenario of recommendation a too verbose explanation is not useful, we consider only the î€›rst î€›ve features in the sorted vector representing the explanation of each recommendation. In order to verify the constancy of the behavior of LIME-RS, given a (ğ‘¢, ğ‘–)pair, we exploit theğ‘›previously generated explanations for this pair. Then forğ‘˜ = 1, 2, . . . , 5, we deî€›ne ğºas the multiset of genres that appear inğ‘˜-th position â€“ for instance, if â€œSci-Fiâ€ occurs in the î€›rst position of 7 explanations, then â€œSci-Fiâ€ occurs 7 times in the multiset ğº, and similarly for other genres and multisets. Then, we compute the frequency of genres in each position as follows: given a positionğ‘˜, a genreğ‘”, and the number ğ‘›of generated explanations for a given pair(ğ‘¢, ğ‘–), the frequency ğ‘“of ğ‘” in ğ‘˜-th position is computed as: where||Â·||denotes the cardinality of a multiset. Then, all this information is collected for each user in î€›ve lists â€” one for each of theğ‘˜positions â€” of pairsâŸ¨ğ‘”, ğ‘“âŸ©sorted by frequency. One can observe that the computed frequency is an estimation of the probability that a given genre is put in that position within the explanation generated by LIME-RS sorted by values. Hence, the pair âŸ¨ğ‘”, max (ğ‘“)âŸ© describes the genre with the highest frequency in theğ‘˜-th position of the explanation for a pair (ğ‘¢, ğ‘–). Finally, it makes sense to compute the meanğœ‡ of the highest probability values in each positionğ‘˜of the explanations for each pair(ğ‘¢, ğ‘–). Formally, by setting a position ğ‘˜, the mean ğœ‡is computed as: whereğ‘ˆis the set of users for whom it was possible to generate a recommendation for. Observing the value of ğœ‡, we can state to what extent LIME-RS is constant in providing the explanations until theğ‘˜-th feature: the higher the value ofğœ‡, the higher the constancy of LIMERS concerning the ğ‘˜-th feature. By looking at Table 2, we can see that for Att-ItemkNN the LIME-RS explanation model is reliable as long as it considers at most three features in the weighted vector presented as an explanation of the recommendation. Extending the explanation to four features, we have a constancy that falls below 65%, while arriving at an explanation with î€›ve features is more likely to run into explanations that exhibit an unacceptably random Table 2 Constancy of LIME-RS. A value equal to 0 means that the genre(s) provided by LIME-RS in the firstğ‘˜position(s) is always diî€˜erent (worst case: completely inconstant behavior); A value equal to 1 means that the genre(s) provided by LIME-RS in the firstğ‘˜position(s) is always the same (total constancy). behavior. On the other hand, we can see that for VSM the values are much more stable. In this case, we have a constancy that, regardless of the length of the weighted vector of the explanation, stabilizes on average around 80%. An aspect emerges that will be discussed in detail later: LIME-RS is conditioned by the ability of the black-box model to discriminate the userâ€™s tastes locally. RQ2.With the aim of providing an answer about the adherence to reality of LIME-RS, we make a comparison between the genres claimed to explain a recommended item and its actual genres. Indeed, the explanations about an item should î€›t the list of genres the item is characterized by. This means that, in an ideal case, all highly weighted features within the explanation should match the genres of the item. From the results in Table 2, we notice that using Att-Item-kNN the constancy of LIMERS reaches a low value after the third feature. Hence, it is a futile eî€˜ort to go deeper in the study of the explanation. To this aim, we intersected each explanation limited to the setğ¸of its î€›rstğ‘˜genres with the set of genresğ¹characterizing the î€›rst recommended item, forğ‘˜ = 1, 2, 3. Upon completion of this operation for all theğ‘›explanations generated for each(ğ‘¢, ğ‘–)pair, we computed the number of times we obtained an empty intersection of these sets, normalized by the total number of explanationsğ‘› Ã— |ğ‘ˆ |, in order to understand to what extent an explanation is (not) adherent to the item. Formally, for a given value ofğ‘˜, the valueğ‘ğ‘‘â„ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’ is computed as: ğ‘ğ‘‘â„ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’=ğ‘› Ã— |ğ‘ˆ|(6) whereğ‘ˆis the set of users of the dataset for whom it was possible to generate a recommendation,ğ‘›is the number of generated explanations for each pair(ğ‘¢, ğ‘–), and byÎ£[Â· Â· Â· ]we mean that we sum 1 if the condition inside[Â· Â· Â· ]is true, and 0 otherwise. One can note that ğ‘ğ‘‘â„ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’âˆˆ [0, 1], where a value of 1 indicates the worst case in which for none of theğ‘›explanations under consideration at least one genre of the item is in the î€›rstğ‘˜features of the explanation. In contrast, the lower the value ofğ‘ğ‘‘â„ğ‘’ğ‘Ÿğ‘’ğ‘›ğ‘ğ‘’, the higher the adherence of LIME-RS. Table 3 Adherence of LIME-RS. For value equals to 1 no genre provided by LIME-RS in the firstğ‘˜real genres of the movie (worst case); For value equals to 0 at least one genre provided by LIME-RS in the firstğ‘˜genres is always among the real genres of the movie. Observing the results from Table 3, Att-Item-KNN performs well in terms of adherence since, in approximately 75% of cases, even considering only the main feature of the explanation, it falls into the set of the item genres, as for Movielens dataset family. This performance is a 10% lower for Yahoo! Movies. In contrast with this result, VSM shows poor performances on both dataset of the Movielens family, by failing half the time about Movielens 1M as regards adherence. A surprising result is achieved for Yahoo! Movies dataset because, enlarging the study to the î€›rst three features among the explanation, the error is almost completely absent. The reasons we found to explain this diî€˜erence in the performances concern the characteristics and the quality of the dataset, as we highlight later on. This work investigates how well a post-hoc approach based on local surrogates â€“ such as the LIME-RS algorithm â€“ explains a recommendation. Instead of studying the impact of explanations on users (that is a well-studied topic in the literature and is beyond our scope), we focus on objective evidences that could emerge. In this respect, we have designed speciî€›c experiments, which introduced two diî€˜erent metrics, to evaluate adherence and constancy for this kind of algorithms. For instance, Table 2 shows a diî€˜erent behavior for Att-Item-kNN and VSM. On the one hand, Att-Item-kNN seems to guarantee a good constancy in explanations up to the third feature. This suggests that an explanation that exploits the î€›rst three features of the list produced by LIME-RS could be barely considered as reliable (i.e., reaching a constancy of 0.69on Movielens 1M). On the other hand, VSM exhibits a much more "stable" behavior, demonstrating in all cases (except for the î€›rst feature with Movielens 1M) better performance than Att-Item-kNN in terms of constancy, with peaks up to 97%. A straightforward consequence of these observations could be analyzed in terms of conî€›dence or probability. If the constancy steadily decreases, it means that the probability that LIME-RS suggests the same explanatory feature decreases. In practical terms, we could say that LIME-RS is less conî€›dent about its explanation. In fact, this is the behavior of Att-Item-kNN. Conversely, VSM shows high values of constancy, resulting in a more "deterministic" behavior. With VSM, LIME-RS is more conî€›dent of its explanations. This could increase userâ€™s trustworthiness, since LIME-RS behavior is more reliable. However, these results could also be interpreted together with the ones from Table 3. They show how often at least one feature â€“ out ofğ‘˜features provided by LIME-RSâ€“ adheres to the features that describe the item being explained. In other words, they measure the probability that LIME-RS succeeds in reconstructing at least one feature of a speciî€›c item. Combining the results of Table 2 and those of Table 3, Att-Item-kNN, as already mentioned, shows good performance regarding adherence and identiî€›es 3 times out of 4 the î€›rst fundamental feature of the explanation among those present in the set of features originally associated with the item. As expected, if the numberğ‘˜of LIME-RS-reconstructed features increases, the number of times such a set has a nonempty intersection (with the features belonging to the item) â€“ i.e., adherence â€“ increases. It could be noted that Att-Item-kNN on Yahoo! Movies shows the worst behavior in terms of adherence. VSM shows a diî€˜erent behavior. Despite the excellent performance regarding constancy, it could be observed that on both Movielens datasets, the performance in terms of adherence is poor, and worse for Movielens 1M than for Movielens Small. Surprisingly, on Yahoo! Movies, VSM performs much better, and the errors are almost negligible. The diî€˜erence between the two modelscould be due to many reasons. In the following we analyze possible relations between such behaviors and two of them: popularity bias in the dataset and characteristics of side information. On the one hand, if the dataset is aî€˜ected by popularity bias, it would be a well-studied cause of confusion for LIME-RS. On the other hand, the characteristics of the side information associated with the datasets could dramatically inî€œuence the performance of the two recommendation models. To assess these hypotheses, we have evaluated (see Table 4) the recommendation lists produced by Att-Item-kNN and VSM considering nDCG, Hit Rate (HR), Mean Average Precision (MAP), and Mean Reciprocal Rank (MRR). Table 4 shows that the chosen datasets are strongly aî€˜ected by popularity bias. Indeed, MostPop is the best performing approach, and the two Table 4 Results of the experiments on the models involved in the experiments. Models are optimized according to the value of nDCG. "personalized" models fail to produce accurate results. This triggers the second aspect that concerns the quality of the content. The results suggest that the side information is not good enough to boost the recommendation systems in producing meaningful recommendations. In fact, the three datasets seem to have an informative content that is not adequate to generate appealing recommendations. We observe that, from an informative point of view, the Yahoo! Movies dataset is slightly more complete: 22 genres against the 18 genres available on Movielens. Although the VSM model does not show excellent performance, in combination with LIME-RS, it provides explanations that are very reliable in terms of constancy (see Table 2) and adherence (see Table 3) to the actual content of the items being explained. From the designer perspective,there is also a pragmatic way to look at the experimental results. Suppose a developer needs an oî€˜-the-shelf way of generating explanations for recommendations, and chooses LIME-RS to do that. Our results suggest that if the explainer employs a Movielens dataset with Att-Item-kNN model, then it is better to run the explainer several times. Indeed, the î€›rst feature obtained for the explanation could change around 1 time every 5 trials (î€›rst column of Table 2), and once such a feature is obtained, it is better to check whether this feature is really among the ones describing the item, since 1 time out of 4 the feature can be wrong (î€›rst column of Table 3). Moreover, if the explainer employs the Yahoo! Movies dataset with VSM model, then probably there is no need to run the explainer twice, since its behavior is constant 97% of the times, while the feature is wrong only 10% of the times. However, the low performance of such a model is to be taken into account. In this paper we shed a î€›rst light on the eî€˜ectiveness of LIME-RS as a black-box explanation model in a recommendation scenario. We propose two diî€˜erent measures to understand how reliable an explanation based on LIME-RS is: (i) constancy was used to assess the impact of the random sampling phase of LIME-RS on the provided explanation â€“ ideally the explanation should remain constant in spite of the sample used to obtain it; (ii) adherence was proposed to understand the reconstructive power of LIME-RS with respect to the features that belong to the item involved in the explanation â€“ ideally, LIME-RS should provide an explanation that always adheres to the actual features of the recommended item. To test both constancy and adherence, we trained and optimized two content-based recommendation models: Attribute Item-kNN (Att-Item-kNN), and a classical Vector Space Model. For each model, and for all datasets exploited in the study, we generated recommendation lists for all users. We exploited the î€›rst item of these top-10 lists to produce the explanations that were then the subject of our investigation. It turned out that for models built with a large collaborative input such as Att-Item-kNN, LIME-RS produces fairly constant explanations up to a length of three features. Moreover, these explanations turn out to be adherent with respect to the item between 65% and 75% of the cases in which only the î€›rst feature of the weighted vector of explanations is considered. VSM shows a diî€˜erent behavior where explanations are much more constant, but suî€˜er a lot in terms of adherence, except for the Yahoo! Movies dataset for which the explanation model showed outstanding performance despite the poor ability of VSM to provide sound recommendations to users. In our experiments, some evidence started to emerge highlighting that the adopted explanation model is conditioned not only by the accuracy of the black-box model it tries to explain but also by the quality of the side information used to train the model. The latter result deserves to be adequately investigated to search for a link at a higher level of detail. We plan to apply our experiments also to other recommendation models, to see whether the problems with adherence and constancy that we found for the two tested models show up also in other situations. We will also investigate what impact structured knowledge has on this performance by exploiting models capable of leveraging this type of content. In addition, it would also be the case to try diî€˜erent reference domains with richer datasets of side information to understand what impact content quality has on this type of explainer. The authors acknowledge partial support of PID2019108965GB-I00, PON ARS01_00876 BIO-D, Casa delle Tecnologie Emergenti della CittÃ  di Matera, PON ARS01_00821 FLET4.0, PIA Servizi Locali 2.0, H2020 Passapartout - Grant n. 101016956, PIA ERP4.0, and IPZS-PRJ4_IA_NORMATIVO.