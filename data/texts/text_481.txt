Selecting the optimal recommender via online exploration-exploitation is catching increasing attention where the traditional A/B testing can be slow and costly, and oî€Ÿine evaluations are prone to the bias of history data. Finding the optimal online experiment is nontrivial since both the users and displayed recommendations carry contextual features that are informative to the reward. While the problem can be formalized via the lens of multi-armed bandits, the existing solutions are found less satisfactorily because the general methodologies do not account for the case-speciî€›c structures, particularly for the e-commerce recommendation we study. To î€›ll in the gap, we leverage the D-optimal design from the classical statistics literature to achieve the maximum information gain during exploration, and reveal how it î€›ts seamlessly with the modern infrastructure of online inference. To demonstrate the eî€ectiveness of the optimal designs, we provide semi-synthetic simulation studies with published code and data for reproducibility purposes. We then use our deployment example on Walmart.com to fully illustrate the practical insights and eî€ectiveness of the proposed methods. â€¢ Information systems â†’ Retrieval models and ranking;â€¢ Computer systems organization â†’Real-time systems;â€¢ Mathematics of computing â†’ Statistical paradigms. Recommender system; Multi-armed bandit; Exploration-exploitation; Optimal design; Deployment infrastructure ACM Reference Format: Da Xu, Chuanwei Ruan, Evren Korpeoglu, Sushant Kumar, and Kannan Achan. 2021. Towards the D-Optimal Online Experiment Design for Recommender Selection. In Procee dings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™21), August 14â€“18, 2021, Virtual Event, Singapore. ACM, New York, NY, USA, 10 pages. https://doi. org/10.1145/3447548.3467192 Developing and testing recommenders to optimize business goals are among the primary focuses of e-commerce machine learning. A crucial discrepancy between the business and machine learning world is that the target metrics, such as gross merchandise value (GMV), are diî€œcult to interpret as tangible learning objectives. While a handful of surrogate losses and evaluation metrics have been found with particular empirical success [20, 44], online experimentation is perhaps the only rule-of-thumb for testing a candidate recommenderâ€™s real-world performance. In particular, there is a broad consensus on the various types of bias in the collected history data [11], which can cause the "feedback-loop eî€ect" if the empirical metrics are used without correction [18]. Recently, there has been a surge of innovations in reî€›ning online A/B testings and correcting oî€Ÿine evaluation methods [18,29,47,52]. However, they still fall short in speciî€›c applications where either the complexity of the problem outweighs their potential beneî€›ts, or their assumptions are not satisî€›ed. Since our work discusses the e-commerce recommendations primarily, we assume the online shopping setting throughout the paper. On the A/B testing side, there is an increasing demand for interleaving the tested recommenders and targeted customers due to the growing interests in personalization [43]. The process of collecting enough observations and drawing inference with decent power is often slow and costly (in addition to the complication of deî€›ning the buckets in advance), since the number of combinations can grow exponentially. As for the recent advancements for oî€Ÿine A/B testing [18], even though certain unbiasedness and optimality results have been shown in theory, the real-world performance still depends on the fundamental causal assumptions [22], e.g. unconfounding, overlapping and identiî€›ability, which are rarely fulî€›lled in practice [49]. We point out that the role of both online and oî€Ÿine testings are irreplaceable regardless of their drawbacks; however, the current issues motivate us to discover more eî€œcient solutions which can better leverage the randomized design of traî€œc. The production scenario that motivates our work is to choose from several candidate recommenders who have shown comparable performances in oî€Ÿine evaluation. By the segment analysis, we î€›nd each recommender more favorable to speciî€›c customer groups, but again the conclusion cannot be drawn entirely due to the exposure and selection bias in the history data. In other words, while it is safe to launch each candidate online, we still need randomized experiments to explore each candidatesâ€™ real-world performance for diî€erent customer groups. We want to design the experiment by accounting for the customer features (e.g. their segmentation information) to minimize the cost of trying suboptimal recommenders on a customer group. Notice that our goal deviates from the traditional controlled experiments because we care more about minimizing the cost than drawing rigorous inference. In the sequel, we characterize our mission as a recommender-wise exploration-exploitation problem, a novel application to the best of our knowledge. Before we proceed, we illustrate the fundamental diî€erences between our problem and learning the ensembles of recommenders [24]. The business metrics, such as GMV, are random quantities that depend on the recommended contents as well as the distributions that govern customersâ€™ decision-making. Even if we have access to those distributions, we never know in advance the conditional distribution given the recommended contents. Therefore, the problem can not be appropriately described by any î€›xed objective for learning the recommender ensembles. In our case, the exploration-exploitation strategy can be viewed as a sequential game between the developer and the customers. In each roundğ‘¡ =1, . . . , ğ‘›, where the role ofğ‘›will be made clear later, the developer chooses a recommenderğ‘âˆˆ {1, . . . , ğ‘˜}that produces the contentğ‘, e.g. top-k recommendations, according to the frontend requestğ‘Ÿ, e.g. customer id, user features, page type, etc. Then the customer reveal the rewardğ‘¦such as click or not-click. The problem setting resembles that of the multi-armed bandits (MAB) by viewing each recommender as the action (arm). The front-end requestğ‘Ÿ, together with the recommended contentğ‘= ğ‘(ğ‘¥), can be think of as the context. Obviously, the context is informative of the reward because the clicking will depend on how well the content matches the request. On the other hand, an (randomized) experiment design can be characterized by a distributionğœ‹overÃ the candidate recommenders, i.e. 0< ğœ‹(ğ‘) <1,ğœ‹(ğ‘–) =1 forğ‘– =1, . . . , ğ‘›. We point out that a formal diî€erence between our setting and classical contextual bandit is that the context here depends on the candidate actions. Nevertheless, its impact becomes negligible if choosing the best set of contents is still equivalent to choosing the optimal action. Consequently, the goal of î€›nding the optimal experimentation can be readily converted to optimizing ğœ‹, which is aligned with the bandit problems. The intuition is that by optimizingğœ‹, we reî€›ne the estimation of the structures between context and reward, e.g. via supervised learning, at a low exploration cost. The critical concern of doing exploration in e-commerce, perhaps more worrying than the other domains, is that irrelevant recommendations can severely harm user experience and stickiness, which directly relates to GMV. Therefore, it is essential to leverage the problem-speciî€›c information, both the contextual structures and prior knowledge, to further design the randomized strategy for higher eî€œciency. We use the following toy example to illustrate our argument. Example 1. Suppose that there are six items in total, and the frontend request consists of a uni-variate user featureğ‘Ÿâˆˆ R. The reward mechanism is given by the linear model: whereğ¼is the indicator variable on whether itemğ‘—is recommended. Consider the top-3 recommendation from four candidate recommenders as follow (in the format of one-hot encoding): If each recommender is explored with the probability, the role ofğ‘is underrated since it is the only recommender that provides information aboutğœƒandğœƒ. Also,ğ‘andğ‘give the same outputs, so their exploration probability should be discounted by half. Similarly, the information provided byğ‘†can be recovered byğ‘†andğ‘†(orğ‘†) combined, so there is a linear dependency structure we may leverage. The example is representative of the real-world scenario, where the one-hot encodings and user features may simply be replaced by the pre-trained embeddings. By far, we provide an intuitive understanding of the beneî€›ts from good online experiment designs. In Section 2, we introduce the notations and the formal background of bandit problems. We then summarize the relevant literature in Section 3. In Section 4, we present our optimal design methods and describe the corresponding online infrastructure. Both the simulation studies and real-world deployment analysis are provided in Section 5. We summarize the major contributions as follow. â€¢We provide a novel setting for online recommender selection via the lens of exploration-exploitation. â€¢We present an optimal experiment approach and describe the infrastructure and implementation. â€¢We provide both open-source simulation studies and realworld deployment results to illustrate the eî€œciency of the approaches studied. We start by concluding our notations in Table 1. By convention, we use lower and upper-case letters to denote scalars and random variables, and bold-font lower and upper-case letters to denote vectors and matrices. We use[ğ‘˜]as a shorthand for the set of:{1,2, . . . , ğ‘˜}. The randomized experiment strategy (policy) is a mapping from the collected data to the recommenders, and it should maximizeÃ the overall rewardğ‘Œ. The interactive process of the online recommender selection can be described as follow. 1. The developer receives a front-end request ğ‘Ÿâˆ¼ ğ‘ƒ. 2. The developer computes the feature representations that combines the request and outputs from all candidate recommender:î€ˆî€€î€î€‰ 3. The developer chooses a recommenderğ‘according to the randomized experiment strategy ğœ‹ (ğ‘|X,Â®â„). 4. The customer reveals the reward ğ‘¦. In particular, the selected recommenderğ‘depends on the request, candidate outputs, as well as the history data:î€Œ and the observation we collect at each round is given by: We point out that compared with other bandit applications, the restriction on computation complexity per round is critical for realworld production. This is because the online selection experiment is essentially an additional layer on top of the candidate recommender systems, so the service will be called by tens of thousands of front-end requests per second. Consequently, the context-free exploration-exploitation methods, whose strategies focus on the cu-Ã mulative rewards:ğ‘„ (ğ‘) =ğ‘¦1[ğ‘ = ğ‘—]and number of appear-Ã ances:ğ‘ (ğ‘) =1[ğ‘ = ğ‘—](assume up to roundğ‘¡) forğ‘ =1, . . . , ğ‘˜, are quite computationally feasible, e.g. â€¢ ğœ–-greedy: explores with probabilityğœ–under the uniform exploration policyğœ‹ (ğ‘) =1/ğ‘˜, and selectsarg maxğ‘„ (ğ‘)otherwise (for exploitation); â€¢ UCB: selectsarg maxğ‘„ (ğ‘) +ğ¶ğ¼ (ğ‘), whereğ¶ğ¼ (ğ‘)characterizes the conî€›dence interval of the action-speciî€›c rewardğ‘„ (ğ‘),î± and is given by:î€€log 1/ğ›¿î€î€ğ‘ (ğ‘) for some pre-determined ğ›¿. The more sophisticatedThompson samplingequips the sequential game with a Bayesian environment such that the developer: â€¢selectsarg maxËœğ‘„ (ğ‘), whereËœğ‘„ (ğ‘)is sampled from the posterior distributionBeta(ğ›¼, ğ›½), andğ›¼andğ›½combines the prior knowledge of average reward and the actual observed rewards. For Thompson sampling, it is clear that the front-end computations can be simpliî€›ed to calculating the uni-variate indices (ğ‘„,ğ‘, ğ›¼,ğ›½). For MAB, taking account of the context often requires em-î€€î€ ploying a parametric reward model:ğ‘¦= ğ‘“ğ“(ğ‘Ÿ, ğ‘(ğ‘Ÿ )), so during exploration, we may also update the model parametersğœ½using ğ‘˜, ğ‘š, ğ‘›menders); the number of top recommendations;the number of exploration-exploitation rounds. ğ‘…, ğ´, ğ‘ŒThe frond-end request, action selected by thedeveloper, and the reward at round ğ‘¡. Â®â„, ğœ‹ (Â· | Â·)randomized strategy (policy) that maps the con-texts and history data to a probability measure I, ğ‘(Â·)The whole set of items and theğ‘–candidaterecommender, with ğ‘(Â·) âˆˆ I. ğ“î€€ğ‘Ÿ, ğ‘(ğ‘Ÿ)î€sentation inR, speciî€›cally for theğ‘¡-roundfront-end request and the output contents for Table 1: A summary of the notations. By tradition, we use uppercase letters to denote random variables, and the corresponding lowercase letters as obser vations. the collected data. Suppose we have an optimization oracle that returnsË†ğœ½by î€›tting the empirical observations, then all the above algorithms can be converted to the context-aware setting, e.g. â€¢ epoch-greedy: explores underğœ‹ (ğ‘) =1/ğ‘˜for a epoch, andî€€î€ selects arg maxË†ğ‘¦:= ğ‘“ğ“(ğ‘Ÿ, ğ‘(ğ‘Ÿ ))otherwise; â€¢ LinUCB: by assuming the reward model is linear, it selects arg maxË†ğ‘¦+ ğ¶ğ¼ (ğ‘)whereğ¶ğ¼ (ğ‘)characterizes the conî€›dence of the linear modelâ€™s estimation; â€¢ Thompson sampling: samplesË†ğœ½from the reward-model-speciî€›c posterior distribution of ğœ½, and selects arg maxË†ğ‘¦. We point out that the per-round model parameter update via the optimization oracle, which often involves expensive real-time computations, is impractical for most online services. Therefore, we adopt the stage-wise setting that divides exploration and exploitation (similar to epoch-greedy). The design ofğœ‹thus becomes very challenging since we may not have access to the most updated Ë†ğœ½. Therefore, it is important to take advantage of the structure of ğ‘“(Â·), which motivates us to connect our problem with the optimal design methods in the classical statistics literature. We brieî€y discuss the existing bandit algorithms and explain their implications to our problem. Depending on how we perceive the environment, the solutions can be categorized into the frequentist and Bayesian setting. On the frequentist side, the reward model plays an important part in designing algorithms that connect to the more general expert advice framework [12]. The EXP4 and its variants are known as the theoretically optimal algorithms for the expert advice framework if the environment is adversarial [4,6,35]. However, customers often have a neutral attitude for recommendations, so it is unnecessary to assume adversarialness. In a neutral environment, the LinUCB algorithm and its variants have been shown highly eî€ective [4,12]. In particular, when the contexts are viewed as i.i.d samples, several regret-optimal variants of LinUCB have been proposed [2,15]. Nevertheless, those solutions all require real-time model updates (via the optimization oracle), and are thus impractical as we discussed earlier. On the other hand, several suboptimal algorithms that follow the explore-then-commit framework can be made computationally feasible for large-scale applications [38]. The key idea is to divide exploration and exploitation into diî€erent stages, like the epochgreedy and phased exploration algorithms [1,28,39]. The model training and parameter updates only consume the back-end resources dedicated for exploitation, and the majority of front-end resources still take care of the model inference and exploration. Therefore, the stage-wise approach appeals to our online recommender selection problem, and it resolves certain infrastructural considerations that we explain later in Section 4. On the Bayesian side, the most widely-acknowledge algorithms belong to the Thompson sampling, which has a long history and fruitful theoretical results [10,40,41,45]. When applied to contextual bandit problems, the original Thompson sampling also requires perround parameter update for the reward model [10]. Nevertheless, the î€exibility of the Bayesian setting allows converting Thompson sampling to the stage-wise setting as well. In terms of real-world applications, online advertisement and news recommendation [3,30,31] are perhaps the two major domains where contextual bandits are investigated. Bandits have also been applied to our related problems such as item-level recommendation [26,32,53] and recommender ensemble [9]. To the best of our knowledge, none of the previous work studies contextual bandit for the recommender selection. As we discussed in the literature review, the stage-wise (phased) exploration and exploitation appeals to our problem because of their computation advantage and deployment î€exibility. To apply the stage-wise exploration-exploitation to online recommender selection, we describe a general framework in Algorithm 1. Input: Reward model ğ‘“(Â·); the restart criteria; the ğœ‹ (ğ‘ | Â·) =, and collect observation toÂ®â„; minimization); The algorithm is deployment-friendly because Step 5 only involves front-end and cache operation, Step 6 is essentially a batchwise training on the back-end, and Step 7 applies directly to the standard front-end inference. Hence, the algorithm requires little modiî€›cation from the existing infrastructure that supports realtime mdoel inference. Several additional advantages of the stagewise algorithms include: â€¢the number of of exploration and exploitation rounds, which decides the proportion of traî€œc for each task, can be adaptively adjusted by the resource availability and response time service level agreements; â€¢the non-stationary environment, which are often detected via the hypothesis testing methods as described in [5,8,33], can be handled by setting the restart criteria accordingly. This section is dedicated to improving the eî€œciency of exploration in Step 5. Throughout this paper, we emphasize the importance of leveraging the case-speciî€›c structures to minimize the number of exploration steps it may take to collect equal information for estimatingğœ½. Recall from Example 1 that one particular structure is the relation among the recommended contents, whose role can be thought of as the design matrix in linear regression. Towards that end, our goal is aligned with the optimal design in the classical statistics literature [37], since both tasks aim at optimizing how the design matrix is constructed. Following the previous buildup, the reward model has one of the following forms: We start with the frequentist setting, i.e.ğœ½do not admit a prior distribution. In each roundğ‘¡, we try to î€›nd a optimal designğœ‹ (Â· | Â·)such that the action sampled fromğœ‹leads to a maximum information for estimatingğœ½. For statistical estimators, the Fisher information is a key quantity for evaluating the amount of information in the observations. For the generalğ‘“, the Fisher information under (2) is given by: whereğœ‹ (ğ‘)is a shorthand for the designed policy. For the linear reward model, the Fisher information is simpliî€›ed to: To understand the role Fisher information in evaluating the underlying uncertainty of a model, according to the textbook derivations for linear regression, we have: â€¢ var(Ë†ğœ½ ) âˆ ğ‘€ (ğœ‹ );î€€î€ â€¢the prediction variance forğ“:= ğ“ğ‘Ÿ, ğ‘(ğ‘Ÿ)is given by Therefore, the goal of optimal online experiment design can be explained as minimizing the uncertainty in the reward model, either for parameter estimation or prediction. In statistics, a D-optimal design minimizesdet |ğ‘€ (ğœ‹ )|from the perspective of estimation variance , and the G-optimal design minimizemaxğ“ğ‘€ (ğœ‹)ğ“ from the perspective of prediction variance. A celebrated result states the equivalence between D-optimal and G-optimal designs. Theorem 1 (Kiefer-Wolfowitz [27]). For a optimal designğœ‹, the following statements are equivalent:î€Œî€Œ â€¢ ğœ‹= maxlog detî€Œğ‘€ (ğœ‹)î€Œ; â€¢ ğœ‹is D-optimal; â€¢ ğœ‹is G-optimal. Theorem 1 suggests that we use convex optimization to î€›nd the optimal design for both parameter estimation and prediction: However, a drawback of the above formulation is that it does not involve the observations collected in the previous exploration rounds. Also, the optimization problem does not apply to the Bayesian setting if we wish to use Thompson sampling. Luckily, we î€›nd that optimal design for the Bayesian setting has a nice connection to the above problem, and it also leads to a straightforward solution that utilizes the history data as a prior for the optimal design. We still assume the linear reward setting, and the prior forğœ½ is given byğœ½ âˆ¼ ğ‘ (0, R)whereRis the covariance matrix. Unlike in the frequentist setting, the Bayesian design focus on the design optimality in terms of certain utility functionğ‘ˆ (ğœ‹). A common choice is the expected gain in Shannon information, or equivalently, the Kullback-Leibler divergence between the prior and posterior distribution ofğœ½. The intuition is that the larger the divergence, the more information there is in the observations. Letğ‘¦be theî€€î€î€€î€ hypothetical rewards forğ“ğ‘Ÿ, ğ‘(ğ‘Ÿ), . . . , ğ“ğ‘Ÿ, ğ‘(ğ‘Ÿ). Then the gain in Shannon information is given by: whereğ¶is a constant. Therefore, maximizingğ‘ˆ (ğœ‹)is equivalentî€Œî€Œ to maximizing log detî€Œğ‘€ (ğœ‹) + Rî€Œ. Compared with the objective for the frequentist setting, there is now an additiveRterm inside the determinant. Notice thatRis the convariance of the prior, so given the previous history data, we can simply plug in the empirical estimation ofR. In particular, letÂ®ğ“ be the collection of feature vectors from the previous explorationî€‚î€€î€€î€î€ƒ rounds:ğ“ğ‘¥, ğ‘(ğ‘¥), . . . , ğ“ğ‘¥, ğ‘(ğ‘¥). ThenRis simplyî€€î€ estimated byÂ®ğ“Â®ğ“. Therefore, the objective for Bayesian optimal design, after integrating the prior from the history data, is given by: maximizelog detî€Œî€Œğ‘€ (ğœ‹) + ğœ†ğ“ğ“î€Œî€Œ, s.t.ğœ‹ (ğ‘) = 1, (6) where we introduce the hyper parameter ğœ† to control inî€uence of the history data. We refer the interested readers to [13,16,25,34] for the historic development of this topic. Moving beyond the linear setting, the results stated in the KieferWolfowitz theorem also holds for nonlinear reward model [46]. Unfortunately, it is very challenging to î€›nd the exact optimal design when the reward model is nonlinear [51]. The diî€œculty liesî€€î€€î€î€ in the fact thatâˆ‡ğ‘“ğ“ğ‘Ÿ, ğ‘(ğ‘Ÿ)now depends onğœ½, so the Fisher information (4) is also a function of the unknownğœ½. One solution which we î€›nd computationally feasible is to consider a local linearization of ğ‘“(Â·) using the Taylor expansion: where ğœ½is some local approximation. In this way, we have: and the local Fisher information will be given byğ‘€ (ğœ‹;ğœ½)according to (4). The eî€ectiveness of local linearization completely depends on the choice ofğœ½. Using the data gathered from previous exploration rounds is a reasonable way to estimateğœ½. We plug inğœ½to obtain the optimal designs for the following exploration rounds: maximizelog detî€Œî€Œğ‘€ (ğœ‹; ğœ½)î€Œî€Œs.t.ğœ‹ (ğ‘ We do not study the Bayesian optimal design under the nonlinear setting because even the linearization trick will be complicated. Moreover, by the way we constructğœ½above, we already pass a certain amount of prior information to the design. Remark 1. Before we proceed, we brieî€y discuss what to expect from the optimal design in theory. Optimizing thelog det |Â·|objective is essentially î€›nding the minimum-volume ellipsoid, also known as the John ellipsoid [19]. According to the previous results from the geometric studies, using the proposed optimal design will do no worse than the uniform exploration if the reward model is misspeciî€›ed. Also,âˆš we can expect an averageğ‘‘improvement in the frequentistâ€™s linearâˆš reward setting [7], which means it only takesğ‘œ (1/ğ‘‘)of the previous exploration steps to estimate ğœ½ to the same precision. In this section, we introduce an eî€œcient algorithm to solve the optimal designs in (9) and (6). We then couple the optimal designs to the stage-wise exploration-exploitation algorithms. The infrastructure for our real-world production is also discussed. We have shown earlier that î€›nding the optimal design requires solving a convex optimization programming. Since the problem is often of moderate size as we do not expect the number of recommenders ğ‘˜to be large, we î€›nd the Frank-Wolfe algorithm highly eî€œcient [17,23]. We outline the solution for the most general non-linear reward case in Algorithm 2. The solutions for the other scenarios are included as special cases, e.g. by replacingğ‘€ (ğœ‹)withğ‘€ (ğœ‹) + R for the Bayesian setting. Referring to the standard analysis of Frank-Wolfe algorithm [23], we show that the it takes the solver at mostO(ğ‘‘ log log ğ‘˜ + ğ‘‘/ğœ–) updates to achieve a multiplicative(1+ğœ–)optimal solution. Each update has anO(ğ‘˜ğ‘‘)computation complexity, butğ‘‘is usually small in practice (e.g.ğ‘‘ =6 in Example 1), which we will illustrate with more detail in Section 5. By treating the optimal design solver as a subroutine, we now present the complete picture of the stage-wise exploration-exploitation with optimal design. To avoid unnecessary repetitions, we describe the algorithms for nonlinear reward model under frequentist setting (Algorithm 3), and for linear reward model under the Thompson sampling. They include the other scenarios as special cases. To adapt the optimal design to the Bayesian setting, we only need to make a few changes to the above algorithm: Algorithm 3:Stage-wise exploration-exploitation with optimal design. Input: Reward model ğ‘“(Â·); restart criteria; initialize optimal design solver under ğœ½=Ë†ğœ½ ; Play ğ‘›rounds of exploration under ğœ‹and collect observation toÂ®â„; â€¢ the optimal design solver is now speciî€›ed for solving (6); â€¢instead of optimizingË†ğœ½via the empirical-risk minimization, we update the posterior of ğœ½ using the history dataÂ®â„; â€¢in each exploitation round, we execute Algorithm 4 instead. Algorithm 4:Optimal design for Thompson sampling at exploitation rounds. prior and collected data ; For Thompson sampling, the computation complexity of exploration is the same as Algorithm 3. On the other hand, even with a conjugate prior distribution, the Bayesian linear regression has an unfriendly complexity for the posterior computations. Nevertheless, under our stage-wise setup, the heavy lifting can be done at the back-end in a batch-wise fashion, so the delay will not be signiî€›cant. In our simulation studies, we observe comparable performances from Algorithm 3 and 4. Nevertheless, each algorithm may experience speciî€›c tradeoî€ in the stage-wise setting, and we leave it to the future work to characterize their behaviors rigorously. In the ideal setting, the online recommender selection can be viewed as another service layer, which we refer to as the system bandit service, on top of the model service infrastructure. An overview of the concept is provided in Figure 1. The system bandit module takes the request (which contains the relevant context), and the wrapped recommendation models. When the service is triggered, depending on the instruction from request distributor (to explore or exploit), the module either queries the pre-computed reward, î€›nds the best model and outputs its content, or run the optimal design solver using the pre-computed quantities and choose a recommender to explore. The request distributor is essential for Figure 1: High-level overview of the system bandit service. keeping the resource availability and response time agreements, since the optimal-design computations can cause stress during the peak time. Also, we initiate the scoring (model inference) for the candidate recommenders in parallel to reduce the latency whenever there are spare resources. The pre-computations occur in the back-end training clusters, and their results (updated parameters, posterior of parameters, prior distributions) are stored in such as the mega cache for the front end. The logging system is another crucial component which maintains storage of the past reward signals, contexts, policy value, etc. The logging system works interactively with the training cluster to run the scheduled job for pre-computation. Another detail is that the rewards are often not immediately available, e.g. for the conversion rate, so we set up an event stream to collect the data. The system bandit service listens to the event streams and determines the rewards after each recommendation. For our deployment, we treat the system bandit serves as a middleware between the online and oî€Ÿine service. The details are presented in Figure 2, where we put together the relevant components from the above discussion. It is not unusual these days to leverage the "near-line computation", and our approach takes the full advantage of the current infrastructure to support the optimal online experiment design for recommender selection. Figure 2: The deployment details of the optimal online experiment design for recommender selection. We î€›rst provide simulation studies to examine the eî€ectiveness of the proposed optimal design approaches. We then discuss the relevant testing performance on Walmart.com. For the illustration and reproducibility purposes, we implement the proposed online recommender selection under a semi-synthetic setting with a benchmark movie recommendation data. To fully reî€ect the exploration-exploitation dilemma in real-world production, we convert the benchmark dataset to an online setting such that it mimics the interactive process between the recommender and user behavior. A similar setting was also found in [9] that studies the non-contextual bandits as model ensemble methods, with which we also compare in our experiments. We consider the linear reward model setting for our simulation. Data-generating mechanism. In the beginning stage, 10% of the full data is selected as the training data to î€›t the candidate recommendation models, and the rest of the data is treated as the testing set which generates the interaction data adaptively. The procedure can be described as follow. In each epoch, we recommend one item to each user. If the item has received a non-zero rating from that particular user in the testing data, we move it to the training data and endow it with a positive label if the rating is high, e.g.â‰¥3 under the î€›ve-point scale. Otherwise, we add the item to the rejection list and will not recommend it to this user again. After each epoch, we retrain the candidate models with both the past and the newly collected data. Similar to [9], we also use the cumulative recallas the performance metric, which is the ratio of the total number of successful recommendations (up to the current epoch) againt the total number of positive rating in the testing data. The reported results are averaged over ten runs. Dataset. We use the MoiveLens 1Mdataset which consists of the ratings from 6,040 users for 3,706 movies. Each user rates the movies from zero to î€›ve. The movie ratings are binarized to{0,1}, i.e.â‰¥2.5 or<2.5, and we use the metadata of movies and users as the contextual information for the reward model. In particular, we perform the one-hot transformation for the categorical data to obtain the feature mappingsğ“(Â·). For text features such as movie title, we train a word embedding model [36] with 50 dimensions. The î€›nal representation is obtained by concatenating all the one-hot encoding and embedding. Figure 3: The comparisons of the cumulative recall (reward) per epoch for diî€erent bandit algorithms. Candidate recommenders. We employ the four classical recommendation models: user-based collaborative î€›ltering (CF) [54], item-based CF [42], popularity-based recommendation, and a matrix factorization model [21]. To train the candidate recommenders during our simulations, we further split the 10% initial training data into equal-sized training and validation dataset, for grid-searching the best hyperparameters. The validation is conducted by running the same generation mechanism for 20 epochs, and examine the performance for the last epoch. For the user-based collaborative î€›ltering, we set the number of nearest neighbors as 30. For itembased collaborative î€›ltering, we compute the cosine similarity using the vector representations of movies. For relative item popularity model, the ranking is determined by the popularity of movies compared with the most-rated movies. For matrix factorization model, we adopt the same setting from [9]. Baselines. To elaborate the performance of the proposed methods, we employ the widely-acknowledged exploration-exploitation algorithms as the baselines: â€¢The multi-armed bandit (MAB) algorithm without context: ğœ–-greedy and Thompson sampling. â€¢Contextual bandit with the exploration conducted in the LinUCB fashion (Linear+UCB) and Thompson sampling fashion (Linear+Thompson). We denote our algorithms by theLinear+Frequentis optimal design and the Linear+Bayesian optimal design. Ablation studies We conduct ablation studies with respect to the contexts and the optimal design component to show the eî€ectiveness of the proposed algorithms. Firstly, we experiment on removing the user context information. Secondly, we experiment with our algorithm without using the optimal designs. Results. The results on cumulative recall per epoch are provided in Figure 3. It is evident that as the proposed algorithm with optimal design outperforms the other bandit algorithms by signiî€›cant margins. In general, even thoughğœ–-greedy gives the worst performance, the fact that it is improving over the epochs suggests the validity of our simulation setup. The Thompson sampling under MAB performs better thanğœ–-greedy, which is expected. The usefulness of context in the simulation is suggested by the slightly better performances from Linear+UCB and Linear+Thompson. However, they are outperformed by our proposed methods by signiî€›cant margins, which suggests the advantage of leveraging the optimal design in the exploration phase. Finally, we observe that among the optimal design methods, the Bayesian setting gives a slightly better performance, which may suggest the usefulness of the extra steps in Algorithm 4. The results for the ablation studies are provided in Figure 4. The left-most plot shows the improvements from including contexts for bandit algorithms, and suggests that our approaches are indeed capturing and leveraging the signals of the user context. In the middle and right-most plots, we observe the clear advantage of conducting the optimal design, specially in the beginning phases of exploration, as the methods with optimal design outperforms their counterparts. We conjecture that this is because the optimal designs aim at maximizing the information for the limited options, which is more helpful when the majority of options have not been explored such as in the beginning stage of the simulation. Finally, we present a case study to fully illustrate the eî€ectiveness of the optimal design, which is shown in Figure 5. It appears that Figure 4: The diî€erence in cumulative recall for system bandit under diî€erent settings. The left î€›gure shows diî€erence in performance between using and not using user context, under the frequentist and Bayesian setting, respectively. The other two î€›gures compare using and not using the optimal design (uniform selection) in the exploration stages, both using the linear reward model. in our simulation studies, the matrix factorization and popularitybased recommendation are found to be more eî€ective. With the optimal design, the traî€œc concentrates more quickly to the two promising candidate recommenders than without the optimal design. The observations are in accordance with our previous conjecture that optimal design gives the algorithms more advantage in the beginning phases of explorations. Figure 5: The percentage of traî€œc routed to each candidate recommender with and without using the optimal design under the frequentist setting. We deployed our online recommender selection with optimal design to the similaritem recommendation of grocery items on Walmart.com. A webpage snapshot is provided in Figure 6, where the recommendation appears on the item pages. The baseline model and we experiment with three enhanced models that adjust the original recommendations based on the brand aî€œnity, price aî€œnity and î€avor aî€œnity. We omit the details of each enhanced model since they are less relevant. The reward model leverages the item and user representations also described in our previous work. Speciî€›cally, the item embeddings are obtained from the Product Knowledge Graph embedding [50], and the user embeddings are constructed via the temporal user-item graph embedding [14]. We adopt the frequentist setting where the reward is linear function ofâŸ¨item emb, user embâŸ©, plus some user and item contextual features: ğœƒ+ğœƒâŸ¨z, zâŸ© + . . . +ğœƒâŸ¨z, zâŸ© + ğœ½[user feats, items feats], and zand zare the user and item embeddings. Figure 7: The testing results for the propose d stage-wise exploration-exploitation with optimal design. We conduct a posthoc analysis by examining the proportion of traî€œc directed to the frequent and infrequent user groups by the online recommender selection system (Figure 8). Interestingly, we observe diî€erent patterns where the brand and î€avor-adjusted models serve the frequent customers more often, and the unadjusted baseline and price-adjusted model get more appearances for the infrequent customers. The results indicate that our online selection approach is actively exploring and exploiting the user-item features that eventually beneî€›ts the online performance. The simulation studies, on the other hand, reveal the superiority over the standard exploration-exploitation methods. Figure 8: The analysis on the proportion of traî€œc directed to the frequent and infrequent customer. We study optimal experiment design for the critical online recommender selection. We propose a practical solution that optimizes the standard exploration-exploitation design and shows its eî€ectiveness using simulation and real-world deployment results.