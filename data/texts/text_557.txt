Knowledge graph is generally incorporated into recommender systems to improve overall performance. Due to the generalization and scale of the knowledge graph, most knowledge relationships are not helpful for a target user-item prediction. To exploit the knowledge graph to capture target-speciî€›c knowledge relationships in recommender systems, we need to distill the knowledge graph to reserve the useful information and reî€›ne the knowledge to capture the usersâ€™ preferences. To address the issues, we propose Knowledge-aware Conditional Attention Networks (KCAN), which is an end-to-end model to incorporate knowledge graph into a recommender system. Speciî€›cally, we use a knowledge-aware attention propagation manner to obtain the node representation î€›rst, which captures the global semantic similarity on the user-item network and the knowledge graph. Then given a target, i.e., a useritem pair, we automatically distill the knowledge graph into the target-speciî€›c subgraph based on the knowledge-aware attention. Afterward, by applying a conditional attention aggregation on the subgraph, we reî€›ne the knowledge graph to obtain target-speciî€›c node representations. Therefore, we can gain both representability and personalization to achieve overall performance. Experimental results on real-world datasets demonstrate the eî€ectiveness of our framework over the state-of-the-art algorithms. â€¢ Information systems â†’ Recommender systems. Network Representation Learning; Graph Convolutional Network; Knowledge Graph; Conditional Attention ACM Reference Format: Ke Tu, Peng Cui, Daixin Wang, Zhiqiang Zhang, Jun Zhou, Yuan Qi, and Wenwu Zhu. 2021. Conditional Graph Attention Networks for Distilling and Reî€›ning Knowledge Graphs in Recommendation. In Proceedings of the 30th ACM Intâ€™l Conf. on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3482331 Nowadays, recommender systems [12,15,47] are widely used in various Internet applications, for example, search engines [2], video websites [7], and E-commerce [31]. The recommender system aims to î€›nd proper items for target users to meet their personalized interests based on the user-item historical network. However, the common critical problem of recommender systems is data sparsity, i.e., the user behaviors or user-item interactions are very limited comparing with the volume of items. Besides, it is hard to recommend items for a new arrival user [32]. To address the limitations, the researchers have proposed to incorporate side information into the recommender systems, such as attributes [18], contexts [20], images [28]. Among the various types of side information, knowledge graphs [9, 44] usually contain more abundant information about the characteristics and connections of nodes. Diî€erent from the normal network, the knowledge graph is composed of a set of triplets, i.e., <head entity, relations, tail entity>. It can describe not only node attributes, such as< Movie,Genre,Actioner >in Figure 1, but also node relationships, such as< Actor,Friend,Director>in Figure 1. Recently, numerous massive knowledge graphs, such as Wordnet [26], Freebase [3] and DBpedia [1], have been published. These knowledge graphs which describe the facts and common sense can be partly aligned to the nodes of most network applications and be regarded as their side information. They can beneî€›t the recommender systems by introducing relatedness among entities, enriching the entity information, and producing the explainability. However, due to the generalization and scale of the knowledge graph, most knowledge relationships are not helpful for a user-item prediction. To exploit knowledge graph to capture target-speciî€›c knowledge relationships in recommender systems [29], we need to address the following new requirements: Figure 1: Illustration of Knowledge-aware recommender system. (1) Knowledge Graph Distillation: Knowledge graphs are massive and comprehensive for containing more information. For a speciî€›c item recommendation, most knowledge relationships maybe not helpful. Thus learning semantic relationships on the full knowledge graph for a given task is very time-consuming and noisy. Distilling the knowledge graph which gives a small sub-structure related to the target task from the full massive knowledge graph is necessary. It is worth noting that the knowledge graph distillation describes the process of transforming the full knowledge graph into a small concentrated one to capture usersâ€™ preference accurately, rather than mimicking a pre-trained, larger teacher model by a small student model like knowledge distillation [16]. (2) Knowledge Graph Reî€›nement: Personalized recommendation mines userâ€™s interests from the past purchasing behaviors. In personalized recommendation with knowledge graph, attention mechanisms are usually used to measure userâ€™s preference [43,45]. However, they give the same weights of knowledge edges for diî€erent target users by edge attentions. For example, in Figure 1, we aim to recommend for targets usersUserandUser. The weights of edge< Movie, genre, Comedy >are only based on the nodesMovieandComedy, and are independent ofUser andUser. ButUserandUsermay have diî€erent preferences on the genre of the movie. That is to say, the weights of edge <Movie2,genre,Comedy> may be diî€erent forUser andUser. Thus, for a given target, we should reî€›ne the knowledge graph to give diî€erent weights for all the knowledge relationships instead of its neighbors. Therefore, we believe that a good knowledge-aware network learning method should distill and reî€›ne the knowledge graphs. Early knowledge graph-aware algorithms are embedding-based models [5,45]. They learn entity and relation representations î€›rst by knowledge graph embedding algorithms [6,48] and then incorporate the latent embeddings into the recommender system. The direct way to exploit the knowledge graph in the embedding space fails to solve the distillation and reî€›nement issues and thus harm the performance. The path-based methods [17,47,49] explore diî€erent meta-paths from knowledge graphs to build relationships of two objects, such asUserâˆ’â†’ Movieâˆ’â†’Actionerâˆ’â†’ Movie. They distill the knowledge graphs into multiple meta-paths. However, these methods heavily depend on the hand-crafted design of the meta-path. Besides, the multiple paths can not handle the diversity of the usersâ€™ preference on the knowledge relationships. Recently, some graph convolutional network-based methods [43,45] are proposed to propagate information on knowledge graphs. They usually use an attention mechanism [39] to produce weights of diî€erent neighbors for knowledge graph reî€›nement. However, the attentions are only based on the two nodes in the same edges and are independent with the target nodes. To overcome the Knowledge Graph Distillation and the Knowledge Graph Reî€›nement issue, we propose a novel model named Knowledge-aware Conditional Attention Networks (KCAN). First, we propose a knowledge-aware graph convolutional network to propagate embedding on the knowledge graph by knowledge-aware attention to capture the global similarity of entities like KGAT [45]. After that, a subgraph sampling strategy is designed to sample target-speciî€›c subgraphs based on the attention weights to distill the knowledge graph. Also, the proposed Local Conditional Subgraph Attention Network (LCSAN) propagates personalized information on the sampled subgraph based on local conditional attention for reî€›ning the knowledge graph. In conclusion, the proposed KCAN can eî€ectively distill and reî€›ne the knowledge graph at the same time. It is worthwhile to highlight the following contributions of this paper: â€¢We highlight the importance of distilling and reî€›ning knowledge graphs in recommender systems and propose to use a conditional attention mechanism on target-speciî€›c subgraphs to capture user preference. â€¢We propose a novel framework named Knowledge-aware Conditional Attention Networks (KCAN) to incorporate the knowledge graph into the recommendation. In particular, the Knowledge-aware Graph Convolutional Network (KAGCN) layer and target-speciî€›c subgraph sampling are designed with the Knowledge Graph Distillation issue in mind. Moreover, the Knowledge Graph Reî€›nement issue is addressed by the Local Conditional Subgraph Attention Network (LCSAN) layer. â€¢Extensive experiments on real-world scenarios are conducted to demonstrate the eî€ectiveness of our framework over several state-of-the-art methods. The rest of the paper is structured as follows. We î€›rst give a brief review of the related works in section 2. Then we formally deî€›ne the solved problems and introduce the details of our proposed model in section 3. In section 4, we report the experimental results. Finally, we give a conclusion in section 5. Knowledge Graph Embedding (KGE) [44] aims to learn latent representations for all components of the knowledge graph including Figure 2: The framework of the proposed KGAN. The framework is composed of four modules: Knowledge Graph Embedding layer, Knowledge Graph Distillation module (KAGCN layer and Target-sepciî€›c Sampling), Knowledge Graph Reî€›nement module (LCSAN layer) and Multi-layer Perceptron (MLP) two-tower prediction layer. entities and relations. Then the low-rank embeddings which preserve the inherent structure and knowledge graph can be used in the downstream tasks such as knowledge graph completion and recommendation. The KGE methods can be roughly divided into two classes: translation distance based models [4,24] and semantic matching based models [27,38,51]. The translation distance based models exploit distance-based scoring functions. The existing relationships in the knowledge graph will have higher scores. The scoring functions usually measure the distance of two entities under the space of the relation. For example, TransE [4] assumes â„ğ‘’ğ‘ğ‘‘ + ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› = ğ‘¡ğ‘ğ‘–ğ‘™and use||â„ğ‘’ğ‘ğ‘‘ + ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘› âˆ’ ğ‘¡ğ‘ğ‘–ğ‘™ ||as scoring function on the(â„ğ‘’ğ‘ğ‘‘, ğ‘Ÿğ‘’ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›, ğ‘¡ğ‘ğ‘–ğ‘™)relationships. TransR [24] î€›rst projects the entity representationsâ„ğ‘’ğ‘ğ‘‘andğ‘¡ğ‘ğ‘–ğ‘™into the space speciî€›c to relationğ‘Ÿand measures the distance between entities â„ğ‘’ğ‘ğ‘‘andğ‘¡ğ‘ğ‘–ğ‘™under that space. The semantic matching models exploit similarity scoring functions. For example, RESCAL [27] uses a bilinear function to learn the similarity of entities. For the recommendation task, the main issue is how to introduce the representations of entities and relations into the recommendation system for enriching the item information. The graph convolutional network (GCN) [22,54] has been proposed to process network data in an end-to-end manner. Earlier works deî€›ne the graph convolutional operations in the spectral domain. Bruna et al. [8] are inspired by the graph signal process [35] and deî€›ne the convolution in the Fourier spectral domain. Kipf et al. [22] propose to use a î€›rst-order approximation to deal with the complexity issue. Recently, lots of non-spectral GCN, which directly deî€›ne convolution on the graph, have been proposed in the spatial domain. GraphSage [13] samples a î€›xed-size neighborhood of each node and then aggregates over it. GAT [40] introduces a self-attention strategy to specify diî€erent weights to diî€erent nodes in a neighborhood. However, these works are designed for homogeneous graphs instead of knowledge graphs. Our method is conceptually inspired by GCN. Similarly, Schlichtkrull et al. [33] and Wang et al. [43] also propose to apply GCN and GAT into the knowledge graph. They î€›rst applies knowledge graph embedding [44] methods to obtain representation for entities. Then they propagate the representation over the user-item bipartite graph and knowledge graph to collect the high-order information. Besides, KGCN-LS [42] extends the previous knowledge based GCN method and adds a label smoothness mechanism to propagates the user interaction labels on the knowledge graph. However, the major diî€erence between our work and these methods is that our model consider the knowledge graph distillation and the knowledge graph reî€›nement whiling learning representations. In general, existing knowledge graph-aware algorithms [6,29] can be roughly categorized into two types: (1) Embedding-based methods [50,53]. These methods learn entity and relation representations î€›rst by knowledge graph embedding algorithms and then incorporate the latent embeddings into the original network. CKE [53] combines collaborative î€›ltering with knowledge graph embedding. Cao et al. [5] use the knowledge graph to augment the modeling of user-item interaction while completing the missing facts in the knowledge graph based on the enhanced user-item modeling at the same time. Nevertheless, these methods only use knowledge graph embedding as regularization and lose the rich topology structure of the knowledge graph. Additionally, Wang et al. [45] use graph convolutional network to explicitly model the high-order relations by recursive propagation in the knowledge graph. But all these methods usually have no constraint between the target users and entities and thus hurt the modeling of user preferences. (2) Path-based methods. These methods [17,46,47] regard the knowledge graph as a heterogeneous information network [52] and explore diî€erent paths from knowledge graphs and measure node relationships by meta-path-based similarity [37]. Besides, some methods leverage the meta-path based random walk to learn better entities representation, such as HIN2Vec [11] and metapath2vec [10]. Deî€›ning eî€ective meta-paths usually requires domain knowledge, and it can not generalize to a new dataset. Recently, some works [36,49] propose to apply reinforcement learning to choose meta-paths while training automatically. However, characterizing the user-item similarity by separate paths may lead to information loss. Our works propose to use target-speciî€›c subgraphs to distill the knowledge graph. Xiao et al. [34] also use subgraphs to distill the knowledge graph. But it pre-computes all user-item subgraphs by the knowledge graph and just learns on these local subgraphs. In such a way, the semantic similarity between entities on the whole knowledge graph is missing. Our model can not only preserve the global semantic similarity but also dynamically generates target-speciî€›c subgraphs to distill and reî€›ne knowledge graph for inferring local user preference accurately. In this section, we will introduce the proposed Knowledge-aware Conditional Attention Networks (KCAN). The framework is shown in Figure 2. The model is composed of four modules: (1) Knowledge Graph Embedding layer. It learns representations for each entity and relation by a traditional knowledge graph embedding method TransH. (2) Knowledge Graph Distillation, which propagates embedding with a knowledge-aware attention mechanism and uses a target-sampling strategy to distill the knowledge graph. (3) Local Conditional Subgraph Attention Network (LCSAN), which propagates personalized information on the subgraph based on local conditional attention to reî€›ne the knowledge graph. (4) Multi-layer Perceptron (MLP) two-tower prediction layer, which combines the embeddings from the above two layers and predicts the î€›nal results with a non-linear layer. Letğº = (ğ‘‰, ğ¸)denotes a network, whereğ‘‰is the set of nodes andğ¸ âŠ† ğ‘‰ Ã— ğ‘‰is the set of edges. For a nodeğ‘£ âˆˆ ğ‘‰,N (ğ‘£) = {ğ‘¢| (ğ‘£, ğ‘¢) âˆˆ ğ¸}is the set of its neighbors. In our setting, we have an external knowledge graphG = {(â„, ğ‘Ÿ, ğ‘¡) |â„ âˆˆ E, ğ‘Ÿ âˆˆ R, ğ‘¡ âˆˆ E}, whereâ„,ğ‘Ÿandğ‘¡denotes the head entity, relation and tail entity of a knowledge graph triple.EandRare the set of entities and relations of knowledge graphG. Actually, our model can be used in any graph tasks with a side knowledge graph. The target setsTrely on the goal of the graph task. For example, in node classiî€›cation, the target set is a single nodeT = {ğ‘£ }. For recommendation task, given a bipartite user-item graph, the goal is to predict the existence or similarityğ‘¦of a targeted user-item pairT = {ğ‘¢, ğ‘£ }. For drug reactions prediction task, the target set is all the drugs which leads reaction togetherT = {ğ‘‘, .., ğ‘‘}. In our paper, we only discuss recommendation task with bipartite user-item graphğºand an external knowledge graphG. A node in networkğºmay also be an entity in knowledge graphG. In general, we use entity to refer all nodes in bothğºandG. For an entityğ‘£, the neighborhood ofğ‘£ in knowledge graph is denoted asN (ğ‘£) = {(ğ‘Ÿ, ğ‘£)|(ğ‘£, ğ‘Ÿ, ğ‘£) âˆˆ G} and we mark the K-hop neighborhood of entityğ‘£asğ‘†(ğ‘£). We mark the embedding of entityâ„inğ‘˜-th layer aseâˆˆ Rwith ğ‘˜ = 0, 1, 2, ..., ğ¾. For simplify, we mark eas e. To distill the whole knowledge graph, we must recognize the importance of the knowledge relationships and eliminate the useless relationships. For this purpose, we î€›rst vectorize the entities by knowledge graph embedding. 3.2.1 Knowledge Graph Embedding. Knowledge graph embedding [6, 19,48] is an eî€ective way to learn entity and relation representations while preserving the topology structure. Since we regard the click relation in the recommendation as a type of knowledge relationships and a user may click many items, so there are a lot of one-to-many mappings in the knowledge graph. Here we use TransH [48] which can solve the one-to-many and many-to-one issues. For a given triple(â„, ğ‘Ÿ, ğ‘¡), its score or distance is deî€›ned as follows: jections of entities embeddingeandeon the relation-speciî€›c hyperplanew, anddis the relation-speciî€›c translation vector; âˆ¥ Â· âˆ¥is theğ¿1-norm. The lower score ofğ‘“(â„, ğ‘¡)indicates the triplet is more likely to be true and vice versa. By projecting to the relationspeciî€›c hyperplane, TransH enables diî€erent roles of an entity in diî€erent triplets. The loss of knowledge graph embedding is Bayesian personalized ranking loss [30], which aims to maximize the margin between the positive samples and the negative samples: L=âˆ’ ln ğœ (ğ‘“(â„, ğ‘¡) âˆ’ ğ‘“ whereğœis the sigmoid function,ğ‘¡is uniformly sampled by replacingğ‘¡with another entity randomly. To increase the representation ability, we also train the model on both the knowledge graphGand the user-item networkğºwith this loss function. For this purpose, we treat all edges in networkğºwith the same typeğ‘Ÿ. Then the networkğºcan be also seen as a knowledge graph with one and the same relation type. 3.2.2 Knowledge-Aware Graph Covolutional Network. However, the knowledge graph embedding only focuses on the separate knowledge triples, thus it fails to capture the high-order similarity between entities. Inspired by KGAT [45] and KGNN [23], we present to capture the high-order similarity of entities by aggregating the knowledge embedding with graph convolutional network. To distinguish the inî€uence of diî€erent types of relations in the knowledge graph, we propose to use a knowledge-aware attention mechanism over the propagation: whereğœ‹(ğ‘£, ğ‘¡)is relation-speciî€›c attention coeî€œcient. The attention measures the importance of the entityğ‘£toğ‘¡under the relation ğ‘Ÿ-speciî€›c space. When the entityğ‘¡is closer to entityğ‘£under the relationğ‘Ÿ-speciî€›c space, the more information should be propagated. Motivated by this, we deî€›ne knowledge-aware attention as follows: wherecos(x, y) =is the cosine similarity. In this way, it will propagate more information for closer entities. Finally, we combine the entity representation eand the neighborhood representationğ‘’and update the entities representation ase. For simplicity, we concatenate two representations and add a nonlinear transformation like GraphSAGE aggregator [13]: where||is the concatenation operation, and LeakyReLU is the activation function.W, bare the trainable weights. By this knowledge-aware propagation mechanism, we can preserve the global similarity between entities after several iterations. 3.2.3 Target-specific Sampling. For a given user-item target(ğ‘¢, ğ‘£), the attentions of KAGCN rely on the knowledge edges and do not concern with the targets. To capture the local inî€uence of the targets, we exploit subgraphs instead of the whole enormous knowledge graph. Compared to the meta-path or random-walk based model which uses path to capture the locality, the subgraph can contain diversity. A target set is marked asT = {ğ‘£, ..., ğ‘£} whichğ‘˜ =2 andTis a user-item pair. Due to the locality of the usersâ€™ preference, we set the receptive î€›eld of each nodeğ‘£as their K-hop neighborhood ğ‘†(ğ‘£). In a real-world knowledge graph, the number of neighbors may vary signiî€›cantly over all entities, and some entities may have a huge number of neighbors. To distill the knowledge graph and reduce the training time, we sample a î€›xed-size set of neighbors for each entity instead of the full neighborhood. We set the sampled size asğ‘€. The traditional way is to sample neighbors uniformly like GraphSAGE [13]. However, the knowledge graph is usually noisy and full of target-independent information. Additionally, considering that we have obtained the global attentionğœ‹(ğ‘£, ğ‘¡), which measures the similarity of entities, we sample neighbors with their attention scoreğœ‹(ğ‘£, ğ‘¡)as the sampled probability. After getting the sampledğ¾-hop neighborhood asË†ğ‘†(ğ‘£)for each nodeğ‘£, we can obtain the receptive î€›eld of the target set by merging them: The knowledge graph distillation module distills the knowledge graph to a target-speciî€›c subgraph as the receptive î€›eld of the target set. In this part, we re-weight the knowledge to reî€›ne the knowledge graph. For that, we propagate entitiesâ€™ information on the distilled target-speciî€›c subgraph. To capture the preference of the target to the knowledge relationships, we propose to use a conditional attention mechanism over propagation to reî€›ne the subgraph as follows: whereğ›¼ (ğ‘£, ğ‘Ÿ, ğ‘¡ |T )is the conditional attention which relies on target T, andN(ğ‘£)is the neighbors of the entityğ‘£in the receptive î€›eld of the target setË†ğ‘†(T ). To better reî€›ne the subgraph based on the target, the conditional attention should contain two aspects: (1) the importanceğ›¼(ğ‘£, ğ‘Ÿ, ğ‘¡) of the knowledge relationship(ğ‘£, ğ‘Ÿ, ğ‘¡). This part is independent of the target, and it measures the importance of knowledge relationship itself upon the task. If the knowledge relationship is a noise edge in the task, we should reduce its inî€uence. In the KAGCN part of the section 3.2.2, we had measured it by the knowledge-aware attentionË†ğœ‹(ğ‘£, ğ‘¡). So we can just setğ›¼=Ë†ğœ‹(ğ‘£, ğ‘¡)for simplicity. (2) the importanceğ›¼(ğ‘¡ |T )of the entity to the target set. This term measures the local preference of the target to the tail entity. The entities which are more similar to the target should be more important for the target. To make the target set and the entity comparable, we calculate the representation of the target set as the concatenation of all contained entities, i.e.,e= âˆ¥e. Subsequently, we can measure the importance of the entity to the target set by their representations: whereais a weight vector.WandWare the linear transformation matrices of targets and entities respectively. Combining the two importance scores, we get the conditional attention: ğ›¼ (ğ‘£, ğ‘Ÿ, ğ‘¡ |T ) = soî‚‰max(LeakyReLU(ğ›¼(ğ‘£, ğ‘Ÿ, ğ‘¡) âˆ— ğ›¼(ğ‘¡ |T ))). (10) The larger attention indicates the more important of the knowledge related to the target. By the conditional attention, the local knowledge graph is reî€›ned into a weighted graph varying with the target. Finally, similar to KAGCN, the target-speciî€›c entities representations can be obtained as follows: e= AGG(e, e Furthermore, in order to capture high-order preference, we further stack more LCSAN layers. Especially, since the subgraphs are composed ofğ¾-hop composed neighborhoods, we stack the LCSAN withğ¾times to make sure that the information of each node can be propagated over all the subgraph. In general, theğ‘–-th LAGCN is as follows: where ğ‘– = 2, ...ğ¾. We set ğ¾ = 2 in all our experiments. After conducting the LAGCN, we obtain the target-speciî€›c representationse. To increase the representability of the embeddings, we concatenate it with the output representationseof KAGCN ase= [eâˆ¥e]. In such way, we can preserve both global similarity and local preference eî€ectively. To automatically balance the two parts, we feed the concatenated representations einto a multi-layer perceptron (MLP) layers to obtain the î€›nal output representations: whereWandbare learnable weights. For recommendation with user-item(ğ‘¢, ğ‘–)prediction, the predicted score is their inner product of user and item representations with the given targetT= {ğ‘¢, ğ‘–}: Since we only observe the positive interactions in the recommender systems, we randomly sample some unobserved relations as the negative samples. To make the learned similarities of the positive interactions are larger than the negative samples, we optimize the model with the Bayesian personalized ranking loss [30]: L=âˆ’ ln ğœ (ğ‘ (ğ‘¢, ğ‘— |T) âˆ’ ğ‘ (ğ‘¢, ğ‘– |T whereğ¸is the observed edges between userğ‘¢and itemğ‘–andğ¸ is the uniformly sampled unobserved edges. Objective Function. Combing the loss in Equation 2 and 16, we get the total objective function as follows: whereÎ˜ = {Ë†E, W, b, W, W, W, b|âˆ€ğ‘– âˆˆ {1,2, .., ğ¾ + 1}, âˆ€ğ‘— âˆˆ {1,2, ..., ğ¾ }}is the set of all parameters, andË†Eis the embeddings of all entities and relations.ğœ†is the weight of regularization. We optimizeLandLalternatively and Adam [21] is used to optimize these parameters. The detailed training algorithm is shown in Algorithm 1. The learning rate for Adam is initially set to 0.025 at the beginning of the training, and the total epoch number is set as 200. Training Complexity. The time cost of our proposed KCAN mainly comes from two part, KAGCN and LCSAN. The time complexity of KCAN isğ‘‚ ((|ğ¸| + |ğ¸|) âˆ— ğ¹)where|ğ¸|is the number of edges in networkğº,|ğ¸|is the number of triples in knowledge graph Gandğ¹is the dimension of the knowledge graph embedding.Ã For LCSAN, the time complexity isğ‘‚ (|ğ¸| âˆ— ğ‘€ âˆ—ğ¹ğ¹), whereğ‘€is the î€›xed-size number of neighbors while sampling subgraphs, andğ¹is embedding size ofğ‘–-th layer. We usually setğ‘† as 20. So the total complexity of KCAN isğ‘‚ ((|ğ¸|+|ğ¸|) âˆ—ğ¹+|ğ¸|âˆ—Ã ğ‘† âˆ—ğ¹ğ¹). Note that theğ‘€andğ¹are small user-speciî€›ed constants, the proposed KCAN is linear to the scale of network and knowledge graph. In this section, we evaluate our method on three real-world datasets to prove its eî€œcacy. We aim to answer the following research questions: â€¢ RQ1: What do the knowledge attention and the conditional attention in the proposed model learn? â€¢ RQ2: Does our proposed KCAN outperform the state-of-theart knowledge-aware methods? â€¢ RQ3: How do our proposed KAGCN, LCSAN layers aî€ect the performance of KCAN respectively? â€¢ RQ4: How do diî€erent choices of hyper-parameters aî€ect the performance of KCAN? Algorithm 1Training Algorithm for Knowledge-aware Conditional Attention Networks (KCAN) Require:User-item networkğº; Knowledge graphG. The conî€›gurationÎ˜ = {Ë†E, W, b, W, W, W, b|âˆ€ğ‘– âˆˆ {1,2, .., ğ¾ + 1}, âˆ€ğ‘— âˆˆ {1, 2, ..., ğ¾ }}. Compute the loss of knowledge graph embedding in Equation 2 and update knowledge graph embeddingğ‘’âˆˆ Î˜ by Adam. Compute the knowledge-aware attentionË†ğœ‹(ğ‘£, ğ‘¡)in Equation 4. Based on Equation 5, propagate over the knowledge graph and the network by the knowledge-aware attentionË†ğœ‹(ğ‘£, ğ‘¡) to obtain embedding ğ‘’. Based onğ‘’andğ‘’, compute the predicted score ğ‘ (ğ‘¢, ğ‘–|T ) by Equation 15. Compute the target loss in Equation 16 and update the conî€›guration Î˜ by Adam. In order to comprehensively evaluate the eî€ectiveness of our proposed method KCAN, we use three public benchmark datasets: MovieLens, Last-FM and Yelp. â€¢ MovieLensis a widely used benchmark dataset in movie recommendations. It contains the explicit ratings (ranging from 1-5) on the MovieLens website. We transform the rating into implicit feedback where each entry is marked with 1 indicating that the user has rated the item over a threshold score (4 for this dataset) and otherwise 0. â€¢ Last-FMis a music listening dataset collected from Last.fm online music systems. The timestamp of the dataset is from Jan, 2015 to June, 2015. To ensure the quality of the dataset, we use the 10-core setting like [45], i.e., retaining users and items with at least ten interactions. â€¢ Yelpis obtained from the 2018 edition of Yelp challenge. The items are local businesses like restaurants and bars. Similarly, we also use 10-core setting on this dataset. Besides, we also need to construct a corresponding knowledge graph from a massive knowledge graph for each dataset. Actually, it is a rule-base rough knowledge graph distill process. Microsoft Satoriis used to build a knowledge graph for MovieLens, we î€›rst select a subset of triplets whose relation name contains "movie" and the conî€›dence level is greater than 0.9 from the whole knowledge graph. We match the entities with the items by their title names. Similarly, the corresponding knowledge graph is built from Freebase for Last-FM. For Yelp2018, we use the local business information network, for example, category, location, and attribute, as the knowledge graph. The detailed statistics of the user-item networks and the knowledge graphs are summarized in Table 1. We compare our proposed KCAN with four representative types of baselines, including regularization-based (CKE [53]), factorizationbased (NMF [15]), path-based (RippleNet [41]) and GCN-based (KGAT [45]). â€¢ CKE[53] combines collaborative î€›ltering with the structural knowledge content, the textual content and visual content in an uniî€›ed framework. In this paper, we implement CKE by combining collaborative î€›ltering and the structural knowledge content. It uses knowledge graph information as regularization to î€›ne tune the collaborative î€›ltering. â€¢ NMF[15] is a novel factorization machine model for prediction under sparse settings. It deepens factorization machine under the neural network framework for learning higherorder and non-linear feature interactions. â€¢ RippleNet[41] combines regularization-based and pathbased methods.It stimulates the propagation of user preferences over the set of knowledge entities. â€¢ KGAT[45] introduces graph attention into recommendation and explicitly models the high-order connectivities in knowledge graph in an end-to-end fashion. We uniformly set the embedding size as 16 for all methods. The hidden size of our method and KGAT is set as a tower structure with 16, 8, 8. For our model and RippleNet, the number of hops is set as 2. The dropout rates of NFM, KGAT, and our model are set as 0.1. The î€›xed-size number of neighbors while sampling subgraphs is set as 20 for our model. We do grid search from {0.01,0.025,0.05,0.1}to tune the learning rate parameter and from {10,10,10,10,10}for the weights of theğ¿normalization. All the experiments are conducted on Intel(R) Xeon(R) CPU E5-2699 v4 @ 2.20GHz with GeForce GTX Titan X GPU. In this subsection, we show what the knowledge attention and the conditional attention in the proposed model learn on the MovieLens dataset in Figure 3. Figure 3 (a) shows the global knowledge attention in the Equation 4. The attention is only related to knowledge relationships which is the same as the KGAT. From this way, we can infer user preferences. The larger attention means the more important edge. As we can see, the pathğ‘¢âˆ’â†’ ğ‘–âˆ’â†’ ğ‘–has the highest attention score between nodesğ‘¢andğ‘–. We can explain that the itemğ‘–is recommend to userğ‘¢because the userğ‘¢clicked itemğ‘–which is similar to item ğ‘–. Besides, the edge ğ‘¢âˆ’â†’ ğ‘–with small weights 0.001 has less information. We can î€›lter it out to distill the knowledge graph. The Figure 3 (b), (c) show the local attention (Equation 10) with targets(ğ‘¢, ğ‘–),(ğ‘¢, ğ‘–)respectively. The red nodes are the nodes which needs to be predicted. We reî€›ne the target-speciî€›c subgraph with diî€erent weights by the local attention. We can see that there are diî€erent attentions for the same edgeğ‘¢âˆ’â†’ ğ‘– in the two subgraphs. The edgeğ‘¢âˆ’â†’ ğ‘–is more important for the prediction of(ğ‘¢, ğ‘–)than the prediction of(ğ‘¢, ğ‘–). All the baselines can not learn this information which is important for predicting user-item pair. We evaluate our method in two recommendation scenarios. (1) TopK recommendation. We use the leave-one-out strategy, which has been widely used in previous works [15,30] to evaluate the recommendation performance. For a user, we randomly sample 100 items that are not interacted by the user, ranking the test item among the 100 items. The performance is judged by Hit Ratio@K (Hit@K) and Normalized Discounted Cumulative Gain@K (NDCG@K) [14]. If the true test item ranks in the topğ¾lists, the Hit@K will be one otherwise zero. Compared with Hit@K, NDCG@K pays more attention to the ranking order. The more front position of the true item will have a larger NDCG@K. Then we compute the two metrics for each user and obtain the average score atğ¾ =10. (2) Click-through rate (CTR) prediction. We predict the score of each user-item pair, including positive items and randomly sampled negative items, by the trained model. The evaluation metric in CTR prediction is set as the area under the curve(AUC). The AUC is equivalent to the probability of positive samples are ranked higher than negative samples [25]. The performance comparison results of the top-K recommendation and CTR prediction are presented in Table 2 and 3, respectively. The observations are illustrated as follows: â€¢Our proposed KCAN achieves signiî€›cant improvements over the baselines on all the datasets in both top-K recommendation and CTR prediction. It demonstrates the eî€ectiveness of our proposed method KCAN. Moreover, KCAN outperforms ğ‘Ÿ0.044ğ‘Ÿ0.043ğ‘Ÿ0.001ğ‘Ÿ0.022 ğ‘Ÿ0.044ğ‘Ÿ0.013ğ‘Ÿ0.085ğ‘Ÿ0.062ğ‘Ÿ0.051 Figure 3: (a) Example from MovieLens dataset.The knowledge attention in the whole knowledge graph and the user-item graph. (b) The conditional attention on the target (ğ‘¢, ğ‘–)-speciî€›c subgraph. (c) The conditional attention on the target (ğ‘¢ speciî€›c subgraph. The red nodes are the nodes which needs to be predicted. Table 2: Hit@10 and NDCG@10 in top-K recommendation. Figure 4: Left: AUC w.r.t. the log value of the weights of ğ¿2 normalization log(ğœ†). Right: AUC w.r.t. the î€›xed-size number of neighbors ğ‘€ in LCSAN. the KGAT in all experiments, and it indicates the eî€ectiveness of conditional attention for reî€›ning the knowledge graph to capture the local user preference. â€¢CKE is the worst one in most cases. It demonstrates that directly using the knowledge graph as a regularization can ğ‘Ÿ0.011ğ‘Ÿ0.031 ğ‘Ÿ0.021ğ‘Ÿ0.050ğ‘Ÿ0.050 not make full use of the knowledge graph. Besides, the NFM outperforms CKE because it preserves the second-order similarity implicitly by the input of the cross feature. â€¢KGAT achieves better performance than CKE, NFM, Ripple in Last-FM and Yelp datasets. It veriî€›es the eî€ectiveness of propagating information in the knowledge graph to preserve global similarity. â€¢In MovieLens dataset, CKE, NFM, KGAT have poorer performance than the performance in the other two datasets. The reason is that MovieLens dataset has very few users and items, and a large number of knowledge graphs, it is more diî€œcult to exploit knowledge graph eî€ectively in this dataset. However, Ripple achieves a good performance in this dataset. Ripple uses paths from an item in a userâ€™s history to a candidate item, and the path-based methods can capture the local preference. It indicates the importance of preserving local preference instead of the whole knowledge graph, especially in a massive knowledge graph. Moreover, the improvements of our method KCAN over Ripple may prove that the way we distill the knowledge graph is better than than the way using multiple paths. In this part, we evaluate how diî€erent parts of KCAN aî€ect the performance. To study their respective eî€ects, we compare our KCAN with three variants: (1)KCAN. It removes the LCSAN layer from KCAN. (2)KCAN. It removes the KAGCN layers and uses the knowledge graph embedding as the input of LCSAN. (3)KCAN. It removes both the LCSAN and KAGCN. The result is shown in Table 4. TheKCANperforms the worst among the three variants, it demonstrates the two layers, KAGCN and LCSAN, are beneî€›cial for knowledge-aware recommendations. This conclusion has been double veriî€›ed since the three variants are worse than the KCAN. Besides, the KCANoutperforms theKCAN, which demonstrates the necessity of distilling and reî€›ning the knowledge graphs. Table 4: Eî€ects of KAGCN, LCSAN on top-K recommendation. Figure 5: The loss w.r.t the number of epoches. In the section, we evaluate the scalability and how diî€erent settings of hyper-parameters aî€ect the performance of KCAN. Especially, we evaluate the eî€ect of the weights ofğ¿2 normalizationğœ†and the î€›xed-size number of neighborsğ‘€in LCSAN. Besides, we also measure the convergence of the alternative optimization in this part. For brevity, we only report the AUC results with Last-FM datasets, and similar trends can be observed on the other datasets. 4.5.1 Weight ofğ¿2 normalizationğœ†. We show how the weights of ğ¿2 normalizationğœ†aî€ect the performance in Figure 4 Left. Theğœ† varies from{10,10,10,10,10}. Whenğœ†increases from 10to 10, the performance is improved, demonstrating that the ğ¿2 normalization can avoid overî€›tting to some extent. Whenğœ† increases from 10to 10, the performance becomes poorer. It makes sense since the normalization is much larger than the losses which need to be optimized in this situation. 4.5.2 The fixed-size number of neighborsğ‘€. In the target-speciî€›c sampling step, we sample a î€›xed-size set of neighbors to distill the knowledge graph and reduce the training time. We vary the î€›xed-size number of neighborsğ‘€from{5,10,20,30}. The result is shown in Figure 4 Right. We can see that the performance raises î€›rstly when the î€›xed-size number of neighborsğ‘€increases. This is reasonable because a largerğ‘€can embody more information in the subgraphs. Afterğ‘€larger than 20, the curve is relatively stable. It demonstrates that our algorithm is not very sensitive to ğ‘€. Besides, we can î€›nd that a smallğ‘€, such as 20, can also achieve a relatively good result. 4.5.3 The convergence of the alternative optimization. In the KCAN model, an alternative optimization is used to optimize the loss of knowledge graph embeddingLand the loss of target prediction L, respectively. In this part, we measures the convergence from the experiment by plotting the loss curve. The number of epoches varies from 1 to 200. The result is shown in Figure 5. We can see that the two curves descends very quickly, indicating the eî€œciency of the framework. About 100 epoches, the two loss are relatively stable, demonstrating our KCAN can converge fast. Besides, we can î€›nd that the loss curve ofLhas very tiny shakes. It is reasonable because the process of sampling target-speciî€›c subgraph will introduce sampling bias. In this paper, we investigate the problem of incorporating the knowledge graph into the recommender system. To address the knowledge graph distillation issue and knowledge graph reî€›nement issue, we propose a novel knowledge-aware graph convolutional network model named Knowledge-aware Conditional Attention Networks (KCAN). The framework consists of two main modules, the Knowledge Graph Distillation module, and the Knowledge Graph Reî€›nement module. We propagates embedding with knowledgeaware attention in a recursive way to capture the global similarity of entities. After that, a subgraph sampling strategy is designed to distill the knowledge graph based on the attention weight of the KAGCN. Also, the LCSAN propagates personalized information on the sampled subgraph based on local conditional attention for reî€›ning the knowledge graph. Beneî€›tting from the two layers, the proposed KCAN can eî€ectively preserve topology similarity and distill and reî€›ne the knowledge graph at the same time. Extensive experiments on three real-world scenarios are conducted to demonstrate the eî€ectiveness of our framework over several state-of-the-art methods. In the future, we aim to further reduce the time complexity of KCAN. Another interesting direction is to make the recommendation more explainable. This work was supported in part by CCF-Ant Group Research Fund.