Institute of Microelectronics of the Chinese Academy of Sciences Learning objectives of recommender models remain largely unexplored. Most methods routinely adopt either pointwise (e.g., binary cross-entropy) or pairwise (e.g., BPR) loss to train the model parameters, while rarely pay attention to softmax loss due to the high computational cost. Sampled softmax loss emerges as an eî€œcient substitute for softmax loss. Its special case, InfoNCE loss, has been widely used in self-supervised learning and exhibited remarkable performance for contrastive learning. Nonetheless, limited studies use sampled softmax loss as the learning objective to train the recommender. Worse still, none of them explore its properties and answer â€œDoes sampled softmax loss suit for item recommendation?â€ and â€œWhat are the conceptual advantages of sampled softmax loss, as compared with the prevalent losses?â€, to the best of our knowledge. In this work, we aim to better understand sampled softmax loss for item recommendation. Speciî€›cally, we î€›rst theoretically reveal three model-agnostic advantages: (1) mitigating popularity bias, which is beneî€›cial to long-tail recommendation; (2) mining hard negative samples, which oî€ers informative gradients to optimize model parameters; and (3) maximizing the ranking metric, which facilitates top-ğ¾performance. Moreover, we probe the model-speciî€›c characteristics on the top of various recommenders. Experimental results suggest that sampled softmax loss is more friendly to historyand graph-based recommenders (e.g., SVD++ [21] and LightGCN [13]), but performs poorly for ID-based models (e.g., MF [22]). We ascribe this to its shortcoming in learning representation magnitude, making the combination with the models that are also incapable of adjusting representation magnitude learn poor representations. In contrast, the history- and graph-based models, which naturally adjust representation magnitude according to node degree, are able to compensate for the shortcoming of sampled softmax loss. We will release our implementations upon acceptance. National University of Singapore, Singapore ,China,Tencent Music Entertainment Group,China Sampled Softmax Loss, Collaborative Filtering, Graph Neural Networks In recent years, studies on the recommender modeling have been extensively conducted. Many model architectures have been proposed to capture user preference from user-item interactions, covering multilayer perceptron [15], attention mechanism [14], and graph neural networks [13,44]. However, fewer eî€orts work on the learning objective â€” the way to train and optimize the model parameters. Speciî€›cally, most of recommenders cast the item recommendation problem into a supervised learning task, and adopt one of three learning objectives: â€¢ Pointwise loss. From the perspective of binary classiî€›cation or regression, it treats the observed user-item interactions as positive instances, while all unobserved items serve as weak negatives. For a user-item pair, it encourages the model prediction to î€›t the corresponding label, so as to approach user preference directly. Widely-used pointwise losses are binary cross-entropy [15, 34] and mean square error [12, 21]. â€¢ Pairwise loss. It accounts for the relative ordering of a positive item over an unobserved item. Numerically, it forces the model to score an observed item higher than its unobserved counterparts. Representative pairwise losses are BPR [32, 38] and WARP [39]. â€¢ Softmax loss. By applying a softmax function, it normalizes the model predictions over all items into a probability distribution. It then maximizes the probability of the observed items as compared with that of the unobserved items. Although aligning well with the ranking metrics that emphasize the top ranked positions [4,30], softmax loss is much less used in recommender systems. One possible reason is in its time complexity â€” in practice, the scale of items easily reaches millions or even larger in real-world scenarios, making the training cost unaî€ordable. Sampled softmax (SSM) lossemerges as a substitute for softmax loss. The basic idea is to use a sampled subset of negatives instead of all items. As such, it not only inherits the desired property of ranking, but also reduces the training cost dramatically. Current studies leverage SSM loss in recommendation mainly for two purposes: (1) Approximating softmax loss. Prior study [2] argues that SSM loss is a biased version of full softmax loss. One possible solution is the logğ‘„correction [2], which samples negative instances from the softmax distribution. Some follow-on eî€orts [1,3,29,37,41,43] devise diî€erent methods to reduce the sampling bias. (2) Performing contrastive learning. Inspired by its success in CV [6,9,36] and NLP [7,8,23] domains, researchers are exploring contrastive learning for recommendation [40,46,47]. They typically use InfoNCE loss [36] for the auxiliary task, maximizing the agreement of positive pairs as compared with that of negative pairs. Essentially, InfoNCE loss is SSM loss, since the observed and unobserved user-item pairs can be viewed as positive and negative instances respectively. Nonetheless, only very limited works [42,46] utilize SSM loss as the main learning objective for model training. Worse still, existing work rarely explores its properties, failing to answer the questions â€œDoes SSM loss suit for item recommendation?â€ and â€œWhat are the conceptual advantages of SSM loss, as compared with pointwise and pairwise losses?â€. In this work, we aim to better understand SSM loss for item recommendation. Firstly, we conduct theoretical analyses, identifying three model-agnostic advantages of SSM loss: â€¢ Alleviating popularity bias.Equipped with in-batch negative sampling, its approximated closed-form solution w.r.t. interactions is inversely proportional to item popularity in log scale, naturally suppressing the prediction scores of popular items. â€¢ Mining hard negative samples.It can discover hard negative samples, which contribute more informative and larger gradients to the optimization and representation learning. â€¢ Maximizing the ranking metric.Optimizing SSM loss is consistent with maximizing the Discounted Cumulative Gain (DCG) metric, which is more suitable for top-ğ¾ task. Empirically, we use SSM loss to train a high-performing recommender, LightGCN [13], on four benchmark datasets. Experimental results validate the superiority of SSM loss over the prevalent losses, in both normal and long-tail test scenarios. We further investigate how SSM loss behaves when applied on diî€erent types of models, so as to uncover itsmodel-speciî€›c characteristics. Without loss of generality, among collaborative î€›ltering (CF) family, we select matrix factorization (MF) [22], SVD++ [21], and LightGCN to represent ID-, history-, and graph-based CF methods, respectively. Experiments show that the loss achieves signiî€›cant performance improvements on SVD++ and LightGCN, but causes performance drop for MF. Scrutinizing these models, we î€›nd that MF has no mechanism to adapt the representation magnitude, as compared with SVD++ and LightGCN. Hence, we argue that SSM loss may be good at learning therepresentation directions, but falls short in adjusting therepresentation magnitudes. Moreover, we give theoretical supporting evidence about our argument. The history- and graph-based recommenders compensate for the weakness of SSM loss. In a nutshell, our contributions are summarized as follows: â€¢We present a comprehensive understanding of SSM loss and theoretically analyze its model-agnostic advantages: reducing popularity bias, mining hard negatives, and maximizing DCG metric. â€¢We point out the weakness of SSM loss in learning representation magnitudes, and show that history- and graph-based recommenders compensates for this weakness. â€¢Extensive experiments on four benchmark datasets demonstrate the superiority of SSM loss. Suppose we have a datasetDwhich consists ofğ‘€users,ğ‘items and|D|observed interactions. The goal of item recommendation is to recommend a user with a list of items she may be interested in. The noise contrastive estimation (NCE) [11] learns to discriminate between the observed data and some artiî€›cially generated noises. DenotingP= {ğ‘–, ğ‘–, Â· Â· Â· , ğ‘–}as the set of items userğ‘¢has adopted, for each observed interaction(ğ‘¢, ğ‘–), ğ‘– âˆˆ P, we sample another set of itemsN = { ğ‘—, ğ‘—, Â· Â· Â· , ğ‘—}from the pre-deî€›ned distributionğ‘as negative items. Denote byI = {ğ‘–, ğ‘—, ğ‘—, Â· Â· Â· , ğ‘—} the union of two sets{ğ‘–}andN, and assign to each tuple(ğ‘¢, ğ‘˜) âˆˆ {ğ‘¢} Ã—Ia binary class labelğ¶: ğ¶= 1ifğ‘˜ = ğ‘–andğ¶= 0ifğ‘˜ âˆˆ N. Since the ideal probability density function is unknown, we model the class-conditional probability ğ‘ (Â·|ğ¶= 1) with ğ‘(Â·), that is, ğ‘ (ğ‘¢, ğ‘˜ |ğ¶= 1) = ğ‘(ğ‘¢, ğ‘˜), ğ‘ (ğ‘¢, ğ‘˜ |ğ¶= 0) = ğ‘ The prior probabilities areğ‘ (ğ¶= 1) = 1/(1 + ğ‘ )andğ‘ (ğ¶= 0) = ğ‘ /(1 + ğ‘ ). Therefore, the posterior probabilities for each class are: Without loss of generality, letğ‘ (ğ¶= 1|ğ‘¢, ğ‘˜) = ğœ (ğ‘“ (ğ‘¢, ğ‘˜)), where ğœ (Â·)is the sigmoid function, andğ‘“ (ğ‘¢, ğ‘˜)measures the aî€œnity score between ğ‘¢ and ğ‘˜. As such, we can get: Note that among the(ğ‘ + 1)tuples, only one tuple is labeled as 1. Our goal is to maximize the following probability: ğ‘ƒ =ğ‘ (ğ‘¢, ğ‘–|ğ¶= 1)ğ‘ (ğ‘¢, ğ‘˜ |ğ¶= 0)Ãğ‘ (ğ‘¢, ğ‘— |ğ¶= 1)Ãğ‘ (ğ‘¢, ğ‘™ |ğ¶= 0) Usually, the negative-logarithm is applied for numerical stability, resulting in the objective function: L= âˆ’1|D|logexp (ğ‘“ (ğ‘¢, ğ‘–))exp (ğ‘“ (ğ‘¢, ğ‘–)) +Ãexp (ğ‘“ (ğ‘¢, ğ‘—)). (6) Here, we omit the regularization term for simplicity. Typically, we use the mini-batch stochastic gradient descent method, e.g., Adam [20], to optimize model parameters. To make full use of parallel computing of modern hardware, in-batch negative trick [16] is commonly adopted, that is, treating positive items of other users in the same batch as the negatives. In expectation, this equals to sampling based on empirical frequency of items in the dataset. Common choices of aî€œnity functionğ‘“ (ğ‘¢, ğ‘–)are cosine similarity or inner product of the representations ofğ‘¢andğ‘–with a temperature coeî€œcient. Recently, [19,40] have demonstrated that cosine similarity with temperature coeî€œcient is a better choice, since it endows SSM loss with the ability to mine hard negatives in a bounded interval. We will go deep into this property later. We should emphasize that throughout this paper, unless otherwise stated, SSM loss is equipped with temperature-aware cosine similarity function and in-batch negative sampling. In this section, we î€›rst prove in theory the properties of SSM loss. Then, we conduct empirical experiments to verify its eî€ectiveness on both normal and long-tail recommendation. 3.1.1Alleviating Popularity Bias. Intuitively, in SSM loss, items with larger frequency are more likely to be involved in a batch. As a consequence, popular items are prone to be penalized as negatives, preventing the model from recommending them to some degree. Here, we prove this statement theoretically. LetLbe the loss on userğ‘¢andğ‘™ (ğ‘¢, ğ‘–)be the individual loss term of positive sample (ğ‘¢, ğ‘–), that is,îƒ• L=ğ‘™ (ğ‘¢, ğ‘–) The gradient ofLw.r.t. ğ‘“ (ğ‘¢, ğ‘–) is as follows: =ğœ•ğœ•ğ‘“ (ğ‘¢, ğ‘–)log1 + ğ‘Eexp (ğ‘“ (ğ‘¢, ğ‘—) âˆ’ ğ‘“ (ğ‘¢, ğ‘˜)). (8) Here, we use the law of large numbers, changing the sum operation to the expectation.ğ‘is the distribution of negative samples which is predeî€›ned. Note that there are two cases whereğ‘“ (ğ‘¢, ğ‘–)is involved: (1)ğ‘˜ = ğ‘–, that is, positive sample is(ğ‘¢, ğ‘–). Note that with probability ğ‘(ğ‘–), itemğ‘–is sampled as a negative item in this case. As such, the gradient of this part is âˆ’1 +1 + ğ‘ ğ‘(ğ‘–)exp (ğ‘“ (ğ‘¢, ğ‘–)) + ğ‘Eexp (ğ‘“ (ğ‘¢, ğ‘—))exp (ğ‘“ (ğ‘¢, ğ‘–)). (9) (2)ğ‘˜ â‰  ğ‘–butğ‘— = ğ‘–, that is, only negative sample is(ğ‘¢, ğ‘–). In this part, the gradient is By adding Equations(9)and(10)together, we obtain the total gradient ofLw.r.t.ğ‘“ (ğ‘¢, ğ‘–). We further enforce the total gradient to be zero and obtain the nearly closed-form solution of ğ‘“ (ğ‘¢, ğ‘–): Here we use the fact thatexp (ğ‘“ (ğ‘¢, ğ‘˜)) â‰ª ğ‘Eexp (ğ‘“ (ğ‘¢, ğ‘—)), ğ‘˜ âˆˆ Pwhenğ‘ â†’ âˆ. Notice that the largerğ‘(ğ‘–)is (i.e., the more popular the item is), the smallerğ‘“(ğ‘¢, ğ‘–)will be. This admits that SSM loss has the potential to suppress popularity bias. We will conduct experiments to verify this in Section 3.2. 3.1.2Hard Negative Mining. In [19,40], the authors analyzed that the InfoNCE loss in self-supervised learning is able to perform hard negative mining. Due to the similar formulae of SSM and InfoNCE, we î€›nd SSM loss exhibits similar power in mining hard negative items when learning user representation. Formally, we denote the î€›nal representations of userğ‘¢and item ğ‘–asğ‘§andğ‘§respectively and adopt cosine similarity with temperature coeî€œcientğœto measure the agreement betweenğ‘¢andğ‘–, that is, ğ‘“ (ğ‘¢, ğ‘–) =, whereğ‘ andğ‘ are the normalized representations, i.e.,ğ‘ =, ğ‘ =. Then the gradient ofğ‘™ (ğ‘¢, ğ‘–)w.r.t. the user representation ğ‘§is as follows: where, Then, the magnitude contribution of negative itemğ‘— âˆˆ Nis proportional to the following term: As analyzed in [40], by setting a properğœ(empirically in the range of 0.1 to 0.3), hard negative items oî€er much larger gradients to guide the optimization, thus making learned representations more discriminative and accelerating the training process. 3.1.3Maximizing Discounted Cumulative Gain. Discounted Cumulative Grain (DCG) is a widely-adopted ranking metric which uses a graded relevance scale to calculate the utility score. The contribution to the utility from a relevant item reduced logarithmically proportional to the position of the ranked list, which mimics the behavior of a user who is less likely to examine items at larger ranking position. Formally, DCG is deî€›ned as follows: whereğœ‹is a ranked list overIinduced byğ‘“for userğ‘¢;ğ‘¦is a binary indicator:ğ‘¦= 1if itemğ‘–has been interacted byğ‘¢, otherwise ğ‘¦= 0;ğœ‹(ğ‘–)is the rank ofğ‘–. It is worth noting that, in practice, there may be more than one item that has been interacted byğ‘¢in I. Inspired by [4], we derive the relationship between SSM loss and DCG:îƒ• Based on the fact I(ğ‘¥ > 0) â‰¤ exp (ğ‘¥), we have: âˆ’ log ğ·ğ¶ğº (ğœ‹, ğ‘¦) â‰¤ âˆ’ log1î€î€‘ As the factlog(1 + ğ‘¥) â‰¤ ğ‘¥, we can safely get that:âˆ’ log ğ·ğ¶ğºis a lower bound of SSM loss, minimizing SSM loss is equivalent to maximizing DCG of I. 3.2.1 Experimental Seî€ings. To justify the superiority of SSM loss, we use LightGCN [13] as the backbone model and compare SSM loss with diverse losses: â€¢BCE Loss: A widely used pointwise loss which is formulated as: L= âˆ’log ğœ (ğ‘“ (ğ‘¢, ğ‘–)) âˆ’log(1 âˆ’ ğœ (ğ‘“ (ğ‘¢, ğ‘— ))) â€¢ BPR Loss: The standard objective function in vanilla LightGCN, which is deî€›ned as follows: â€¢SM Loss: It is short for softmax loss which maximizes the probability of the observed items normalized over all items by softmax function, that is: L= âˆ’1|D|logexp (ğ‘“ (ğ‘¢, ğ‘–))Ãexp (ğ‘“ (ğ‘¢, ğ‘—))(20) â€¢CCL Loss [25]: Itâ€™s a contrastive loss proposed most recently by maximizing the cosine similarity of positive pairs, while minimizing the similarity of negative pairs below a certain margin: L= âˆ’1|D|ï£¯ï£¯1 âˆ’ ğ‘“ (ğ‘¢, ğ‘–) +ğ‘¤|N |(ğ‘“ (ğ‘¢, ğ‘—) âˆ’ ğ‘š)ï£ºï£º where(Â·)indicatesmax(0, Â·),ğ‘šis the margin for similarity score of negative samples,ğ‘¤is a hyper-parameter to balance the loss terms of positive samples and negative samples. Detailed comparison between SSM loss and competing losses is attached in Appendix A. We conduct extensive experiments on four benchmark datasets: Gowalla [13,38], Yelp2018 [13,38], AmazonBook [13,38] and Alibaba-iFashion [40]. Following [13,38], we use the same 10-core setting for the î€›rst three datasets. For AlibabaiFashion, as processed in [40], we randomly sample 300k users and collect all their interactions over the fashion outî€›ts. The statistics of all four datasets are summarized in Table 1. We follow the same strategy described in [38] to split the interactions into training, validation and test set with a ratio of7 : 1 : 2. For users in the test set, we follow the all-ranking protocol [13,38] to evaluate the top-K recommendation performance and report the average Recall@20 and NDCG@20. 3.2.2 Hyper-parameter Seî€ings. For fair comparison, all methods are trained from scratch which are initialized with Xavier [10]. In line with LightGCN, we î€›x the embedding size to 64 and optimize it via Adam [20] with the default learning rate of 0.001 and default mini-batch size of 2048. Theğ¿regularization term is added as in LightGCN, with the coeî€œcient in the range of{1ğ‘’, 1ğ‘’, Â· Â· Â· , 1ğ‘’}. The layer combination coeî€œcient is set towhereğ¾is the number of GCN layers, which is searched in range of{1, 2, 3, 4}. For BCE loss, we randomly sample non-observed interactions to form the negative setDin each training epoch, the ratio of positive to negative is set to1 : 4. For BPR loss, we randomly sample a noninteracted item of user as negative for each observed interaction. For SM loss, we examine both cosine similarity and inner product similarity between user and item representations, and report the best performance. For CCL loss, since the authors have reported the results on Yelp2018 and Amazon-Book datasets under the same experimental setting, we just copy their results. While for the results of the rest two datasets which are not reported in the original paper [25], we report the best results based on our implementations. As suggested in the paper, we tune the marginğ‘šamong0.1 âˆ¼ 1.0 at an interval of 0.1 and weightğ‘¤in range of{1, 150, 300, 1000}. The number of negative samples is searched from 1 to 2000. While for SSM loss, we î€›nd cosine similarity always leads to better performance. Since the temperature coeî€œcientğœin both SM loss and SSM loss is of great importance [40], we use the following strategy to tune it: we î€›rst perform grid search in a coarse grain range of {0.1, 0.2, 0.5, 1.0}, then in a î€›ner grain range, which is based on the result of coarse tuning. For example, if the model achieves the best performance whenğœ = 0.2in the î€›rst stage, then we tune it in the range of{Â· Â· Â· , 0.16, 0.18, 0.22, 0.24 Â· Â· Â· }in a î€›ne-grained. The reported results are the average value of 5 runs. 3.2.3 Performance Comparison. We report the overall results in Table 2. We have the following observations: â€¢Among the compared losses, BCE Loss performs worst on all four datasets, which suggests the limitations of pointwise objective. Speciî€›cally, as it approaches the task as a binary classiî€›cation machine learning task, î€›tting the exact value of labels may introduce noise in item recommendation. BPR loss consistently performs better than BCE Loss, verifying the superiority of pairwise objective to capture relative relations among items. Both SM Loss and CCL Loss have signiî€›cant gains over BCE Loss and BPR Loss on Yelp2018 and Amazon-Book datasets. A common point of SM Loss and CCL loss is that they both compare one positive sample with multiple negative samples. This suggests that enlarging the number of negative samples during training is beneî€›cial to representation learning. On Gowalla and Alibaba-iFashion datasets, SM Loss and CCL loss have on-par performance compared to BPR Loss. â€¢SSM Loss performs best across the board, empirically verifying the advantages of SSM loss for implicit item recommendation. We attribute the improvements to: (1) Comparing with pointwise and pairwise loss, SSM loss uses multiple negatives for each positive sample at every iteration. In addition, with a proper temperature coeî€œcient, SSM loss beneî€›ts from dynamic hard negative mining (see Section3.1.2), in which the hard negative items oî€er larger gradients to guide the optimization. Moreover, SSM loss has strong connection with DCG ranking metric as we have analyzed in Seciton 3.1.3 â€” meaning that SSM loss directly optimize the goal of model evaluation. (2) Compared with SM loss, SSM loss only uses a fraction of items from the whole item set for optimization, this greatly alleviates the training cost and diî€œculty, especially when the set of whole items goes extreme large. An evidence supporting this assertion is that the improvements of SSM loss over SM loss become more signiî€›cant on two larger datasets (i.e., Amazon-Book and Alibaba-iFahsion) than smaller ones (i.e., Gowalla and Yelp2018). Another possible reason lies in that by introducing randomness, SSM loss can avoid the phenomenon that a few extremely hard negative items dominant the optimization in SM loss. Moreover, we conduct the signiî€›cant test, whereğ‘-value < 0.05 indicates that the improvements of SSM loss over BPR loss are statistically signiî€›cant in all cases. â€¢Itâ€™s worth mentioning that through our empirical experiments, we î€›nd that SSM loss converges much faster than BPR loss. We also ascribe such eî€œcient training to the superiority of hard negative mining. On the contrary, for BPR loss, we randomly sample an unobserved item as negative, which is more likely to be an easy negative sample â€” that is, the positive observation has larger predicted score than that of the selected negative sample and thus making the gradient magnitude vanishing [31]. 3.2.4 Long-tail Recommendation. As analyzed above, SSM loss is of promise to alleviating the popularity bias. To verify this property, we follow [40], splitting items into ten groups based on the frequency meanwhile keeping the total number of interactions of each group the same. The items in group with larger GroupID have larger degrees. As such, we decompose the Recall@20 metric of the whole dataset into ten parts, each of which represents the contribution of a single group as follows: whereğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™measures the recommendation performance over theğ‘”-th group,ğ‘™andğ‘™are the items in the top-ğ¾recommendation list and relevant items for userğ‘¢, respectively. We report the results in Figure 1 and have the following observations: â€¢Implemented on LightGCN recommender, BCE loss, BPR loss and SM Loss all show strong inclination to recommend high-degree items, while leaving less-popular relevant item seldom exposed. We should emphasize that the most popular group, i.e., the 10-th group only contains less than 1% of item spaces (0.65%, 0.83%, 0.83%, 0.22% respectively) but contributes more than 35% of the total Recall scores in most cases on four datasets. On the contrary, the least popular group, i.e., the 1-st group contains most of item spaces (25.51%, 35.70%, 31.82%, 64.43% respectively) however contributes less than 3% of the total Recall scores in most datasets. This indicates that paired with LightGCN recommender, BCE Table 3: Performance comparison among diî€erent recommendation models using BPR loss and SSM loss. RI stands for relative improvement of SSM loss over BPR loss under same recommender. loss, BPR loss and SM Loss hardly learn high-quality representations for long-tail items. As such, the recommendation models suî€er from popularity bias. Whatâ€™s worse, the feedback loop of recommender system will further intensify popularity bias over time, resulting in Matthew eî€ect [5]. â€¢CCL loss and SSM loss perform well on long-tail groups (those with smaller group ID), showing the potential in alleviating the popularity bias. This is consistent with our analyses in Section 3.1.1. Making comparisons between CCL and SSM loss, we î€›nd CCL loss performs better in the groups with the smallest group ID on three out of four datasets. In contrast, SSM loss exhibits stronger competitiveness in the waist and head groups. These admit that SSM loss balances well on normal and long-tail recommendation tasks, that is, SSM loss can not only promote the exposure of less popular items but also guarantee overall performance. Surprisingly, for SSM loss, the contribution of each group on Amazon-Book is nearly uniformly-distributed. This again justiî€›es that, by suppressing the predicted scores of popular items, the representation learning sheds more lights on long-tail items, so as to establish better representations of these items. â€¢Another interesting observation is that: for long-tail groups, the performance rank of losses typically exhibits such order: pointwise loss < pairwise loss < setwise loss; however, for the most popular group, the loss order reverses: setwise loss < pairwise loss < pointwise loss. This suggests that the limited expressiveness of binary pointwise loss and pairwise loss cannot fully capture user preference towards items, instead, they adopt a conservative fashion by recommending the popular items. In this section, we î€›rst conduct experiments of SSM loss paired with diî€erent recommender models on diî€erent benchmark datasets. Based on the experimental results, we then study the performance discrepancies of SSM loss w.r.t. diî€erent recommenders. In the previous section, we have veriî€›ed the superiority of SSM loss when using LightGCN as the backbone model. Here, we want to check if SSM loss is consistently a good choice for diî€erent collaborative î€›ltering models. Toward this goal, besides LightGCN [13] which is a graph-based recommender, we additionally select the following models as the backbones: â€¢MF [22]: Itâ€™s an ID-based CF model which directly projects the single ID of a user/item into a latent embedding. â€¢SVD++ [21]: Itâ€™s a history-based CF model which takes into consideration the userâ€™s historical interactions for user representation learning. It can be regarded as a single-side single-layer GCN model. Formally, we can formulate it from a message passing perspective as follows: whereğ‘andğ‘are the ID embeddings for userğ‘¢and itemğ‘–, whileğ‘andğ‘are the î€›nal representations forğ‘¢andğ‘–respectively;Pis the set of users who have interacted withğ‘–;ğ›¼andğ›¼ are the normalization factor. For the vanilla SVD++,ğ›¼is î€›xed as 0. We tune ğ›¼in range of {0, 0.5, 1.0}. In Table 3, we report the results of diî€erent CF models optimized with BPR loss and SSM loss. We can î€›nd that: â€¢Similar to LightGCN, SVD++ paired with SSM loss has signiî€›cant performance improvements over the combination of SVD++ and BPR loss. This again veriî€›es the superiority of SSM loss for implicit item recommendation. However, we observe performance drop when training MF via SSM loss as compared to BPR loss. This admits that, SSM loss is more friendly to model history-based methods and graph-based methods, but may not suit for models like ID-based methods. We argue that the key lies in message passing since both SVD++ and graph-based methods adopts message passing technique to reî€›ne representations. We will make in-depth analyses on this problem later, from the perspective of learning representation magnitude. â€¢Focusing on BPR loss or SSM loss only, we î€›nd that as the recommender becomes more complicated, the performance improves gradually. This is in line with our intuition that adding GCN layers will capture higher-order collaborative signal which is of beneî€›t to recommendation. However, as we can see, the combination of SVD++ and SSM loss outperforms the combination of LightGCN and BPR loss. This suggests that BPR loss is less Table 4: Performance Comparison among diî€erent similarity function combination during training and testing phase. â€˜IPâ€™ indicates â€™Inner Productâ€™ similarity while â€˜COSâ€™ indicates â€˜Cosineâ€™ similarity. We report Recall@20 metric on four datasets. eî€ective to mine user preference underlying observed interactions. In contrast, the performance gain provided by SSM loss is larger than that from adding GCN layers, which further justiî€›es its eî€ectiveness. Moreover, we î€›nd some interesting phenomenons for SSM loss using LightGCN as base model, as shown in Table 4. Speciî€›cally, we adopt diî€erent similarity measurement combinations for training and testing â€” we use either Inner Product similarity or Cosine similarity in SSM loss during training, and use one of them to make predictions during testing, resulting in four diî€erent cases. Empirically, we î€›nd that: (1) Cosine similarity is a better choice than Inner-Product similarity for SSM loss during training. This is consistent with our analysis in Section 3.1.2: equipped with cosine similarity, SSM loss is capable of performing hard negative mining. A nice property of Cosine similarity is that the similarity value is bounded in the interval[âˆ’1, 1], making it easier to train. (2) Comparing with Cosine similarity, Inner Product similarity performs better for SSM loss during testing. The underlying reason is that besides the angel between user and item representations, Inner Product similarity additionally considers the magnitude of two representations, thus making full use of the learnt representations for prediction. Based on the aforementioned empirical studies and a general idea that a high quality representation should have a proper direction and a proper magnitude, we conjecture that SSM loss falls short in learning proper magnitude of representations, since it uses cosine similarity which cares only about the angle between two latent vectors while neglecting the impact of their magnitude. Note that MF merely projects ID into latent embedding, it does not provide any design to adjust the magnitude of representations. As such, the combination of MF and SSM loss can not compensate for the î€aws in adjusting the magnitude of representations and hence performs poorly. On the contrary, as we will prove in the following section, a message-passing method like SVD++ or GCN is capable of adjusting the magnitude of node representations inherently. Therefore, paired with a message-passing method, SSM loss will lead to excellent performance. Without loss of generality, we suppose the message passage rule in user-item interaction graph is deî€›ned as: We here only analyze from the item side, since the user side can be derived in the same way. Suppose the degree of user node|P|and item node|P|follow Pareto distribution [27] with parameter(ğ›¼, ğ‘¥= 1),ğ‘is initialized with some distribution D (ğœ‡, ğœ), that is E(|P|) =E(|P|) =ğ›¼ğ›¼ âˆ’ 1,V(|P|) =V(|P|) =ğ›¼(ğ›¼ âˆ’ 1)(ğ›¼ âˆ’ 2) whereE(Â·)andV(Â·)are the expectation and variance, respectively. Therefore, for a given itemğ‘–, the expectation of the square of its magnitude ğ‘is We leave the detailed derivation of Equation(26)in Appendix C.1 due to the limited space. Since|P|andğ‘are independent, we have therefore, where, Specially, whenğ›¼= 0.5, ğ›¼= 0.5is the case of LightGCN,E((ğ‘)) monotonically linearly increases as |P| increases. As analyzed above, the normalization factorğ›¼andğ›¼, which determine the message passing rule, play an important roles in adjusting the magnitude of learnt representations. To see howğ›¼andğ›¼inî€uence the ranking performance, we adopt SVD++ that can be viewed as the simplest message passing based method as the backbone, and train it using SSM loss. Note that the vanilla SVD++ only propagate messages on the user side, here we introduce a variant that only propagates messages on the item side. Similar to Equation(23), we deî€›ne the propagation rule of the variant of SVD++ on the item side as follows: Table 5 shows the Recall@20 metrics on Yelp2018, Amazon-Book and Alibaba-iFashion datasets. We have the following observations: â€¢With diî€erentğ›¼andğ›¼, the ranking performance varies a lot. Typically, compared withğ›¼, changing the value ofğ›¼will cause larger performance î€uctuation. This is consistent with our analyses thatğ›¼determines the order of the magnitude whileğ›¼ inî€uences its multiplication factor. â€¢The best choice ofğ›¼andğ›¼is diî€erent for user-side SVD++ and item-side SVD++. Speciî€›cally, on all three datasets, the best performance of user-side SVD++ is achieved whenğ›¼= 1.0and ğ›¼= 0. While for item-side SVD++,ğ›¼= ğ›¼= 0.5is the best option across three datasets. â€¢On Yelp2018 and Amazon-Book datasets, item-side SVD++ can obtain better performance than user-side SVD++, which is in contrast to the case on Alibaba-iFashion. Table 5: Impact of message passing strategy. â€™Userâ€™ indicates propagating messages on the user side, while â€™Itemâ€™ indicates propagating messages on the item side. In this section, we review the popular objectives for item recommendation which cast the task into a supervised machine learning problem. Many traditional item recommendation methods perform learning by minimizing thepointwisedivergence of the reconstructed interaction matrix from the observed interaction matrix. According to the way the divergence is deî€›ned, pointwise losses can be further categorized into mean square error loss [17,21,26,45], binary cross entropy loss [15,34], and hinge loss [33], to name a few. However, for item recommendation, the primary goal is not to score the interaction exactly to 1 or 0. Instead, the relative ordering of a positive item over an unobserved item is of central importance. Toward this goal,pairwiselosses are proposed to optimize the preference structure consistency between the reconstructed interaction matrix from the observed interaction matrix. Speciî€›cally, pairwise losses treat training data as a set of triplet instances{(ğ‘¢, ğ‘–, ğ‘—)}to capture the intention that userğ‘¢prefers to positive itemğ‘–over irrelevant itemğ‘—. BPR [32] is one of the most popular pairwise losses in item recommendation which is proven to optimize AUC scores. WARP [39] is another pairwise algorithm customized for item recommendation. It encourages the predicted score of positive item larger than that of negative item above a margin, which is in line with the hinge loss. Similarly, Park et al. [28]proposed a max-margin hinge ranking loss to minimize the ranking risk in the reconstructed matrix. Kabbur et al. [18]devised a pairwise MSE loss which computes the relative diî€erence between the actual non-zero and zero entries and the diî€erence between their corresponding predicted entries. Besides pointwise losses and pairwise losses, a third option for item recommendation is to model the recommendation predictions over all items into a probability distribution which is normalized using a softmax function, termedsoftmaxloss [30]. Bruch et al. [4] veriî€›ed that softmax loss aligns well with the ranking metrics. However, calculating the partition function of softmax is computational costly. A substitute for softmax loss is sampled softmax loss which reduces the computational cost by employing the partition function only on a small subset of negatives. However, common view on sampled softmax loss is that itâ€™s a biased version of full softmax loss which can be corrected by logğ‘„correction [2]. Agreeing with this point of view, some follow-on works in recommendation devise diî€erent methods to get unbiased estimation[1,3,37,41,43]. For example, Blanc and Rendle[3]proposed a divide and conquer algorithm to estimate the proposal distribution with kernel based sampling strategy. Yi et al. [43]presented a method to estimate item frequency for streaming data without requiring î€›xed item vocabulary. Wang et al. [37]devised a cross-batch negative sampling with the FIFO memory bank to improve the accuracy of estimating item distribution via enlarging the number of negative samples. Most recently, a surge of works introduced contrastive loss like InfoNCE loss into recommendation [25,35,40,46]. At its core is maximizing the agreement of positive pairs as compared with that of negative pairs. Interestingly, when directly utilizing the supervision signal (whether the interaction is observed) of the data to construct positive and negative pairs, InfoNCE loss becomes sampled softmax loss. However, to the best of our knowledge, only very limited works [24,35,46] utilized sampled softmax loss as their main task objective to train recommendation model. Zhou et al. [46]proved that both contrastive loss and multinomial IPW loss optimize the same objective in principle but failed to answer why does contrastive loss work. Liu et al. [24]presented a debiased contrastive loss to remedy the negative eî€ects of false negative samples in the randomly sampled subset. Tang et al. [35]is mostly related to our work which also realized the superiority of using sampled softmax loss to train recommender. However, it diî€ers from our work in: (1) [35] only empirically veriî€›ed the eî€ectiveness of sampled softmax loss for graph-based recommender. On the contrary, we reveal theoretically three advantages of sampled softmax loss, that is, mitigating popularity bias, mining hard negative samples and maximizing the ranking metrics; (2) We also uncover the shortcoming of sampled softmax loss in adjusting the representation magnitudes and present theoretical evidence to support our argument. In this work, we present insightful analyses of sampled softmax loss for item recommendation. Speciî€›cally, we î€›rstly disclose theoretically model-agnostic advantages of sampled softmax loss in mitigating popularity bias, mining hard negative samples and maximizing ranking metrics. We then probe the model-speciî€›c characteristics of sampled softmax loss and point out its potential shortcoming in adjusting representation magnitudes. To justify our argument, we further present theoretical support that message passing based methods are capable of adjusting magnitude. We conducted extensive experiments on four benchmark datasets, demonstrating the superiority of training history- and graph-based models using sampled softmax loss for both normal and long tail recommendation tasks. We believe the comprehending of SSM loss is inspirational to future developments of recommendation community. In future work, we would like to further improve the drawbacks of SSM loss, making it more powerful for item recommendation. Moreover, since practical recommender systems usually involve many useful features, exploring the potential of SSM loss for feature-based CTR task is another promising direction.