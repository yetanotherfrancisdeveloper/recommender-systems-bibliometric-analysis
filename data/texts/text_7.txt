Recent studies in recommender systems have managed to achieve signiî€›cantly improved performance by leveraging reviews for rating prediction. However, despite being extensively studied, these methods still suî€er from some limitations. First, previous studies either encode the document or extract latent sentiment via neural networks, which are diî€œcult to interpret the sentiment of reviewers intuitively. Second, they neglect the personalized interaction of reviews with user/item, i.e., each review has diî€erent contributions when modeling the sentiment preference of user/item. To remedy these issues, we propose aSentiment-awareInteractive FusionNetwork (SIFN) for review-based item recommendation. Speciî€›cally, we î€›rst encode user/item reviews via BERT and propose a light-weighted sentiment learner to extract semantic features of each review. Then, we propose a sentiment prediction task that guides the sentiment learner to extract sentiment-aware features via explicit sentiment labels. Finally, we design a rating prediction task that contains a rating learner with an interactive and fusion module to fuse the identity (i.e., user and item ID) and each review representation so that various interactive features can synergistically inî€uence the î€›nal rating score. Experimental results on î€›ve real-world datasets demonstrate that the proposed model is superior to state-of-the-art models. ACM Reference Format: Kai Zhang, Hao Qian, Qi Liu, Zhiqiang Zhang, Jun Zhou, Jianhui Ma, Enhong Chen. 2021. SIFN: A Sentiment-aware Interactive Fusion Network for Review-based Item Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3459637.3482181 Recommender Systems (RS) have been widely adopted by many platforms, e.g. Amazon and Yelp, thanks to their capability of î€›ltering information based on the user interests [5, 16]. In these platforms, user reviews of items contain rich semantic information. Therefore, to better estimate userâ€™s rating, review-based recommendation has gained wide attention in both academia [1] and industry [7, 12]. In the literature, there are many eî€orts for this recommendation problem, especially on the rating prediction task. Among them, the most representative methods are based on Matrix Factorization (MF) model, which decomposes the user-item rating matrix into two matrices corresponding to user and item features. However, these methods represent user and item information only based on ratings, which suî€er from sparsity issue. To mitigate this problem, many RS have been proposed to exploit the semantic information from user reviews [1,2,13,14,18]. Among these review-based methods, the earlier works adopt MF to extract the semantics from user/item reviews, such as PMF[8] and ConvMF+ [6]. Recently, the focus of research shifts to learn latent features from reviews via neural network methods. Among them, DeepCoNN [18] applies convolution to process the reviews to learn user and item representations. D-Attn [12] leverages global and local attention to select decisive words in the review documents for rating prediction. NARRE [2] is a review retrieval model that adopts an attention mechanism to select appropriate reviews for items. Moreover, CARP [7] utilizes a sentiment capsule network to estimate user-item ratings and provide interpretability with a î€›ne-grained manner. Although these works achieved signiî€›cant performance improvement in the review-based recommendation, they still suî€er from intrinsic issues. First, most previous works directly encode reviews to extract implicit semantics while largely ignoring the explicit sentiment polarity of reviews [12,14], which carry the user attitudes and preferences (i.e., which kinds of item user may like or dislike). Therefore, implicitly mining the semantic information of reviews may lead to sub-optimal prediction because the reviewsâ€™ sentiment label has not been applied to the training process [4,10,14,17]. Second, in the rating prediction scenario, reviewsâ€™ sentiment polarity plays distinct roles in representing the overall sentiment preference of the user/item. For instance, the impact of a negative review may be far more signiî€›cant than a positive review for the user/itemâ€™s semantic preference representation. Therefore, it is necessary to weigh and fuse the interactions between each review and user/item features for the î€›nal rating prediction. Figure 1: (a) The overall architecture of SIFN model; (b) the rating learner which can exploit interactive fusion knowledge. To address these aforementioned issues, we propose aSentimentaware Interactive Fusion Network (SIFN), which includes a sentiment prediction task to exploit the sentiment polarity from each review, and a rating prediction task to estimate user rating of items. Speciî€›cally, we î€›rst utilize the pre-trained BERT to encode text reviews and introduce a light-weighted sentiment learner to mine the sentiment-aware features. Then, we design a sentiment prediction task that explicitly extracts important sentiment features in each review for user/item. Finally, in the rating prediction task, we develop a rating leaner which contains two novel operations (i.e., interactive & fusion) to fuse the identity representation of user/item and review representation so that various interactive features can synergistically aî€ect the î€›nal rating prediction. The main contributions of this paper are as follows: 1) We highlight the explicit sentiment polarity in each review, and focus on modeling the multiple feature interactions between each review and user/item. 2) We propose a novelSentiment-awareInteractiveFusionNetwork (SIFN) model with two main components, Sentiment Leaner and Rating Learner. 3) We conduct extensive experiments on î€›ve datasets so that the results demonstrate the eî€ectiveness of the proposed method. Figure 1 illustrates the overall architecture of SIFN, which mainly consists of three components. The details of each component will be explained in the following sections. In this section, we present how to extract deep semantic representations of reviews progressively. Since the semantic learning process of reviews for user and item are quite similar, we mainly introduce user reviewsâ€™ semantic representation for space saving. BERT Encoding.Given a user reviewx= (ğ‘¤, ğ‘¤, ..., ğ‘¤) âˆˆ Rwhich is formed by merging the totalğ‘™words in a review written by a user. For word embedding, we adopt the pre-trained BERT [3] to encode each word and use the last hidden state of the pre-trained BERT for word representations [15]. The formulation is as follows: R= (e, e, ..., e) = BERT(ğ‘¤, ğ‘¤, ..., ğ‘¤),(1) whereRâˆˆ Ris the embedding of a user review,eâˆˆ Ris the embedding of theğ‘™-th word, whereğ‘˜is the word embedding size. Similarly, an item review can be mapped asR= (ğ’†, ğ’†, ..., ğ’†) âˆˆ R. With the powerful pre-trained models, the general semantic knowledge of the text reviews can be fully extracted. Sentiment Learner.As each word in a review carries distinct semantic and sentimental information, which is crucial to estimate user rating and sentiment polarity. Therefore, to better suit the downstream tasks, we devise a sentiment learner to adaptively underline the informative words. Speciî€›cally, we employ the attention mechanism, in which the attention score ofğ‘–-th word is calculated as: Wandbare weight matrix and bias, respectively.ğ›¼is attention score ofğ‘–-th word in a review. Therefore, words with smaller attention score are less important. Further, with a weighted sum pooling over the words in a review, the eî€ects of uninformative words are diminished and the semantic representation of the user review sâˆˆ Ris calculated as: Through the encoding module, we obtain each reviewâ€™s representation of the user, i.e.,s, via aggregating feature vectors of the words. Similarly, we use the same method to generate each reviewâ€™s representation, i.e., sâˆˆ R, for the item. Since the sentiment polarity of review carries the intrinsic user preference toward item, we design a sentiment prediction task for each review to learn sentiment features explicitly (e.g.,sands). Note that, the ground-truth is obtained by converting the user-item rating to sentiment label with a threshold of 3, i.e., we category them into positive (i.e., higher than 3 stars), negative (i.e., lower than 3 stars) or neural (i.e., equal to 3 stars). Hereby, we utilize a cross entropy loss function to estimate the sentiment polarity of each user and item review as follows: where^ğ‘¦= ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (s)is the predicted sentiment polarity of the ğ‘—-th user review. Similarly, we can get sentiment prediction of item review^ğ‘¦= ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (s).ğ‘‚denotes the set of observed user-item rating pairs.ğ¶is the number of sentiment polarity categories and ğ‘š is the number of reviews for a user/item. In this subsection, we apply our SIFN model for the rating prediction task. To this end, we propose a rating learner to comprehensively extract the latent interactions among the identity representation of user/item and review representation, as shown in î€›gure 1 (b). To begin with, there is user ID feature, which is encoded into low-dimensional vector denoted as eâˆˆ Rthrough: whereWâˆˆ Ris weight matrix andğ‘›is the total number of user ID features. Rating Learner.As previous work states [2], diî€erent reviews do not contribute equally to the preference of user/item. Besides, user also carries distinct style when writing reviews. For example, an optimistic user is more likely to write positive reviews while a cranky user tends to write something even rude. Therefore, in order to better model the user/itemâ€™s personalized preference in diî€erent reviews, it is indispensable to incorporate both text review and user/item features. Hereby, we concatenate them as follows: where Wâˆˆ Ris a projection weight matrix. 1) Aggregation.Then, we utilize the user-aware review representationoâˆˆ Rto adaptively select important reviews with attention function which is formulated as:î€î€î€‘î€‘ wheredâˆˆ Ris the user review representation with aggregated information from all user reviews under the inî€uence of user features. Similarly, we can get the embedding vector of item ID denoted as eâˆˆ Rand the item review representation dâˆˆ R. 2) Fusion Network.So far, the user/item reviews representation (i.e.,oando), that integrates the features of identity and reviews, are learned from separate hierarchical representation modules without interactions. Since we aim at learning the user-item rating, it is necessary to extract the mutual inî€uence among them. However, as user and item features may carry diî€erent characteristics in diî€erent spaces, we design a novel fusion network that projects features into a shared hidden space before learning feature interactions as: wheref âˆˆ Ris the fusion representation,Wâˆˆ Ris the weight matrix that maps user and item features to the same latent space. 3) Interactive Network.In addition, to underline the contribution of the diî€erent extracted representations (e.g.,d,dand f), we further design an interactive network so that the user-item rating estimation is fully explored as follows: whereWandbare weight matrix and bias.âŠ™denotes the elementwise product of vectors.p âˆˆ Ris the user-item preference representation with aggregated interactions from reviews and identity. Table 1: Statistics of the Amazon datasets. Through the above learning processes, the predicted rating of user toward item^ğ‘Ÿis obtained by a linear projection and the regression loss function for rating prediction is as: wherewis the weight parameter.ğ‘andğ‘are biases for user and item, respectively. ğ‘Ÿ is the ground truth user-item rating. Finally, we utilize the joint learning process to optimize both objectives with a hyper-parameters ğœ† as: We choose î€›ve Amazon datasets (i.e., Music Instruments, Oî€œce Products, Digital Music, Tools, Video Games) from [7] to conduct our experiments. We preprocessed it to ensure that all users and items have at least one rating î€›ve reviews in our experiment. The dataset consists of numerous data samples on which we follow a randomized 80:10:10 train/test/validation split. For concrete information, please refer to Table 1. To show the performance of our proposed model, we compare SIFN with the state-of-the-art (SOTA) models. The benchmarks are: â€¢ MF-based: PMF [8] models the latent factors by introducing Gaussian distribution. ConvMF+ [6] incorporates convolutional neural network into Matrix Factorization. â€¢ Neural-based: DeepCoNN [18], D-Attn [12], NARRE [2], CARP [7]. These methods have been introduced in section 1. In our SIFN model, we use BERT encoding from Hugging Face. User and item embedding size is set to 16. The batch size is 100, learning rate is 0.001, dropout rate is 0.2, andğœ†is tuned amongst [0.1,1,10]. For baseline methods, we follow the hyper-parameter conî€›gurations in their papers. Following previous works [7,12], we utilize Mean Squared Error (MSE) as the evaluation metric and select Adam as optimizer for all models. In this section, we evaluate the performance of SIFN model on Amazon datasets along with detailed comparison of results in Table 2. The major results are summarized as follows: (1) MF-based methods (e.g., PMF and ConvMF+) consistently fall behind other methods, which indicates the limitations of Matrix Factorization to learn Table 2: Experimental results in Amazon datasets. Percentage in ( ) denotes the relative improvement of SIFN over the baseline method, which is achieved with paired t-tests at the signiî€›cance level of 0.01. We underline the best performed baseline. DeepCoNN [18] Figure 2: Results from variants of SIFN. In each plot, the x-axis denotes the name of variants, in which the preî€›x SIFN is omitted for space saving. The y-axis denotes the value of evaluation loss. The blue bar represents the loss from SIFN while the stacked orange part denotes increased loss, which indicates a worse performance of variant than SIFN. semantic information from sparse rating dataset. (2) Neural-based methods (e.g., DeepCoNN, D-Attn, NARRE, CARP) outperform MFbased ones by a large margin, which validates the powerful feature extraction capability of neural networks. Among them, CARP performs best, which suggests itâ€™s helpful to encode viewpoint of user and sentiment aspect of item for rating prediction. (3) Furthermore, our proposed SIFN model still outperforms CARP by 1.81%âˆ¼3.41%, which demonstrates the superiority of the well designed interactive& Fusion module that extracts the mutual inî€uence among user and item features and the eî€ectiveness of the sentiment prediction task that attends the sentimental phrases in text reviews. To explore the impact of each component of SIFN, multiple ablation studies are carried out by removing one sub-module at a time. â€¢ SIFN_sa: replaces the sentence attention with a simple average sum pooling over all the reviews. â€¢ SIFN_fn: removes the fusion network so that user and item features are disentangled without explicit interactions. â€¢ SIFN_in: replaces the interactive network with commonly used Factorization Machine (FM) [11] to estimate the ratings. â€¢ SIFN_w2v: replaces the BERT encoding of text reviews with commonly used pre-trained word embedding GloVe [9]. â€¢ SIFN_sp: removes the sentiment prediction task so that the model focuses on user-item rating prediction. We report the results of ablation studies in Figure 2. Speciî€›cally, the performance ofSIFN_sawith average pooling drops as it just assumes every review contributes equally for user-item rating, which overlooks the impact of various user/item information. Without the fusion network, the performance ofSIFN_fnalso declines since Figure 3: Attention visualization of the diî€erent words. The word attention value from SIFN and SIFN_sp are associated with the row (1), row (2) respectively in both examples. the interactions of user and item reviews are not fully exploited. As user and item features carry diî€erent characteristics, it is ineî€œcient to perform second-order operations in the same space with FM, which leads to the decrease of the performance ofSIFN_in. Not surprisingly, without BERT encoding, the word embedding in SIFN_w2vis incapable of representing deep semantic information of text reviews. Without sentiment prediction inSIFN_sp, there is no supervision towards attending sentiment-aware words in reviews, which are vital for the rating prediction. As shown in Figure 3, we randomly sample two reviews of a user from the Music Instrument dataset and visualize the attention results to explain the capability of SIFN in extracting sentiment knowledge. Speciî€›cally, in review (a), SIFN aligns the sentiment words, e.g., â€œhatedâ€ and â€œcheapâ€, with a rating of 1.08, which is consistent with actual value. In contrast, we utilize the attention results from SIFN_spas described in Section 3.4, which overlooks these sentiment words thus predicting a relatively higher rating. Similarly, in review (b), SIFN accurately predicts a rating of 4.98 by extracting the sentiment words, e.g., â€œloveâ€ and â€œperfectlyâ€, whileSIFN_sp is incapable of achieving this. These examples demonstrate that SIFN is eî€ective in extracting the latent semantics of reviews and interpret the corresponding sentiment, which is helpful for the î€›nal rating prediction. In this paper, we proposed a novel Sentiment-aware Interactive Fusion Network (SIFN) model for review-based item recommendation. Speciî€›cally, we î€›rst employed the encoding module which contains BERT encoding and a sentiment learner to learn sentiment-aware features of each review sentence. Then, we designed a sentiment prediction task to guide the learn process of sentiment features. Finally, we developed a rating learner for the î€›nal rating prediction, which contains two novel operations (i.e., interactive & fusion) to fuse the identity features of user/item and review representation. Extensive experiments on î€›ve public datasets demonstrate the eî€ectiveness of our proposed method. This research was partially supported by grants from the National Natural Science Foundation of China (Grants No. 61922073 and U20A20229).