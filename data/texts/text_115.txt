{bencheng.ybc,pengjie.wpj,vjinquan.ljq,kuang-chih.lee,xiyu.xj,bozheng}@alibaba-inc.com,lwsaviola@163.com Nowadays, deep learning models are widely adopted in web-scale applications such as recommender systems, and online advertising. In these applications, embedding learning of categorical features is crucial to the success of deep learning models. In these models, a standard method is that each categorical feature value is assigned a unique embedding vector which can be learned and optimized. Although this method can well capture the characteristics of the categorical features and promise good performance, it can incur a huge memory cost to store the embedding table, especially for those web-scale applications. Such a huge memory cost signiî€›cantly holds back the eî€ectiveness and usability of EDRMs. In this paper, we propose a binary code based hash embedding method which allows the size of the embedding table to be reduced in arbitrary scale without compromising too much performance. Experimental evaluation results show that one can still achieve 99% performance even if the embedding table size is reduced 1000Ã—smaller than the original one with our proposed method. â€¢ Information systems â†’ Recommender systems. Embedding learning for categorical features plays an important role in embedding-based deep recommendation models (EDRMs) [3,7,23]. A standard method, often referred to as full embedding, for embedding learning is to learn the representation of each feature value [19]. Speciî€›cally, letğ¹be a categorical feature and|ğ¹ |be its vocabulary size, each feature valueğ‘“âˆˆ ğ¹is assigned an embedding indexğ‘˜so that theğ‘˜-th row of the embedding tableğ‘Š âˆˆ R is the embedding vector ofğ‘“, whereğ‘ = |ğ¹ |in the full embedding method and ğ· is the embedding dimensionality (see Fig 1 (a)). However, such full embedding learning suî€ers from severe memory cost problems. Actually, the memory cost of the embedding Figure 1: Comparisons of diî€erent embedding methods. Step 1, 2, and 3 refer to feature hashing, embedding index generation, and embedding generation respectively. table isğ‘‚ (|ğ¹ |ğ·)which grows linearly with|ğ¹ |. For web-scale applications, one may need to store a huge embedding table since the vocabulary size may be millions or even billions. For example, suppose|ğ¹ | =500 million andğ· = 256, the corresponding memory cost will be 475GB. In practice, such a huge cost becomes a bottleneck in deploying EDRMs in memory-sensitive scenarios. Therefore, it is crucial to reduce the size of the embedding table [2,19]. In this paper, we highlight two challenges: (1)Challenge one: Flexibility. The memory constraint varies with diî€erent scenarios (from distributed servers to mobile devices). The embedding reduction methods need to be î€exible enough to meet diî€erent memory requirements. Especially for mobile devices, a tiny EDRM is needed to meet the limited memory requirement. (2)Challenge two: Performance Retention. Since a big model usually has a better capacity and hence a better performance, embedding reduction may bring a performance gap due to the fewer parameters used in the reduced model. Hence, how to keep high performance when the memory size is reduced is a big challenge, especially for the memory-sensitive scenarios (e.g., in mobile devices). In general, there are two directions to reduce the embedding table size, i.e., reducing the size of each embedding vector and reducing the number (i.e.,ğ‘) of the embedding vectors in an embedding table. The embedding table size of the former methods (e.g., product quantization [5,9], K-D method [2,11,13,15,20], and AutoDim [6, 10,16,17,28,29]) is still linearly increased with|ğ¹ |, failing to tackle the memory problem caused by a large vocabulary size in web-scale applications [19]. Hence these methods are not considered in our paper. For the latter methods, they typically apply a mod-based hash embedding to reduceğ‘. The key idea of them is to apply modulo operation on the unique Hash ID of each feature value, i.e., focusing on Step 2 in Fig 1 (b). For example, Hash embedding [24] Table 1: Comparison about embedding methods. takes the remainder of the Hash ID divided byğ‘€as the embedding index, reducing the embedding size fromğ‘‚ (|ğ¹ |ğ·)toğ‘‚ (ğ‘€ğ·). The problem of this method is that diî€erent feature values may have the same embedding index and hence the same embedding vector, leading to poor performance. Multi-Hash (MH) [22] adopts multiple embedding indices for one feature value, reducing the collision rate among feature values. But diî€erent feature values may still be indistinguishable especially for a tiny model, failing to the challenge two. Q-R trick [19] uses both the remainder and the quotient as embedding indices to identify a feature value. However, Q-R trick fails to the challenge one since its minimal reduced size is relatedî° to|ğ¹ |rather than any scales. Although the generalized Q-R tries to address this problem, it needs a lot of eî€ort to design the divisor [19]. The comparisons are summarized in Table 1. In this paper, unlike the existing methods which adopt a modulo (collision) operation, we bring the idea of binary code (e.g., the binary code of integer 13 is1101) which is unique for diî€erent Hash ID and propose a binary code based hash embedding method to tackle this reduction problem (see Fig 1 (c)). Speciî€›cally, we î€›rst binarize the Hash ID into a binary code. Then, to address the challenge one, we propose a code block strategy and reduce the embedding table size by adjusting the code block length î€exibly. To address the challenge two, the generated embedding index is designed to be unique for diî€erent feature values at any reduction ratios. The uniqueness at any reducing ratios allows EDRMs to distinguish diî€erent feature values, leading to a good performance even for a tiny model. Furthermore, Step 2 of our method is a deterministic and non-parametric process and can be computed onthe-î€y. This property is friendly for EDRMs both on the convenient application and handling new (out-of-vocabulary) feature values. We also note that we are aware of some recent works using similar terms such as learning binary embedding [8,14,26]. We want to point out that they are in totally diî€erent contexts. In these works, binary refers to that each element in an embedding vector is a binary number for a fast similar embedding search. While in our work, binary refers to binarize the integer ID into a binary code. To summarize, the main contributions are listed as follows: (1) We propose binary code based hash embedding, a simple but eî€ective embedding method, to reduce the embedding table size and keep a high performance at the same time. (2) A code block strategy is presented to adjust the embedding table size î€exibly and a lossless embedding index generation process is elaborately designed to allow the model to distinguish diî€erent feature values and achieve better performance. (3) Experimental results on large-scale realworld datasets show that with the help of the proposed method, the model size can be 1000Ã—smaller than the original model, and keep 99% performance as the original model achieves at the same time. In this section, we introduce the framework (see Fig 2) of our methods. In general, we also adopt three steps as introduced in Fig 1. Figure 2: The framework of the proposed method. In practice, the raw categorical feature values may be represented as various types, such as String and Integer values. To handle diî€erent types of categorical feature values, in practice, a feature hashing [19,22,24] is î€›rstly applied to map these raw feature values into a uniformed integer number, called Hash ID (see Fig 2). Formally, the feature hashing process can be expressed asâ„= H (ğ‘“)where Hrefers to a hash function (e.g., Murmur Hash [25]) andâ„is an integer number, called the Hash ID ofğ‘“. In practice, the output length ofHis always a large value (e.g.,â„is a 64-bits integer) to make the collision amongâ„as small as possible. In this case,â„ can be basically taken as a unique ID for diî€erent ğ‘“[12, 24]. In this section, we introduce the embedding index generation process including binarization, code block strategy, and decimalization. 2.2.1 Binarization. After Step 1, each feature valueğ‘“is mapped toâ„, which is basically regarded as a non-collision mapping due to the large output space [12,24]. Then the binary codeğ‘âˆˆ {0, 1} (whereğ‘†refers the binary code length) ofğ‘“can be generated by transforming this uniqueâ„to a binary form (e.g., the binary code of integer 13 is 1101). Note ğ‘is also unique for diî€erent ğ‘“. 2.2.2 Code Block Strategy. To allow the model can î€exibly reduce memory, we propose a novel strategy called code block strategy. Generally speaking, the code block strategy divides each 0-1 value inğ‘to diî€erent blocks. Then, the ordered 0-1 values (i.e., 0-1 code) in each block can representğ¾ = 2unique integers whereğ‘›is the number of 0-1 values in this block (see Step 2.2 in Fig 2). If we take the decimal form of 0-1 code in each block as an embedding index and map each index to an embedding tableğ‘Š âˆˆ R, the size of the embedding table can be î€exibly adjusted byğ‘›. For example, whenğ‘› = 1(i.e., the number of 0-1 values in each block is 2), the embedding table size isğ‘‚ (2ğ·). When all 0-1 values inğ‘are arranged into one block, the embedding table size isğ‘‚ (|ğ¹ |ğ·)(i.e., full embedding). In other words, by controlling the value ofğ‘›, we can adjust the embedding table size to meet various scenarios (from distributed services to mobile devices). Formally, we deî€›neğµ= [ğµ; ğµ; ...; ğµ; ...]as the sequential code blocks produced by a code block strategy onğ‘, and|ğµ|refers to the number of blocks. Then theğ‘š-th code blockğµâˆˆ {0, 1} can be represented as Figure 3: Code block strategy examples. The binary code ğ‘= 100100110101100111000011. The 0-1 values plotted with the same color in each case are divided into the same block. where the functionAllocis the allocation function which allocates each 0-1 value to diî€erent blocks.Orderis a function which gives an order for the 0-1 value in each block and generates a 0-1 code for each code block. Here we give two code block strategies (including Succession and Skip) as examples to show how it works (other possible strategies can also be allowed). Succession.As shown in Fig 3 (a), the succession strategy puts the ğ‘¡successive 0-1 values in a binary code into the same block. The Orderfunction keeps 0-1 values in the same relative position inğ‘. Note if the number of the last 0-1 values inğ‘is less thanğ‘¡, all of the left values are divided into a new code block. Skip.As shown in Fig 3 (b), if the number of interval values of two 0-1 values in a binary code isğ‘¡, they will be divided into the same block. The Order function is the same as that in Succession. Note given one of the above code block strategies, we can obtain a unique sequence of code blocksğµfor the binary codeğ‘. This property guarantees the process of code block strategy is lossless. 2.2.3 Decimalization. The embedding index of each block can be obtained by decimalizingğµ(e.g.,ğ·ğ‘’ğ‘ğ‘–ğ‘šğ‘ğ‘™ğ‘–ğ‘§ğ‘’ (1101) = 13), i.e., ğ‘˜= ğ·ğ‘’ğ‘ğ‘–ğ‘šğ‘ğ‘™ğ‘–ğ‘§ğ‘’ (ğµ)whereğ‘˜is the embedding index of When obtaining multiple indices forğ‘“, to get its embedding, two steps are proposed, i.e., embedding lookup and embedding fusion. 2.3.1 Embedding Lookup. As introduced above, each code block ğµinğµcan obtain an embedding indexğ‘˜. The number of blocks is|ğµ|, leading to a total of|ğµ|embedding indices. Then we can map each embedding index into an embedding vector, i.e., ğ‘’= E (ğ‘Š, ğ‘˜)whereğ‘Šis a embedding table,ğ‘’refers to the embedding ofğµandEis a embedding lookup function which usually returns theğ‘˜-th row ofğ‘Š. In practice, keeping |ğµ|embedding tables for diî€erentğµmay also cost a lot memory consumption. Therefore, it is common to keep a single embedding table and share this table among all ğµ[22]. 2.3.2 Embedding Fusion. To generate the î€›nal embedding vector ğ‘¥of ğ‘“, an embedding fusion function ğ‘” is applied, The design of the fusion function can be various, such as pooling, LSTM, concatenation and so on. In this paper, by default, we adopt sum pooling as the fusion function (others can also considered). 2.4.1 Desiderata. There are several key desiderata of our method, which EDRMs can be beneî€›ted. (1)Determinacy.The indices generation is a deterministic and non-parametric process. It is computed on the î€y, making it simple to practical implementations and friendly to new feature values. (2)Flexibility.The size of embedding tableğ‘Š âˆˆ Ris mainly determined byğ‘›(i.e., the number of 0-1 values in each code block). It means the memory reduction ratio can be î€exibly adjusted from2/|ğ¹ |to 1 (assuming adopting embedding table sharing strategy). This beneî€›ts EDRMs can be developed on memory insensitive scenarios to sensitive scenarios. (3) Uniqueness.No matter what the reduction ratio is,ğµis unique for each feature value. This enables the model to distinguish diî€erent feature values and further improve the model performance. 2.4.2 Sub-collision Problem. We should point out althoughğµis unique, there may exist sub-collision among two feature values (e.g., ğµâ‰  ğµbutğµ= ğµ), called sub-collision problem. Actually, it is an open problem which exists in most mod-based hash methods [19,22,24]. We leave it as one of the future work. In practice, a hash function (e.g., Murmur hash [25]) which can randomly map values to a large space is used to relieve this problem. 2.4.3 The Relation with Existing Methods. Here, we discuss the relation between ours and other methods. (1)Full Embedding.Both of full embedding and ours can distinguish diî€erent feature values. Besides, our method has the ability to reduce memory î€exibly. (2) Hash Embedding.It is a simpliî€›ed form of ours, where the code block strategy is Succession, and only the î€›rst topğ‘¡0-1 values are used as the embedding index. (3)Multi-Hash Embedding.Both of them create multiple embedding indices. But our method goes further, i.e., keeping a uniqueness constraint for these indices. (4) Q-R Trick.Q-R trick is a special case of our method. When we utilize Succession and the block number is set to 2. The î€›rst topğ‘¡ 0-1 code and the left 0-1 code can be taken as the quotient and the remainder in Q-R trick respectively. Datasets.(1) Alibaba is an industrial dataset which is obtained from Taobao. There are a total 4 billion samples, 100 million users. (2) Amazonis collected from the Electronics category on Amazon. There are total 1,292,954 samples, 1,157,633 users. (3) MovieLens is a reviews dataset and is collected from the MovieLens web site. There are total 1,000,209 samples, 6,040 users. Baselines.(1) Full Embe dding (Full) is a standard embedding learning method. (2) Hash Embedding (Hash)[24] applies the modulo operation on the Hash ID to obtain an embedding index. (3) MultiHash Embedding (MH) [22] applies multiple hash functions to the feature value to obtain multiple indices. (4) Q-R Trick (Q-R) [19] take both the remainder and the quotient as indices. Training Details.All methods have the same EDRM architecture. The embedding dimensionality is also set the same for all methods. The methods (i.e., MH, Q-R trick, and ours) employ embedding table sharing strategy in diî€erent indices for memory reduction purpose and take sum pooling as the fusion function. For MH, we use 2 hash functions as suggested by authors [22]. For our method, the code block strategy is Succession. We use the Adagrad optimizer with a learning rate of 0.005. The batch size is 1024 for all datasets. Table 3: The results of memory size when all the methods archive 99 % performance as the full embedding method achieves in AUC score. Top 3 Figure 4: Convergence of diî€erent methods. We conduct experiments on CTR prediction tasks and compare the performance with diî€erent memory reduction ratios of the full embedding. AUC (%) [4] score is reported as the metric. Note 0.1% absolute AUC gain is regarded as signiî€›cant for the CTR task [3, 21, 30]. Results are shown in Table 2. BH refers to our method. Comparison with Mod-based Hash Embedding Methods.In general, BH performs best on all cases, and the gain gap between BH and baselines is increased for a smaller model. For example, compared with Q-R on MovieLens, BH can achieve 0.24% gains when the reduction ratio is 37.5 %. While when the ratio becomes 0.1 %, the gain gap is increased to 2.28 %. It indicates that due to the nice properties of BH (see Section 2.4.1), BH can better represent each categorical feature value, especially for a tiny model. Comparison with the Full Embedding.We can observe that: (1) Since the full embedding method contains signiî€›cant parameters, it gets better performance. (2) In most cases, BH can obtain competitive performance with the full embedding method. In this section, we conduct experiments to evaluate the model size of all methods when achieving similar performance. Speciî€›cally, we take the dataset Alibaba as an example due to its closeness with the web-scale application. Then we report the model size of diî€erent embedding methods when they achieve 99% performance as the full embedding method achieves in AUC score. Besides, to further evaluate the reduction ratio, we also report the size of the embedding table of the top 3 largest for each method. The results are shown in Table 3. Some observations are summarized as follows: (1) Table 4: The results of diî€erent code block strategies. Compared with other embedding methods, when all of them archive similar scores, BH can cost the smallest memory size. (2) Compared with the full embedding method, BH can adopt an extremely tiny model (i.e., 1000Ã—smaller) to achieve 99 % performance. Such a small model with high performance is urgently needed to develop EDRMs on a memory-sensitive scenarios (e.g., mobile devices). In this section, we evaluate the performance of the proposed two code block strategies, i.e., Succession and Skip. To have a fair comparison, we keep the same reduction ratio for these two strategies. Table 4 shows the results. Note we also provide the best performance (Q-R) among baselines for comparison. We can observe that Succession and Skip achieve similar performance overall datasets, and perform better than the best baselines. It indicates the uniqueness of code block strategy is helpful to improve the embedding performance no matter what kind of code block strategies we choose. We conduct experiments to analyze the convergence of diî€erent models. Speciî€›cally, we keep the same reduction ratio for all methods and report the AUC and the loss value of these methods on test data of MovieLens within 100 epochs (similar conclusions can be found in other datasets). As shown in Fig 4, we can î€›nd: (1) Compared the AUC and loss curves of mod-based hash embedding methods, BH converges faster than that of other baselines. Furthermore, BH can achieve a higher AUC score and a lower loss value. It demonstrates the eî€ectiveness of our method when reducing EDRMs into a small-scale size. (2) Due to more parameters adopted in full embedding, it archives the best performance. But, BH can also achieve competitive performance compared with full embedding. In this paper, to tackle the memory problem in embedding learning, we propose a binary code based hash embedding. A binary code is î€›rstly generated to guarantee a unique index code. Then a code block strategy is designed to î€exibly reduce the embedding table size. Finally, the feature embedding vector is obtained by combining the embedding vectors from diî€erent code blocks. Experimental results show that even if the model size is 1000Ã—smaller, we can still obtain the 99% performance by binary code based hash embedding.