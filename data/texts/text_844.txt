Implicit feedback recommendation has attracted increasing attention in recent years as the convenience of data collection[1] and it can take full advantage of usersâ€™ interactions (such as view, click, purchase and so on) for recommendation. Prior eï¬€orts have shown the importance of exploiting usersâ€™ behaviors for comprehending usersâ€™ interest[2]â€“[4]. Existing works beneï¬t from learning short-term preference by exploring usersâ€™ recent historical interactions. As integrating the short-term preference, recommender systems have fully considered the evolution of usersâ€™ preferences. However, in such an implicit feedback setting, a userâ€™s exact preference level is hard to quantify through observing his behaviors directly[5]. More than this, there are even some unexpected behaviors in the historical interactions. For examples, as shown in ï¬gure 1, a businessman has bought a baby T-shirt for his son, and clicked a handicraft by accident or due to curiosity, as shown in red boxes. Consider another Manuscript received August 1, 2019. Manuscript revised August 1, 2019. School of Computer and Information Engineering, Tianjin Normal University, Tianjin 300387, China a) E-mail: gxuchenjie04@gmail.com b) E-mail: wxxjlf@sina.com c) E-mail: mcmxhd@163.com d) E-mail: sunhuazhi@tjnu.edu.cn DOI: 10.1587/trans.E0.??.1 , Lifen JIANG, Chunmei MA, and Huazhi SUN, scenario, someone has watched a bunch of movies, but not every movie was what he likes. In these two cases, we argue that these unexpected behaviors are irrelevant to the userâ€™s preference. It will hurt the recommendation performance dramatically when we can not exploit these behaviors in a discriminating way. That means we need to eliminate the eï¬€ects of unexpected behaviors. However, existing works do not have clear information to guide the model to eliminate the eï¬€ects of unexpected behaviors and they achieve inferior performances. Fig. 1: Unexpected behaviors in the historical interactions, as shown in red boxes. In this work, we aim to ï¬ll the research gap by developing a solution that eliminates the eï¬€ects of unexpected behaviors under the guidance of the target item. Towards this end, we propose a novel method, named Multi Preference Model (MPM), which can not only eliminate the eï¬€ects of unexpected behaviors but also retain the predictive information as much as possible. Speciï¬cally, we ï¬rst extract the usersâ€™ instant preferences from their recent historical interactions by a ï¬ne-grained preferences module. And then we train an unexpected-behaviors detector to judge whether these instant preferences are biased by unexpected behaviors. This is done under the guidance of the target item, as the instant preferences should stay in line with the positive target items and keep a distance from the negative ones. We also integrate a general preference module, which is a neural latent factor model[6] based on collaborative ï¬ltering. The integration of the general preference module in our model can ensure the recommendation will not deviate far away from the usersâ€™ real general preferences. Thereafter, an output module is performed to eliminate the eï¬€ects of unexpected behaviors and aggregate all information to obtain the ï¬nal recommendation. We conduct extensive experiments on two datasets about movie and e-retailing to verify our method, demonstrating signiï¬cant improvements in our Multi-Preferences Model over the state-of-the-art methods. MPM gets a massive improvement in HR@10 and NDCG@10, which relatively increased by 3.643% and 4.107% compare with AttRec model on average. The contributions of this work are threefold: 1. We highlight the importantance of eliminating the unexpected behaviorsâ€™ eï¬€ect in historical interactions, which is crucial for recommendations. 2. We propose an end-to-end neural network model which can eliminate the eï¬€ects of unexpected behaviors in historical interactions. 3. We conduct experiments on public datasets to demonstrate the eï¬€ectiveness of the proposed model, and it outperform the competitive baselines. As a personalized ï¬ltering tool, recommender systems have been widely adopted to alleviate the information overload problem. In this section, we brieï¬‚y review related works from three perspectives: session-based recommendation, latent factor models and a new trend in recommendation. In the session-based setting, recommender systems have to rely only on the interactions of the user in the current sessions to provide accurate recommendations[7]. And, our work mainly integrates the recent historical interactions to the conventional latent factor model, meanwhile we eliminate the eï¬€ects of unexpected behaviors. In the session-based recommendation, Hidasi et al.[8] take the lead in exploring GRUs for the prediction of the next userâ€™s action in a session. Li et al.[9] argue that both the userâ€™s sequential behaviors and the main purpose in the current session should be considered in recommendations. Tuan et al.[10] describe a method that combines session clicks and content features to generate recommendations. Ruocco et al.[11] use a second RNN to learn from recent sessions, and predict the userâ€™s interest in the current session. Wu et al.[12] incorporate diï¬€erent kinds of user search behaviors such as clicks and views to learn the session representation, which can get a good result in recommendations. Session-based recommendation systems share many similarities with our work, they both can take the advantage of historical interactions for recommendations. Latent factor models get its name from mapping users and items to latent factor vectors. In the age of machine learning, latent factor models are ï¬rst proposed based on matrix factorizations to deliver accuracy superior to classical nearestneighbor techniques[13]. Latent factor models are canonical for capturing userâ€™s general preference, as they are trained by considering the whole historical interactions. After entering the era of deep learning, latent factor model has been developed in diï¬€erent ways[14]. Salakhutdinov et al.[15] ï¬rst use Restricted Boltzmann Machine to extract latent features of user preferences from the user-item preference matrix. In[16], Autoencoder is used in deep latent factor models to learn a non-linear representation of the user-item matrix. He et al.[6] replace the inner product with an MLP as the interaction function. They present a general framework named NCF and implement a GMF and NeuMF model under the NCF framework. Travis et al.[17] integrate the latent factor model and the neighborhood-based model by memory network and propose the collaborative memory network model. In our work, we integrate a latent factor model to capture the userâ€™s general preference, which endows our model with particular robustness. From the literature, we can ï¬nd that session-based recommender systems produce recommendations base on shortterm preference and latent factor models mainly rely on longterm (general) preference. Recently, a surge of interest in combining long-term and short-term preference for recommendation has emerged. Liu et al.[4] propose a novel shortterm attention/memory priority model (STAMP), which is capable of capturing usersâ€™ general interests from the longterm memory of a session context, whilst taking into account usersâ€™ current interests from the short-term memory ofthe last-clicks. BINN learns historical preference and present motivation of the target users using two LSTM-based architecture by discriminatively exploiting user behaviors[2]. Zhang et al.[18] propose an AttRec model which learns the short-term intention by self-attention module and extract long-term intention in the traditional manner which measure the closeness between item ğ‘– and user ğ‘¢ by Euclidean distance. These works have achieved considerable success. However, as mentioned before, there are usually some unexpected behaviors in historical interactions, and these works do not have clear information to guide the model to learn to eliminate the eï¬€ects of unexpected behaviors. Towards this end, we propose a Multi-Preferences Model to address this problem. In this section, we will elaborate on the multi-preference model, as illustrated in ï¬gure 2. Here, we ï¬rst formally deï¬ne the recommendation problem in implicit feedback datasets and then present MPM in detail. In the implicit feedback setting, the interactions between users and items such as purchase, browse, click, view or even mouse movements are informative for recommendations. However, usersâ€™ opinion canâ€™t be deduced directly through observing these behaviors[5]. This brings great challenges to the recommendation problem. We assume there are ğ‘€ users and ğ‘ items in the dataset and deï¬ne the user-item interaction matrix ğ‘Œ = ğ‘…as Fig. 2: The framework of Multi-Preferences Model. (1) Fine-grained preferences module learns the instant preferences from historical interactions, (2) unexpected-behaviors detector detect the unexpected behaviors, (3) general-preference module capture userâ€™s general preference, (4) output module eliminate the eï¬€ects of unexpected behaviors and produce ï¬nal recommendation. equation 1. If exist interactions between user u and item i set ğ‘¦= 1, otherwise, ğ‘¦= 0 . ğ‘¦=1 if interactions (user u, item i) are observed;0 otherwise. Our task can be formulated as follows: Given a user ğ‘¢, a target item ğ‘–, and the last ğ¾ interactions ğ»= [ğ‘¥, ğ‘¥, ğ‘¥, ..., ğ‘¥]. The holistic goal is to estimate the interaction by: Where ğ‘“ denotes the underlying model with parameters Î˜, and Ë†ğ‘¦presents the predicted score for the interaction. We can explain Ë†ğ‘¦as the likelihood of the user ğ‘¢ interact with the item ğ‘– based on the recent historical interactions Multi-preferences model takes the userâ€™s last ğ¾ historical interactions as input and outputs a score indicating how possible the user will interact with the target item. As illustrated in ï¬gure 2, there are four key components in MPM: (1) ï¬negrained preferences module learn the ï¬ne-grained instant preferences from historical interactions, (2) unexpectedbehaviors detector validate whether the instant preferences are biased by unexpected behaviors, (3) general-preference module is a conventional neural latent factor model for capturing general preference,(4) at last, the output module is performed to eliminate the eï¬€ects of unexpected behaviors and fuse all information to produce a ï¬nal recommendation. Given the recent historical interactions, we need to extract useful knowledge for recommendations. Instead of learning a single representation of the historical interactions in an autoencoding manner like previous works[18], we output an instant preference in each step. This workï¬‚ow is consistent with the conventional sequential modeling, which obey the autoregressive principle. Here, we implement the ï¬ne-grained preference module based on the Temporal Convolutional Network. Temporal Convolutional Network (TCN) is a popular architecture. Studies have shown that TCN outperforms canonical recurrent networks such as LSTMs across a diverse range of tasks and datasets[19]. More formally, an input of 1 âˆ’ ğ· items sequence ğ»âˆˆ ğ‘…and a ï¬lter ğ‘“ :{0, ..., ğ‘˜ âˆ’ 1}â†’ ğ‘…, the dilated convolution operation ğ¹ on element ğ‘¥of the sequence is deï¬ned as ğ¹ (ğ‘¥) = (ğ»âˆ—ğ‘“ )(ğ‘¥) =ğ‘“ (ğ‘–).ğ»(3) Where ğ‘‘ is the dilation factor, ğ‘˜ is the ï¬lter size, and ğ‘  âˆ’ ğ‘‘ğ‘– accounts for the direction of the past. Dilated convolution is illustrated in ï¬gure 3. And zero padding of length (ğ‘˜ âˆ’ 1) is added to keep subsequent layers as the same length as previous ones. TCN also incorporates residual connection[20], weight normalization[21] and spatial dropout[22] to improve performance, as shown in ï¬gure 2 (a). In our model, the ï¬ne-grained preferences module can be abstracted as equation 4 Where â„contains all the semantic information of the interactions before position ğ‘–, that is â„= Î¨(ğ‘¥, ğ‘¥, ..., ğ‘¥). â„represents the userâ€™s instant preference at this moment. However, some of these instant preferences always are biased as the existence of unexpected behaviors in historical interactions. For example, ğ‘¥is a gift that the user buys for his friend or he click the item by accident. As a consequence, some instant preferences will be destroyed by unexpected behaviors. Fig. 3: A dilated convolution with dilation factors ğ‘‘ = 1, 2, 4 and ï¬lter size ğ‘˜ = 3. To eliminate the eï¬€ects of unexpected behaviors, we ï¬rstly builds an unexpected-behaviors detector to judge whether an instant preference is biased by unexpected behaviors. More speciï¬cally, in the training phase, unexpected-behaviors detector incorporates the target items to judge whether an instant preference is biased, as the instant preferences should stay in line with the positive target items and keep a distance from the negative ones. Once the model trained, the detector has the ability to make a judgment by itself. The unexpected-behaviors detector is shown in ï¬gure 4, which consist of ğ¾ multi-layers perceptron (MLP). Every instant preference â„will be concatenated with a target item embedding ğ‘’, and then send into an MLP. The MLP can model the interactions between instant preference and item latent features. Finally, it outputs a predictive vector ğ‘, which contains the information about whether the instant preference is distorted. Unexpected-behaviors detector is deï¬ned as follows. Where ğ‘Š, ğ‘, and ğ‘represent the corresponding weight matrix, bias, and activation function in the ğ‘¥ âˆ’ğ‘¡â„ layer of the MLP respectively. All in all, unexpected-behaviors detector consumes the instant preferences and outputs a bunch of predictive vectors ğ‘ƒ = [ğ‘, ğ‘, ..., ğ‘], which indicate whether the instant preference is distorted. So far, we have described the unexpected-behaviors detector in detail and the predictive vectors ğ‘ƒ are informative for recommendations. However, due to the complexity of the real world, we still need to introduce general preference to make sure recommendations go in the right direction. Here, we build a general preference module base on conventional neural latent factor model[6], which is proï¬cient in capturing the general preference by considering the whole user-item matrix. General preference module takes the user latent vector ğ‘’and item latent vector ğ‘’as input and outputs a predictive vector ğ‘, as illustrated in ï¬gure 5. As the user latent factor vector contains the userâ€™s general preference and item latent factor vector contains the characteristics of the item ğ‘–[13], the predictive vector ğ‘indicates whether the target item ğ‘– ï¬t the userâ€™s general preference. The integration of the General Preference Module in our model can ensure the recommendation will not deviate from the general preferences of users. It can improve the robustness of the model by a large margin. The general preference module is deï¬ned as follows, ğ‘= ğ‘(ğ‘Š(ğ‘(...ğ‘(ğ‘Š[ğ‘’, ğ‘’]+ ğ‘)...)) + ğ‘) (9) Until now, we still havenâ€™t eliminated the eï¬€ects of unexpected behaviors yet. In our model, the Output Module has two important tasks: (1) eliminates the eï¬€ects of unexpected behaviors base on the detection of unexpected-behaviors detector as much as possible, (2) integrates all the information to make a ï¬nal prediction. We build our Output Module atop MLP, it will automatically learn how to complete these two mentioned tasks. MLP is a powerful architecture, it is sufï¬cient to represent any function in theory[23]. The Output Module is illustrated in ï¬gure 6. Fig. 6: Output Module. It eliminates the eï¬€ects of unexpected behaviors and outputs the ï¬nal prediction. Output module consumes the predictive vectors ğ‘ƒ = [ğ‘, ğ‘, ğ‘, ..., ğ‘] to eliminate the eï¬€ects of unexpected behaviors and integrate ğ‘make the ï¬nal prediction Ë†ğ‘¦, as deï¬ned in equation 10. Ë†ğ‘¦indicate the likelihood of user ğ‘¢ interacts with the item ğ‘– in the next time. Similar to the spirit in recent works[6], the recommendation problem in implicit feedback datasets can be regarded as a binary classiï¬cation problem, where an observed useritem interaction is assigned a target value 1, otherwise 0. We choose the binary cross-entropy as our loss function, and learn the parameters of MPM in a pointwise learning manner. The loss function is deï¬ned as equation 11. ğ‘™ğ‘œğ‘ ğ‘  = âˆ’ğ‘¦ğ‘™ğ‘œğ‘” Ë†ğ‘¦+ (1 âˆ’ ğ‘¦)ğ‘™ğ‘œğ‘”(1 âˆ’ Ë†ğ‘¦ Here ğ‘¦ denotes the positive sample set and ğ‘¦means the negative sample set. We elaborate on the implementation details in the section of Experimental Settings. In this section, we conduct experiments on two real-world datasets to evaluate our proposed method. We aim to answer the following research questions: 1. RQ1: Compared with the state-of-the-art methods in implicit feedback setting, how does our method perform? 2. RQ2: How do recent historical interactions aï¬€ect the performance of our method? 3. RQ3: Will a larger model size be helpful for improving performance? We consider two scenarios: movie recommendation and eretailing recommendation. Datasets[24], which are typically used as a benchmark dataset for the recommender systems. We chose to experiment with the ml-20m, ml-10m and ml-1m subsets of MovieLens. 2. For e-retailing domain: we use the Taobao User- Behavior Dataset[25], which is a behavior dataset provided by Alibaba for the study of implicit feedback recommendation problem. We sample three subsets from Taobao user-behavior data set, denoted as Taobao-1m, Taobao-10m, and Taobao-20m. Detail statistics of the datasets are presented in Table 1. Following previous eï¬€orts[1], [6], [17], we evaluate the performance of our proposed model by the leave-one-out evaluation. For each dataset, we hold out the last one item that each user has interacted with and sample 99 items that unobserved interactions to form the test set, a validation set is also created like the test set and remaining data as a training set. For each positive user-item interaction pair in the training set, we conducted the negative sampling strategy to pair it with four negative items. Our datasets are formed as follows, ğ‘ˆğ‘’ğ‘Ÿ ğ‘¢, ğ¼ğ‘¡ğ‘’ğ‘š ğ‘–, ğ»= (ğ‘¥, ğ‘¥, ğ‘¥, ..., ğ‘¥), ğ‘¦(12) We rank the test item with the 99 negative items and then select the ğ‘¡ğ‘œğ‘ âˆ’ ğ¾ items as a recommendation list. We evaluate our model by HR (Hit-Rate) and NDCG (Normalized Discounted Cumulative Gain) metrics, which are deï¬ned as follows: ğ‘ ğ·ğ¶ğº@ğ¾ =1ğ‘€1ğ¼ğ·ğ¶ğº2âˆ’ 1ğ‘™ğ‘œğ‘”(ğ‘– + 1)(15) ğ¼ğ·ğ¶ğº=2âˆ’ 1ğ‘™ğ‘œğ‘”(ğ‘– + 1)(16) Here, In the HR metric, ğ‘…is the rank generated by the model for item ğ‘–. If a model ranks ğ‘– among the ğ‘¡ğ‘œğ‘ âˆ’ ğ¾, the indicator function will return 1, otherwise 0. In the NDCG, ğ‘ is the rank position of ğ‘– in the recommendation list, and IDCG means the ideal discounted cumulative gain[26]. Intuitively, HR measures the presence of the positive item within the ğ‘¡ğ‘œ ğ‘ âˆ’ ğ¾ and NDCG measures the itemâ€™s position in the ranked list and penalizes the score for ranking the item lower in the list. We compare our proposed approach against the competitive baselines list below. â€¢ MF[27], a general matrix factorization model which is one of the most eï¬€ective techniques for capturing long-term preferences. â€¢ NeuMF[6] is a composite matrix factorization jointly coupled with a multi-layer perceptron model for item ranking. â€¢ AttRec[18] take both short-term and long-term intentions into consideration. We use an MLP network replace Euclidean distance to measure the closeness between item ğ‘– and user ğ‘¢. â€¢ MPM-attn, we also implemented a simple version of the MPM model in the conventional fashion[18], in which we learn a short-term preference from instant preferences by self-attention mechanism[28], and then combine the short-term preference and general preference for recommendations. For a fair comparison, we learn all models by binary crossentropy loss and optimize all models with Adaptive Moment Estimationâ€“Adam[29]. We apply a grid search to ï¬nd out the best settings of hyperparameters. After a grid search was performed on the validation set, the hyperparameters were set as follows, the ğ‘™ğ‘’ğ‘ğ‘Ÿğ‘›ğ‘–ğ‘›ğ‘” âˆ’ ğ‘Ÿğ‘ğ‘¡ğ‘’ was set to 0.001, ğ‘ğ‘’ğ‘¡ğ‘was set to 0.9, ğ‘ğ‘’ğ‘¡ğ‘was set to 0.999, ğ‘’ğ‘ğ‘ ğ‘–ğ‘™ğ‘œğ‘› was set to 1e-8, ğ‘’ğ‘šğ‘ğ‘’ğ‘‘ğ‘‘ğ‘–ğ‘›ğ‘” âˆ’ ğ‘ ğ‘–ğ‘§ğ‘’ was set to 32 and ğ‘ğ‘ğ‘¡ğ‘â„ âˆ’ ğ‘ ğ‘–ğ‘§ğ‘’ was set to 1024. We employed a four layers architecture [64, 128, 64, 32] for multi-layer perceptron. In the TCN module, the number of dilated convolution layers is set to 4, our kernel size is 3 and the dilation factors is [1, 2, 4, 8]. 4.3 Performance Comparison (RQ1) Table 2 reports our experimental results for HR and NDCG with cut oï¬€ at 10 on the MovieLens and Taobao UserBehavior datasets, in which the number of historical interactions (history size) is set to 9. From the experimental results, we have the following ï¬ndings: â€¢ MF gives poor performance in both datasets. This indicates that the conventional latent factor models, which rely heavily on the general preference, may fail to capture the slight variation in the preference and have not considered the evolution of usersâ€™ preferences. As a consequence, they achieve an inferior performances. â€¢ NeuMF outperform MF, which demonstrates that incorporating an MLP to model the interaction between users and items is beneï¬cial. NeuMF learn an arbitrary function from data by replacing the inner product with a neural architecture. â€¢ AttRec achieves better performance than NeuMF. It makes sense since by incorporating short-term intention learned from recent historical interactions as ï¬ne adjustment. AttRec fully explore both long-term intent and short-term preference for recommendations. â€¢ It is worth mentioning that MPM-attn also achieves comparable performance to AttRec. MPM-attn is an alternative choice for integrating both long-term preference and short-term preference to make a recommendation, it extracts userâ€™s short-term preference from the recent historical interactions by an self-attention mechanism[28]. â€¢ MPM substantially outperforms AttRec ğ‘¤.ğ‘Ÿ.ğ‘¡ HR@10 and NDCG@10, achieving the best performance. By introducing the positive interactions which are in conformity with the userâ€™s real preferences to train the model, after training, MPM has the ability to eliminate the eï¬€ect of unexpected behaviors. MPM gets a massive improvement in HR@10 and NDCG@10, which relatively increased by 3.643% and 4.107% on average. This demonstrate the importance of eliminating the effects of unexpected behaviors. It also illustrates the eï¬€ectiveness of proposed model. 4.4 Study of MPM (RQ2) For a comprehension of our method, we dive into an indepth model analysis. We start by exploring the inï¬‚uence of multi-preferences to investigate whether MPM can eliminate the eï¬€ects of unexpected behaviors. We then study how the history size aï¬€ects the performance. We only conduct experiments on the Taobao-20m dataset for saving space. To our knowledge, there are two methods to integrate the short-term preference and long-term preference for recommendations in the literature. But, actually, the main diï¬€erence between them is the way of learning short-term preference. One learns a single representation of short-term preference from the recent historical interaction in an autoencoding manner directly[18], in which it doesnâ€™t need to follow the principle of autoregression. The other extracts instant preferences by sequential model in an autoregression manner and then encode these instant preferences to obtain the short-term preference by self-attention mechanism, just like MPM-attn does. However, these works fail to eliminate the eï¬€ect of unexpected behaviors, as no information to guide model learning. To address this problem, we propose a MultiPreferences Model, it eliminates the eï¬€ect of unexpected behaviors through a multi-preferences way. We explore the inï¬‚uence of multi-preferences on Taobao-20m dataset, as illustrated in ï¬gure 7. From ï¬gure 7, we can see that MPM-attn achieved comparable performance to AttRec, even outperforms slightly. After eliminating the eï¬€ects of unexpected behaviors, the Multi-Preferences Model substantially outperforms the stateof-the-art method by a larger margin in both HR@10 and NDCG@10, which increased by 5.47% and 3.23%. This demonstrates that unexpected behaviors can hurt the recommendation performance dramatically and our model can eliminate the eï¬€ects of unexpected behaviors. The other important thing about our model is to decide how many historical interactions should be considered. We cannot simply assume that the longer the history, the more information MPM can capture, and the better performance we got. As usersâ€™ preferences evolve over time, too big a history size can only add the burden on the model and bring in some noise. Here, we study the eï¬€ect of history size on Taobao20m dataset, we search the history size on{5, 7, 9, 11, 13}, as illustrated in ï¬gure 8. From ï¬gure 8, we canâ€™t observe signiï¬cant diï¬€erences between the performances as history size growing from 5 to 13. However, looking from the other side, this can be saw as evidence of MPM can eliminate the eï¬€ects of unexpected behaviors eï¬ƒciently, as the larger of history size, the more unexpected interactions it will encounter possibly, but the performance still keeps in a reasonable region. In addition, we can see a slight improvement on NDCG@10 metric. This indicates that we get a higher quality recommendation list from the model. But, in practices, we should tune the history size for diï¬€erent dataset, this is a trade-oï¬€ between performance and complexity. 4.5 Is Larger Model Size Helpful ? (RQ3) Considering the eï¬ƒciency of model, it is curious to see whether employing a larger model size will be beneï¬cial to the recommendation task. Towards this end, we further investigated MPM with diï¬€erent model size. To ensure the extraction of instant preferences, we keep the architecture of TCN unchanged. The other factors of model size are embedding size and the scale of the MLP module. We combine these two factors to determine the model size. When the embedding size is set to 16, the architecture of MLP is adopted to [32, 64, 32, 16], when the embedding size is 64, the MLP is set to [128, 256, 128, 64] and so on. We conduct experiments on ml-1m dataset to study the eï¬€ects of model size. The experimental result is shown as ï¬gure 9. Fig. 9: The performances of MPM ğ‘¤.ğ‘Ÿ.ğ‘¡ diï¬€erent model size. We can see that the larger the embedding size is, the better the performance of the model is. However, we cannot guarantee a larger model size will lead to better model performance, as overï¬tting could be a possible consequence. On the other hand, the improvement is not signiï¬cant enough in both HR@10 and NDCG@10, as the model size grows up. This may cause by the fact that when model size grows larger, the gain from increasing model size canâ€™t trade oï¬€ the complexity it brings in. As a consequence, we should tune the model size carefully for diï¬€erent datasets. In this work, we explored the inï¬‚uence of unexpected behaviors, which is harmful to recommendations. We propose a multi-preferences model which can eliminate the eï¬€ect of unexpected behaviors. The proposed model extracts instant preferences from recent historical interactions and detect which instant preferences are biased by unexpected behaviors under the guidance of the target item. Then, the output module eliminates the eï¬€ects of unexpected behaviors based on the detection. We also integrate a conventional latent factor model to learn the general preferences for recommendations. Extensive experiments are performed to show the eï¬€ectiveness of our model. In the future, we will extend our work in three directions. First, we attempt to mimic the interactions between users and items via graph neural networks to capture general preferences of users and characteristics of the items, since precise user and item embedding learning is the key to building a successful recommender system. Second, we plan to design a dedicated loss function instead of the general binary cross-entropy loss to get the most out of our modelâ€™s performance. In the end, we should also try some eï¬€ect to bring in more auxiliary information for building a more robust recommender system.