Recently, sequential recommendation systems are important in solving the information overload in many online services. Current methods in sequential recommendation focus on learning a ï¬xed number of representations for each user at any time, with a single representation or multi-interest representations for the user. However, when a user is exploring items on an e-commerce recommendation system, the number of this userâ€™s interests may change overtime (e.g. increase/reduce one interest), affected by the userâ€™s evolving self needs. Moreover, different users may have various number of interests. In this paper, we argue that it is meaningful to explore a personalized dynamic number of user interests, and learn a dynamic group of user interest representations accordingly. We propose a Reinforced sequential model with dynamic number of interest representations for recommendation systems (RDRSR). Speciï¬cally, RDRSR is composed of a dynamic interest discriminator (DID) module and a dynamic interest allocator (DIA) module. The DID module explores the number of a userâ€™s interests by learning the overall sequential characteristics with bi-directional self-attention and Gumble-Softmax. The DIA module allocates the historical clicked items into a group of sub-sequences and constructs userâ€™s dynamic interest representations. We formalize the allocation problem in the form of Markov Decision Process(MDP), and sample an action from policyğœ‹for each item to determine which sub-sequence it belongs to. Additionally, experiments on the real-world datasets demonstrates our modelâ€™s effectiveness. ACM Reference Format: Weiqi Shao, Xu Chen, Jiashu Zhao,Long Xia, Dawei Yin. 2022. Gumble Softmax For User Behavior Modeling. In Proceedings of ACM Conference (Conferenceâ€™17). ACM, New York, NY, USA, 9 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn With the development of Internet technologies, recommender systems have been widely applied to many online services such as Figure 1: From the click sequences of user A and user B, there are multi conceptually distinct items in a userâ€™s click behaviors which indicates the change of dynamic number of userâ€™s interest and different interest number between users by the time. e-commerce, advertising, social media, and etc. Recommender systems serve to alleviate the information overload problem and enhance user experiences. Traditional recommender systems mostly focus on promoting generalized user interests, such as collaborative ï¬ltering [22,23]. In recent years, more and more researchers study the sequential recommendation problem to capture the dynamic user behaviors, which assumes that a userâ€™s information need changes over the time [19]. The existing sequential recommendation solutions represent a user as a ï¬xed number of representations, including a single representation or multiple representations. For the single representation recommendation, only one user embedding representation is generated for the next-item prediction. Early solutions usually adapted the Markov Chain [19] which assumes that the next-item prediction is closely related to the previous item [20]. With the breakthrough of deep learning in many areas (e.g. computer vision and natural language processing) [36], sequential neural networks such as recurrent neural network [9,17]and Transformer [29]have been adopted to the sequential recommendation tasks. These sequential neural networks can characterize the sequential item interactions and learn informative representations for user behaviors [14]. Additional context information can also be considered to enhance the performance of neural sequential recommendation [10,37]. For multi-representation recommendation approaches, a user is assumed to have multiple interests and these interests jointly affect the userâ€™s next item selection. From the empirical analysis, a user usually interacts with several types of items that are conceptually different over time. For example, zhang [37] identiï¬es that the items in a userâ€™s recent behaviors belong to different categories on Taobao dataset. Various approaches have been adopted to model the multiple interests from the userâ€™s historical behaviors, including Capsule routing network [21] and multi-head self-attention [33]. The temporal information in the sequence can also be considered to enhance the recommendation performance [4]. All the multi-interest modeling approach rely on a pre-given ï¬xed number to generate the corresponding number of representations, which assumes that the numbers of interests for all users are the same and do not change over the time. However, the ï¬xed-number of interest assumption is not necessarily true in the real applications. For example, one user may have very broad interests, and another user have more focused intents. Figure 2 shows two users each with a sequence of interacted (i.e. clicked) items, user A overall has three interests (furniture, electronic products, and sport products), while user B has two interests only ((furniture and electronic products). On the other hand, throughout the user behaviours over the time, a user may have more/less interests. In Figure 2, user A is only interested in furniture at the beginning, then A gradually start to show interest in electronic products and sport products. So we can see that user the number of Aâ€™s interest changes from one to three. Therefore, modeling a ï¬xed number of interests can not fully simulate the real user intents. If a user has more interests than the given ï¬xed number, then the userâ€™s intent can not be accurately represented. On the other hand, if a user has less interests than the given ï¬xed number, then the userâ€™s intent will be represented with noise. Therefore, it is important to consider userâ€™s dynamic interest number in recommendation. In this paper, we propose a promising alternative method to learn a dynamic group of embedding representations for a userâ€™s behavior sequence, where each embedding representation encodes one aspect of the userâ€™s intends. Inspired by the above observations, we introduce Learning Reinforced Dynamic Representations for Sequential Recommendation(RDRSR) to learn the a dynamic of group representations. Specifically, we design Dynamic Interest Discriminator(DID) to detect the dynamic number of a userâ€™s interests using self-attention [29] and Gumble-Softmax [11]. With self-attention, the items with high attention weights are clustered together to form different interests. And then with the informative item representations, Gumble-Softmax determines the interest number with the Gumble distribution as the noise to improve the exploration of the user interest number. Furthermore, we design the Dynamic Interest Allocator(DIA) to allocate the userâ€™s click sequence into a dynamic group of interest sub-sequences, where DIA formalizes the allocation process in the form of Markov Decision Process(MDP) and sample the action for each item to determine which sub-sequence it belongs to. Here each sub-sequence forms a userâ€™s interest representation with average-pooling method. As for the next-item prediction, we input the candidate item into the policyğœ‹to decide which sub-sequence it belongs to and use the corresponding user interest representation to calculate the compatibility between the sub-sequence and candidate item for prediction. To summarize, the main contributions of this paper are: â€¢To the best of our knowledge, we are the ï¬rst to consider a dynamic number of interest in sequential recommendation. The explore of user interest number improves the performance in the sequential recommendation. â€¢We propose the RDRSR model. The RDRSR model includes DID to learn the userâ€™s dynamic interest number over the time and leverages the DIA to allocate the click into different sub-sequence to form multi interests through average-pooling method for the next item prediction. â€¢We conducted experiments on several real datasets with several public benchmarks to verify the effectiveness of the model. We analyze the DID module and DIA module to valid the proposed RDRSR model through ablation study. Before introducing the details of the proposed model, in this section, we introduce the related literature about recommendation systems, including general model, sequential model, multi-interest recommendation systems and attention mechanism we used in the paper. The main methods in traditional recommendation system is extracting usersâ€™ general tastes from their historical behaviors to make recommendation. Typical methods include Collaborative Filtering [22, 38], Matrix Factorization [15] and Factorization Machines. Collaborative Filtering method is based on the similarity of users [38] or the similarity of items [22] for recommendation. But it is a non-trivial work to quickly and accurately ï¬nd the similar users or items. Matrix Factorization(MF) [15] as one the most popular technique in recommendation system, map users and items into joint latent space and estimate user-item scores through the inner product between their embedding vectors. Factorization Machines(FM) [19] methods consider all the variable interaction information which not only improve the recommendation results but also achieve good results even when the data is sparse. With the success of deep learning in computer vision and natural language processing [36], more and more efforts has been done to apply deep learning to the recommendation system [34]. He [7,7,8] makes a great success, NCF [7]uses multi-layer perceptions to replace the inner product operation in MF for interaction estimation. [7,8]use deep learning to obtain higher-oeder interactive expressions of interaction with a fast calculation trick. These deep learning based methods achieve good performance. Moreover, several attempts also tried to apply graph neural networks [6, 13, 26]. In relevant literature,many sequential recommendation models have been proposed to leverage user historical records in a sequential manner to capture the userâ€™s preference for the next item. By integrating the good performance of matrix factorization and the sequential pattern of Markov chains, factorized personalized Markov chains (FPMC) [20] embeds the sequential information between adjacent clicked items into the ï¬nal prediction for recommendation, and later the hierarchical representation model (HRM) [30] simultaneously consider the sequence behaviors and user preferences. Though they make progress in sequential recommendation, these methods only model the local sequential patterns between every two adjacent clicked item [35]. To model longer sequential behaviors, [9] ï¬rst adopted recurrent neural network to model the long sequence pattern for recommendation, RNN care too much about the sequence pattern which could be disturbed by the noise in the click sequence while neglect the userâ€™s main intent, [17,18] not only consider the sequence pattern in the sequence and also explore the userâ€™s main purpose through the attention mechanism. Later, [14] consider the importance of each item and other items in the click sequence achieve great progress in many real datasets [24] with unsupervised learning to learn the hidden relationships between items and make a difference. The main difference between multi-interest recommendation and single embedding recommendation is that multi interest recommendation uses multi vectors to represent the user while only one vector in other methods. The classic method [3,16] use a capsule routing based method to extract the userâ€™s multi interest. [33]explore userâ€™s with multi-head self-attentive, where the multi-head number as the multi-interest number through sum-pooling method. [4]consider the time interval to extract the multi interest and [27] infer a sparse set of concepts for each user from the large concept as its multi interest. Those methods have achieved good performance in recommendation, but non of them consider the different interest number between different users at different time and the dynamic user interest number over time. The originality of attention mechanism is in computer vision [2,25] to make the target object get more weight, but its great success in various ï¬elds in artiï¬cial intelligence comes only in recent years with the development of deep learning. It ï¬rst come to the center of the stage is in machine translation [1,29] and is rather useful and efï¬cient in real-world application tasks. It is also been successfully applied in recommendation applications [32] which learns the importance of each feature interaction from data via a neural attention network. Whatâ€™s more, [14,24] use the different relationships between items in the clicked sequence to capture both the long-term semantics and short-term semantics make a difference. In this section, before going into the details of our proposed model. We ï¬rst describe the problem statement in our work. And then we will give an overview of the proposed Learning Reinforced Dynamic Representations for Sequential Recommendation(RDRSR) framework(as shown Figure 2), which consists two main modules DID and DIA for dynamic interest number detector and user behavior allocation. The key claim of sequential recommendation is that the current user preference should be related with the historical behaviors. Formally, suppose we have a user setU={ğ‘¢,ğ‘¢, ...,ğ‘¢}, an item set I={ğ‘–,ğ‘–, ...,ğ‘–}, andğ‘›andğ‘šare the numbers of users and items in the sequential recommendation task. Unlike general recommendation, which only captures the correlation between a user and an item without considering the order of the click sequence. We use C={ğ‘¥,ğ‘¥, ...,ğ‘¥,ğ‘¥}to denote a sequence of items in chronological order that a user has interacted, and theğ‘¥âˆˆ I. The goal of sequential recommendation is to predict the next itemğ‘¥depending on the precious click sequence {ğ‘¥,ğ‘¥, ...,ğ‘¥}. We create an item embedding matrixğ¸âˆˆ Rand an user embedding matrixğ¸âˆˆ R, where d is the latent dimension andğ‘›andğ‘šare the number of user and item. We retrieve the click item in the click sequenceC={ğ‘¥,ğ‘¥, ...,ğ‘¥}with a latent vector in the item embedding embedding and get the item sequence embedding ğ¸={ğ‘’,ğ‘’, ...,ğ‘’}and the corresponding user embeddingğ‘’, where t is the click sequence length and we process the datasets like [14] Furthermore, we incorporate a learnable position encoding matrix ğ‘ƒ âˆˆ ğ‘…to enhance the input representations. In this way, the input representationsğ¸ âˆˆ ğ‘…for the generator can be obtained by summing two embedding matrices: ğ¸ = ğ¸+ğ‘ƒ. As mentioned before, the userâ€™s dynamic interest number is evolving and changing by the time, a new click item would indicate user get one more interest or reduce a interest due to he may get what he want. DID aims to ï¬nd the user dynamic interest number with the userâ€™s current click sequence. First, we stack multiple bi-directional architecture self-attention [29] block based on the embedding layer. With the bi-directional architecture self-attention block, interest relevant items in the click sequence are clustering more close and get a more informative item representation. Self-AttentionFrom the formula, the attention layer calculates a weighted sum of all values, where the weight between query and value, which could cluster those items belong to the same interest and effectively ï¬nd the dynamic interest number. And the scale factorâˆšğ‘‘is to avoid overly large values of the inner product when the dimension is very high. We take E as input, convert it to three matrices through linear projections, and feed them into an attention layer: where the projections matricesğ‘Šğ‘Šğ‘Šâˆˆ R. The projections make the model more ï¬‚exible. Feed Forward NetworkThough the attention calculation is able to aggregate previous itemsâ€™ embeddings with corresponding weights, it is still a linear model. In order to enforce the model with non-linearity and to get more high-order interaction information, we apply a two-layer feed-forward network to all S. whereğ‘Š,ğ‘Šare d x d matrices andğ‘,ğ‘are d-dimensional vectors. In order to get the userâ€™s dynamic interest number, we set an attention mechanism with the F andğ‘’to get the united user general purpose representation. where f is a k-dimension vector represent the probability of each possible interest number,ğ‘Šis a dxk matrix and k is the max dynamic interest number set in our model. Figure 2: An overview of our model RDRSR. The input of our model is a user behavior sequence and those items are fed into the embedding layer and transformed into the item embeddings. Dynamic Interest Discriminator concentrates on exploring the dynamic interest number with a bi-directional architecture self-attention and Gumble-Softmax. Dynamic Interest Allocator activates the corresponding allocation policy according the learning interest number in DID. Then DIA allocates the click item into the subsequence with activeated policy and form different interests through average-pooling method. Last, we put the target item into the policy ğœ‹ to decide which sub-sequence it belongs to and use the corresponding user interest representation to calculate the rewards between the interest and target item which will be used for prediction. Gumble Softmax SamplingWe employ the Gumble Softmax [11] sampling method to produce the user dynamic interest number. DID(Dynamic Interest Discriminator) draws z from a categorical distribution with class probabilities ğ‘“ = {ğ‘“,ğ‘“, ...,ğ‘“}. whereâ„is current generated dynamic interest number and{ğ‘”,ğ‘”, ...,ğ‘”} are sample drawn fromğºğ‘¢ğ‘šğ‘ğ‘™ğ‘’(0,1) distributions. In practice, we sample theğºğ‘¢ğ‘šğ‘ğ‘™ğ‘’(0,1) distribution using inverse transform sampling by drawing u from a uniform distribution. Whatâ€™s more, those added new Gumble distribution as the noise changes the probability distributions and give other original non-max alternative interest number chance to be chosen, which improve the exploration of the user interest number and make our model more solid. where u is sampling from Uniform(0,1). The argmax operation in Eq. (6) is non-differentiable, but we can resort to the Gumbel Softmax distribution, which adopts softmax as a continuous relaxation to argmax in order to alleviate the nondifferentiable problem used in Eq. (11)(12). ğ‘§=ğ‘’ğ‘¥ğ‘ ((log ğ‘“+ğ‘”)/T)Ãğ‘“ ğ‘œğ‘Ÿ ğ‘– = 1, 2, . . . , ğ‘˜(8) whereTis a temperature parameter to control the discreteness of the output vectorğ‘§, which is set 10 in our model. Now, we get the probabilityğ‘§for each dynamic interest number. During the forward pass, we sample the dynamic interest numberâ„using Eq. (6) for the click item sequence. As for the backward pass, we are able to estimating the gradients of the discrete samples by computing the gradients of the continuous softmax relaxation ğ‘§ in Eq. (8). After the userâ€™s dynamic interest numberâ„generated in Eq. (6) and more informative representationğ¹in Eq. (3) of item vector found in DID, DIA split the click sequence into different sub-sequences, where each sub-sequence represents a userâ€™s interest and we use average-pooling method to get the userâ€™s interest representation of those sub-sequence. DIA formalizes the allocation click sequence problem in the form of Markov Decision Process(MDP), and sample action form policyğœ‹for each item to determine which sub-sequence it belongs to. Item representationğ¹with the bi-directional architecture self-attentive, our policyğœ‹can foresee future sequential information when making a decision, which could offer insightful clues to determine item-level relevance without direct supervision signals. We consider an episodic RL approach to allocate the click sequence C={ğ‘¥,ğ‘¥, ...,ğ‘¥}into h sub-sequenceğ‘†={ğ‘ ğ‘¢ğ‘,ğ‘ ğ‘¢ğ‘, ...,ğ‘ ğ‘¢ğ‘}and each sub-sequence represent a user interest represenation. Episode RLWe see the allocation sequence split as an episode RL approach. At each time T, the process is in some stateğ‘ âˆˆ ğ‘†. According to the stateğ‘ , the agent performs an actionğ‘modeled by a policyğœ‹ (ğ‘|ğ‘ ). The action space isğ‘ âˆˆ {ğ‘,ğ‘, ...,ğ‘}, where ğ‘is that at time T, theğ‘–ğ‘¡ğ‘’ğ‘šis belongs toğ‘ ğ‘¢ğ‘ âˆ’ğ‘ ğ‘’ğ‘ğ‘¢ğ‘’ğ‘›ğ‘ğ‘’. The following is the policy ğœ‹. ğœ‹ (ğ‘|ğ‘ ) = ğ‘†ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘…ğ‘’ğ¿ğ‘ˆ (ğ‘ ğ‘Š+ğ‘))ğ‘Š+ğ‘(9) whereğœ‹ (ğ‘|ğ‘ )is the discrete probability distribution thatğ‘–ğ‘¡ğ‘’ğ‘š belongs to sub-sequenceğ‘andğ‘Šis a d xğ‘‘matrix andğ‘Šis a ğ‘‘x h matrix. State TransitionWe give each sub-sequence an initial multi interest representation at time 0ğ‘ƒ={ğ‘,ğ‘, ...,ğ‘}where eachğ‘ is a d-dimension vector and initialize with the corresponding user embeddingğ‘’. At time T, we put theğ‘ into the policyğœ‹to get the actionğ‘(the sub-sequenceğ‘ğ‘–ğ‘¡ğ‘’ğ‘šbelongs to). We then use our well-designed pooling method to update the corresponding interest representation embedding with the new addedğ‘–ğ‘¡ğ‘’ğ‘š, where the representation of ğ‘–ğ‘¡ğ‘’ğ‘šis ğ¹. In reality, there are complex relationships between the userâ€™s click sequence, like point level,union level with or without skip[28]. For accurately capturing those relationships, we use a well-designed attention mechanism to deï¬ne the state transition, which explore the relationships between the new click item and the generated subsequence with a weighted sumğ‘,ğ¹in Eq. (3) and dynamic interest number distribution probability informationğ‘§in Eq. (8) through a Neural Networks to get the ğ‘  where ğ‘Šis a 2ğ‘‘ğ‘¥ğ‘‘ matrix and (Â·) represent the inner product. Even we use a hard allocate, but some information from other other sub-sequence is transitioning into the dynamic interest representation when we deï¬ne the state transition, which makes our model more solid. Reward SettingAfter the allocation process, we get the multi interests representation at time tğ‘ƒ={ğ‘,ğ‘, ...,ğ‘}. With the generated dynamic multi interests, here comes to the question that which interest representation is related to the target item. To conï¬rm the target interest,we use the target itemğ‘to get the current state ğ‘ through formulasğ¸ğ‘.(11)(12)and put it into the policy net ğœ‹ (ğ‘|ğ‘ )set in Eq. (9) to sample action for getting the subsequenceğ‘the target item belongs to. Here we leverage a Sampled Softmax technique [5,12] to calculate reward where the relationshipğ‘with the target item and other candidate item will be considered. o is the sample item number in the dataset. Throughğ‘…consider other items when calculate the reward, it doesnâ€™t use other generated multi interests which means that only when our target interest selection is correct, the reward is the optimal result. In order to promote the policyğœ‹to choose the right action, we employ a baseline in the reward function which use the average scores of the all generated multi interests, deï¬ned as: with the baseline reward setting, and the advantage of selected dynamic interest representation reward setting is as below: In order to enforce the learned dynamic multi interests representation orthogonally. Speciï¬c, we denote theğ‘…as the mean of the absolute value of the inner product between all different generated dynamic interest representations ğ‘in ğ‘ƒ. where| Â· |represents the absolute value of inner product between ğ‘andğ‘inğ‘ƒ. Combine the two reward above, the ï¬nal reward function of our model is: whereğœ†is the trade-off parameter to balance the two rewards, which is set 0.001 in our experiments. We treat the allocation task as a RL problem and apply the classic policy gradient to learn the model parameters. Speciï¬cally, the corresponding probability of generatingğ‘sub-sequence isP(ğ‘ ğ‘¢ğ‘) which is calculated as follows: P(ğ‘ ğ‘¢ğ‘) =ğœ‹ (ğ‘|ğ‘ , ğœƒ ) âˆ— ğ‘ƒ (ğ‘ |ğ‘ , ğ‘, ğœƒ ) =ğœ‹ (ğ‘|ğ‘ , ğœƒ ) Theğ‘is then used for the dynamic interest selection for target itemğœ‹ (ğ‘|ğ‘ )in Reward Setting. Thus, the probability of the generated sample action sequence is as followed: Formally, the objective of the policy network is to maximize the expected reward at the ï¬nal prediction. whereğ‘…is deï¬ned in Eq.(17) and its gradient will be detached in the training process, C is the all the generated action sequence of target sub-sequence andğœƒis the parameters of the model including the parameters of DIA and DID. The gradient of the objective function âˆ‡J(ğœƒ) regard to the model parameters ğœƒ can derived as: âˆ‡J(ğœƒ) = âˆ‡ğ‘…âˆ— P(ğ‘ ) Therefore, the optimization of the policy network is calculate with a log trick as follow: Here we use the standard cross-entropy and a Sampled Softmax technique [5, 12] to calculate the classiï¬cation loss: o is the same as the negative sample number in reward calculation. Finally, we jointly train the allocation task and classiï¬cation task with a trade-off parameter ğ›½: ğ›½control the weight of theLloss, which is set 1 in our experiments. When we do the prediction, we ï¬rst scan the user click session and select each action with the maximal probability at policyğœ‹Eq. (9) and corresponding state transition Eq. (11)(12) , which can be written as follows: For each candidate item, we put it into policy net to get which subsequence it belongs to and get its corresponding reward. We then rank all candidate items according to their rewards at Eq. (13) and return the top-ğ‘ rewards item as the ï¬nal recommendations. In this section, we conduct experiments on sequential recommendation to evaluate the performance of our proposed method RDRSR on three benchmark datasets compare with several state-of-the-art baselines. We ï¬rst brieï¬‚y introduce the datasets and the state-of-theart methods, then we conduct experimental analysis on the proposed model and the benchmark models. Speciï¬cally, we try to answer the following questions: â€¢How effective is the proposed method compared to other stateof-the-art baselines? Q1 â€¢What are the effects of the DIA(Dynamic Interest Allocator) and DID(Dynamic Interest Discriminator) modules through ablation studies? Q2 â€¢How sensitive are the hyper-parameter the max dynamic interest number ğ‘˜ in proposed model RDRSR? Q3 In this section, we introduce the details of the three experiment datasets, evaluation metrics, and comparing baselines in our experiments. DatasetsWe perform experiments on three publicly available dataset, including MovieLens, Lastfm and Foursquare. And the relative statistics information of the three datasets are shown in Table 1. Ml âˆ’ 100kis a dataset about userâ€™s rating score for movies. In experiments, we follow [8] to preprocess the dataset. Foursquareis a location based social networks datasets which contains check-in, tip and tag data of restaurant venues in NYC collected from Foursquare from 24 October 2011 to 20 February 2012. Lastfmrecords the music records of users from Last.fm. In experiments, we only use the click behaviors. For Foursquare and Movielens datasets, we ï¬lter items and users interacted less ten times, and ï¬ve times in Lastfm datasets. And all datasets are taken Leave-one-out method in [14] to split the datasets into training, validation and testing sets. Speciï¬cally, we split the historical sequence for each user into three parts: (1) the most recent action for testing, (2) the second most recent action for validation, and (3) all remaining actions for training. And if the click sequence length is less than t, we repeatedly add a â€˜paddingâ€™ item to the left until the length is t. Note that during testing, the input sequences contain training actions and the validation actions. BaeslinesWe compare our proposed model RDRSR with the following state-of-the-art sequential recommendation baselines, including single representation methods and multi representation methods. Single representation modelsThe most common sequential recommendation methods which generates a single embedding representation for the next-item prediction. â€¢ GRU4Rec[9] is a pioneering work which ï¬rst leverages GRU to model user behavior sequences for prediction. â€¢ Caser[28] is a recently proposed CNN-based method capturing sequential pattern by applying convolutional operations on the embedding matrix for the most recent items, and achieves state-ofthe-art sequential recommendation performance. â€¢ BERT4Rec[24] is a recently proposed BERT-based method which achieves state-of-the-art sequential recommendation performance. â€¢ STAMP[18] is a neural sequential model by incorporating user short-term memories and preferences. Table 2: Overall comparison between the baselines and our models. The best results are highlighted with bold fold. All the numbers in the table are percentage numbers with â€™%â€™ omitted. Multi representation modelSequential recommendation methods that generates multi representation to model user click behavior for the next-item prediction. â€¢ MCPRN[31] is a recent representative work for extracting multiple interests which designs a mixture-channel purpose routing networks with a purpose routing network to detect the purposes of each item and assign them into the corresponding channels to form multi presentations. Parameter Conï¬guration.For a fair comparison, all baseline methods are implemented in Pytorch and optimized with Adam optimizer with a mini-batch size of 2048. The learning rate is tuned in the ranges of [0.01,0.001]. We tuned the parameters of comparing methods according to values suggested in original papers and set the embedding sizeğ‘‘as 64, and sequence length t=10. For our method, it has three crucial hyper-parameters: the trade-off parameterğœ†,ğœ† and the max dynamic interest numberğ‘˜. We searchğ‘˜from 3, 4, 5, and we setğœ†,ğœ†0.001 and 1. In order to keep the policy consistent in Dynamic Interest Allocator, we put the same user traing dataset in a batch to train the model. The conï¬guration of the other two parameters max dynamic interest number k and neg samples o for three datasets are reported in Table 3. Table 3: The optimal setting of our hyper-parameters for our model. Other parameters like dimension ğ‘‘ and learning rate ğ›¾ are set as 64 and 0.001, respectively. Evaluation Metrics.For each user in the test set, we treat all the items that the user has not interacted with as negative items. We use two commonly used evaluation criteria [7]: Hit Rate (HR) and Normalized Discounted Cumulative Gain (NDCG) to evaluate the performance of our model. Table 1 summarizes the performance of RDRSR and baselines including single-representation and multi-presenation methods on three benchmark datasets. Obviously, RDRSR achieves comparable performance to other the baselines on the evaluation metrics in general. In the baselines of single representation methods, we ï¬nd that GRU4Rec obtains good performance over other singlerepresentation methods. Whatâ€™s more, compare single representation methods with multi representation methods, it is obvious that recommendation with multiple presentations ( MCPRN, RDRSR) for a user click sequence perform generally better than those with single representation (Caser, GRU4Rec, BERT4Rec ...). Therefore, it is necessary to explore multiple representation to model userâ€™s diverse intents. Moreover, we can observe that the improvement introduced by capturing userâ€™s various intentions is more signiï¬cant for Movielens and Lastfm datasets due to their density. The users in denser datasets like Movielens and Lastfm tend to exhibit more diverse interests in online activity than rating datasets Movielens, which veriï¬es the necessity of our motivation to model the dynamic interest number in the user behavior and the effectiveness of the DID module in exploring the userâ€™s dynamic interest number. The improvement of RDRSR over the ï¬xed interest number multi representation method(MCPRN) shows that dynamic interest exploration serves as a better multi-interest extractor than ï¬xed multi interest. Considering the RDRSR and other baselines results, RDRSR consistently outperforms them on three datasets over all evaluation metrics. This can be attributed to two points: 1) The Dynamic Interest Discriminator explores userâ€™s dynamic interest number which takes the advantage of single representation methods when userâ€™s intent is one and multi representation methods when userâ€™s intents are more than one. 2) RMRSR could correctly explore userâ€™s dynamic interest number and generates corresponding dynamic interest representation for next-item prediction while all other methods could be seen as ï¬xed interest number method which are without enough ï¬‚exibility. In our model, a major novelty is that we want to explore the userâ€™s dynamic interest number and form the corresponding dynamic interest representation. To obtain a better understanding why RDRSR performs better than other models, shown in Figure 3, we further construct a case study on Movielens dataset. Speciï¬cally, we present a snapshot of the interaction sequence for a sampled user, which contains seven items, and top-one as the recommendatio result. Here we use different colors to represent the different dynamic interest sub-sequences, which is captured by the DID and DIA modules, and the total number of colors is equal to the userâ€™s dynamic interest number. The ï¬rst ï¬ve items are userâ€™s click behavior. In the ï¬rst line at time t=6, the new movie doesnâ€™t not increase the userâ€™s dynamic interest number and the dynamic interest number is still 2. The second line at time t=6, the new movie with one more color yellow means that the userâ€™s dynamic interest number is increasing from 2 to 3. Next, the userâ€™s new interest in sci-ï¬ movie is main for the next-item prediction at time t=7. The result shows that our model can correctly explore the userâ€™s dynamic interest number and makes better recommendation. Figure 3: Case study. The left before t=5 is the user behavior, at t=6 the user clicks two different movies and get different recommendation results. Here we use colors to represent dynamic interest sub-sequences. The picture of each movie is downloaded from https://movie.douban.com. We introduce one variant (RDRSR-F) to validate the effectiveness of the proposed model. Speciï¬cally, RDRSR-F shuts down the module Dynamic Interest Discriminator, and the the module Dynamic Interest Allocator set a ï¬xed dynamic interest number. We conduct experiments on all three datasets. Table 4 reports the results in terms of NDCG@10. RDRSR-F3 and MCPRN-3 means that the ï¬xed interest number is 3 and the max dynamic interest number of RDRSR-3 is 3. Obviously, RDRSR-3 signiï¬cantly outperforms the variant RDRSR-F3 on all datasets. The substantial difference between RDRSR-F3 and RDRSR-3 shows that the learning dynamic user dynamic interest number in DID module is better than those ï¬xed interest number in RDRSR-F3. And it veriï¬es our motivation to explore the user dynamic interest number in sequential recommendation and the effectiveness of the proposed module DID. Whatâ€™s more, the improvement of RDRSR-F3 over MCPRN-3 validates that our DIA module is useful to model userâ€™s dynamic interest representations for next-item recommendation. Table 4: Ablation study. Performance comparison of RDRSR-3 (max dynamic interest number 3), its variant RDRSR-F3 and MCPRN-3 (ï¬xed interest number 3) over three datasets. And all the numbers in the table are percentage numbers with â€™%â€™ omitted. We also investigate the sensitivity of the max dynamic interest numberğ‘˜to RDRSR in all three datasets. Figure 4 reports the performance of our model in the metrics of HR and NDCG. In particular, We keep the other parameters in the model consistent with the Q1 settings. From the ï¬gure, we can observe that RDRSR obtains the best performance of HR and NDCG whenğ‘˜equals 3 or 4. With the ï¬xed sequence length t the result increases with the increase of the max dynamic interest numberğ‘˜, which indicates that the userâ€™s dynamic interest number is multi and bigger max dynamic interest number set in the model may meet the requirements better. RDRSR becomes a single representation method(RDRSR-1) when the max dynamic interest number is 1. The sub-optimal results achieved Figure 4: Hyperparameter study, where the horizontal coordinates is the max dynamic interest number from 1 to 7, and the the vertical coordinates are the metric HR@10 and NDCG@10. by RDRSR-1 gives evidences that single representation is not the best solution for sequential recommendation and the necessity of dynamic interest representations methods. The recommendation performance increases at the beginning, but decreases after reaching a peak due to the complex model structures with bigger max dynamic interest number k, which brings more noise and makes sub-optimal recommendation. In this article, we learning a dynamic group of representations for user to improve the performance of the sequential recommender system. In order to achieve this goal, we design DID and DIA to capture the dynamic interest number and form the corresponding dynamic interest representations. Whatâ€™s more, we conducted a ablation study to explore the effectiveness of DID and DIA modules and veriï¬ed the effectiveness of RDRSR on several real datasets with SOTA methods. To the best of our knowledge, we are the ï¬rst to consider the personalized dynamic interest number in sequential recommendation. However, the proposed model also exists shortcomings in computing speed, where we formulate the allocation task in DIA module as a MDP problem which is computing cost and unstable in training. In the future we will consider how to allocate the click sequence in a more effective way.