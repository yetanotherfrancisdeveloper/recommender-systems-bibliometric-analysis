Abstractâ€”Graph neural networks (GNN) have shown outstanding applications in many ï¬elds where data is fundamentally represented as graphs (e.g., chemistry, biology, recommendation systems). In this vein, communication networks comprise many fundamental components that are naturally represented in a graph-structured manner (e.g., topology, conï¬gurations, trafï¬c ï¬‚ows). This position article presents GNNs as a fundamental tool for modeling, control and management of communication networks. GNNs represent a new generation of data-driven models that can accurately learn and reproduce the complex behaviors behind real networks. As a result, such models can be applied to a wide variety of networking use cases, such as planning, online optimization, or troubleshooting. The main advantage of GNNs over traditional neural networks lies in its unprecedented generalization capabilities when applied to other networks and conï¬gurations unseen during training, which is a critical feature for achieving practical data-driven solutions for networking. This article comprises a brief tutorial on GNNs and their possible applications to communication networks. To showcase the potential of this technology, we present two use cases with state-of-the-art GNN models respectively applied to wired and wireless networks. Lastly, we delve into the key open challenges and opportunities yet to be explored in this novel research area. Network modeling is a fundamental component for efï¬cient operation, control and management of communication networks. For example, a network model enables to make what-if analysis by predicting key performance indicators (e.g., latency, jitter, loss) for particular network scenarios (e.g., check new conï¬gurations, trafï¬c changes, topology upgrades, potential failures). Likewise, it can be used for autonomous network operation by pairing the model with an optimization algorithm (e.g., local search, reinforcement learning) [1], [2]. Traditionally, network models have been implemented with analytical approaches, mainly based on ï¬‚uid models or queueing theory. However, these models offer limited capabilities to reproduce the behavior of real-world networks (e.g., real trafï¬c, multi-hop routing) [3]. In this context, Machine Learning (ML) has been recently positioned as a promising technique to achieve more accurate network models, while offering limited execution cost Â´es, Krzysztof Rusek, Fabien Geyer, Xiangle Cheng, for operation in large-scale environments. Particularly, Deep Learning is gaining increasing attention in communication networks, motivated by the outstanding results that these models have already shown in other domains (e.g., computer vision, natural language processing). Neural networks (NN) are data-driven, this means they can directly learn from real data. As a result, they expand the possibilities to accurately model the behavior of real networks at a ï¬ne-grained level of detail (e.g., protocols, physical effects, hardware-level impact), without the need for introducing theoretical assumptions as those of classical network models (e.g., ï¬‚uid models, queuing theory). At the same time, NNs are highly parallelizable (e.g., on GPUs, TPUs), allowing to effectively scale to large real-world networks and Big Data environments. Recent years have seen great efforts to produce network models by leveraging well-known NN types with popular applications in other domains (e.g., Fully-Connected NNs, Convolutional NNs, Autoencoders). However, such types of models are not suited to learn from network-related data, and â€” as a result â€” they often do not perform well when applied to other networks and conï¬gurations unseen during the training phase. The main reason behind this lack of generalization is that communication networks comprise relational information at many different levels (e.g., topology, routing, user connections, signal interference), and traditional NNs are not designed to capture such type of information. The natural way to represent relational information is in the form of graphs, i.e., as sets of elements and connections encoding their relationships. Indeed, the networking community has traditionally relied on graphs as a central element to represent networks and solve a plethora of control and optimization problems [2]. This eventually calls for employing deep learning methods more suitable for graph-structured relational data. In light of the above, this article posits graph neural networks (GNN) [4] as a key enabler to produce a new generation of data-driven network models with strong generalization capabilities over network data. GNNs are precisely a NN family tailored to process and learn from graphs. Unlike more traditional NN models, GNNs exhibit unique properties to learn and exploit relational patterns between the different elements in graphs, which is also referred to as strong relational inductive bias [5]. This eventually extends the possibility to achieve better performance and accurately generalize to other networks and conï¬gurations unseen during training. As such, this new breed of Deep Learning models have already produced a variety of applications showing an unprecedented level of generalization over communication networks (e.g., wired/wireless networks, data centers, IoT, SDN, NFV) [6]. Graphs are an essential data type to generate structured and self-contained representations of many real-life elements, such as molecules in chemistry, gravitational systems in physics, proteins in biology, or user relations in social networks. In the ï¬eld of communication networks, graphs are pervasively used to represent many fundamental network components, such as the topology, routing, dependencies between ï¬‚ows, user connections, interference, and many others. In general, graphs enable to represent in a structured manner the different elements that compose a network scenario (e.g., devices, users, applications) and the underlying relationships between such elements, which are crucial to solve many networking problems [2]. Table I shows a comparison of some popular NN models available at the time of this writing. The most classical and generic models are Fully-Connected NNs. These models are considered as universal approximators, which can be applied to virtually any problem and data type. Then, Convolutional NNs and Recurrent NNs emerged as new architectural variants to target speciï¬c data types: images, and sequences respectively. The main essence behind these two latter NN types is that they are tailored to capture and exploit meaningful information from the structure of their input data, thus adding this new dimension to the knowledge learned during training. Particularly, Convolutional and Recurrent NNs are respectively invariant to spatial and temporal translations, which represent opportunistic biases to generalize better over their target data types (i.e., images and sequences) [7]. In this context, a lesson learned from the recent history of Deep Learning is that generic Fully-Connected NNs systematically show weaker performance when applied to problems where special-purpose NNs can be leveraged to exploit the target data structure (e.g., Convolutional NN, Recurrent NN) [7]. GNNs [4] are thus a more recent family of NN models speciï¬cally designed to process and learn from graphstructured data. For this purpose, these models implement a modular NN architecture that explicitly represents the elements (nodes) and connections (edges) of input graphs, and introduce some inductive biases intended to capture meaningful patterns from the graphs seen during training (e.g., they are equivariant to node and edge permutation) [5]. As a result, these models have already demonstrated groundbreaking applications in ï¬elds where graphs are ubiquitous (see applications in Table I). Indeed, Transformers â€” which are gradually replacing Recurrent NNs in natural language processing â€” are considered a special case of GNNs in which sentences are represented as fully-connected graphs of words, which allows to better capture the relationships between words (i.e., their semantic context) regardless of their position in the sentence. This section provides an overview of Message-Passing Neural Networks (MPNN) [8], which are a subclass of GNNs especially interesting from the networking point of view, as they can be naturally distributed over different NN modules deployed in network devices that communicate with their neighbors via message passing, which ï¬ts the distributed nature of todayâ€™s communication network standards. As shown in Figure 1, the execution of a MPNN can be divided in three main phases: 1) Initialization, 2) Message passing, and 3) Readout, which are described below. 1) Initialization (Fig. 1, Phase 1): Given an input graph, the GNN generates an associated state vector for each node â€” known as the nodeâ€™s hidden state â„â€” and initializes it with a set of features (ğ‘‹) already included in the input graph. To describe this process, we will refer to an illustrative example where the input graph of the GNN is a direct representation of a wired network topology (see Fig. 1, left). In this case, each graph node represents a router; hence the initial features ğ‘¥would be some router-level characteristics (e.g., switching capacity, buffer size). Hidden states â„are represented by n-element vectors that are typically larger than the initial feature vectors (ğ‘¥-ğ‘¥), so they can be simply zero-padded to ï¬ll the vector size. Likewise, graph edges represent the physical links of the network, and can also include some initial features ğ‘’(e.g., the link capacity). 2) Message-passing phase (Fig. 1, Phase 2): Once all the routerâ€™s hidden states are initialized, an iterative process (message-aggregation-update) is executed over the input graph (i.e., the network topology). Figure 1, Phase 2 illustrates a message passing (MP) iteration on a router (node ğ‘–). Note that this process would run in parallel for each router. In each MP iteration, the router combines the hidden state with its neighbors, and applies three main functions along this process: a) Message function (ğ‘€): It encodes information about two connected nodes in the graph (i.e., neighboring routers in the graph of Fig. 1). The ğ‘€ function has as input the states of two connected routers (â„and â„) and some additional properties of the link connecting them (ğ‘’). As output, it produces a message (ğ‘š), which is a new n-element vector that encodes some information about that node pair. b) Aggregation function (ğ´ğ‘”ğ‘”ğ‘Ÿ): After messages (ğ‘š) are generated for all pairs of routers connected in the graph, each router combines the messages computed with its neighbors using an aggregation function (ğ´ğ‘”ğ‘”ğ‘Ÿ). This function produces the aggregated message (ğ‘š), which is a n-element vector seeking to encode relevant information of the messages aggregated in that node. This ğ´ğ‘”ğ‘”ğ‘Ÿ function is typically an element-wise summation. c) Update function (ğ‘ˆ): It is tasked to update the node states at the end of each MP iteration (Fig. 1, Phase 2). To this end, it combines the current state of the router (â„) with the newly aggregated message (ğ‘š) â€” which encodes information from its neighborhood â€” and produces an updated state vector for the router (â„). Note that in each MP iteration routers only receive information from their direct neighbors. To ensure that each router potentially receives information from all the other nodes in the graph, the GNN executes a number of MP iterations ğ‘‡, which is a conï¬gurable parameter of the model. For instance, in the graph of Fig. 1, the GNN would need at least two MP iterations to propagate information from node 1 to node ğ‘–. 3) Readout phase (Fig. 1, Phase 3): This last phase translates the encoded information in node hidden states to the output values of the GNN. Note that GNNs may typically produce two output types: (ğ‘–) nodelevel, or (ğ‘–ğ‘–) global graph-level features. Following the example of Fig. 1, the GNN may predict features at the router level (e.g., buffer occupancy), or infer global network-level properties (e.g., congestion level). In the ï¬rst case, a readout function (ğ‘…) is individually applied to each router state to produce the outputs, while in the second case global outputs are obtained by ï¬rst aggregating all router states (e.g., elementwise sum) and then applying a global readout function (ğ‘…). In the end, the previous GNN comprises four main building blocks, which are the four functions described above: message (ğ‘€), aggregation (ğ´ğ‘”ğ‘”ğ‘Ÿ), update (ğ‘ˆ), and readout (ğ‘…). Typically, ğ‘€, ğ‘ˆ, and ğ‘… are approximated by three separate NNs (e.g., Fully-Connected NNs), and the ğ´ğ‘”ğ‘”ğ‘Ÿ function is often implemented as an element-wise summation. Thus, the GNN is dynamically built by combining copies of the previous four functions based on the nodes and connections of the input graph. Note that, once the GNN model is assembled, it forms a recurrent network. This means it is possible to train the whole GNN architecture end-to-end. As a result, functions approximated by NNs (ğ‘€, ğ‘ˆ, and ğ‘…) are jointly learned across all their copies in the GNN, by applying a common backpropagation method as in any other NN. As a result, after training, these NNs learn generic functions â€” according to the purpose the GNN was trained for â€” that can be applied to other graphs with different structures (i.e., nodes and connections) unseen during training. This section motivates the beneï¬ts of GNNs for building practical ML-based solutions for network modeling. Figure 2 shows a black-box representation of a generic network model. This model has as input a network scenario (e.g., topology, trafï¬c, conï¬guration), and produces as output some performance metrics predicted at different levels of granularity (e.g., ï¬‚ow, link, port statistics). Network models can be used â€” for instance â€” to evaluate the network performance under topology changes (e.g., upgrades, failures), or new conï¬gurations (e.g., routing, scheduling policy, VNF placement) without the need to implement them in the real network infrastructure. As a result, they enable a plethora of network control and management tasks, such as what-if analysis or automatic network optimization. Networks comprise graph-structured information at many different levels [2]. In this vein, more traditional NN models, such as Fully-Connected NNs, are not designed to directly process and capture meaningful patterns from graphs, which introduce non-Euclidean data with variable size and structure. GNN thus represents the most suitable ML technique nowadays for processing such graph-structured information. GNN models have unique properties to generalize over graphs. For example, unlike other ML models, GNNs are focused on relational reasoning and combinatorial generalization over graphs [5]. Leveraging the distributed message-passing architecture described in the previous section, GNNs learn from the information locally processed on graph nodes. For example, in the scenario of Fig. 1 the internal NN functions of the message-passing phase (i.e., message, update) learn from the individual perspective of each router in the network (i.e., its local context within the topology graph). This feature eventually endows the model with strong generalization capabilities, as it learns from the experiences locally seen by all routers during training and can then apply this knowledge to other routers in different networks with variable sizes and structures. Moreover, GNNs are equivariant to node and edge permutations; this means that if we represent network scenarios as graphs, these models are able to ï¬nd symmetries or equivalent patterns between the network scenarios seen during training and the new graphs where the model is applied [7]. Following with the example of Fig. 1, the GNN model would be able to identify clusters within the network topology that are equivalent or similar to others seen during training. Note that in practice different types of networks (e.g., wireless, data centers, IoT) may comprise graphs with different elements and relation types. However, these generalization properties are extensible to any network problem as long as it is represented as a graph. As a result of the generalization properties previously mentioned, GNN unlocks fundamental practical limitations of previous ML-based solutions for network modeling: Ofï¬‚ine training: Training a Deep Learning model often requires big amounts of data with enough diversity to abstract deep insights during the training phase (e.g., test different conï¬gurations, inject various trafï¬c loads). Indeed, models typically need to observe edge cases that may break the normal operation of the network (e.g., link failures), so that they can then make good estimates in case these rare events occur once they are deployed. For obvious reasons, it is not feasible to reproduce these edge cases in real networks in production; hence a more realistic approach is to train ML models ofï¬‚ine (e.g., in a controlled testbed), and make products that are readily available for deployment in customer networks, without the need for re-training on the target network. However, this requires to rely on ML models that can generalize to new networks unseen during training (e.g., new topologies, conï¬gurations, trafï¬c patterns), which makes GNN a key enabler for achieving such type of data-driven network models that can be fully trained ofï¬‚ine. Testing and deployability: Networks are critical infrastructures and â€” as a result â€” nowadays networking products are extensively tested before being deployed in real networks. This makes it unrealistic to rely on ML-based solutions that can be trained online in the target network, without strong supervision mechanisms. Thus, from a deployability standpoint, GNNs enable to train network models ofï¬‚ine, test their behavior under a wide range of operational network scenarios, and ï¬nally generate certiï¬cations that can determine the safe operational ranges where the model offers guarantees (e.g., network sizes, maximum trafï¬c aggregates). This conforms to the standard commercialization process of networking products nowadays. A GNN-based model like the one described in the previous section has as many applications as traditional network models, with the additional beneï¬t that it can be used in use cases where accurate ï¬ne-grained performance metrics are relevant (e.g., end-to-end delays, jitter, loss, ï¬‚ow completion time). This makes these models especially useful for SLAdriven network optimization tasks, where traditional modeling techniques do not meet the requirements to achieve accurate estimates with limited cost. Figure 3 depicts a generic optimization architecture in the context of SDN-based networks. Note that we opportunistically use this architecture to better illustrate the operational workï¬‚ow, while similar optimization mechanisms could be implemented in networks with distributed control. This optimization architecture envisions two main operation modes, depending on whether there is human intervention (open loop), or not (closed loop). Based on this, two main paradigmatic applications can be differentiated: What-if analysis (open loop) This operation mode contemplates the intervention of a network administrator and/or engineer that aims to evaluate the behavior of the network under some speciï¬c what-if scenarios (e.g., new conï¬gurations, upgrades, potential failures). For this purpose, the network model can be used to predict relevant performance metrics for those alternative scenarios. This, in the end, can be leveraged for a plethora of common networking tasks, such as planning, troubleshooting, optimization, or building network recommendations systems. Automatic optimization (closed loop) This second operation mode envisions autonomous optimization tasks, with an eye on future self-driving networks. Automatic network optimization can be achieved by combining a network model with an optimization algorithm [1]. In this well-known architecture, the algorithm (e.g., local search, branch and bound, reinforcement learning) generates candidate conï¬gurations (e.g., routing, scheduling) that pursue a particular optimization goal (e.g., minimize end-to-end delay); while the model is responsible for predicting the performance if these conï¬gurations were applied in the network. Thus, through an iterative generate-evaluate process the optimizer can ï¬nally produce a new conï¬guration that meets the optimization goals. Note that in the previous applications, it is essential to count on a network model that can accurately reproduce the target performance metrics (e.g., delay, jitter); otherwise there could be a problematic mismatch between the performance predicted by the model and the actual result after applying the new conï¬guration in the network. This makes the use of traditional network models (e.g., queuing theory) arguably limited for such control and management tasks, as they have limited capabilities to reproduce the behavior of real networks (e.g., real trafï¬c, physical effects, hardware-level impact). Likewise, in the context of ML, to achieve effective operation the network model must generalize well to a broad range of network state descriptions, either produced by network administrators (what-if analysis) or by optimization algorithms (automatic optimization). In practice, this means we need a model that can be trained ofï¬‚ine on a broad collection of network samples (e.g., from a controlled testbed), and then can produce accurate performance estimates when operating online over new network scenarios unseen in advance (e.g., new conï¬gurations, trafï¬c). This again calls for the use of GNNs, since they are the only ML models that demonstrated capabilities to generalize well over networks, as we discussed in more detail in the previous section. GNNs have already been applied to a wide spectrum of networking use cases, such as routing optimization [1], [9], Multipath TCP [10], network calculus [11], or power control in wireless networks [12], [13]. In order to illustrate the potential of GNNs, we present two representative examples of custom GNN-based architectures respectively applied to two use cases of wired and wireless networks: RouteNet [1], and WCGCN [12]. To this end, we implement these models with IGNNITION [14], and make some experiments focused on showcasing the capabilities of these models to generalize over networks. RouteNet [1] is a GNN-based model for wired networks. As illustrated in Figure 4, this model has as input a network state description, deï¬ned by: a network topology, a trafï¬c matrix, and a routing conï¬guration. As a result, it produces estimates of key performance indicators at a ï¬‚ow-level granularity (e.g., delay, jitter, loss). This network model can be used for performance evaluation of what-if network scenarios (e.g., check new conï¬gurations), as well as automatic optimization by combining the model with an optimization algorithm, as shown in Figure 3. As an example, in [1] they leverage this model for several SLA-driven optimization use cases, such as minimizing the delay and/or jitter in the network, fast link failure recovery, or ï¬nding the optimal link upgrades. To showcase the generalization capabilities of this model, we train RouteNet on 177,500 samples simulated in two realworld network topologies: NSFNET (14 nodes) and Germany50 (50 nodes), including different trafï¬c matrices and routing conï¬gurations. Then, we evaluate the predictions of the model in different sets with 40,000 samples respectively from NSFNET, Germany50, and a new network topology: GBN (17 nodes). Figure 4 shows the mean relative error (MRE) produced by RouteNet when predicting ï¬‚ow-level delays on samples from these networks. Note that all these evaluation samples include new combinations of routing conï¬gurations and trafï¬c matrices unseen during training. As we can see, the model can accurately generalize to these new samples, even for those of the GBN network, which was never seen during training (MRE<3%). For the sake of comparison, we repeat the same experiment with a Fully-connected NN with equivalent inputs to RouteNet and â€” as shown in Fig. 4 â€” we observe that this model produces a signiï¬cantly larger error, which especially raises when applied to samples of the new GBN network unseen during training (MREâ‰ˆ35%). These results evince the unprecedented capability of the GNN-based model (RouteNet) to generalize over all its input network parameters (i.e., topology, trafï¬c, routing), which are internally represented by the model in a graph-structured manner. Beyond the accurate predictions produced by this model, a main advantage is its low computational cost. As a reference, in our experiments RouteNet takes 65 ms on average to evaluate samples of the 50-node network (Germany50), while evaluating the same samples with a packet-level simulator (OMNet++) took more than 10 minutes. WCGCN [12] is a GNN-based model for radio resource management in wireless networks. This model can represent different optimization problems of wireless networks as graphs, and solve them via self-supervised training. In the original paper [12] they show how this model can be applied to maximize the sum rate on networks, which is a classical optimization problem applicable to a variety of wireless network use cases (e.g., power control, beamforming). As an example, we apply this model to power control. In this case the model has as input a description of the network state, including the channel state and the distances between end nodes, and produces as output the recommended Tx power on each node pair connection (see Figure 5). We train this model on 1,000 samples of 50 links (i.e., 50 connected node pairs) and then evaluate its performance on different sets with network samples of increasing size (50-400 links). Figure 5 shows the performance difference with respect to the classical weighted minimum mean square error (WMMSE) algorithm, which represents a close-to-optimal approach for this problem. As we can observe, even if WCGCN was only trained on 50-link scenarios, it achieves similar performance to the near-optimal WMMSE algorithm in networks up to 400 links (â‰ˆ+4%); this demonstrates the unprecedented scalability and generalization capabilities of this ML model on the target optimization problem. Likewise, we observe a signiï¬cant reduction in the execution time with respect to WMMSE. For example, in scenarios with 400 links WCGCN produces results in 16 ms on average, while WMMSE takes more than 10 seconds. This section outlines some opportunities to explore and several technological challenges yet to be addressed in order to achieve production-ready GNN-based solutions for communication networks. GNNs can be virtually applied to a plethora of networkrelated domains as long as they can be formulated as graphs. Indeed, these models have been recently validated for several representative use cases, showing outstanding applications in different communication paradigms (wired, wireless, SDN/NFV, IoT) [1], [9], [10], [11], [12], [13]; while there is still an ocean of opportunities to apply these novel methods to different use cases (e.g., optimization, troubleshooting, planning, what-if analysis) and expand their horizon towards other types of communication networks barely explored (e.g., satellite communications). However, this will not be a straightforward task, as standard GNN models are not directly applicable to many networking use cases, but a thorough designing process is often required to come up with custom GNN architectures well suited to the target networking problem. Despite the promising applications of GNNs to networking, and the several successful applications to date, these models are still in an early stage of technology readiness. We describe below some crucial open challenges that remain to be addressed: dented generalization capabilities of GNNs in communication networks, existing GNN solutions have limited capabilities to scale to large real-world networks (e.g., due to over-smoothing in message aggregations). To achieve production-ready solutions, it would be convenient to create GNN models that can be trained ofï¬‚ine in network testbeds of limited size (e.g., at a networking lab) and that can effectively scale to much larger real-world networks (e.g., with up to several hundreds or thousands of nodes). Indeed, beyond the scope of GNNs, scaling to larger networks is a generic open challenge among ML solutions nowadays. probabilistic models that entail some degree of uncertainty. Given the critical nature of network infrastructures, this can be a potential limitation for deploying data-driven solutions in networks. In this vein, more mature GNN-based solutions can be achieved by relying on explainability methods that can produce human-readable interpretations for the actions selected by GNN models [15]. Also, some works from the ML community propose to deal with uncertainty by predicting the posterior probability on model estimates (e.g., Bayesian NNs). Another possibility is to leverage expert knowledge to design custom methods that produce meaningful interpretations for speciï¬c network models, or to design testing procedures (e.g., in controlled testbeds) that systematically determine the safe operational ranges of GNN-based products before deployment (e.g., maximum network size, trafï¬c loads), so that it would be possible to issue certiï¬cations guaranteeing bounded conï¬dence levels within certain operational ranges. As a ï¬nal thought, we have already witnessed the revolution of Convolutional NNs applied to computer vision, or Recurrent NNs applied to natural language processing. The main reason behind these success stories is that they started to use NNs speciï¬cally tailored to understand the underlying structure of the data in their respective domains. In this context, GNNs can be the perfect partner to materialize the revolution of Deep Learning in the ï¬eld of communication networks, as data in networks is pervasively represented as graphs. Moreover, from a practical standpoint, GNNs can accurately generalize across networks, which represents a crucial aspect for achieving commercial data-driven solutions that can be trained ofï¬‚ine in controlled testbeds and be directly ready for deployment in any network in production. Nevertheless, in this article we have raised some relevant technological challenges and potential applications yet to be explored before achieving widespread adoption of GNN-based solutions for communication networks. This publication is part of the Spanish I+D+i project TRAINER-A (ref. PID2020-118011GB-C21), funded by MCIN/AEI/10.13039/501100011033. This work is also partially funded by the Catalan Institution for Research and Advanced Studies (ICREA).