How to predict precise user preference and how to make eï¬ƒcient retrieval from a big corpus are two major challenges of large-scale industrial recommender systems. In tree-based methods, a tree structure T is adopted as index and each item in co rpus is attached to a leaf node on T . Then the recommendation problem is converted into a hierarchical retrieval problem solved by a beam search process eï¬ƒciently. In this paper, we argue that the tree index used to support efï¬cient retrieval in tree-based methods also has rich hierarchical information about the corpus. Furthermore, we propose a novel context-aware tree-based deep model (ConTDM) for reco mmender systems. In ConTDM, a context-aware user preference prediction model M is designed to ut iliz e b oth horizontal and vertical contexts on T . Horizontally, a graph convolutional layer is used t o enrich the representation of both users and nodes on T with their neighbours. Vertically, a parent fusion layer is designed in M to transmit the user preference representation in higher levels of T to the current level, grasping the essence that tree-based metho ds are generating the candidate set from coarse to detail during the beam search retrieval. Besides, we argue that the proposed user preference model in ConTDM can be conveniently extended to other tree-based methods for recommender systems. Both experiments on large scale real-world datasets and online A/B test in large scale industrial applications show the signiï¬cant improvements brought by Co nTDM. â€¢ Computing methodologies â†’ Classiï¬cation and regression trees; Neural networks; â€¢ Information systems â†’ Recommender systems. recommender systems, context-aware model, tree-based retrieval, large-scale problem ACM Reference Format: Daqing Chang, Jintao Liu, Ziru Xu, Han Li, Han Zhu, and Xiaoqiang Zhu. 2021. Context-aware Tree-based Deep Model for Re commender Systems. In Proceedings of DLP-KDD 2021. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Recommendation problem is generally to retrieve for a candidate set comprised by usersâ€™ most preferred items from the entire corpus. In large-scale industrial recommender systems, making precise user preference prediction and eï¬ƒcient retrieval from a big corpus is extremely important. The linear retrieval complexity of traversing the entire corpus is usually unacceptable. Together with a user preference prediction model, a proper index structure is usually ne c essary in retrieving the candidate set. Recently, vector representation learning methods have become more and more popu lar in recommender systems [2, 14, 17, 29]. In these methods, both users and items are ï¬rstly represented by vectors in a same space. Then the inner-product of the user vector and the item vector is used as the metric of user-item preference. As a main beneï¬t, the candidate generation for these methods e quals to a classic k-nearest neighbour problem, which can be accelerated by quantization-based index [10, 16], hierarchical graph index[18] etc.. Many eï¬€orts such as se quential model [17], graph convolutional network [29] have been made to learn better user vectors and item vectors. However, the inner-product form of user-item preference modeling required by these methods is still a bottleneck for improving user preference predictio n accuracy[7, 39]. To break this bottleneck, a tree st ructure is used as index in tree-based methods [20, 38, 39, 41]. The general framework of treebased methods for recommender systems is shown in Figure 1(a). Firstly, each item in the corpus is carefully indexed to a leaf node of T and a user node preference prediction model M is trained. T is usually a ful l binaray tree and t he relationship between leaf nodes and items in corpus can be made by clustering [39] or joint learning [38]. In retrieval, a top-down beam search process o n T guided by M is used in candidate generation, which complexity is logarithmic w.r.t. the corpus size. With tree index, restrictions on the structure of M required by the kNN-based retrieval are removed and many advanced user preference prediction model such as Deep Interest Network [37], Wide & Deep [1], Deep Interest Evolution Network [36], xDeepFM [15] can be naturally used to achieve better user preference accuracy [39]. Figure 1: (a). A general framework of tree-based model for recommender systems. In retrieval, a top-down beam search on the tree index is ï¬rstly processed to generate the ï¬nal candidate set guided by a u ser preference prediction model. Then items indexed on nodes in the beam of the leaf level are generated as candidate sets. In this example, the b eam size is 2 and item 5, 6 are ï¬nally selecte d for recommend ation. (b) A toy example to show the value of contexts between nodes on T . However, all models above are originally designed for general user preference prediction and the useful contexts between nodes on tree index are not fully considered. Since nodes with a common parent are equally represented by this parent both in training and retrieval, each node on T is actually an abstraction of its children. The beam search retrieval is actually to generate the ï¬nal candidate set from coarse to detail. Intuitively, contexts between nodes on T should be useful auxiliary knowledge in user preference predictio n. To better illustrate this, a toy example is shown in Fig 1(b). According to the girlâ€™s historical behaviors, she probably prefers cloths at this time. Suppose M has rightly predicted the left node in level 2 as the girlâ€™s coarse preference, this vertical context should be useful for M to generate the ï¬nal candidate set in leaf level. Besides, relationships between nodes on the same level are also u seful horizontal contexts in prediction. For this toy example, shoes are also probably preferred by the girl in Fig 1 because clothes and shoes are usually co rrelative things in daily life. However, shoes may not be chosen by M if only historical behaviors are used as features in prediction. In this paper, we propose a Context-aware Tree-based Deep Model (ConTDM) to utilize both vertical and horizontal contexts on tree index in user preference prediction for tree-based recommender systems. Generally, a novel context-aware user node preference prediction model M is proposed in ConTDM. Our main contributions are listed as follows: â€¢ Hor iz ontally, contexts between nodes on the same level of T are aggregated with a graph convolutional layer. For tree-based models, a hierarchical graph structure is necessary to utilize vertical contexts on all levels of T . We propose a novel hierarchical graph construction algorithm according to r aw user behavior sequences and tree index. â€¢ Vertically, a parent fusion layer is designed in M. In ConTDM, we take the user preference representations predicted on higher levels of T as vertical contexts. Through the parent fusion layer, they are imp orted as an auxiliary input in prediction on the current level. â€¢ We argue that the p roposed user preference mod el can be conveniently extended to other tree-based methods. The training of ConTDM in this paper follows the framework proposed in TDM [39] without loss of generality. Oï¬„ine experiments and ablation study on open data sets shows the signiï¬cant improvements o f ConTDM compared with baseline methods. â€¢ ConTDM has been ap plied in full production to the display advertising scenario of Gu ess What You Like column of Taobao App Homepage at the candidate generation st age. Online A/B test shows the signiï¬cant improvements on click-through rate (CTR) and revenue per mille (RPM), which are key performance indicators for online display ad vertising. The rest of the paper is organized as follows: We introduce related works in Section 2. The proposed context-aware user preference prediction model and the training framework of ConTDM are introduced in Section 3. Experimental results are analysed in Section 4. We conclude our work in Section 5. Vec tor representation learning methods beginning from matrix factorization based collaborative ï¬ltering have been widely used in recommender systems [2, 13, 14 , 27, 29, 3 2]. In these methods, both users and items are mapped to vectors in the same space and the user-item preference is measured by the inner-product of user and item vectors. As an early representative work, a multi-layer fully connected network is used to project users and items into a latent space in industrial YouTube video recommender systems [2]. As a main beneï¬t, the candidate retrieval for these methods equal s to a k-nearest neighbour problem, which can be accelerated by quantization based index [10, 16], hierarchical graph index[18] etc.. Variants of improvements have been made in learning the vector mappings. For example, Lv et al. [17] use both recurrent neural network and attention mechanism in learning user vectors. Wang et al. [29] use a graph neural network to aggregate local information from the graph structure. However, the simple inner-product form of user preference modeling required by the kNN-based retrieval is still a key bottleneck for recommendation accu racy due to its limited learning capacity [7, 39]. Many other user preference models that has b een shown to be eï¬€ective such as Deep Interest Network [37], Deep & Wide [1], Deep Interest Evolution Network [36], xDeepFM [15] usually can not be applied directly in these methods. In past years, tree-based methods are actively studied in the ï¬eld of extreme classiï¬cation [3, 20, 22, 23, 25, 30, 34], which is also closely related with recommender systems [8, 22]. To break the bottleneck o f kNN-based methods, tree-based methods [38, 39 , 41] take a tree structure as index and each item in corpus is attached to Figure 2: The backbone of us er preference prediction mo del in ConTDM. Highlights: a) A graph convolutional layer is used to aggregate horizontal contexts on the tree index. Both the user embeddings and target node embedding are enhanced by this layer. b) The parent fusion layer takes both the output of the multi-layer fully connect ed network on the target node and its parent as inputs and a fusion unit is used to utilize vertical contexts on the tree index. a leaf node by clustering [39] or joint learning [38]. A beam search process from top to bottom is used in retrieval and a logarithmic complexity w.r.t. the corpus size is achieved. In these methods, restrictions on the form of user preference prediction model is removed and the use of arbitrary advanced user preference models is enabled to improve the recommendation accuracy. In training, a joint optimization framework of the user preference predict io n model and the tree index [38] is proposed. More recently, An o ptimal beam search aware training framework of tree-based deep models [41] is proposed to eliminate the mismatch in tr aining and retrieval. On the other hand, graph-based methods have also at tracted much attention in recommender systems [4, 28, 31, 33, 35, 40]. The general idea of graph-based methods is to make eï¬€ective information aggregation from the local subgraph with a well constructed graph structure. Variants of aggregator architectures have b een proposed in literatures [5, 12, 26]. In industrial community, Ying et al. [33] propose the PinSage algorithm used in Pinterest by adding a random walk based neighbour sampling strategy to GraphSage [5]. By importing side information to DeepWalk [21], Wang et al. [28] propose the graph embedding algorithm used in Alibaba to generate the item embeddings. They are used to calculate the similarity matrix for subsequent It em-CF [24] based recommendation. However, to our best knowledge, there is no existing work appl ied in industrial community simultaneously utilizing the superiority of bot h tree-based methods and graph-based models. In this section, we introduce t he proposed Context-aware Treebased Deep Model for reco mmender systems. As a main highlight of ConTDM, the prop osed context-aware user node preference prediction model is given in Section 3.1. We show the training framework of ConTDM in Section 3.2. In tree-based methods for reco mmendation, a tree stru cture T is used as index and each item in corpus is carefully indexed to a leaf node on T by clustering [39], joint learning [38] etc., as introduced in Section 1 and Fig 1. T hen a beam search process guided by a user node preference prediction model M is made to generate the candidate set for recommendation in retrieval. Obviously, the accuracy of user node preference prediction has great inï¬‚uence on the ï¬nal recommendation quality for tree-based methods. Inspired by the essence of tree-based method s is to generate the candidate set from coarse to detail, we hope to fu lly u tilize the rich hierarchical information on the tree index about the co rpus in designing the structure of M. More speciï¬cally, both vertical and horizontal contexts contained on T are properly utiliz ed to improve the accuracy of M in ConTDM. The backbone of M is shown in Fig 2. Figure 3: (a) An example to generate edges based on tree index and raw user behaviors. Firstly, hierarchical behavior sequences are contructed. Then an edge is put into the graph between an co-occurrence pair of nodes if the gap of their time stamps is not bigger than a threshold. (b) The constructed hierarchical gr aph based on sequences in (a). The weight of an edge is the count of co-occurrence between its two endpoints. Denote (ğ‘, ğ‘Â· Â· Â· ğ‘) as the historical behavior sequence of the user ğ‘¢ and denote ğ‘(ğ‘) as the ancestor of node ğ‘ on level ğ‘— of T . We use hierarchical user behavior sequences [38] that have been shown eï¬€ective in tree-based methods for recommender systems as our user features. More exactly, the user feature is represented as (ğ‘(ğ‘), ğ‘(ğ‘) Â· Â· Â· ğ‘(ğ‘)) when the target node in prediction is on level ğ‘— of T . Other useful features such as user proï¬les can be conveniently added if needed. In estimating the user node preference probability, the embeddings of both the hierarchical user behavior sequence and the target node are ï¬rstly put into a graph convolutional layer to import horizontal contexts o n T . Then user historical behaviors are divided into d iï¬€erent time windows and graph embeddings in the same time window are averaged to reduce the network complexity, which is optional to meet the practical time constraint in large scale industrial application without much loss of the eï¬€ect at the same time. Next, a fusion unit is used to fuse the graph embeddings in user behaviors wit h the graph embedd ing of the target node similarly as the att ention mechanism, which can also been employed in practice. We use the fusion unit here for consistency. After this, the user embeddings and the target embedding are concatenated as the input of a multi-layer fully connected network with PReLU activation function. The outputs of the network on both the target node and its parent are further fused by a parent fusion layer to import vertical contexts. Finally, a softmax layer is used to compute the user-node preference probability. As key co mponents to u tilize c ontexts between nodes, the mechanism and eï¬€ect of the graph convolutional layer and the parent fusion layer are further analysed in the following two subsections. 3.1.1 Graph Convolutional Layer. Horizontally, the relevance between nodes on the same level of T can be use d to enrich both the user and the target node features. As discussed in Section 2, graph structures have been proven to be powerful to grep the relationship between items in corpus. In ConTDM, a non-parameterized graph convolutional layer based on GraphSage[5] is used considering the eï¬ƒciency in training and inference, as shown in Fig 2. For each node ğ‘› on the tree index, its graph embedding is computed by: ğ¸ğ‘šğ‘ where ğ¸ğ‘šğ‘(ğ‘›) is the embedding of node ğ‘› and ğ‘ ğ‘’ğ‘–ğ‘”â„(ğ‘›) is the set of ğ‘›ğ‘  neighbours on the graph. The graph convolutional layer concatenates the averaged embeddings of ğ‘›ğ‘  neighbours with ğ‘›ğ‘  embedding. The purpose is to import the neighbour context as well as highlight t he role of ğ‘›. With the graph convolutional layer, the representation of each node is enriched by its neighbou rs. On the one hand, the impact of sparsity in training data that has been w idely observed in recommender systems [24] is alleviated. On the other hand, the scope of the user feature could jump out of historical behaviors and the diversity of the candidate set is promoted, which is usually preferred by most recommender systems. Besides, it is worth to point out that graph embeddings of all nodes can be computed eï¬ƒciently before the trained model is put online according to the backbone of M. Therefore, as another beneï¬t brought by the graph convolutional layer, the time cost of online forward computation which is usually strictly bounded in industrial recommendation scenario can be saved. Obviously, the quality of the graph matters. Besides, a hierarchical graph is required in ConTDM to aggregate horizontal contexts on all levels of T . We build the hierarchical graph according to the co-occurrence of nodes in hierarchical user behavior sequences {(ğ‘(ğ‘), ğ‘(ğ‘) Â· Â· Â· ğ‘(ğ‘))}. The process is explained in Fig 3(a). The user behavior sequence is ï¬rstly divided into sessions according to the gap between timestamps. Then an edge between each co-occurrence pair in the same session is added to the graph. The weight of an edge is the count of all co-occurrence pairs between its two endpoints. Generally, our main idea is that two items behaved by a user se quentially are probably related with each other and the hierarchical behavior sequence is the abstract user behaviors on diï¬€erent levels of T .A formal statement about the construction of the hierarchical graph is given in Alg 1 where ğ‘¡ is the threshhold (e.g. 40ğ‘šğ‘–ğ‘›) to divide the sessions in behavior sequences and ğ‘˜ is t he max number of neighbours used in truncation (e.g. 10 in Algorithm 1 Building Hierarchical Graph in ConTDM Input: Tree index T with max level ğ‘™, raw user behavior sequences of training data {(ğ‘, ğ‘Â· Â· Â· ğ‘)}, max number of neighbours ğ‘˜, max time interval ğ‘¡ G â† âˆ… for each raw sequence (ğ‘, ğ‘Â· Â· Â· ğ‘) do quence up to level ğ‘™ of T . if node ğ‘›and ğ‘›are both contained in (ğ‘(ğ‘), ğ‘(ğ‘) Â· Â· Â· ğ‘(ğ‘)) and the time interval between them â‰¤ ğ‘¡ minutes then end if for each node ğ‘› in level ğ‘™ of T do containing ğ‘› in G. Sort {ğ‘›}in the descending order of (ğ‘›, ğ‘›)â€™s weight. end for Output: Undirected hierarchical graph G. practice) to avoid the number of neighbours for hot nodes is too big and only keep the solid relationship as well. 3.1.2 Parent Fusion Layer. Given a user ğ‘¢ and a target node ğ‘› on level ğ‘™ of T , we denote ğ‘ğ‘(ğ‘›) as t he parent node of ğ‘›in level ğ‘™ âˆ’ 1 and denote ğ‘£ (ğ‘›) as the output of the multi-layer fully co nnected network. The parent fusion layer takes ğ‘£ (ğ‘›) and ğ‘£ (ğ‘ğ‘(ğ‘›)) as input and returns the fuse d user node preference representation through a fusion unit. The detaile d formulation is ğ‘£= ğ‘ƒğ‘…ğ‘’ğ¿ğ‘ˆ (ğ‘Š âˆ— ğ¶ğ‘œğ‘›ğ‘ğ‘ğ‘¡ (ğ‘£ (ğ‘›), ğ‘£ (ğ‘›) âŠ™ ğ‘£ (ğ‘ğ‘(ğ‘›)), ğ‘£ (ğ‘ğ‘(ğ‘›))) + ğ‘) . where ğ‘Š is the weight term and ğ‘ is the bias term to be optimized. With the parent fusion layer, the vertical contexts on higher levels of T are imported to the current p rediction. The impact of the parent fusion layer is shown as follows: â€¢ Explainability. In the tree index, each node is an abstraction of its children. Traditionally, ğ‘£ (ğ‘›) is used as the input vector of the ï¬nal softmax layer to compute the ï¬nal user preference probability, which indicates that ğ‘£ (ğ‘›) should contain useful user preference information. Besides, the essence of the hierarchical retrieval is to generate the ï¬nal candidate set from coarse to ï¬ne. Therefore, the user preference representation on the parent node ğ‘£ (ğ‘ğ‘(ğ‘›)) is a useful coarsegrained auxiliary feature in the prediction of ğ‘›. Algorithm 2 The Training Framework of ConTDM Input: Initial context-aware user preference model M, tree index T , raw training d ata {(ğ‘¢, ğ‘)}. and nodes on T as vertex with Al g 1. Output: Trained model M used for beam search retrieval. â€¢ Non-sparsity. In ConTDM, the training samples for each node in higher levels is much more enriched than lower levels, since the total number of nodes decreases exponentially with the going up of levels on T . By importing contexts in higher levels, the impact of sparsity in lower levels is also largely reduced. â€¢ Eï¬ƒciency. Since the hierarchical retrieval is made from top to bottom on T , ğ‘£ (ğ‘ğ‘(ğ‘›)) can be eï¬ƒciently reused in predicting ğ‘›. Therefore, the total increase of computation in retrieval is brought by the fusion unit, which is usually acceptable. With the proposed context-aware user preference predictio n model, the training framework o f ConTDM is shown in Alg 2 following the tree-based deep model proposed in [39]. The input of Alg 2 contains an initial context-aware user preference mod el M, the tree index T and the raw training data set {(ğ‘¢, ğ‘)}where ğ‘¢denotes user ğ‘– and ğ‘denotes the label item ğ‘¢prefers (e.g. ğ‘¢clicks ğ‘before). The initial tree index can be constructed by clustering following [3 9] without loss of generality. Before training M, we ï¬rstly construct the hierarchical graph G used by Alg 1 . Next, M is optimize d under the total empirical loss as follows [39]: where ğ‘™is the max level of tree index T . ğœ½ is the parameter of M to be optimized. ğ‘(ğ‘) returns item ğ‘ğ‘  ancestor node on level ğ‘— of T .Ë†ğ‘(ğ‘|ğ‘¢) is the estimated probability ğ‘¢ prefers ğ‘ by M. In Eq (3), the total negative logarithm of the estimated user-node probability between each pair (ğ‘¢, ğ‘) and their ancestors is minimized. In each iteration, we randomly sample a mini-batch samples from the raw data set and tracing them up to all levels of T as positive data. Besides, negative sampl ing [2, 9] is used in estimatingË†ğ‘(ğ‘|ğ‘¢) with negative data sampled from the corresponding levels of T . As the proposed context-aware user node preference prediction model in ConTDM only relies on the tree index and raw training behavior sequences, it is convenient to be applied to other existing tree-based frameworks for recommender systems such as [20, 38, 41]. In this section, we show both online and oï¬„ine performance of ConTDM. Firstly, dat asets utilized in oï¬„ine experiments are brieï¬‚y summarized. Secondly, we compare the overall performance of ConTDM with other baseline recommendation models to show the effectiveness of the context-aware modeling. Thirdly, ablation st udy is followed up to help comprehend how each part o f ConTDM works in detail. At last, we show the performance of Co nTDM in Taobao display advertising platform with real online tr aï¬ƒc. Our oï¬„ine experiments are conducted with two large-scale realworld datasets: 1) user-book review dataset from Amazon[6, 19]; 2) user-item behavior dataset from Taobao called UserBehavior[39]. The details are as follows: â€¢ Amazon Books: This dataset is composed by product reviews from Amazon. Here we use its largest subset, i.e., Books. The users with less than 10 books reviewed are excluded. Each review record is in the format of (user ID, book ID, rating, timestamp). â€¢ UserBehavior: It is a subset of Taobao user behavior data containing about 1 million randomly sampled users who had behaviors from November 25 to December 03, 2017. Similar to Amazon Books, only users with at l east 10 behaviors are kept. Each user-item behavior is corresponding to a record in the form of (user ID, item ID, category ID, behavior type, timestamp). All behavior types are treated equal in our experiments. Table 1 summarizes the above two d atasets after preprocessing. Table 1: Details of the two datasets after preprocessing. One record is a us er-item pair that represents user feedback. In our oï¬„ine experiments, Precision, Recall and F-Measure are used as metrics for performance evaluation of diï¬€erent method s as in most related works for candidate generation in recommender systems. For a user u, denote P(|P| = ğ‘€) as the recalled candidate set and Gas the ground truth set. The deï¬nitions of these metrics are as follows: Precision@ğ‘€ (ğ‘¢) =|Pâˆ© G||P|, R ecall@ğ‘€ (ğ‘¢) =|Pâˆ© G||G|, F-Measure@ğ‘€ (ğ‘¢) =2 âˆ— Precision@ğ‘€ (ğ‘¢) âˆ— Recall@ğ‘€(ğ‘¢)Precision@ğ‘€(ğ‘¢) + Recall@ğ‘€(ğ‘¢). The user average of the above three metrics in testing set are used to compare the following methods: â€¢ Item-CF [24], namely the classic item-based collaborative ï¬ltering, maintains an item-item matrix measuring similarities between pairs of items. The recommended items are generated according to the userâ€™s historical behaviors and the matrix. â€¢ YouTube product-DNN [2], the representative work of kNNbased methods, is a practical method used in YouTube video recommendation. The inner-product of the learnt user and itemâ€™s vector representation denotes the preference. â€¢ HSM [20] is shor t for the hierarchical softmax model, which utilize multiplication of level-wise conditional probabilities to obtain item preference probability w ithout the normalization term. â€¢ TDM [39] is a representative tree-based deep model for recommender systems. The backbone of its user preference prediction model is comprised by an attention layer and a multilayer plain-DNN. â€¢ ConTDM is the proposed context-aware user preference model along with the tree index. The structural information on the tree index is incorporated by a graph convolutional layer and a p arent fusion layer contained in the preference model. We randomly sample 5,000 and 10,000 disjoint users to create testing set for Amazon Books and UserBehavior respectively. The other users in two datasets compose the training set. For each user in testing set, we take the ï¬rst half of behaviors along the time line as known features and the latter half as ground truth. In negative samples generation, we deploy the same sampling str ategy for all methods except Item-CF and use the same sampling ratio. For fairness, both HSM and TDM use t he same user preference prediction model, which contains an at tention layer before a three-layer plainDNN. In ConTDM, the parameter size of the fusion unit is taken as closely as the attention layer in TDM and the plain-DNN layers are the same as o ther baseline methods. Note that we do not apply attention module to YouTube product-DNN because pairwise attention is not applicable in industrial scenario for user preference mod els with the inner-product form t o achieve acceleration in retrieval. Besides, the same tree index learnt by the joint learning framework [ 38] wit h a plain-DNN user preference prediction model is shared by HSM, TDM and ConTDM to make fair comparison. The quantitative results of al l methods in t wo datasets under settings above is shown in Table 2 . Firstly, compared with the traditional Item-CF and kNN-based retrieval models, TDM signiï¬cantly improves the recommendation accuracy in all metrics. This result clearly shows the superiority of tree-based methods by removing the restrictions on the form of user preference modeling and enabling the use of more eï¬€ective models. Actually, together with a carefully designed joint learning framework[38], TDM could outperform the brutal-force traverse of the whole corpus with the same preference model trained on raw training data only. Besides, a direct application of hierarchical softmax model to recommendation problem does not show much improvements on Item-CF and knn based method, which is consistent with the conclusion in [39]. Secondly, ConTDM still outp erforms the strongest baseline TDM with a 6.3% and 10.5% recall lift in Amazon Books and UserBehavior respectively. The comparison result shows the eï¬€ectiveness of importing contexts contained on the tree index to user preference modeling. Notice that the improvements are mostly achieved by the proposed context-aware user preference p rediction model with a common tree index shared between diï¬€erent tree-based methods. A ï¬ne-tuned tree index and more eï¬€ective training framework such as [38, 41] can be naturally applied in ConTDM to achieve better overall performance. In ConTDM, a graph convolutional layer and a parent fusion layer are designed in the user preference prediction model to utiliz e both horizontal and vertical contexts on T respectively. In this subsection, we make an ablation analysis about the eï¬€ectiveness of these two layers. All settings are kept unchanged as the last subsection other than removing one of these two l ayers from M. Experimental results are shown in Table 3 with TDM as the baseline. Graph Convolutional Layer. In this case, we remove the par ent fusion layer in ConTDM and keep other parts unchanged. We denote this model as ConTDM-GC. From Table 3, ConTDM-GC lifts the recall wit h the relative percentage of 6.3% and 5.9% in Amazon Books and UserBehavior respectively. This result conï¬rms the advantage of u tilizing the co-occurrence amo ng nodes from the same level in the tree index. Parent Fusion Layer. In retrieval, the top-down path from the root to the leaf layer fo rms the de cision chain of the user preference model, which naturally indicates the user interests granularity evolves from coarse to ï¬ne. From a probabilistic p erspective, the top-down beam search process can be regarded as a sequence generation process. We remove the graph convolutional layer from ConTDM and denote the preference model with t he parent fusion layer as ConTDM-PF . Results in Table 3 shows the eï¬€ectiveness of the parent fusion layer. In UserBehavior, ConTDM-PF gains 6.6% recall lift, which beats the ConTDM-GC with 5.9%. Nevertheless, in Amazon Books, the recall yields by ConTDM-PF and TDM are roughly the same. We attribute this result to the impact of dataset. Since most items in this dataset are books and the coar se description of user preference on higher levels of T does not beneï¬t much to prediction on child nodes. Table 3: Ablation results for Graph Convolutional Layer and Parent Fusion Layer in Amazon Books and UserBehavior. ConTDM has been applied in the display advertising scenario, i.e. Guess What Yo u Like column of Taobao App, with full online trafï¬c at the stage of candidate generation. In Taobaoâ€™s advertising systems, advertisers bid on the reveals that show items to users. When a user opens the Taobao App, the advertising engine should choose a few proper ads from the large scale corpus of ads to be revealed. Generally, the whole process in practice can be devided into three subsequent stages: candidate generation, ranking and strategy. After each stage, the candidate set is reduced gradually from the whole corpus containing millions of it ems to few ads. Besides, the target considered in each stage also varies to meet diï¬€erent business goal s. To measure the performance, we conduct online A/B comparison by replacing ConTDM with t he st rongest baseline method TDM. Each comparison experiment has 2% of all online traï¬ƒc. We use click-through rate (CTR) and revenue per mille (RPM) that are the key performance indicators for online display advertising as metrics. The deï¬nitions of these two metrics are as follows: CTR =# of clicks# of impressions, R PM =Ad revenue# of impressionsâˆ— 1000. Besides, the diversity deï¬ned by the size of diï¬€erent categories in the candidate set is also considered. Table 4 reveals the lift on all online metrics. 3.8% growth on CTR exhibits that more precise it ems have be en recommended. RPM with t he increase of 4.0% proves ConTDM can bring more income for Taobao advertising platform. Thanks to the horizontal Table 4: Online results in Guess What You Like column of Taobao App Homepage. contexts brought by the hierarchical graph, the diversity of candidate set is also signiï¬cantly improved by 14.0%. It means more potential improvements can be made by subsequent stages since their corpus is largely enriched. Notice that there are sevaral diï¬€erent methods working simultaneously online at the candidate generation stage and ConTDM is only one of them. Besides, the cancandidate set returned in the candidate generation stage will be reranked by the subsequent stages to pick the few ï¬nal ads. Therefore, the improvements achieved by ConTDM is very signiï¬cant. Besides, as discussed in Section 3.1.1, graph embe ddings of ConTDM are aggregated oï¬„ine and all improvements compared with TDM are achieved without increase of the stress of the online engine. In this paper, we study the eï¬€ect of the tree index in user preference modeling and propose a context-aware user preference prediction model, which can be conveniently applied in general tree-based methods for recommender systems. Both horizontal and vertical contexts o n T are utilized through a novel graph convolutional layer and a parent fusion layer. Both online and oï¬„ine results show the signiï¬cant improvements brought by ConTDM. In ConTDM, we mainly focus on how to improve the recommendation accuracy of user preference model in tree-based methods. Actually, the quality of the tree index also matters [38]. Intuitively, we can naturally extend the binary tree index used in ConTDM to a multi-path tree index with the constructed hierarchical graph and its capacity is much increased. We will fu rther study how to make eï¬€ective and eï¬ƒcient training and retrieval on the more challenging multi-path tree index in our future work.