Oî€-policy Evaluation (OPE), or oî€Ÿine evaluation in general, evaluates the performance of hypothetical policies leveraging only oî€Ÿine log data. It is particularly useful in applications where the online interaction involves high stakes and expensive setting such as precision medicine and recommender systems. Since many OPE estimators have been proposed and some of them have hyperparameters to be tuned, there is an emerging challenge for practitioners to select and tune OPE estimators for their speciî€›c application. Unfortunately, identifying a reliable estimator from results reported in research papers is often diî€œcult because the current experimental procedure evaluates and compares the estimatorsâ€™ performance on a narrow set of hyperparameters and evaluation policies. Therefore, it is diî€œcult to know which estimator is safe and reliable to use. In this work, we develop Interpretable Evaluation for Oî€Ÿine Evaluation (IEOE), an experimental procedure to evaluate OPE estimatorsâ€™ robustness to changes in hyperparameters and/or evaluation policies in an interpretable manner. Then, using the IEOE procedure, we perform extensive evaluation of a wide variety of existing estimators on Open Bandit Dataset, a large-scale public real-world dataset for OPE. We demonstrate that our procedure can evaluate the estimatorsâ€™ robustness to the hyperparamter choice, helping us avoid using unsafe estimators. Finally, we apply IEOE to realworld e-commerce platform data and demonstrate how to use our protocol in practice. oî€-policy evaluation, recommender systems, counterfactual estimation ACM Reference Format: Yuta Saito, Takuma Udagawa, Haruka Kiyohara, Kazuki Mogi, Yusuke Narita, and Kei Tateno. 2021. Evaluating the Robustness of Oî€-Policy Evaluation. In Fifteenth ACM Conference on Recommender Systems (RecSys â€™21), September 27-October 1, 2021, Amsterdam, Netherlands . ACM, New York, NY, USA, 17 pages. https://doi.org/10.1145/3460231.3474245 Interactive bandit and reinforcement learning algorithms have been used to optimize decision making in many real-life scenarios such as precision medicine, recommender systems, advertising, etc. We often use these algorithms to maximize the expected reward, but they also produce log data valuable for evaluating and redesigning future decision making. For example, the logs of a news recommender system record which news article was presented and whether the user read it, giving the decision maker a chance to make its recommendation more relevant. Exploiting log data is, however, more diî€œcult than conventional supervised machine learning. This is because the result is only observed for the action chosen by the algorithm but not for all the other actions the system could have taken. The logs are also biased, as the logs overrepresent the actions favored by the algorithm used to collect the data. Online experiment or A/B test is a potential solution to this issue. It compares the performance of counterfactual algorithms in an online environment, enabling unbiased evaluation and comparison. However, A/B testing counterfactual algorithms is often diî€œcult, since deploying a new policy to a real environment is time-consuming and may damage user satisfaction [7, 19]. This motivates us to study Oî€-policy Evaluation (OPE), which aims to estimate the performance of an evaluation policy using only log data collected by a behavior policy. Such an evaluation allows us to compare the performance of candidate policies safely and helps us decide which policy to deploy in the î€›eld. This alternative oî€Ÿine evaluation approach thus has the potential to overcome the above issues with the online A/B test approach. With growing interest in OPE, the research community has produced a number of estimators, including Direct Method (DM) [2], Inverse Probability Weighting (IPW) [17,21], Self-Normalized IPW (SNIPW) [25], Doubly Robust (DR) [4], Switch-DR [29], and Doubly Robust with Optimistic Shrinkage (DRos) [22]. One emerging challenge with this trend is that there is a need for practitioners to select and tune appropriate hyperparameters for OPE estimators for their speciî€›c application [23,28]. For example, DM î€›rst estimates the expected reward function using an arbitrary machine learning method, then uses its estimate for OPE. Therefore, one has to identify a good machine learning method to estimate the expected reward before the oî€Ÿine evaluation phase. Identifying the appropriate machine learning method for DM is diî€œcult, because its accuracy cannot be easily quantiî€›ed from bandit data [8]. Sophisticated estimators such as Switch-DR [29] and DRos [22] show improved oî€Ÿine evaluation performance in some experiments. However, these estimators have a larger number of hyperparameters to be tuned compared to the baseline estimators. A diî€œculty here is that the estimation accuracy of OPE estimators is highly sensitive to the choice of hyperparameters, as implied in empirical studies [20,28]. When we rely on OPE in real-world applications, it is desirable to use an estimator that is robust to the choice of hyperparameters and achieves accurate evaluations without requiring signiî€›cant hyperparameter tuning. Moreover, we want the estimators to be robust to other possible conî€›guration changes such as evaluation policies. An estimator of this type is preferable, because tuning hyperparameters of OPE estimators with only logged bandit data is challenging in nature, and we often apply an estimator to several diî€erent policies to compare the performance of candidate policies oî€Ÿine. The aim of this paper is thus to enable a safer OPE practice by developing a procedure to evaluate the estimatorsâ€™ robustness. Current dominant evaluation procedures.The current evaluation procedure used in OPE research is not suitable for evaluating the estimatorsâ€™ robustness. Almost all OPE papers evaluate the estimatorâ€™s performance for a single given set of hyperparameters and an arbitrary evaluation policy [4,6,13â€“15,20,22,24,27,29]. Even though it is common to iterate trials with diî€erent random seeds to provide an estimate of the performance, this procedure cannot evaluate the estimatorsâ€™ robustness to hyperparameter choices or the changes in evaluation policies, which is critical in real-world scenarios. The estimatorâ€™s performance derived from this common procedure does not properly account for the uncertainty in oî€Ÿine evaluation performance, as the reported performance metric is a single random variable drawn from the distribution over the estimatorâ€™s performance. Consequently, choosing an appropriate OPE estimator is diî€œcult, as their robustness to hyperparameter choices or the changes in evaluation policies are not quantiî€›ed in existing experiments. Contributions.Motivated towards promoting a reliable use of OPE in practice, we develop an interpretable and scalable evaluation procedure for OPE estimators that quantiî€›es their robustness to the choice of hyperparameters and possible changes in evaluation policies. Our evaluation procedure compares several OPE estimators as depicted in Figure 1. This î€›gure compares the oî€Ÿine evaluation performance of IPW and DM by illustrating their accuracy distributions as we vary their hyperparameters, evaluation policies, and random seeds. The x-axis is the squared error in oî€Ÿine evaluation; a lower value indicates that an estimator is more accurate. The î€›gure is visually interpretable, and in this case, we are conî€›dent that IPW is better, having lower squared errors with high probability, being robust to the changes in conî€›gurations, and being more accurate even in the worst case. In addition to developing the evaluation procedure, we have implemented open-source Python Figure 1: An example output of the proposed evaluation procedure for oî€line evaluation software,pyIEOE, so that researchers can easily implement our procedure in their experiments, and practitioners can identify the best estimator for their speciî€›c environment. Using our procedure and software, we evaluate a wide variety of existing OPE estimators on Open Bandit Dataset [20] (Section 5) and several classiî€›cation datasets (Appendix A). Through these extensive experiments, we demonstrate that IEOE can provide informative results, in particular the estimatorsâ€™ robustness to the hyperparameter settings and evaluation policy changes, which could not be obtained using typical experimental procedure in OPE research. Finally, as a proof of concept, we use our procedure to select the best estimator for the oî€Ÿine evaluation of coupon treatment policies on a real-world e-commerce platform. The platform uses OPE to improve its coupon optimization policy safely without implementing A/B tests. However, the platformâ€™s data scientists do not know which OPE estimator is appropriate for their setting. We apply our procedure to provide an appropriate estimator choice for the platform. This real-world application demonstrates how to use our procedure to reduce uncertainty and risk that we face in real-world oî€Ÿine evaluation. Our contributions are summarized as follows. â€¢We develop an experimental procedure called IEOE that is useful for identifying robust estimators and avoid the use of estimators sensitive to conî€›guration changes. â€¢We have implementedpyIEOE, open-source Python software, that facilitates the use of our experimental procedure both in research and in practice. â€¢We conduct comprehensive benchmark experiments on public datasets and demonstrate that IEOE is useful for identifying estimators sensitive to conî€›guration changes, and thus can help avoid potential failures in OPE. â€¢We apply IEOE to a real-world OPE application and demonstrate how this procedure helps us safely conduct OPE in practice. We consider a general contextual bandit setting. Letğ‘Ÿ âˆˆ [0, ğ‘Ÿ] denote a reward or outcome variable (e.g., whether a coupon assignment results in an increase in revenue) andğ‘ âˆˆ Abe a discrete action. We letğ‘¥ âˆˆ Xbe a context vector (e.g., the userâ€™s demographic proî€›le) that the decision maker observes when picking an action. Rewards and contexts are sampled from unknown probability distributionsğ‘ (ğ‘Ÿ | ğ‘¥,ğ‘)andğ‘ (ğ‘¥), respectively. We call a functionğœ‹:X â†’ Î”(A)a policy. It maps each contextğ‘¥ âˆˆ Xinto a distribution over actions, whereğœ‹ (ğ‘ | ğ‘¥)is the probability of taking action ğ‘ given context vector ğ‘¥. LetD:= {(ğ‘¥, ğ‘, ğ‘Ÿ)}be a historical logged bandit feedback withğ‘›observations.ğ‘is a discrete variable indicating which action inAis chosen for individualğ‘–.ğ‘Ÿandğ‘¥denote the reward and the context observed for individualğ‘–. We assume that a logged bandit feedback dataset is generated by a behavior policy ğœ‹as follows: where each context-action-reward triplet is sampled independently from the identical product distribution. Then, for a functionğ‘“ (ğ‘¥, ğ‘, ğ‘Ÿ ),Ã we useE[ğ‘“ ]:= ğ‘›ğ‘“ (ğ‘¥, ğ‘, ğ‘Ÿ)to denote its empirical expectation overğ‘›observations inD. We also useğ‘(ğ‘¥, ğ‘):= E[ğ‘Ÿ | ğ‘¥, ğ‘]to denote the mean reward function for a given context and action. In OPE, we are interested in using historical logged bandit data to estimate the following policy value of a given evaluation policy ğœ‹which might be diî€erent from ğœ‹: Estimatingğ‘‰ (ğœ‹)before deployingğœ‹in an online environment is useful in practice, becauseğœ‹may perform poorly. Additionally, this makes it possible to select an evaluation policy that maximizes the policy value by comparing their estimated performances without incurring additional implementation cost. Given the policy value as the estimand, the goal of researchers is to propose an accurate estimator. OPE estimatorË†ğ‘‰estimates the policy value of an arbitrary evaluation policy asğ‘‰ (ğœ‹) â‰ˆË†ğ‘‰ (ğœ‹;D,ğœƒ), whereDis an available logged bandit feedback dataset, andğœƒis a set of pre-deî€›ned hyperparameters ofË†ğ‘‰ . Below, we summarize the deî€›nitions and properties of several existing OPE estimators. We also summarize their built-in hyperparameters in Table 1. Direct Method (DM). DM [2] î€›rst trains a supervised machine learning method, such as ridge regression, to estimate the mean reward function ğ‘. DM then estimates the policy value as whereË†ğ‘(ğ‘¥, ğ‘)is the estimated mean reward function. IfË†ğ‘(ğ‘¥, ğ‘)is a good approximation to the mean reward function, this estimator accurately estimates the policy value of the evaluation policy. If Ë†ğ‘(ğ‘¥, ğ‘)fails to approximate the mean reward function well, however, the î€›nal estimator tends to fail in OPE. Inverse Probability Weighting (IPW). To alleviate the issue with DM, researchers often use IPW [17,21]. IPW re-weights the rewards by the ratio of the evaluation policy to the behavior policy, as whereğœŒ (ğ‘¥, ğ‘):= ğœ‹(ğ‘ | ğ‘¥)/ğœ‹(ğ‘ | ğ‘¥)is called the importance weight. When the behavior policy is known, IPW is unbiased and consistent for the policy value. However, it can have high variance, especially when the evaluation policy deviates signiî€›cantly from the behavior policy. To reduce the variance of IPW, the following weight clipping is often applied. whereğœ† â‰¥0 is a clipping hyperparamter. A lower value ofğœ†greatly reduces the variance while introducing a large bias. Following Su et al. [22], we call IPW with weight clipping as IPW with Pessimistic Shrinkage (IPWps). When ğœ† = âˆ, IPWps is identical to IPW. Doubly Robust (DR). DR [4] combines DM and IPW as follows. DR uses the estimated mean reward function as a control variate to decrease the variance of IPW. It is also doubly robust in that it is consistent to the policy value if either the importance weight or the mean reward estimator is accurate. The weight clipping can also be applied to DR as follows. := E[E[Ë†ğ‘(ğ‘¥, ğ‘)] + min{ğœŒ (ğ‘¥, ğ‘), ğœ†}(ğ‘Ÿâˆ’Ë†ğ‘(ğ‘¥, ğ‘))], whereğœ† â‰¥0 is a clipping hyperparamter. DR with weight clipping is called DR with Pessimistic Shrinkage (DRps). Whenğœ† = âˆ, DRps is identical to DR. Self-Normalized Estimators. SNIPW [25] is an approach to address the variance issue of IPW. It estimates the policy value by dividing the sum of weighted rewards by the sum of importance weights as: SNIPW is more stable than IPW, because the policy value estimated by SNIPW is bounded in the support of rewards, and its conditional variance given action and context is bounded by the conditional variance of the rewards [12]. IPW does not have these properties. We can deî€›ne Self-Normalized Doubly Robust (SNDR) in a similar manner as follows. Switch Estimator. DR can still be subject to the variance issue, particularly when the importance weights are large due to low overlap between behavior and evaluation policies. Switch-DR [29] aims to further reduce the variance by using DM where the importance weight is large: Note:Ë†ğ‘ is an estimator for the mean reward function constructed by an arbitrary machine learning method. ğ¾ is the number of folds in the cross-î€›tting procedure.Ë†ğœ‹is an estimated behavior policy. This is unnecessary when we know the true behavior policy, and thus it is in parentheses. ğœ and ğœ† are non-negative hyperparameters for deî€›ning the corresponding estimators. whereI{Â·}is the indicator function andğœ â‰¥0 is a hyperparameter. Switch-DR interpolates between DM and DR. Whenğœ =0, it is identical to DM, while ğœ â†’ âˆ yields DR. Doubly Robust with Optimistic Shrinkage (DRos). Su et al. [22] proposes DRos based on a new weight functionË†ğœŒ:X Ã— A â†’ R that directly minimizes sharp bounds on the mean-squared-error (MSE) of the resulting estimator. DRos is deî€›ned as whereğœ† â‰¥0 is a hyperparameter andË†ğœŒis deî€›ned asË†ğœŒ (ğ‘¥, ğ‘;ğœ†):= ğœŒ (ğ‘¥, ğ‘). Whenğœ† =0,Ë†ğœŒ (ğ‘¥, ğ‘;ğœ†) =0 leading to DM. On the other hand, as ğœ† â†’ âˆ,Ë†ğœŒ (ğ‘¥, ğ‘; ğœ†) = ğœŒ (ğ‘¥, ğ‘) leading to DR. Cross-Fitting Procedure. To obtain a reward estimator,Ë†ğ‘, we sometimes use cross-î€›tting to avoid the substantial bias that might arise due to overî€›tting [16]. The cross-î€›tting procedure constructs a model-dependent estimator such as DM and DR as follows: (1)Take ağ¾-fold random partition(D)of sizeğ‘›of logged bandit feedback datasetDsuch that the size of each fold isğ‘›= ğ‘›/ğ¾. Also, for eachğ‘˜ =1,2, . . . ğ¾, we deî€›neD:= (2)For eachğ‘˜ =1,2, . . . ğ¾, construct reward estimators{Ë†ğ‘} using the subset of data D. (3)Given{Ë†ğ‘}and model-dependent estimatorË†ğ‘‰, estimateÃ the policy value by ğ¾Ë†ğ‘‰ (ğœ‹; D,Ë†ğ‘). Hyperparameter Tuning Proce dure. As Table 1 summarizes, most OPE estimators have hyperparameters such asğœ†,ğœ,ğ¾, andË†ğ‘that should appropriately be set. Su et al. [22] proposes to select a set of hyperparameters based on the following criterion. whereV(ğœƒ;D)is the sample variance in OPE, andBiasUB(ğœƒ;D) is the upper bound of the bias estimated usingD. There are several ways to derive the bias upper bound as stated in Su et al. [22]. One way is the direct bias estimation: whereğ›¿ âˆˆ (0,1]is the conî€›dence delta to derive the high probability upper bound, andğœŒ:= maxğœŒ (ğ‘¥, ğ‘)is the maximum importance weight.Ë†ğœŒ (ğ‘¥, ğ‘;ğœƒ)is the importance weight modiî€›ed by a hyperparameter. For example, for IPWps and DRps,Ë†ğœŒ (ğ‘¥, ğ‘;ğœ†) = min{ğœŒ (ğ‘¥, ğ‘), ğœ†}, and for Switch-DR,Ë†ğœŒ (ğ‘¥, ğ‘;ğœ) = ğœŒ(ğ‘¥, ğ‘)I{ğœŒ (ğ‘¥, ğ‘) â‰¤ ğœ}. So far, we have seen that the OPE community has developed a variety of OPE estimators. What every OPE research paper should do in their experiments is to compare the performance (estimation accuracy) of the existing estimators and report the results. A typical and dominant method to do so is to estimate the following meansquared-error (MSE) as the estimatorâ€™s performance metric: MSE(Ë†ğ‘‰ ; ğœ‹, ğœƒ ) := Eğ‘‰ (ğœ‹) âˆ’Ë†ğ‘‰ (ğœ‹; D, ğœƒ), whereğ‘‰ (ğœ‹)is the policy value andË†ğ‘‰is an estimator to be evaluated. MSE measures the squared distance between the policy value and its estimated value; a lower value means a more accurate OPE byË†ğ‘‰. Researchers often calculate the MSE of each estimator several times with diî€erent random seeds and report its mean. The issue with this procedure is that most of the estimators have some hyperparameters that should be chosen properly before the estimation process. Moreover, the estimation performance can vary when evaluating diî€erent evaluation policies (especially in î€›nite sample cases). However, the current dominant procedure for evaluating OPE estimators uses only one set of hyperparameters and an arbitrary evaluation policy for each estimator, and then discusses the derived results [1,6,24,27,29].This type of simpliî€›ed experimental procedure does not accurately capture the uncertainty in the performance of OPE estimators. Speciî€›cally, it cannot evaluate the robustness to hyperparameter choices and evaluation policy settings, as the reported score is for a single arbitrary set of hyperparameters and for a single evaluation policy. What is often critical in oî€Ÿine evaluation practices is to identify an estimator that performs well for a variety of evaluation policies without problem-speciî€›c hyperparameter tuning. An estimator robust to the changes in such conî€›gurations is usable reliably in uncertain real-life scenarios. In contrast, an estimator which performs well only on a narrow set of hyperparameters and evaluation policies entails a higher risk of failure in its particular application. Therefore, we want to avoid using such sensitive estimators as these estimators are more likely to fail. In the next section, we describe an experimental procedure that can evaluate the estimatorsâ€™ robustness to experimental conî€›gurations, leading to informative estimator comparisons in OPE research and a reliable estimator selection in practice. Here, we outline our experimental protocol, Interpretable Evaluation for Oî€Ÿine Evaluation (IEOE). As we have discussed, the expected value of performance (e.g., MSE) alone is insuî€œcient to properly evaluate the real-world applicability of an estimator, as it discards information about its robustness to hyperparameter choices and changes in evaluation policies. We can conduct a more informative experiment by estimating the cumulative distribution function (CDF) of an estimatorâ€™s performance, as done in some studies on reinforcement learning [5,9,10]. CDF is the function,ğ¹:R â†’ [0,1], where ğ‘ is a random variable representing the performance metric of an estimator (e.g., the squared error).ğ¹(ğ‘§)maps a performance metricğ‘§to the probability that the estimator achieves a performance better or equal to that score, i.e., ğ¹(ğ‘§) := P(ğ‘ â‰¤ ğ‘§). When we have sizeğ‘šof realizations ofğ‘, i.e.,Z:= {ğ‘§, . . . , ğ‘§}, we can estimate the CDF by Using the CDF for evaluating OPE estimators allows researchers to compare diî€erent estimators with respect to their robustness to the varying conî€›gurations. Speciî€›cally, we can use the CDF to evaluate OPE estimators by examining the CDF of the estimatorsâ€™ performance visually or computing some summary scores of the CDF as the estimatorsâ€™ performance metric. For example, we can score an estimator by the area under the CDF curve (AU-âˆ« CDF):AU-CDF(ğ‘§):=ğ¹(ğ‘§)ğ‘‘ğ‘§.Another possible summary score is conditional value-at-risk (CVaR) which computes the expected value of a random variable above a given probabilityğ›¼: CVaR(ğ‘ ):= E[ğ‘ | ğ‘ â‰¥ ğ¹(ğ›¼)],whereğ¹(ğ›¼):= argmin{ğ‘§ | ğ¹(ğ‘§) â‰¥ ğ›¼ }is the inverse of the CDF. When using CVaR, the estimators are evaluated based on the average performance of the bottom 100Ã— (1âˆ’ ğ›¼)percent of trials. For example,CVaR(ğ‘ ) is the average performance of the worst 30% of trials. In addition, we can use standard deviation (Std),E[(ğ‘ âˆ’ E[ğ‘ ])], and some other moments such as the skewness ofË†ğ¹ (ğ‘§) as summary scores. IEOE with Synthetic or Classiî€›cation Data. In research papers, it is common to use synthetic or classiî€›cation data to evaluate OPE estimators [4,11,12,22,29]. We î€›rst present how to apply the IEOE procedure to synthetic or classiî€›cation data in Algorithm 1. To evaluate the estimation performance ofË†ğ‘‰, we need to specify a candidate set of hyperparametersÎ˜, a set of evaluation policies Î , a hyperparameter sampling functionğœ™, and a set of random seedsS. Then, for every seedğ‘  âˆˆ S, the algorithm samples a set of hyperparametersğœƒ âˆˆ Î˜based on samplerğœ™. What kind ofğœ™ we use can change depending on the purpose of the evaluation of OPE. For example, we can use a hyperparameter tuning method for OPE estimators such as the method described in Section 2.2 as ğœ™, assuming practitioners use it in real-world applications. When we cannot implement such a hyperparameter tuning method for OPE due to its implementation cost or risk of overî€›tting, we can be conservative and use the uniform distribution asğœ™in the evaluation of OPE. Next, the IEOE algorithm samples an evaluation policyğœ‹âˆˆ Î from the discrete uniform distribution. Then, it replicates the data generating process using the b ootstrap sampling fromD. A bootstrapped logged bandit feedback dataset is deî€›ned asD:= {(ğ‘¥, ğ‘, ğ‘Ÿ)}where each tuple(ğ‘¥, ğ‘, ğ‘Ÿ)is sampled independently fromDwith replacement. Finally, for sampled tuple (ğœ‹, D, ğœƒ ), it computes a performance metric (e.g., the squared error). After applying Algorithm 1 to several estimators and obtaining the empirical CDF of their evaluation performances, we can visualize them or compute some summary scores to evaluate and compare the estimatorsâ€™ robustness. IEOE with Real-World Data. It is also possible to apply IEOE to real-world logged bandit data. Algorithm 2 presents IEOE that can be used in real-world applications. To evaluate the performance ofË†ğ‘‰with real-world data, we need to prepare several logged bandit feedback datasets{D}where each datasetDis collected by a policyğœ‹. Then, for every seedğ‘  âˆˆ S, the algorithm samples a set of hyperparametersğœƒ âˆˆ Î˜based on a samplerğœ™. Next, the algorithm samples an evaluation policyğœ‹âˆˆ Î from the discrete uniform distribution. Then, the evaluation and test sets are deî€›nedÃ asD= DandD=Dwhere the evaluation set is used in OPE and the test set is used to calculate the ground-truth performance ofğœ‹. Then, the algorithm replicates the environment using the bootstrap sampling fromD. A bootstrapped logged bandit feedback dataset is deî€›ned asD:= {(ğ‘¥, ğ‘, ğ‘Ÿ)}where each tuple(ğ‘¥, ğ‘, ğ‘Ÿ)is sampled independently fromDwith replacement. Finally, for a sampled tuple(ğœ‹, D, ğœƒ ), it computes the squared error as follows. whereğ‘‰(ğœ‹;D) = E[ğ‘Ÿ]is the on-policy estimate of the policy value of ğœ‹estimated with the test set. Following Algorithm 2, researchers can benchmark the robustness of OPE estimators using public real-world data. In addition, practitioners can avoid using unstable estimators by applying Algorithm 2 to their own bandit data. In this section, we use IEOE and evaluate the robustness of a wide variety of OPE estimators on Open Bandit Dataset (OBD). We run the experiments using our pyIEOE software. By using it, anyone can replicate the results easily. OBD is a set of logged bandit feedback datasets collected on a largescale fashion e-commerce platform provided by Saito et al. [20]. There are three campaigns, "ALL", "Men", and "Women". We use size 30,000 and 300,000 of randomly sub-sampled data from the "ALL" campaign. The dataset contains user context as feature vectorğ‘¥ âˆˆ X, fashion item recommendation as actionğ‘ âˆˆ A, and click indicator as rewardğ‘Ÿ âˆˆ {0,1}. The dimensions of the feature vectorğ‘¥is 20, and the number of actions is 80. The dataset consists of subsets of data collected by two diî€erent policies, the uniform random policy and the Bernoulli Thompson Sampling policy [26]. We letDdenote the dataset collected by uniform random policyğœ‹andDdenote that collected by Bernoulli Thompson Sampling policyğœ‹. We apply Algorithm 2 to obtain a set of SEs as the performance metric of the estimators. We use our protocol and evaluate DM, IPWps, SNIPW, DRps, SNDR, Switch-DR, and DRos in an interpretable manner. In the experiment, we use the true behavior policy contained in the dataset to derive importance weights. In this setting, SNIPW is hyperparameter-free, while the other estimators need to be tested for robustness to the choice of the pre-deî€›ned hyperparameters and changes in evaluation policies. In addition, we use the hyperparameter tuning method described in Section 2.2 to tune estimator-speciî€›c hyperparameters such as ğœ† and ğœ. Then, we use RandomizedSearchCV implemented in scikit-learn withn_iter =5 to tune hyperparameters of reward estimatorË†ğ‘. Tables 2 and 3 describe hyperparameter spacesÎ˜for each estimator. Finally, we set S = {0, 1, . . . , 499}. Note: LR/RR means that LogisticRegression (LR) is used when ğ‘Œ is binary and RidgeRigression (RR) is used otherwise. RF stands for RandomForest. Ë†ğœ‹is an estimated behavior policy. This is unnecessary when we know the true behavior policy. We estimate the behavior policy only in the experiments with classiî€›cation data in Appendix A. Therefore, ğœ‹is in parentheses. ğ¾ = 1 means that we do not use cross-î€›tting and train the whole D. Note: We follow the scikit-learn package as to the names of the hyperparameters As default, we use max_iter = 10, 000 for LogisticRegression, n_estimators = 100 for RandomForest, and max_iter = 100 for LightGBM. Figure 2 visually compares the CDF of the estimatorsâ€™ squared error. Table 4 reports AU-CDF, CVaR, and Std as summary scores. When the dataset size is small (ğ‘› =30,000), we see that the typical way of reporting only the mean of the squared error cannot tell which estimator is accurate or robust. However, some other summary scores show that DM has more robust and stable estimation performance than other estimators, having lower CVaRand Std. Moreover, Figure 2 provides more detailed information about the estimatorsâ€™ performance. Speciî€›cally, DM performs better in the worst case while the other estimators show better performance Ë†ğ‘ (and behavior policy estimatorË†ğœ‹) in the region where squared error is lower than 0.2. Thus, when we are conservative and prioritize the worst case performance, DM is the most appropriate choice. Otherwise, other estimators might be a better choice. We cannot obtain this conclusion by comparing only the mean (typical metric) of the squared error. When the dataset size is large (ğ‘› =300,000), we conî€›rm that IPWps and SNIPW are more accurate than other model-based estimators. In particular, Figure 2 shows that IPWps performs better than other estimators in all region, meaning that we should use it whether we prioritize the best or the worst case performance. Overall, the results indicate that an appropriate estimator can drastically change depending on the situation such as the data size. Note: Larger value is better for AU-CDF and lower value is better for Mean, CVaR, and Std. Note that we normalize the scores by dividing them by the best score among all estimators. We use ğ‘§= 1.0 Ã— 10for ğ‘› = 30, 000 and ğ‘§ redand greenfonts represent the best and second-best estimators, respectively. The blue Therefore, we argue that identifying a reasonable estimator before conducting OPE is essential in practice. Moreover, we demonstrate that the IEOE procedure can provide more informative insight as to the estimatorsâ€™ performance compared to the typical metric. In this section, we apply the IEOE procedure to a real-world application. To show how to use IEOE in a real-world application, we conducted a data collection experiment on a real e-commerce platform in September 2020. The platform wants to use OPE to improve the performance of its coupon optimization policy safely without conducting A/B tests. However, it does not know which estimator is appropriate for its speciî€›c application and environment. Therefore, we apply the IEOE procedure with the aim of providing a suitable estimator choice for the platform. During the data collection experiment, we constructedD,D, andDby randomly assigning three diî€erent policies (ğœ‹,ğœ‹, andğœ‹) to users on the platform. In this application,ğ‘¥is a userâ€™s context vector,ğ‘is a coupon assignment variable (where there are four diî€erent types of coupons, i.e.,|A| =4), andğ‘Ÿis either a userâ€™s content consumption indicator (binary outcome) or the revenue from each user observed within the 7-day period after the coupon assignment (continuous outcome). The total number of users considered in the experiment was 39,687, and each ofD, D, and Dhas approximately one third of the users. Note that, in this application, there is a risk of overî€›tting due to the intensive hyperparameter tuning of OPE estimators, as the size of the logged bandit feedback data is not large. Moreover, the data scientists want to use an OPE estimator to evaluate the performance of several candidate policies. Therefore, we aim to î€›nd an estimator that performs stably for a wide range of evaluation policies with fewer hyperparameters. To apply our evaluation procedure, we need to deî€›ne a performance metric (in step 8 of Algorithm 2). We can do this by using our realworld data. We î€›rst pick one of the three policies as evaluation policyğœ‹and regard the others as behavior policies. When we chooseğœ‹as the evaluation policy, we deî€›neD= Dâˆª D andD= D. Then, by applying Algorithm 2, we obtain a set of SEs to evaluate the robustness and real-world applicability of the estimators. We use the IEOE protocol to evaluate the robustness of DM, IPWps, SNIPW, DRps, SNDR, Switch-DR, and DRos. Then, we utilize the experimental results to help the data scientists of the platform choose an appropriate estimator. During the data collection experiment, we logged the true action choice probabilities of the three policies, and thus SNIPW is hyperparameter-free. We use the hyperparameter spaces deî€›ned in Tables 2 and 3 for our real-world application. In addition, we use the hyperparameter tuning method described in Section 2.2 to tune estimator-speciî€›c hyperparameters such asğœ†andğœ. Then, we use the uniform distribution asğœ™to sample hyperparameters of reward regression modelË†ğ‘. Finally, we setS = {0,1, . . . ,999}and We applied Algorithm 2 to the above estimators for the binary and continuous outcome data, respectively. Figure 3 compares the CDF of the estimatorsâ€™ squared error for each outcome. First, it is obvious that SNIPW is the best estimator for the binary outcome case, achieving the best accuracy in almost all regions. We can also argue that SNIPW is preferable for the continuous outcome case, because it reveals the most accurate estimation in the worst case and is hyperparameter-free, although it underperforms DM in some cases. On the other hand, IPWps performs poorly for both outcomes, because our dataset is not large and some behavior policies are near deterministic, making IPWps an unstable estimator. Moreover, Switch-DR fails to accurately Note: Binary Outcome is the results when the outcome is each userâ€™s content consumption indicator. Continuous Outcome is the results when the outcome is the revenue from each user observed within the 7-day period after the coupon assignment. Larger value is better for AU-CDF and lower value is better for Mean, CVaR, and Std. Note that we normalize the scores by dividing them by the best score among all estimators. We use ğ‘§= 1.0 Ã— 10for the binary outcome and ğ‘§= 1.0 Ã— 10for the continuous outcome to calculate AU-CDF. The red represent the best and second-best estimators, respectively. The bluefonts represent the worst estimator. evaluate the performance of the evaluation policies. Thus, it is unsafe to use these estimators in our application, even though we tune their hyperparameters (ğœ† or ğœ). We additionally conî€›rm the above observations in a quantitative manner. For both binary and continuous outcomes, we compute AU-CDF, CVaR, and Std of the squared error for each OPE estimator. We report these summary scores in Table 5, and the results demonstrate that SNIPW clearly outperforms other estimators in almost all situations. In particular, SNIPW is the best with respect to CVaRand Std for both binary and continuous outcomes, showing that this estimator is the most stable estimator in our environment. Moreover, SNIPW is hyperparameter-free, and overî€›tting is less likely to occur compared to other estimators having some hyperparameters to be tuned. Through this evaluation of OPE estimators, we concluded thatthe e-commerce platform should use SNIPW for its oî€line evaluation.After comprehensive accuracy and stability veriî€›cation, the platform is now using SNIPW to improve its coupon optimization policy safely. In this paper, we argued that the current dominant evaluation procedure for OPE cannot evaluate the robustness of the estimatorsâ€™ performance. Instead, the IEOE procedure can provide an interpretable way to evaluate how robust each estimator is to the choice of hyperparameters or changes in evaluation policies. We have also developed open-source software to streamline our interpretable evaluation procedure. It enables rapid benchmarking and validation of OPE estimators so that practitioners can spend more time on real decision making problems, and OPE researchers can focus more on tackling advanced technical questions. We perform an extensive evaluation of a wide variety of OPE estimators and demonstrated that our experiments are more informative than a typical procedure, showing which estimators are more sensitive to conî€›guration changes. Finally, we applied our procedure to a real-world application and demonstrated its practical usage. Although our procedure is useful to evaluate the robustness of estimators, we need to prepare at least two logged bandit feedback datasets collected by diî€erent policies to apply it to real-world applications, as described in Algorithm 2. Thus, it would be beneî€›cial to construct a procedure to enable the evaluation of OPE estimators with only logged bandit data collected by a single policy. The authors would like to thank Masahiro Nomura, Ryo Kuroiwa, and Richard Liu for their help in reviewing the paper. Additionally, we would like to thank the anonymous reviewers for their constructive reviews and discussions.