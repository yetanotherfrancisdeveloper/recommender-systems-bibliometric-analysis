User modeling plays a fundamental role in industrial recommender systems, either in the matching stage and the ranking stage, in terms of both the customer experience and business revenue. How to extract usersâ€™ multiple interests eî€ectively from their historical behavior sequences to improve the relevance and personalization of the recommend results remains an open problem for user modeling. Most existing deep-learning based approaches exploit item-ids and category-ids but neglect î€›ne-grained features like color and material, which hinders modeling the î€›ne granularity of usersâ€™ interests. In the paper, we present Multiple interest and Fine granularity Network (MFN), which tackle usersâ€™ multiple and î€›ne-grained interests and construct the model from both the similarity relationship and the combination relationship among the usersâ€™ multiple interests. Speciî€›cally, for modeling the similarity relationship, we leverage two sets of embeddings, where one is the î€›xed embedding from pretrained models (e.g. Glove) to give the attention weights and the other is trainable embedding to be trained with MFN together. For modeling the combination relationship, self-attentive layers are exploited to build the higher order combinations of diî€erent interest representations. In the construction of network, we design an interest-extract module using attention mechanism to capture multiple interest representations from user historical behavior sequences and leverage an auxiliary loss to boost the distinction of the interest representations. Then a hierarchical network is applied to model the attention relation between the multiple interest vectors of diî€erent granularities and the target item. We evaluate MFN on both public and industrial datasets. The experimental results demonstrate that the proposed MFN achieves superior performance than other existed representing methods. â€¢ Information systems â†’ Recommender systems;â€¢ Learning to rank; â€¢ Personalization; Multiple interest, î€›ne granularity, user modeling, search ranking ACM Reference Format: Jiaxuan Xie, Jianxiong Wei, Qingsong Hua, Yu Zhang. 2021. Multiple Interest and Fine Granularity Network for User Modeling. In Proceedings of ACM Conference (Conferenceâ€™17). ACM, New York, NY, USA, 4 pages. https://doi. org/10.1145/1122445.1122456 In recent years, recommender systems have become increasingly prevalent and gained success in various applications such as news distribution, video watching and e-commerce. Instead of only considering categories or keywords, current recommender approaches attempt to merge more personalized information into modeling with the goal of understanding exactly the intention of users and showing what they are most interested in. The user historical behavior sequences, which have been proved to be of great value for personalization, play a signiî€›cant role in user modeling for extracting usersâ€™ hidden interests. A popular user modeling strategy is to obtain user interest representations by leveraging the mean pooling or the weighted pooling over the historical behavior sequences. Several methods follow a similar Embedding&MLP paradigm [2,3,14]. They capture the representation of usersâ€™ interests by averaging the embedding vectors of user behaviors and transform them into a î€›xed-length vector as usersâ€™ interest representation. While attention-based methods like DIN [16], DIEN [15] and DSIN [4], they adaptively extract the insterest vector by considering the relevance of historical behaviors given a target item using attention mechanisms, which consequently assigns higher activated weights to those historical behaviors with higher relevance. DIN and its successors achieve better performance than Embedding&MLP methods, but a single interest vector is still insuî€œcient to capture the varying characteristics of usersâ€™ interests. To address this problem, MIND [6] and ComiRec [1] leverage capsule routing mechanism for clustering historical behaviors and obtaining usersâ€™ multiple interest vectors. However, except item-ids, category-ids and similar id features, both of them lack the modeling of î€›ner grained features like color and materials. [5,8,12] exploit transformers or similar hierarchical attention structure to capture multiple interest representations of users. Yet, their complicated model structures make serving online nearly infeasible without speciî€›c optimization for practical usage. In this paper, we propose Multiple Interest and Fine granularity Network (MFN), for user modeling to handle with usersâ€™ multiple and î€›ne-grained interests. There are two key components in MFN, one is for modeling the similarity relationship and the other is for the combination relationship among usersâ€™ multiple interests. Similaritymeans two interests are similar in terms of physical characteristics,e.g.a customer likes black windbreakers and black coats.Combinationmeans two interests share latent collocation, e.g.a father buys beers and pampers. Speciî€›cally, for modeling the similarity relationship, we leverage two sets of embeddings, where one is the î€›xed embedding from pretrained models (e.g. Glove) to give the attention weights and the other is trainable embedding to be trained with MFN together. For modeling the combination relationship, self-attentive layers are exploited to build the higher order combinations of diî€erent interest representations. By modeling the similarity and the combination relationship separately, we are able to build more expressive and eî€ective usersâ€™ multiple and î€›ne-grained interest representations. In a nutshell, the main contributions of this paper can be concluded as following: â€¢We propose to study the problem of extracting multiple and î€›ne-grained interest representations from usersâ€™ historical behavior sequences for user modeling and present the Multiple interest and Fine granularity Network (MFN). â€¢We propose to model both the similarity relationship and the combination relationship among usersâ€™ multiple interests. For the former, we leverage two sets of embeddings, where the î€›xed one is for giving he attention weights and the trainable one is trained for the main task. For the latter, we leverage self-attentive layers to learning high-order interest interactions. â€¢We conduct extensive experiments on both public and industrial datasets. Experimental results demonstrates the proposed MFN achieves superior performance than other representing methods with other important attributes like good explainability and little online response time. In recent decades, user interest modeling has attracted much attention in industrial applications such as recommender systems and online advertising, which concentrates on learning the representation of usersâ€™ interests from the historical behavior sequences. DIN [16] leverages an attention mechanism to capture the diverse interests of a user on diî€erent candidate items. DIEN [15] considers the temporal relationship among the historical behaviors and proposes to model the evolution of usersâ€™ interests with an interest extraction layers based on GRU. DSIN [4] highlights that user behaviors are highly homogeneous in each session and heterogeneous cross sessions and designs a self-attention network with bias encoding to get the corresponding interest representation of each session. MIND [6] and ComiRec [1] emphasize that a single interest vector is insuî€œcient to capture the varying nature of usersâ€™ interests. They exploit capsule network and dynamic routing to obtain the representation of usersâ€™ interests as multiple interest vectors. Due to the success of transformers in other areas, [8,11] propose to leverage transformers or similar structures with multiple head attentions to extract usersâ€™ multiple interests from the behavior sequences. There are also a series of works that focus on long-term even lifelong user interest modeling. [9] demonstrates that the longer historical behavior sequences in the user interest modeling module can support the CTR model with better performance. MIMN [9] embeds user long-term interest into î€›xed-sized memory network to decrease the burden of the latency and storage of online serving. SIM [10] leverages a general search unit to get a sub user behavior sequence that is relevant to the candidate item and proposes an exact search unit to model the precise relationship between the candidate item and the sub sequence. In this section, we formulate the problem and illustrate the proposed nt Multiple interest and Fine granularity Network (MFN) in detail. Considering the deployment of MFN in practical scenarios, we build MFN with a hierarchical structure with two levels. The î€›rst level is for category and the second level is for î€›ne-grained features like item entities. Suppose we have a set of usersUand a set of itemsI. For simplicity, here we consider a userğ‘¢ âˆˆ Uand a candidate itemğ‘–âˆˆ I. The historical behavior sequence ofğ‘¢isğ¸ =(ğ‘–, . . . , ğ‘–, . . . , ğ‘–)âˆˆ R, whereğ¸ âŠ‚ I,ğ‘¡denotes theğ‘¡-th interaction itemğ‘‘is the feature size,ğ‘is the length of the sequence. The feature î€›elds of items include id features ( e.g. iid-item id, cid-category id, sid-shopid, bid-brand id) and î€›ne-grained features (e.g. entity features likeî€î€‘ color or materials), i.e.ğ‘–=ğ‘–; ğ‘–, whereğ‘–âŠ† {ğ‘–, ğ‘–, ğ‘–, ğ‘–}, ğ‘–denote the features of coarse id features and î€›ne-grained features, respectively. The feature construction here follows the basic embedding paradigm [2? ]. Similarity means two interests are similar in terms of physical characteristics, e.g. a customer likes black windbreakers and black coats. Hence, we can divide and extract usersâ€™ multiple interests according to the similarity of the items in the behavior sequences. Here, to model the similarity relationship among the usersâ€™ multiple interests, we construct two sets of embeddings,ğ¸andE, ğ¸, E âˆˆ R. One is provided by pretrained models and is î€›xed in the model to measure the similarity, still denoted asğ¸. For id features like iid, cid, sid, the pretrained embedding can be obtained from a vanilla model under an auxiliary CTR prediction task based on longterm behavior data. If the î€›ne-grained features are from words, we can exploit Golve or Word2Vec to get the pretrained embeddings. And the other set of embeddingEis trainable and is trained under the main task in the proposed MFN model. For clustering multiple interest vector centers, [1,6] leverage random centers as the initial centers. However, Using random centers not only does require more iteration in the following clustering, but also lose the expressive power and the explainability to some degree. In this paper, we propose to pretrain the cluster centers. Suppose one userâ€™s multiple interest cluster centers areğ¶ = {ğ¶, . . . , ğ¶. . . , ğ¶} âˆˆ R, where ğ¾is the number of centers,ğ¶denotes theğ‘—-th center. Recall the historical behavior sequenceğ¸ =(ğ‘–, . . . , ğ‘–, . . . , ğ‘–)âˆˆ R, we assume the probability that behaviorğ‘–belongs to centerğ¶isğ‘ƒ. On the one hand, we hope that the centers are not redundant,i.e. each cluster is î€›lled with samples and each center can be covered, hence we maximum the entropy of the sum along the rows of ğ‘ƒ, On the other hand, one behaviorğ‘–ought to be assigned to only one centers, hence we minimum the entropy of each ğ‘ƒ, From Eqn. (1) and Eqn. (2), the optimization target is to minimize Considering the large-scale situation, we can use the mini-batch version. The algorithm to obtain the multiple interest centers is demonstrated as Alg. 1. Note that here we leverage backward propagation to optimize the trainable centersğ¶, and the stopping criterion can be set to stop at a given number of iterations. Data: Trainable centers ğ¶ âˆˆ R, ğ‘€ usersâ€™ historical behavior sequence ğ¸= {ğ¸}, ğ¸âˆˆ R, learning rate ğ›¼ Result: Trained centers ğ¶. {1, . . . , ğ‘€}; And in the next we introduce how to leverage the multiple interest centers in the main task of MFN. After obtaining the multiple interest centersğ¶, using attention mechanism we are able to give the similarity weights,ğ‘–.ğ‘’.the probabilities that the behaviorsE can be divided into interest centers ğ¶, And the multiple interest representations of a user ğ‘™ is where ğ‘…âˆˆ R. Besides the similarity attribute in terms of physical or semantic characteristics, another signiî€›cant relationship among the usersâ€™ multiple interests is combination. Combination here means two interests share latent collocation, e.g. a father buys beers and pamper at the same time, or a student buys an English book and an electronic dictionary. Here, we adapt the self attentive method [7] for capturing high-order combinations of multiple interests. Recall E âˆˆ Ris the trainable embedding for behavior sequences, using self-attention mechanism, we get the combinational weight matrix ğ´ as whereswish(Â·)denotes theswishactivation,MSA(Â·)denotes the Multi-head self-attention,WandWare trainable parameters with size ğ‘‘Ã— ğ‘‘ and ğ‘‘Ã— ğ¾, respectively. The combinational interest representationsğ‘…can be computed whereğ‘…âˆˆ R,ğ¾is the number of interests andğ‘‘is the dimension of features. In this section, we introduce how to aggregate the interest representations from both the similarity and combination relationships. After extracting the multiple interests for each user from the historical behavior sequences, we have the whole interests R as whereR âˆˆ R. We can get the interest representationR(ğ‘–) with regard to the candidate item ğ‘–âˆˆ R, R(ğ‘–) = ğ‘“ (R, ğ‘– whererdenotes theğ‘—-th interest inR(i.e.theğ‘—-th row ofR),ğ‘(Â·) andğ‘(Â·)is a feed-forward network with output as the weight, andÃÃ Leveraging the interest representationR(ğ‘–), along with the User Proî€›le ğ‘‹, theItem proî€›le ğ‘‹andContext features ğ‘‹, we train the MFN model by optimizing the cross entropy loss Table 1: Experimental results on industrial datasets whereDis the training set,ğ‘¥is represented by[ğ‘‹, ğ‘‹, ğ‘‹]that is the input of the network,ğ‘¦ âˆˆ {0,1}denotes the labels, where 1 means the user has an inveraction with the candidate item,ğ‘ (Â·)is the î€›nal output of the model which represents the output probability. In this section, we conduct experiments on both public and industrial datasets to evaluate the eî€ectiveness of the proposed MFN compared with other representative methods. We introduce the datasets, competitors, evaluation metrics and then report and analyse the experimental results. In this paper, we evaluate the model on both the public and industrial datasets. We collect the industrial datasets from an e-commerce app and leverage 3 daysâ€™ logs with usersâ€™ historical sequences for constructing the training dataset and use the next one dayâ€™s data for testing. For evaluating the performance on public datasets, the Area Under the Curve (AUC) is exploited and for the industrial datasets we use the RelaImpr [13] to measure the relative improvement, which is deî€›ned asî€’î€“ ğ‘…ğ‘’ğ‘™ğ‘ğ¼ğ‘šğ‘ğ‘Ÿ =ğ´ğ‘ˆ ğ¶ (ğ‘‡ ğ‘’ğ‘ ğ‘¡_ğ‘šğ‘œğ‘‘ğ‘’ğ‘™) âˆ’ 0.5ğ´ğ‘ˆ ğ¶ (ğµğ‘ğ‘ ğ‘’_ğ‘šğ‘œğ‘‘ğ‘’ğ‘™) âˆ’ 0.5âˆ’ 1 We conduct our experiments on Tensorî€ow 1.4 and use the Adam optimizer. The initial learning rate is 1e-4, the epoch is set to 1. Other methods are set as their recommend settings. The results on the industrial datasets is illustrated as Table. 1 In the paper, we present Multiple interest and Fine granularity Network (MFN), which tackle usersâ€™ multiple and î€›ne-grained interests and construct the model from both the similarity relationship and the combination relationship among the usersâ€™ multiple interests. We evaluate MFN on both public and industrial datasets. The experimental results demonstrate that the proposed MFN achieves superior performance than other existed representing methods.