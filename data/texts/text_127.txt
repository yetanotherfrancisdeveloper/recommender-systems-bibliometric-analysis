Knowledge Distillation (KD), which transfers the knowledge of a well-trained large model (teacher) to a small model (student), has become an important area of research for practical deployment of recommender systems. Recently, Relaxed Ranking Distillation (RRD) has shown that distilling the ranking information in the recommendation list signiî€›cantly improves the performance. However, the method still has limitations in that1)it does not fully utilize the prediction errors of the student model, which makes the training not fully eî€œcient, and2)it only distills the user-side ranking information, which provides an insuî€œcient view under the sparse implicit feedback. This paper presents Dual Correction strategy for Distillation (DCD), which transfers the ranking information from the teacher model to the student model in a more eî€œcient manner. Most importantly, DCD uses the discrepancy between the teacher model and the student model predictions to decide which knowledge to be distilled. By doing so, DCD essentially provides the learning guidance tailored to â€œcorrectingâ€ what the student model has failed to accurately predict. This process is applied for transferring the ranking information from the user-side as well as the item-side to address sparse implicit user feedback. Our experiments show that the proposed method outperforms the state-of-the-art baselines, and ablation studies validate the eî€ectiveness of each component. â€¢ Information systems â†’ Learning to rank; Collaborative î€›ltering; Retrieval eî€œciency. Recommender System; Knowledge Distillation; Learning to Rank; Model Compression; Retrieval eî€œciency ACM Reference Format: Youngjune Lee,Kee-Eung Kim. 2021. Dual Correction Strategy for Ranking Distillation in Top-N Recommender System. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/3459637.3482093 In this era of information explosion, Recommender Systems (RS) are widely used in various industries to provide personalized user experience [3,12,16,18]. For achieving higher recommendation accuracy, the recommendation model has become very large to capture the complexity of personalized recommendations [11,17, 19,26,28,30]. However, large models incur correspondingly large computational cost as well as high latency for inference, which has become one of the major obstacles for real-time service [9,11,17]. To reduce the inference latency, early methods adopt hash techniques [13,20,21,30] or tree-based data structure [1]. However, they have problems such as easily falling into a local optimum or applicable only to speciî€›c models [9,28]. To address the problems, Knowledge Distillation (KD) has been actively studied for RS [9,10,17,19,26,28]. KD is a model compression technique that improves the performance of a small student model by transferring the knowledge of a pre-trained large teacher model [2,4,6,25]. During the distillation, the teacher model provides additional supervision which is not existent in the usersâ€™ feedback, so the student model can achieve a higher recommendation accuracy compared to the student model trained only on the original feedback. The state-of-the-art method, Relaxed Ranking Distillation (RRD) [9], formulates the distillation process as a ranking matching problem between the recommendation list of the teacher model and that of the student model. In other words, it utilizes the ranking orders among the items from the teacher model as additional supervision to guide the student model, and trains the student model to preserve the ranking orders of the teacher model. This ranking-distillation approach transfers the relative preference order among the userâ€™s preferred items, which is the key knowledge directly aî€ecting topğ‘recommendation accuracy. As a result, it signiî€›cantly improves performance over the previous methods [19,26] that do not directly utilize the ranking information [9]. Still, there are limitations in RRD. First, it transfers the knowledge without consideration of the prediction errors of the student model. As the student model gets more accurate in matching the prediction of the teacher model, repeatedly distilling the ranking information that the student model already correctly predicts cannot eî€ectively enhance the student model and makes the training ineî€œcient. We argue that the knowledge to be distilled should be dynamically changed based on the student modelâ€™s prediction error, enabling â€œcorrectionâ€ for what the student model has not yet predicted accurately. Second, it only transfers user-side ranking information, i.e., ranking orders among the items. Previous studies have pointed out that learning only with the user-side ranking information degrades the quality of user representation [7] and provides a view insuî€œcient to fully understand the sparse implicit feedback [10,14]. Particularly in KD where the student modelâ€™s capacity is limited, these problems can be further exacerbated and severely degrade the performance. In this work, we propose a novel Dual Correction strategy for Distillation (DCD), which aims to address the aforementioned shortcomings. To this end, DCD î€›rst computes discrepancy between the ranking list of the teacher model and that of the student model, then decides what knowledge to be distilled based on the discrepancy. By doing so, DCD provides guidance tailored to correct what the student model has failed to correctly predict, which helps to î€›nd an eî€ective path for the student modelâ€™s training. This process is conducted for dual-side ranking, i.e., for the user-side and the item-side, providing a comprehensive view to better understand both users and items [7,14]. We validate the superiority of the proposed method with extensive experiments on real-world datasets, and provide an ablation study showing the eî€ectiveness of each proposed component. We focus on top-ğ‘recommendation task for implicit feedback [8, 18]. Given implicit user-item interactions, a recommender system provides a ranked list of top-ğ‘unobserved items for each user. The distillation process is conducted as follows: First, we train a large model (teacher) using the implicit feedback. Then, we train a small model (student) with the same feedback data along with the ranking list predicted from the teacher. The ranking information reveals the detailed preference orders among the unobserved items, which helps the training of the student. Our goal is to design a distillation strategy that allows the student to eî€ectively follow the teacherâ€™s ranking list. We denote the teacher byğ‘‡and the student byğ‘†.ğ‘…andğ‘… denote the user-side ranking list for userğ‘¢(i.e., the list of the unobserved items) predicted by the teacher and the student, respectively. ğ‘…(ğ‘–)denotes the rank of itemğ‘–in the ranking list where a lower value means a higher ranking position, i.e.,ğ‘…(ğ‘–) =0 is the highest ranking. For the item-side ranking list, we simply reverse the notation of the user-side. Concretely,ğ‘…andğ‘…denote the itemside ranking list for itemğ‘–(i.e., the list of the unobserved users) predicted by the teacher and the student, respectively, andğ‘…(ğ‘¢) denotes the rank of user ğ‘¢ in the ranking list. The ranking distillation (RRD) [9] formulates the distillation as a ranking matching problem between the ranking list of the teacher and that of the student (i.e.,ğ‘…andğ‘…). Speciî€›cally, the method trains the student to preserve the orders of ranking inğ‘…by using a variant of ListMLE [29]. The core idea is to deî€›ne a permutation probability based on the the studentâ€™s ranking scores, and train the student to maximize the likelihood of the teacherâ€™s ranking ğ‘…. To make the student better focus on top-ranked items, we also adopt relaxed permutation probability [9] that ignores the lowranked itemsâ€™ detailed orders. Formally, letğ‘…is decomposed to two sub-ranking listsğ‘…= [ğœ‹;ğœ‹], whereğœ‹includes a few top-ranked items andğœ‹includes the remaining items (ğ‘¢andğ‘‡are omitted for simplicity). The relaxed permutation probability of the ranked list ğœ‹ for the student ğ‘† is deî€›ned as follows: ğ‘ (ğœ‹ |ğ‘†) =exp ğ‘†(ğ‘¢, ğœ‹)ÃÃ whereğœ‹is theğ‘˜-th item inğœ‹,ğ‘† (ğ‘¢, ğœ‹)is the score of the user-item interaction predicted by the student. By maximizing the probability, the student learns the detailed ranking orders inğœ‹while lowering all the ranks of items inğœ‹below the lowest rank of items inğœ‹, which allows the student to focus more on top-ğ‘ranking orders. The student is trained by the ranking knowledge distillation (RKD) loss as follows: whereLis the loss for training the base model using the im-Ã plicit feedback data, andL= âˆ’log ğ‘ (ğœ‹|ğ‘†)is the permutation loss deî€›ned for the users in mini-batch ğµ. We present Dual Correction strategy for Distillation (DCD) that adaptively assigns more concentrations on training instances that the student fails to predict correctly, unlike the prior methods such as RRD that generate training instances solely based on the teacherâ€™s predictions. This correction is used for transferring the ranking information from the user-side as well as the item-side, providing a comprehensive view to understand both users and items. 2.3.1Identifying discrepancy between Teacher model and Student model. DCD î€›rst identiî€›es discrepancy betweenğ‘…and ğ‘…to decides what knowledge to be distilled. We deî€›ne two types of discrepancy: 1) underestimation error and 2) overestimation error. The underestimation error means that the student predicts a low ranking position whereas the teacher predicts a higher ranking position, i.e.,ğ‘…(ğ‘–) > ğ‘…(ğ‘–). Thus, the student needs to be corrected to give a lower rank value for(ğ‘¢, ğ‘–). The overestimation error means the opposite, the student predicts a high ranking position whereas the teacher predicts a lower ranking position, i.e.,ğ‘…(ğ‘–) < ğ‘…(ğ‘–), which needs to be corrected to give a higher rank value. The userside errors are computed as follows: We useğ‘¡ğ‘ğ‘›â„, which is a saturated function, to treat the errors above a certain threshold equally, allowing the student to learn the teacherâ€™s knowledge on most of the discrepant predictions.ğœ‡is a hyperparameter that controls the sharpness of the tanh function. Using the computed errors, we identify the discrepant predictions that need to be corrected. Concretely, we sampleğ‘€underestimated items andğ‘€overestimated items. Both sampling probabilities are proportional to the degree of a discrepancy. whereğ‘(ğ‘–)is the sampling probability of underestimated items andğ‘(ğ‘–)is the sampling probability of overestimated items for userğ‘¢. These discrepant items are dynamically changed based on the prediction errors of the student during the training, and will be corrected by the correction loss (Sec. 2.3.2). DCD also provides the corrections for discrepancy in terms of the item-side ranking. As pointed out in the previous work [7,14], learning only the user-side ranking degrades the quality of user representation [7] and also provides a restricted view insuî€œcient to understand the sparse implicit feedback [14]. Especially, in KD where the studentâ€™s capacity is highly limited, these problems can be further exacerbated, which leads to degraded performance. Similar to the user-side, we identify the discrepant predictions on the item-side. We sampleğ‘€underestimated users andğ‘€overestimated users based on the discrepancy betweenğ‘…andğ‘…. The sampling probabilities are as follows: whereğ‘(ğ‘¢)is the probability of underestimated users andğ‘(ğ‘¢) is the probability of overestimated users for itemğ‘–. Without loss of generality, ğ·(ğ´, ğµ) = ğ‘¡ğ‘ğ‘›â„(ğ‘šğ‘ğ‘¥ (ğœ‡(ğ‘…(ğ‘¢) âˆ’ ğ‘…(ğ‘¢)), 0)). 2.3.2Dual Correction Distillation Loss. Now, we correct the discrepant predictions in the user-side (summarized byğ‘€andğ‘€) and the item-side (summarized byğ‘€andğ‘€). From the points of the teacher, the underestimation errors contain the predictions that should be higher-ranked, whereas the overestimation errors contain the predictions that should be relatively lower-ranked compared to the former. As consistently shown in the existing distillation work [9,19,26], the student takes a huge beneî€›t by learning the teacherâ€™s knowledge with a particular emphasis on the high-ranked items, because it directly aî€ects the top-ğ‘recommendation accuracy. In this regard, we design the correction loss that corrects the ranks of the underestimation errors in detail and lowers the ranks of the overestimation errors overall. LetğœŒdenote the sorted lists ofğ‘€by the original order in ğ‘…, andğœŒdenote the sorted lists ofğ‘€by the order inğ‘…. The user-side correction distillation (UCD) for userğ‘¢is conducted by maximizing the following relaxed permutation probability: ğ‘ (ğœŒ|ğ‘†) =ÃÃ, (7) whereğœŒis theğ‘˜-th item inğœŒ. UCD is applied for the users inÃ mini-batchğµ, i.e.,L= âˆ’log ğ‘ (ğœŒ|ğ‘†). Analogously, item-side correction distillation (ICD) for itemğ‘–is conducted by maximizing the following relaxed permutation probability: ICD is also applied for correcting errors with respect to the items inÃ the batch, i.e.,L= âˆ’log ğ‘ (ğœŒ|ğ‘†). Finally, the proposed DCD trains the student with the following loss function. whereğœƒis the learning parameters of the student.ğœ†andğœ† are hyperparameters controlling the user-side and item-side corrections, respectively. Note thatLis computed for the same ground-truths regardless of the discrepancy during the training. Finally, our dual correction loss provides dynamically changing guidance to correct the student errors for more eî€ective training. Table 1: The number of parameters and inference time for generating recommendation list for every user. We closely follow the setup of the state-of-the-art method, RRD [9]. Speciî€›cally, datasets, base models, evaluation protocol, and the metrics are the same as [9]. Due to the limited space, we omit the detailed explanations of the setup. Please refer to [9]. Datasets.We use CiteULike [27] and Foursquare [22] which are public real-world datasets. After the preprocessing [9], CiteULike has 5,220 users, 25,182 items, and 115,142 interactions. Foursquare has 19,466 users, 28,594 items, and 609,655 interactions. Base models.We use two base models for the top-ğ‘recommendation: BPR [24] and NeuMF [5], which have diî€erent architectures and optimization strategies. For both models, the dimension of user/item representations are set to 200 for the teacher model, and 20 for the student model. Following [9], we denote the student model trained without distillation as "Student". Table 1 presents the number of parameters and inference time. The inferences are made using PyTorch with CUDA from TITAN Xp GPU and Intel i7-4770 CPU. It shows that the smaller model has lower inference latency. Evaluation protocol and metrics.We use the leave-one-out evaluation protocol whereby two interacted items for each user are held out for test/validation, and the rest are used for training [9]. We adopt two top-ğ‘ranking evaluation metrics, namely Hit Ratio (H@ğ‘) and Mean Reciprocal Rank (M@ğ‘). We compute the average score of those two metrics for each user. Finally, we report the average of the î€›ve independent runs. Baselines.We compare DCD with the state-of-the-art ranking distillation method, RRD [9]. Note that we do not include the previous methods distilling point-wise information (e.g., RD [26], CD [19]), because RRD already outperforms them by a huge margin [9]. Implementation details.We use PyTorch [23] for implementation and train all models with Adam optimizer [15]. For each base model and dataset, we tune the hyperparameters by grid search on the validation set. We tune learning rate and L2 regularizer âˆˆ {10,10,10,10,10,10}. In the case of RRD-speciî€›c hyperparameters, we tune them in the ranges suggested by the original paper. For the dual correction loss, we tuneğœ†, ğœ†âˆˆ {1,10,10,10,10,10}, and conduct the sampling process every 5 epochs. The number of discrepant users/items (ğ‘€,ğ‘€) is set to 40, but it can be further tuned. Lastly, ğœ‡ is set to 10. 3.2.1Overall Evaluation. Table 2 presents top-ğ‘recommendation accuracy of the methods compared. DCD achieves significantly higher performance than RRD on both datasets and both Table 2: Performance comparison. improve.r denotes the improvement of DCD over RRD and improve.s denotes the improvement of DCD over Student. * and ** indicate ğ‘ â‰¤ 0.005 and ğ‘ â‰¤ 0.0005 for the paired t-test of DCD vs. RRD on H@5. NeuMF base models. Also, in terms of the number of recommended items (ğ‘), DCD shows larger improvements for H@5/M@5 compared to H@10/M@10. Namely, DCD has a better performance at predicting the top-ranked items than RRD, which is practically advantageous for real-world RS, which gives the users the most preferred items. 3.2.2Ablation Study. We provide ablation study of the key components of DCD in Table 3. We compare the following ablations: 1)w/o Correctiontransfers the user-side and item-side ranking information without the correction strategy, i.e., RRD + item-side RRD. 2)w/o Item-sideandw/o User-sideablate ICD and UCD from DCD, respectively. 3)w/o Samplingdeterministically selects items/users with the largest underestimation error and overestimation error without the sampling process (Sec. 2.3.1). We observe that each proposed component is indeed eî€ective in distilling the ranking information. This result supports our claim that the supervision from the teacher model should be dynamically changed based on the studentâ€™s errors (w/o Correction) and distilling the single-side ranking is insuî€œcient (w/o Item-side and w/o User-side). Also, the comparison with â€œw/o Samplingâ€ shows that a certain degree of î€exibility is beneî€›cial in choosing the discrepant predictions for the correction strategy. 3.2.3Further Analysis. We provide further analysis on DCD. For the sake of the space, we report the results of BPR on CiteULike. First, Figure 1a presents the average ranking-discrepancy of various methods. In speciî€›c, we compute the discrepancy as |ğ‘…(Â·) âˆ’ Table 3: Ablation analysis on Foursquare dataset. BPRw/o Correction 0.5318 0.3408 0.6710 0.3594 NeuMFw/o Correction 0.5147 0.3214 0.6704 0.3424 Figure 1: Eî€ects of DCD. (a) The average discrepancy from Teacher, (b) H@5 with varying ğœ†and ğœ†. ğ‘…(Â·)|for the user-side (and for the item-side) top-50 recommendation list produced by the teacher model. We compute it for all users (and for all items), then report the average value. We observe that the proposed correction strategy eî€ectively reduces the discrepancy between the teacher model and the student model. All the correction-based methods (i.e., ICD, UCD, and DCD) achieves lower discrepancy than RRD. Also, DCD achieves the lowest discrepancy in both user-side and item-side, which supports its superior recommendation performance. This also again shows the importance of the dual-side ranking correction. Lastly, Figure 1b shows the eî€ects ofğœ†andğœ†. Note thatğœ†=0 &ğœ†=0 corresponds to RRD. We again observe that both user-side and item-side corrections are indeed eî€ective. The best performance is achieved when ğœ†is around 10-10and ğœ†is around 10-10. We propose DCD, a dual correction strategy for ranking distillation in top-ğ‘RS. Unlike the existing method based on unilateral distillation, DCD provides guidance designed to correct the errors that the student model has failed to learn. By considering the prediction errors of the student model, DCD helps to î€›nd an eî€ective path for the student modelâ€™s training. DCD also considers the user-side ranking and item-side ranking simultaneously, providing a comprehensive view to understand both users and items. We validate the eî€ectiveness of DCD with extensive experiments on real-world datasets. Also, we provide in-depth ablation study to ascertain the validity of each proposed component. For future work, we will investigate the eî€ects of DCD on various base models. Acknowledgement.The authors thank SeongKu Kang for contributing to the implementation and improvement of DCD. This work was supported by IITP grant funded by the MSIT: (No.20190-00075, Artiî€›cial Intelligence Graduate School Program(KAIST)) and the ETRI: (Contract No. 21ZS1100).