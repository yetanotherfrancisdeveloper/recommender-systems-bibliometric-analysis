Cross-domain cold-start recommendation is an increasingly emerging issue for recommender systems. Existing works mainly focus on solving either cross-domain user recommendation or cold-start content recommendation. However, when a new domain evolves at its early stage, it has potential users similar to the source domain but with much fewer interactions. It is critical to learn a userâ€™s preference from the source domain and transfer it into the target domain, especially on the newly arriving contents with limited user feedback. To bridge this gap, we propose a self-trained CrossdOmain User Preference LEarning (COUPLE) framework, targeting cold-start recommendation with various semantic tags, such as attributes of items or genres of videos. More speciî€›cally, we consider three levels of preferences, including user history, user content and user group to provide reliable recommendation. With user history represented by a domain-aware sequential model, a frequency encoder is applied to the underlying tags for user content preference learning. Then, a hierarchical memory tree with orthogonal node representation is proposed to further generalize user group preference across domains. The whole framework updates in a contrastive way with a First-In-First-Out (FIFO) queue to obtain more distinctive representations. Extensive experiments on two datasets demonstrate the eî€œciency of COUPLE in both user and content cold-start situations. By deploying an online A/B test for a week, we show that the Click-Through-Rate (CTR) of COUPLE is superior to other baselines used on Taobao APP. Now the method is serving online for the cross-domain cold micro-video recommendation. â€¢ Computer systems organization â†’ Embedded systems; Redundancy; Robotics; â€¢ Networks â†’ Network reliability. cross-domain, cold-start recommendation, user preference learning Figure 1: Overview of the proposed cross-domain cold-start recommendation system. A userâ€™s preference is captured from diî€erent perspectives with jointly modeling behaviors in all domains. Personalized recommendation system (RS) plays a vital role in an e-commerce platform, where eî€ective strategies have been made to alleviate information overload and facilitate better user experiences. In recent years, deep learning has been widely applied in RS to overcome obstacles of conventional recommendation techniques. Great eî€ort has been made to achieve better performances in heterogeneous multi-modal recommendation [11,13,38], debiased recommendation [14,48], reliable and explainable recommendation [6, 43], etc. Among all, cross-domain recommendation [10, 20, 46] and cold-start recommendation [7,30,34] problems draw a lot of attention. Cross-domain recommendation systems aim to transfer knowledge available in other domains (known as the source domain) to the target domain where users have much sparser feedback or interactions. Overlapped users are often selected to learn the mapping of interest between two domains so that same pattern can be applied to those cold-start users of target domain [20,46]. On the other hand, cold-start recommendation is often referred to cold-start content recommendation where no or few user feedback can be found when new items arrive. It is challenging to recommend such items to users due to the unavailability of the statistical information which RS relies heavily on. Common solutions extract and analyze content information of items to match usersâ€™ potential interests based on their historical behaviors [7, 13]. Previous works normally treat cold-start user and cold-start content recommendation as two separate scenarios. However, when a new domain evolves at its early stage in real-world, it may suî€er both issues at the same time. For example, Taobao is the largest e-commerce platform in China where billions of products are interacted with hundreds of millions of users each day. When microvideos were î€›rst served to the platform, they shared the same set of users as product domain but had few (or none) user interactions. How to perform reliable recommendation on such emerging new contents appears to be one of the most challenging problems for RS. A recent work [44] proposes an internal contextual attention network to deal with the cross-domain cold-start recommendation on contents with very few interactions. However, this approach has limited ability to model brand new contents which are never seen before. In this paper, we further extend the work [44] and provide a uniî€›ed Cross-dOmain User Preference LEarning (COUPLE) framework where items with limited interactions can be reliably recommended. COUPLE diî€ers from previous cross-domain works in that we jointly model all usersâ€™ behaviors across domains instead of using just the overlapped users, as shown in Figure.1. With historical feedback modeled, a userâ€™s preference across domains is obtained from content perspective and user group perspective. When a new item comes, we push it to the corresponding users with matched interest based on its content feature and the userâ€™s general preference learnt by the system. Main contributions of our proposed work are summarized as follows: â€¢We propose a uniî€›ed user preference learning framework to solve cross-domain cold-start problem, which delicately models a userâ€™s history preference, content preference and group preference. With proper choice of a First-In-First-Out (FIFO) queue, the whole framework can be self-trained in an eî€œcient contrastive way. â€¢ With semantic tags extracted for item representation, a frequency encoder based on cross-layer attention is utilized. It further boosts the interactions between high-level representation with low-level content source input. To the best of our knowledge, it is the pioneering work to solve the crossdomain cold-start issue with the use of tag information. â€¢A hierarchical memory tree with orthogonality property is proposed to learn usersâ€™ preference across domains. It can generalize userâ€™s diverse interest with topğ¾leaf-node representations based on his/her historical behaviors. This is proved to be a good way of user preference generalization in cross-domain scenario. The rest of the paper is organized as follows. In Section 2, we review the related work. Section 3 presents the details of our proposed method. Oî€Ÿine experimental results and ablation study are presented in Section 4. We introduce the online deployment of the system in Section 5 and conclude our work in Section 6. In this section, we introduce the related work on personalized recommendation system, user preference learning, and the orthogonal regularization mechanism we use in the paper. A standard matching (i.e. candidate generation) stage of a personalized recommender system aims to retrieve a small subset of items from the huge pool for the sophisticated ranking scheme. Previous methods typically employ collaborative î€›ltering (CF) strategies to exploit user preferences from their explicit or implicit feedback [19,22]. These methods achieve good performance where user-item interactions are dense but are not able to handle the cold-start situations where the interactions are limited. Content-based methods alleviate this problem by mining the content features of items or meta information about users [11â€“13,16,37]. Among them, tags are proved to be useful [12,16], especially for cross-domain recommendation where semantically similar tags may exist in both domains and bridge the gap to some extent. Despite their success, both types of the methods treat user-item interactions in a static way and cannot capture the dynamic interests of users over time. As a result, sequential recommender systems are proposed to model the sequential nature of usersâ€™ interactions [5,32,35]. Besides, attention mechanism has been widely adapted to recommender systems since its out-breaking success in NLP [9,33]. It is leveraged between users and contents to capture the most representative information or provide good interpretability for recommendation [3, 27, 29]. In real-world applications, hybrid recommendation framework is usually applied which makes up for the shortcomings of using a single method and maximizes the advantages. Our work is typically within the hybrid system for cross-domain cold-start recommendation, where all strategies mentioned above are involved. The core of a recommender system is to model the dynamic user preferences. Previous works usually represent a userâ€™s historical behavior with one single latent vector which may suî€er from correlation loss [5]. Clustering-based approaches oî€er an alternative to traditional model-based methods and cluster similar users or items together [31,40]. However, these methods rely heavily on manual selection of a userâ€™s attribute and proî€›le, or treat diî€erent domains separately. Recently, recommendations with memory network [5,49] are proposed to model user preference and store the long term interest. The basic idea is to maintain a key-value style memory matrix with numbers of slots. A userâ€™s embedding is fed into the matrix where similarity between the user and each key vector is computed and converted to relevance probability using softmax function. In our work, a hierarchical memory tree is utilized for user preference learning. Diî€erent from previous memory-based network, our tree structure has a much larger number of memory slots (i.e. few hundred or thousand vs. few dozens). What is more, we only pick the top memory slots each time with calculated correlation score instead of using all the memory slots. The orthogonality implies energy preservation and is proved to be eî€œcient for stabilizing the distribution of activations over layers with CNNs. Orthogonal regularizers are extended to fullyconnected layers and a Spectral Restricted Isometry Property (SRIP) regularizer is proposed to guarantee better convergence [2]. Later on, more investigations [4,26] further extend SRIP regularization bounds and bring compatible results on applications like image classiî€›cation and Person Re-Identiî€›cation. We also apply such orthogonal regularization scheme for the user group preference learning, which will be illustrated in detail in Section 3. In this section, we present the problem formulation of cross-domain cold-start recommendation î€›rst. Then we explain in detail how we model user preference from three diî€erent aspects in COUPLE and train the framework eî€œciently in a contrastive way. The overall design of our proposed COUPLE network is shown in Figure 2. 3.1.1 Problem Formulation. Cross-domain recommendation in realworld may involve several channels [44]. Without loss of generality, we classify those channels into source domain(s)Aand target domain(s)B, where user interactions are much denser inAcompared toB. We denoteU,Uas the sets of users andI,Ias the sets of interacted items with content features (semantic tags in our case) respectively. We also have a cold-start item set in target domain denoted asIwhere items are with limited (or none) interactions. For the cross-domain cold-start recommendation, we have a) user overlapUâˆ© Uâ‰  âˆ…; b) item overlapIâˆ© I= âˆ…; c) cold-start item overlapIâˆ© I= âˆ…. We deî€›ne two tasks with regard to the training and inference processes of COUPLE, respectively: â€¢ Joint recommendation for training, i.e., make recommendation on items in Iâˆª Ito users Uâˆª U. â€¢ Cold-start recommendation for inference, i.e., make recommendation on items in Ito users Uâˆª U. It is noted that our work can be easily extended to recommendation on items with limited interactions as well. Notations are summarized in Table 1. ğ’†, ğ’†, ğ’†âˆˆ Ruser, item, tag embedding feature ğ’†, ğ’†, ğ’†âˆˆ Ruser history, content, group preference ğ’”âˆˆ Rtree node vector at position ğ‘— in layer ğ‘– As illustrated in Figure 2, we model a userâ€™s preference from three interactive stages. First, userâ€™s history preference is modeled with a domain-aware multi-head attention scheme for sequential behaviors from diî€erent domains. Then it will attend to the underlying tag-level features to encode tag importance for user content preference. A hierarchical memory tree is used to represent userâ€™s historical preference with â€œorthogonal" leaf-node vectors. Finally, user preferences from three aspects are weighted aggregated. 3.2.1 User History Preference. We model a userâ€™s historical behavior using a deep sequential model, where item-level attention scheme is applied. In standard sequential models, a sequence of item index is often given for embedding lookup [27,35]. However, in cold-start content recommendation, items may be never seen or interacted before. To solve this issue, we use the global semantic tags to represent each item. To be more speciî€›c, we obtain a cross-domain tag setTfrom items across domainsIâˆª I. The tags can be extracted from hashtag made by users, item attributes or genres, or from images and texts pre-processed by vision and language models. A fair assumption can be made that the tag setT is able to cover most of the items in Ifrom the target domain. Our input for each item is a group of tags[ğ‘¡, ğ‘¡, . . . , ğ‘¡], where ğ‘¡âˆˆ {1,2, . . . , ğ‘ }is the index of the tag in the whole setTwith sizeğ‘. A tag embedding tableHâˆˆ Rtakes tag index sequence as the input and outputs the representation of the tag features [ğ’†, ğ’†, . . . , ğ’†], whereğ’†âˆˆ R. In this way, we represent an itemğ’†âˆˆ Rby aggregating the tag features by average pooling: whereğ‘›is the number of tag features capped for each item. Note that the index-based embedding lookup scheme can be easily extended to using multi-modal embeddings from any pre-trained models. To explicitly involve the domain-speciî€›c knowledge, we follow the design in Bert [9] and add a domain embedding tableHâˆˆ R(ğ‘„is the number of channels) to the current item feature embedding, for training. For inference and online serving, we only use the tag features for item embedding, as suggested by the recent recommendation work [48]. To model user history preference, we apply a multi-head attention (MHAttn) mechanism [27] to the sequence of temporally sorted items[ğ’†, ğ’†, . . . , ğ’†]clicked (rated) by users. Self-attention is applied to aggregate all item embeddings with adaptive weights and followed by two-layer feed-forward networks (FFN) to increase non-linearity. whereğ‘™is the max sequence length andğ‘šis the number of attention heads. Furthermore, we perform weighted aggregation (WA) to obtain user history preferenceğ’†on the output representations {ğ’†}of Eq. (2) as: Figure 2: Framework of our proposed method. Three levels of user preferences are modeled, including user history, user content and user group. The whole framework updates in a contrastive way with a Fisrt-In-First-Out (FIFO) queue, where a large number of negative samples from latest steps is maintained for more consistant training. with a parameter matrixM âˆˆ Rlearnt and updated, the following procedure is performed: 3.2.2 User Content Preference. User history feature obtained from Section 3.2.1 focuses on the item-level attention, where each tag contributes equally for an item representation. However, same tags appearing multiple times in the item sequence are supposed to contribute more to a userâ€™s interest. The similar conclusion on the importance of item frequency for next-bucket recommendation has been made recently in [21]. As a result, we propose a frequency encoder which emphasizes the appearances of the same tags. It is implemented by a cross-layer attention process. We concatenate all the tag embeddings in the item sequence together as{ğ’†}=[ğ’†, ğ’†, . . . , ğ’†], and let them go through a one-layer MLP with a hyperbolic tangent function. This results in a hidden representationğ’‰. Then the softmax function is applied betweenğ’‰and the userâ€™s history representationğ’†to obtain the attention weights. The user content representationğ’†is computed as the weighted sum of attention weights and the tag features: In this way, the same tag with multiple attention weights will be augmented in the î€›nal representation of user content preference. A simple illustration of how this frequency encoder works is shown in Figure.3. From the design point of view, the user history representation is modeled based on a bottom-up attention while the user content preference is in a top-down fashion. Such cross-layer attention is proved to be critical to boost interactions between the high-level representation with the low-level content source [1,33]. 3.2.3 User Group Preference. Unlike previous cross-domain works where only the overlapped users are considered, we take into account all the usersâ€™ behavior patterns and try to î€›nd out a uniform representation for user group preference across domains. Consequently, a hierarchical memory tree is designed to remember a userâ€™s general interest and automatically classify it into groups with closest interests. It diî€ers from the original memory networks in that : a) we are able to have more memory slots as proved in the recent work [41] (i.e. few hundreds or thousands vs. few dozens); and b) we donâ€™t use all slots as memory networks do, instead we pick the top ğ¾ leaf-node slots each time with highest scores. Figure 3: Tag-level Frequency Encoder. Representation of the same tag will be aggregated and augmented after usertag attention scheme (best viewed in color). Tree Structure Design.With user history representation from equation(4), we follow the similar procedure as [41] to use a hierarchically fully-connected tree where the root node allocates weights to children nodes and î€›nally leaf nodes. To enforce further interpretability, the sum of the node weights at the same layer is equal to 1 and the sum of children node weights inherited from the same parent node is equal to the parent node weight. Given user history representation ğ’†and a parent node at position ğ‘— in layer ğ‘– with â„children nodes, whose weight is denoted asğ‘¤, the weights of the children nodes in layerğ‘– +1 can be calculated using softmax function: ğ‘¤= ğ‘¤Â· soî‚‰max(ğ’†Â· ğ’”) where ğ’”is the memory slot vector at position ğ‘˜ of layer ğ‘– + 1. Dead-node Solution.During the training process of retrieving topğ¾leaf-node vectors, we met the same problem as [41] where almost the same set of nodes where visited at each step. The paper dealt with this problem by uniformly choosingğ¾random candidates each time thus solving the dead-node problem at the early stage of training. However, as the model parameters update along time, real top-ğ¾ node vectors are desired to be retrieved. In our work, we use Gumbel-Softmax [25] which is an ideal sampling trick for distribution on discrete vectors (also known as reparameterization trick). The basic idea of Gumbel-Softmax is to add a Gumbel noise and temperature factor to the original softmax function so that it is able to approximate a probability distribution made up of discrete categories (which are the normalized leaf-node weights in our case). With calculated leaf-node weights (i.e. probabilities)ğ‘¤, ğ‘¤, . . . , ğ‘¤ from softmax function using Eq.(6), the Gumbel-Softmax is conducted as: ğ‘¦= soî‚‰max((log(ğ‘¤) + ğ‘”)/ğœ) whereğ‘”, ğ‘”, . . . , ğ‘”are i.i.d samples drawn from the standard Gumbel distribution havingğœ‡andğ›½as 0 and 1 respectively, with PDF (Probability Density Function) ofğ‘’. In practice,ğ‘”can be sampled using inverse transform sampling by drawingğ‘¢âˆ¼ uniform(0, 1) and calculated as ğ‘”= âˆ’ log(âˆ’ log(ğ‘¢)) [36]. One of the great property of the Gumbel-Softmax is that the output value approaches the real distribution with low temperature factorğœand tends to be uniform sampling with largerğœ. As a result, we start the training with a big temperature and then anneal it towards small values. In this way, the model tends to explore random nodes for update in the beginning and gradually stick to the real top ğ¾ selection for a better convergence. For inference, the Topğ¾leaf-node vectors with highest weight scores are used to form the user group representation ğ’†: Orthogonality. To make the nodes at each layer more diverse and representative, orthogonality regularizer is applied to the fullyconnected layers W in the tree: whereğœ†is the penalty parameter andğœ (W) = sup is the spectral norm ofW, i.e. the largest singular value ofW. Though computation of Eq.(9)involves expensive eigen-decomposition, it can be approximated via power iteration method [2]. Starting with a random initialized vectorğ’— âˆˆ R, we iteratively perform the following procedures a small number of times (2 by default): ğ’– â† (WW âˆ’ I)ğ’—, ğ’— â† (Wğ‘¾ âˆ’ I)ğ’–, ğœ (WW âˆ’ I) â†âˆ¥ğ’— âˆ¥âˆ¥ğ’– âˆ¥. (10) Our hierarchical tree-based memory module is similar to previous work [41] in design. However, the main purpose and realization is quite diî€erent: â€¢The PreHash module proposed in [41] is designed to solve user embedding problem where certain anchor vectors are manually selected and kept unchanged during training. In our work, the memory tree is designed for user group classiî€›cation and is fully automatic with parameters updating. All the user behavior representations will go through this module and fall into most related leaf slots. â€¢Orthogonality property is applied to our memory tree as it is a strong regularization for the diverse representation and ensures better convergence of our tree module while PreHash in [41] doesnâ€™t have this for model update. â€¢Sampling strategy during training is also diî€erent. Prehash uses the uniform sampling throughout training while the Gumbel-softmax sampling applied in COUPLE is more eî€œcient for node representation update. After obtaining user history representation (in Section 3.2.1), user content representation (in Section 3.2.2) and user group representation (in Section 3.2.3), we apply weighted aggregation (WA) again to get the î€›nal user representation ğ’†using Eq. (3): Unsupervised representation learning in a contrastive way is highly successful in recently research [9,17,39]. Following a standard contrastive deî€›nition [15], we deî€›ne a contrastive loss between our user representaion ğ’†and item representation ğ’†,ğ’†,...,ğ’†: L= âˆ’E[logexp(ğ’†Â· ğ’†/ğœ”)exp(ğ’†Â· ğ’†/ğœ”) +Ãexp(ğ’†Â· ğ’†/ğœ”)], (12) whereğœ”is a temperature hyper-parameter. The contrasitve loss has a low value whenğ’†is similar to its positive item sampleğ’† and dissimilar to all other negative item samples {ğ’†}. In earlier study, softmax-based classiî€›er is often utilized to classify between positive and negative samples. In [17], an eî€œcient momentum contrastive scheme is proposed to maintain the dictionary as a queue of data samples. In this way, a rich set of negative samples can be involved for training and achieve positive results in various computer vision tasks. In our framework, a First-In-FirstOut (FIFO) quque is maintained and updated with the current batch enqueued and the oldest batch dequeued for each training step. So that each batch of positive samples can be encoded with the hard negative samples over the latest steps. It makes the loss tracking more consistent and has a debiasing eî€ect in large-scale production environment as proved in [48]. Thus, our network is trained under the loss functionL = L+ Lconsisting of a contrastive loss and a orthogonal constraint on weights of memory tree. In this section, we evaluate our proposed cross-domain user preference learning (COUPLE) model for top-ğ¾retrieval task under diî€erent scenarios. We introduce the experimental setup and the comparing baselines î€›rst, and present the performance comparison with other state-of-the-arts. Then the ablation study is carried out both quantitatively and qualitatively. 4.1.1 Dataset Overview. We compare the performance of our proposed framework with related methods for cross-domain cold-start recommendation on two publicly accessible datasets. Amazon review datasetis one of the most widely-used public dataset [18] for e-commerce recommendations. We use subsets of Books as the source domain while Movies and TV the target domain. The other dataset is called Tao-Product-Micro-Video (TPMV), selected from the real-world online service of Taobao Recommendation. User interactions are much denser in product domain than micro-video domain, making the dataset suitable for cross-domain cold-start recommendation. The detailed statistics of the two datasets are demonstrated in Table 2. 4.1.2 Experimental Setup. To compose valid training and testing datasets for cross-domain cold-start recommendation, we use "click" as the implicit feedback for TPMV dataset and "review rating" for Amazon dataset. For training and evaluation, clicked items of TPMV and review ratings over 3 (ratings range from 1 to 5) of Amazon are regarded as positive samples and temporally sorted with timestamps of a userâ€™s action. For testing, we use leave-one-out strategy and make the last-position items from target domain as the groundtruth, while erasing these items from the training pool to make sure they are totally cold-start contents. To further make the situation tougher as the real circumstances, we randomly select half of the testing users, and erase all the items of the target domain (if any) from their historical sequences to make them cold-start users. For all the models compared in this paper, the detailed numbers of training and testing samples are listed in Table 3. 4.1.3 Evaluation metrics. To evaluate the performance of all the methods, Hit Ratio (HR), and Normalized Discounted cumulative gain (NDCG) are used. HR measures whether the positive item retrieved within the top-ğ¾and NDCG penalizes the score if positive item positioned lower in the ranking list. Higher values in these metrics indicate better recommendation performance. For each ground-truth positive item, we pair it with 100 randomly sampled negative items from the pool where users have no historical interactions with, following the prior work [19,23]. To make the comparison more reliable, 50 of them are sampled randomly, while the other 50 items are sampled according to the popularity [23]. We compare our proposed framework with six baselines with neural deep learning structures, including DSSMs, Youtube DNNs and the Sequential methods. â€¢ DSSMs. The original Deep Semantic Similarity Model (DSSM) [24] serves as a strong baseline widely used in information retrieval, which can perform semantic matching between a query and a document. DeepCoNN [47] further extends DSSM to two parallel convolutional neural networks. One of them models user behaviors and the other models item properties from the review texts. It achieves a strong performance in content recommendation. â€¢ Youtube DNNs. Youtube [8] is a classical deep-based matching model for building user and item embeddings collaboratively. Layers of depth on the top is proved eî€ective to model non-linear interactions between features. The latest work [44] solves multi-channel cold-start issue based on Youtube DNN structure where SOTA result is achieved. â€¢ Sequential methods. SASRec [27] encodes userâ€™s behavior based on a variant of Transformer and is used as the backbone for many sequential models including ours. And we also compared to the latest sequential method ComiRec Table 4: HitRate and NDCG of diî€erent methods on the two datasets, where best performance is in boldface. HP denotes hyperparameters, including ğ‘› the number of tags and ğ‘™ the item length for se quential modeling. [3] which further improves for controllable multi-interest matching based on the previous MIND [29]. For methods mentioned above, we follow their implementations with tuned parameters for better comparison. Original DSSM [24] is implemented by maximizing the conditional likelihood of clicked items given only a closest query (item in our case). For DeepCoNN [47], it models similarity between user and item representations. We feed in a sequence of items represented by tag features where convolution is applied. In terms of sequential methods, we î€›rst average tag representations for an item as our method does. Then item sequence is modeled the same way as the original papers, where position embedding is added. To implement ICAN [44], we adjust the ID input feature with tag embeddings to î€›t in the content cold-start scenario, while keeping all other settings unchanged. The overall performance of all the methods on both datasets is summarized in Table 4. We can observe that our proposed COUPLE network, which models user preference from diî€erent perspectives, consistently yields the best performance in terms of HitRate@N and NDCG@N (N = 5, 10). We would like to note that, among three diî€erent types of baselines, Youtube DNNs and Sequential methods obtain better performance than DSSMs. It implies that suî€œcient interactions between user and item features are crucial for content-based recommendation. Within two-towel models, DeepCoNN gains apparent improvement over original DSSM by modeling series of items for user presentation instead of only one item. By adding channel-wise attention between source and target domain, ICAN is able to achieve better performance over original Youtube network. The same improvement can be found on ComiRec over simple sequentially self-attention method SASRec, where multi interests of users are learnt across domains. Furthermore, we observe that the improvement of our method over the strongest baselines is more signiî€›cant on TPMV data (7.44% vs. 2.19% on HitRate@10) and the performances of all the methods on TPMV Dataset are generally better than those on Amazon Dataset. The reasons may be that: a) TPMV data have more overlapped tags in target domain with source domain than Amazon (60% vs. 25%), so that models can learn a better correlation between domains; and b) user behaviors are more consistent on TPMV data which is collected from a range of a few days compared to Amazon with a wider range of months (over 4 years). Both attributes of TPMV favors our model where user-content attention and usergroup preference learning are performed to generalize and transfer the interests across domains. We perform ablation study to further explore the eî€ect of individual components of our COUPLE model on performance. As shown in Table 5, our base model with user history and domain embeddings achieves similar performance with SASRec. An apparent improvement is observed by applying user group preference and user content preference learning modules. The variant with hierarchical memory tree outstands others and raise the HitRate@10 by a large margin of 3.88%, further illustrating the eî€ectiveness of the generalizing user preference across domains. After combining all modules together to represent the user, the performance is further improved. We also experiment with the batch negative sampling strategy compared to the contrastive learning using a FIFO queue. It can be seen that with a larger number of negative samples, model is able to distinguish data with higher eî€œciency. With all modules put together, our proposed model achieve a competitive performance for the real-world cold-start recommendation on Taobao platform. One critical assumption of our work is that it helps with the crossdomain recommendation by joint modeling user behaviors from both source and target domains. In this section, We conduct experiments on two datasets the sensitivity of our model against the amount of source domain data. By manipulating the amount of training data, we gradually decrease the portion of data from the source domain. As part of the testing cases is cold-user recommendation where user behaviors only exist in the source domain, we further disassemble HR@10 and NDCG@10 for diî€erent situations. Table 5: Ablation Study on TPMV Dataset. Figure 4: The impact of the source domain data. Figure 4 shows the impact of source domain data on the performance. We can see a clear drop on both HitRate and NDCG as the percentage of training data from the source domain decreases. Recommending cold items to cold users are more diî€œcult as the overall performance is worse than recommending to users with interactions in target domain already. What is more, with less data from the source domain, the performance on cold-user recommendation decreases more than the warm-user situations with increasing gap between two lines. An interesting î€›nding is that cold-user recommendation on Amazon data is more sensitive to the varying source data than that on TPMV data. We believe that it is related to the correlations between tags extracted from both domains. With less overlapping tags, it becomes more diî€œcult to match interest between domains with progressively sparser source triggers. A good orthogonal representation tree should be able to project and classify users into distinct groups. To further verify whether orthogonal regularization works, We visualize the user group representations with t-distributed stochastic neighbor embedding (t-SNE) [42]. First, we obtain the memory slot vector of leaf nodes{ğ’” } via fully-connected projection of user history representationğ’†. Then the leaf node groupğ‘˜with largest weight calculated using Eq.(6)is assigned to the user. Following t-SNE, We treat the each component ofğ’”as an individual point and keep only the two components that have the highest conî€›dence levels. For better illustration, we randomly sample 10 user groups with 10,000 points within each and the result is shown in Figure 5. It is obvious that the clustering result with orthogonal regularization has higher similarity within a cluster and can separate diî€erent clusters better. When node vectors approach orthogonal, they become de-correlated so that the responses are much less redundant. By applying orthogonality constraints together with Gumbel-Softmax sampling trick, we observe a stable convergence and layer-wise distribution of the hierarchical memory tree. Figure 5: t-SNE of clustering samples of the memory tree. We conduct online experiments by deploying our work to guessyou-like recommendation after purchase at Taobao platform, where brand new micro-videos with extracted tags are displayed to users with related interest. This scenario has over dozens of millions of active users and hundreds of thousands of new micro-movies uploaded each day. For comparison, all the matching methods share the same ranking stage, so that click-through-rate (CTR) can be a fair metric and is used for evaluation. We compare our work with two baselines that are already running online, one is author-based CF where new videos created by the same author will be retrieved and the other is Youtube DNN where tag features are fed into the network for similarity measurement. Users assigned to the experiment group experience COUPLE recommendation, while the other two control groups experience author-CF and Youtube DNN, respectively. We run the A/B test in the heavy-traî€œc scenario for a week. The CTR of the experiment group with COUPLE improves over the control group by8.855%. Further compared to author-CF, COUPLE has a lower CTR rate (-5.6%) but provides signiî€›cant gain on exposure rate by58.91%. And It illustrates that with contentbased recommendation scheme, more cold micro-videos get the opportunity to be shown to users which contributes to the sustainable development of recommender systems. In general COUPLE + author-CF improve CTR over Youtube + author-CF by3.15%. We replace Youtube DNN with COUPLE and it is now serving online for the cross-domain cold micro-video recommendation. In this paper, we proposed a uniî€›ed framework for cross-domain cold-start recommendation in real world. It learns a userâ€™s preference from three diî€erent perspectives including user history, user content and user group. By jointly modeling the userâ€™s behaviors with extracted semantic tags across domains, our work is able to perform reliable recommendation in a new domain with limit user interactions. Fed into a contrastive learning scheme, COUPLE achieve state-of-the-art results on two datasets and demonstrates superior performance compared to other baselines running at Taobao APP. It has been deployed in production at scale for cold microvideo recommendation. For the future, we plan to integrate more heterogeneous content features into our framework to further help with the cold-start recommendation. Another exciting direction is to incorporate knowledge graph for information propagation towards explainable recommendation.