Deep learning based models have dominated the current landscape of production recommender systems. Furthermore, recent years have witnessed an exponential growth of the model scaleâ€”from Googleâ€™s 2016 model with 1 billion parameters to the latest Facebookâ€™s model with 12 trillion parameters. Signiî€›cant quality boost has come with each jump of the model capacity, which makes us believe the era of 100 trillion parameters is around the corner. However, the training of such models is challenging even within industrial scale data centers. This diî€œculty is inherited from the staggering heterogeneity of the training computationâ€”the modelâ€™s embedding layer could include more than99.99%of the total model size, which is extremely memory-intensive; while the rest neural network is increasingly computation-intensive. To support the training of such huge models, an eî€œcient distributed training system is in urgent need. In this paper, we resolve this challenge by careful co-design of both the optimization algorithm and the distributed system architecture. Speciî€›cally, in order to ensure both the training eî€œciency and the training accuracy, we design a novel hybrid training algorithm, where the embedding layer and the dense neural network are handled by diî€erent synchronization mechanisms; then we build a system called Persia (short forparallel recommendation trainingsystem with hybridacceleration) to support this hybrid training algorithm. Both theoretical demonstrations and empirical studies up to 100 trillion parameters have been conducted to justiî€›ed the system design and implementation of Persia. We make Persia publicly available (at https://github.com/PersiaML/Persia) so that anyone would be able to easily train a recommender model at the scale of 100 trillion parameters. A recommender system is an important component of Internet services today. Tasks such as click-through rate (CTR) and buythrough rate (BTR) predictions are widely adopted in industrial applications, inî€uencing the ad revenues at billions of dollar level for search engines such as Google, Bing and Baidu [78]. Moreover, 80%of movies watched on Netî€ix [30] and60%of videos clicked on YouTube [25] are driven by automatic recommendations; over 40%of user engagement on Pinterest are powered by its Related Pins recommendation module [58]; over half of the Instagram community has visited recommendation based Instagram Explore to discover new content relevant to their interests [12]; up to35%of Amazonâ€™s revenue is driven by recommender systems [18,104]. At # of Recommender Model Parameters (by Trillion) 100 0.001 2016 2017 2018 2019 2020 2021 2022 Figure 1: Model sizes of diî€erent recommender systems, among which only XDL and AIBox (via PaddlePaddle) are open-source. Persia is an open-source training system for deep learning-based recommender systems, which scales up models to the scale of 100 trillion parameters. Kwai, we also observe that recommendation plays an important role for video sharingâ€”more than 300 million of daily active users explore videos selected by recommender systems from billions of candidates. Racing towards 100 trillion parameters.The continuing advancement of modern recommender models is often driven by the ever increasing model sizesâ€”from Googleâ€™s 2016 model with 1 billion parameters [24] to Facebookâ€™s latest model (2022) with 12 trillion parameters [62] (See Figure 1). Every jump in the model capacity has been bringing in signiî€›cantly improvement on quality, and the era of 100 trillion parameters is just around the corner. Interestingly, the increasing parameter comes mostly from the embedding layer which maps each entrance of an ID type feature (such as an user ID [50,83] and a session ID [79,85,86]) into a î€›xed length low-dimensional embedding vector. Consider the billion scale of entrances for the ID type features in a production recommender system (e.g., [28,89]) and the wide utilization of feature crosses [23], the embedding layer usually domains the parameter space, which makes this component extremely memoryintensive. On the other hand, these low-dimensional embedding vectors are concatenated with diversiî€›ed Non-ID type features (e.g., image [95,98], audio [87,96], video [20,46], social network [27,33], etc.) to feed a group of increasingly sophisticated neural networks (e.g., convolution, LSTM, multi-head attention) for prediction(s) [24, 63,107,116,121,122]. Furthermore, in practice, multiple objectives can also be combined and optimized simultaneously for multiple tasks [48,60,93,94,118]. These mechanisms make the rest neural network increasingly computation-intensive. Challenges.This paper is motivated by the challenges that we were facing when applying existing systems, e.g., XDL from Alibaba [38] and PaddlePaddle from Baidu [9], to the training of models at the 100 trillion parameter scale. This is a challenging taskâ€”as illustrated in Figure 1, most open source systems were only designed for a scale that is at least one order of magnitude smaller; even the largest proprietary systems such as the one reported by Facebook [62] just two month ago (September 2021) are still8.3Ã— smaller. The goal of this paper is to enable, to our best knowledge, the î€›rst open source system that is able to scale in this regime. As 100 trillion parameters require at least 200TB to simply store the model (even infp16), a distributed training system at this scale often consists of hundreds of machines. As so, the communication among workers is often a system bottleneck. While state-of-the-art systems [38,45,63,66,70,100,114,115] often employ a carefully designed heterogeneous architecture (e.g., CPU/GPU, DRAM/SSD) to accommodate the heterogeneity in a recommender model (as illustrated in Figure 2), all of them are using a homogeneous training algorithm (either synchronous or asynchronous stochastic gradient based algorithms), for the model as a whole. At the scale of 100 trillion parameters and hundreds of workers, this homogeneity in the training algorithm starts to become a signiî€›cant problem: (1) Synchronous algorithms always use the up-to-date gradient to update the model to ensure the model accuracy. However, the overhead of communications for synchronous algorithms starts to become too expensive to scale out the training procedure, causing ineî€œciency in running time. (2) While asynchronous algorithm have better hardware eî€œciency, it often leads to a â€œsigniî€›cantâ€ loss in model accuracy at this scaleâ€”for production recommender systems (e.g., Baiduâ€™s search engine [114]). Recall that even 0.1% drop of accuracy would lead to a noticeable loss in revenueâ€”this is also consistent with our observation at Kwai. Motivated by these challenges, we ask: Q1. Can we design an algorithm that can take beneî€›ts from both synchronous and asynchronous updates avoiding their disadvantages, to further scale up a recommender system with 100 trillion parameters? Q2. How can we design, optimize, and implement a system to eî€œciently support such an algorithm? Persia.In this paper, we describe Persia, an open source distributed training system developed at Kwai to support models at the scale of 100 trillion parameters. As we will show in Section 6, this is only possible given its signiî€›cant speed-up over state-ofthe-art systems including both XDL and PaddlePaddle, achieved by the careful co-design of both the training algorithm and the training system.Open Source and Reproducibility:We also believe that the ability to train such a large model should be made easily and widely available to everyone instead of just being locked in the hands of a handful of largest companies. We make Persia open Figure 2: An example of a recommender models with 100+ trillions of parameter in the embedding layer and 50+ TFLOP computation in the neural network. source so that everyone who has access to cloud computing service, e.g., Google cloud platform (where we perform Persiaâ€™s capacity test) can easily setup a distributed training system to reproduce our 100-trillion-parameter model test and to train their own models at this scale. Contributions.Persia is enabled by a set of technical contributions. The core technical hypothesis of Persia is that, by using a hybrid and heterogeneous training algorithm, together with a heterogeneous system architecture design, we can further improve the performance of training recommender systems over state-of-the-arts. Ourî€›rst contributionis a natural, but novel hybrid training algorithm to tackle the embedding layer and dense neural network modules diî€erentlyâ€”the embedding layer is trained in an asynchronous fashion to improve the throughput of training samples, while the rest neural network is trained in a synchronous fashion to preserve the statistical eî€œciency. We also provide a rigorous theoretical analysis on its convergence behavior and further connect the characteristics of a recommender model to its convergence to justify its eî€ectiveness. This hybrid algorithm requires us to revisit some decisions with respect to the system architecture and optimizations, to unleash its full potential. Oursecond contributionis on the system side. We design a distributed system to manage the hybrid computation resources (CPUs and GPUs) to optimize the co-existence of asynchronicity and synchronicity in the training algorithm. We further implement a wide range of system optimizations for memory management and communication optimizationâ€”such system optimizations are the key to fully unleash the potential of our hybrid training algorithm. We also develop a fault tolerance strategy to handle various potential failures during the training procedure. Last but not least,we evaluate Persia using both publicly available benchmark tasks and real-world tasks at Kwai. We show that Persia scales out eî€ectively and leads to up to7.12Ã—speedup compared to alternative state-of-the-art approaches [9,38]. We further conduct larger-scale scalability experiments up to 100 trillion parameters on public clouds to ensure public reproducibility. Overview.The rest of the paper is organized as follows. We î€›rst provide some preliminaries for deep learning recommender systems in Section 2. We introduce our hybrid training algorithm in Section 3 and discuss Persia system design and implementation in Section 4. We show the theoretical analysis of the hybrid algorithm in Section 5, present the experimental results in Section 6, summarize related work in Section 7, and conclude in Section 8. This section introduces the basic principle of a typical deep learning based recommendation system (e.g., DLRM [62]) illustrated in Figure 2. We î€›rst formalize the distributed recommender training problem; and then give an anatomy of the existing architectures. A typical recommender system takes the training sample in the whereğœ‰denotes the index of the sample in the whole dataset.x:= {ğ‘¥, ğ‘¥, . . .}is the collection of ID type features in the sample,î€Œî€Œ î€Œî€Œxî€Œî€Œdenotes the number of IDs,xdenotes the Non-ID type features, andydenotes the label. The ID type feature is the sparse encoding of large-scale categorical information. For example, one may use a group of unique integers to record the microvideos (e.g., noted asâŸ¨VideoIDsâŸ©) that have been viewed by a user; similar ID type features may include location (âŸ¨LocIDsâŸ©), relevant topics (âŸ¨TopicIDsâŸ©), followed video bloggers (âŸ¨BloggerIDsâŸ©), etc. In our formalization,xis the collection of all ID type featuresâ€”for the above example, it can be considered as: x:=[âŸ¨VideoIDsâŸ©, âŸ¨LocIDsâŸ©, âŸ¨TopicIDsâŸ©, âŸ¨BloggerIDsâŸ©, ...] The Non-ID type featurexcan include various visual or audio features. And the labelymay include one or multiple value(s) corresponding to one or multiple recommendation task(s). The parameterwof the recommender system usually has two components: wherewâˆˆ Ris the parameter of the embedding layer and wâˆˆ Ris the parameter of the rest dense neural network. Weî€î€‘ use:lookupxto denote the concatenation of all embedding vectors that has correspondence inx;NN(Â·)to denote a function parameterized bywimplemented by a deep neural network that takes the looked up embeddings and Non-ID features as input and generates the prediction. In the remaining part of this paper, â€œNNâ€ is short for â€œneural networkâ€. The recommender system predicts one or multiple values ^y by: It is worth mentioning that while thewinvolved computation can be10Ã—more than thewinvolved computation, the size of wcan be10Ã—larger than that ofw, especially whenw contains many cross features. The imbalance between model scale and model computation intensity is one of the fundamental reasons why the recommender system needs an elaborate training system other than a general purpose deep learning system. Formally, the training system essentially solves the following optimization if we useLto denote some loss function over the prediction and the true label(s) y, ğ¹ can be materialized as: ğ¹ (w; ğœ‰) := LNNlookupx, x, y. Lastly, we use to denote the gradients of wand wrespectively. Existing distributed systems for deep learning based recommender models are usually built on top of the parameter server (PS) framework [47], where one can add elastic distributed storage to hold the increasingly large amount of parameters of the embedding layer. On the other hand, the computation workload does not scale linearly with the increasing parameter scale of the embedding layerâ€”in fact, with an eî€œcient implementation, a lookup operation over a larger embedding table would introduce almost no additional computations. Thus, a fundamental goal of these systems is to resolve the inconsistency between the computation-intensive neural network and the memory-intensive embedding layer. One natural solution is to deploy the training task over a hybrid infrastructure as an extension of the original PS framework: the large number of embedding parameters can be sharded across multiple CPU PS nodes for storage and update, whilst the intensive FLOP computation can be assigned to GPU nodes. Thus, the optimization in these systems focuses on avoiding frequent communications of the memory-intensive embedding layer. Towards this end, diî€erent designs have been proposed. For example, Alibaba proposes XDL [38] that moves the computation of the embedding layer from the GPU worker to the CPU PS node so that only the embedding outputs (instead of all parameters) are transformed to the GPU workers along with the neural network parameters; Baidu designs a hierarchical PS framework [59] using a hierarchical storage to cache the frequently used parameter close to the GPU workersâ€”only the infrequently used parameters would trigger the slow communication from SSD to GPU. Our Motivation.Existing systems such as XDL and PaddlePaddle update the model in either the pure synchronous fashion or the pure asynchronous fashion. Both generally perform well on training small models. However, when training a large-scale model, the synchronous updating suî€ers from low hardware eî€œciency while the asynchronous updating usually incurs low accuracy (recall that a tiny loss in accuracy often means a huge loss in revenue in the recommender system). Therefore, it motivates us to design a (sync-async) hybrid algorithm and an eî€œcient system that is able to take beneî€›ts from both synchronous and asynchronous updates but avoid their disadvantages. This section starts with explaining the intuition of the proposed hybrid algorithm, followed by the detailed algorithm description. Training a recommendation mode requires the following essential steps in each iteration as shown in Figure 3-left: â€¢ Preparation of embedding for the training sample(s); â€¢ Forward propagation of the neural network; â€¢ Backward propagation of the neural network; â€¢ Synchronization of the parameters of the neural network; â€¢ Update of embedding based on the corresponding gradients. One illustration of the training workî€ow is shown on in Figure 3left. Note that a homogeneous view of the embedding layer and NN would set obstacles for either hardware or statistical eî€œciency. For example, consider the fully synchronous mode that we illustrate in Figure 3-right, the above î€›ve steps have to happen sequentiallyâ€”it would be hard to speed up the training procedure based on this mechanism with low hardware eî€œciency. Asynchronous distributed training.It is important to consider appropriate system relaxations for stochastic gradient-based optimizationsdeployed in a hybrid distributed runtime. There are two key observations from the ML community about asynchronous distributed training of neural networks: â€¢Asynchronous updating is eî€œcient with sparse access. When individual updating (e.g., SGD iteration) only modiî€›es a small portion of the modelâ€™s parameters, overwrites are rare and introduce barely no bias into the computation when they do occur [55,65]. â€¢Staleness limits the scalability and convergence of asynchronous SGD. On the other hand, when the updates are heavily overlapped (e.g, dense neural networks), the bias introduced by the discrepancy would limit SGDâ€™s scalability and convergence [19, 52]. Without considering the statistical eî€œciency, the fully asynchronous mode (second row in Figure 3-right) would provide the optimal hardware eî€œciency for distributed recommender model training, where the time of preparing embeddings, synchronizing dense parameters, and updating embedding parameters can be hidden within the computation time for forward and backward propagations of the dense neural network. Unfortunately, as we will illustrate in Section 6.2, the asynchronicity would hurt the statistical eî€œciency and diminish the generalization performance, which is not acceptable for production recommender models [114]. Hybrid training algorithm. Based on these observations, one may consider to design diî€erent mechanisms for the embedding and dense neural network separately to optimize the training efî€›ciency. For this purpose, we introduce a natural, but novel syncasync hybrid algorithm, where the embedding module trains in Algorithm 1 Asynchronous updating algorithm for w. Context: Forward task for embedding layer. Input: Embedding layer ğ‘™ğ‘œğ‘œğ‘˜ğ‘¢ğ‘(Â·), training data D. while Not converge do /* Without any lock: */ Select a sample from the training set: xâˆ¼ D;î€î€‘ Get embedding vector(s) w.r.t the sample: wâ† getx; Send the embedding vector(s) to proceeding units: wâ†“. end while Context: Backward task for embedding layer. Input: Embedding layer ğ‘™ğ‘œğ‘œğ‘˜ğ‘¢ğ‘(Â·), optimizer Î©. while Activationâ€™s gradients keeps arriving do /* Without any lock: */ Receive gradient of embedding: ğ¹;î€î€‘ Send gradients to parameter storage: putx, ğ¹;î€î€‘ Update embedding parameter:wâ† wâˆ’ Î©ğ¹{ğœ‰}. end while an asynchronous fashion while the dense neural network is updated synchronously. Considering the inherited heterogeneity of deep learning based recommender systems, this design would exploit the strength whilst avoid the weaknesses of asynchronous SGD from the algorithmic perspective. Brieî€y, as we illustrate in the third and forth rows in Figure 3-right, a naive hybrid mode is able to hide the steps of preparing embeddings, and updating embedding parameters within the synchronous training of the dense neural network; further, with some advanced system optimizations (e.g., overlapping computation and communication listed in Section 4.2.3), the synchronization of the dense parameters would also be able to mostly hidden within the backward propagation of the dense neural network. Thus, the hybrid algorithm would achieve almost similar hardware eî€œciency as the fully asynchronous mode without sacriî€›cing the statistical eî€œciency. To discuss the asynchronous algorithm to update embedding parameterw, given training sample indexed byğœ‰, we introduce the following operations: â€¢ getx: fetch the subset of parameters to generate the embedding of the ID type featurexâ€”we use the notationwto represent this subset of parameters activated by the sample.î€î€‘ â€¢ putx, ğ¹: communicate the gradientğ¹w.r.txto the storage of parameter w. Algorithm 1 shows the asynchronous updating algorithm for the embedding layerw. Both forward computation task and backward computation task will be executed without any lock for synchronization: the forward computation task takes training samples, retrieves the embedding vectors, and sends the corresponding embedding vectors to the proceeding compute units (which handle the NN SGD computation); the backward computation task will receive the gradients of the embedding vectors, and sends it to the parameter storage for parameter updates. Figure 3: Left: deep learning based recommender model training workî€ow over a heterogeneous cluster. Right: Gantt charts to compare fully synchronous, fully asynchronous, raw hybrid and optimized hybrid modes of distributed training of the deep learning recommender model. Algorithm 2 Synchronous updating algorithm for w. Context: A task for dense module (indexed by ğ‘˜, ğ‘˜ = 1, 2, ..., ğ¾). Input:Neural networkNN(Â·), embeddingsw, optimizerÎ©. while Not converge do Randomly select ğ‘ buî€ered embeddings: w, ..., wâˆ¼ w; Forward of batch B: ğ¹=LNNw, x, y; /* With locks as synchronization barrier: */ Backward of batch B: compute gradient ğ¹, and ğ¹; Sync gradients with optimization: ğ¹â†î€‚ğ¹î€ƒ Our training algorithm for the rest neural network looks similar to the standard distributed training of deep neural networksâ€”the key diî€erence is that our algorithm concatenates the embedding activations as part of input. The algorithm is illustrated in Algorithm 2. Notice that we adopt a mini-batch based SGD algorithm in contrast to the sample-based SGD in the asynchronous counterpart. Since the hybrid algorithm is quite diî€erent from what has been assumed by existing systems [9,38,47,59,73]. To fully unleash its potential, we have to carefully design the system and optimize its performanceâ€”we introduce the system design and implementation of Persia in Section 4. On the other hand, it is also important to understand the statistical eî€œciency of the proposed hybrid training algorithm, thus we provide a theoretical analysis about the convergence guarantee of the hybrid algorithm in Section 5. In this section, we î€›rst introduce the design of Persia to support the hybrid algorithm; then we discuss a wide range of implementations to optimize the computation and communication utilization. The system design includes two main fundamental aspects: i) the placement of the training workî€ow over a heterogeneous cluster, and ii) the corresponding training procedure over the hybrid infrastructure. To enlighten the system implementation, we also list the implementation goals of the Persia system here. Workî€ow placement over heterogeneous cluster.To support the distributed training of deep learning based recommender model, astraightforward utilization of the PS paradigm(provided by general purpose deep learning frameworks, e.g., TensorFlow [11]) would place the storage and update of both embedding and NN parameters in a group of PS nodes (i.e., CPU machines) and the computation of forward and backward propagations in a group of worker nodes (i.e., GPU machines). However, this would be far from eî€œcient and even impossible for deployment. For example, such recommender model would easily exceed the GPU RAM; and the uniform view of embedding and NN modules would introduce a large amount of unnecessary network traî€œc. Someoptimized PS architecturesare proposed to optimize the training of deep learning recommender models by rearrange the functionalities in the PS paradigm. For example, XDL [38] designs the advanced model server to extend the original functionality of PS node to manage the learning (forward and backward propagations) of the embedding module. The hierarchical PS architecture proposed by Baidu [114] adopts a colocated PS framework with a sophisticated caching schema to reduce the communication overhead. New challenges for the layout of 100-trillion-parameter models. In order to support the recommender models with one or two magnitude larger models, Persia should provide eî€œcient autoscaling. Thus, we introduce the following modules, where each module Figure 4: The architecture of Persia. Persia includes a data loader mo dule, an emb edding PS module, a group of embedding workers over CPU nodes, and a group of NN workers over GPU instances. can be dynamically scaled for diî€erent model scales and desired training throughput: â€¢Adata loaderthat fetches training data from distributed storages such as Hadoop, Kafka, etc; â€¢An embedding parameter server (embedding PSfor short) that manages the storage and update of the parameters in the embedding layer w; â€¢A group ofembedding workersthat runs Algorithm 1 for getting the embedding parameters from the embedding PS; aggregating embedding vectors (potentially) and putting embedding gradients back to embedding PS; â€¢A group ofNN workersthat runs the forward-/backward- propagation of the neural network NN(Â·). Considering the heterogeneity between the embedding and NN modules, Persia adopts diî€erent communication paradigms for training process: i) the PS paradigm between embedding PS and embedding workers (running on CPU nodes) to manage the training of the embedding layer, while ii) the AllReduce paradigm among NN workers (running on GPU nodes) for the NN. Distributed training procedure.Logically, the training procedure is conducted by Persia in a data dispatching based paradigm as below (see Figure 4): (1)The data loader will dispatch the ID type featurexto an embedding workerâ€”the embedding worker will generate a unique sample IDğœ‰for this sample, buî€er this sample ID with the ID type featurexlocally, and return this IDğœ‰back the data loader; the data loader will associate this sampleâ€™s Non-ID type features and labels with this unique ID. (2)Next, the data loader will dispatch the Non-ID type feature andî€î€‘ label(s)x, yto a NN worker. (3)Once a NN worker receives this incomplete training sample, it will issue a request to pull the ID type featuresâ€™ (x) embeddingwfrom some embedding worker according to the sample IDğœ‰â€”this would trigger the forward propagation in Algorithm 1, where the embedding worker will use the buî€ered ID type featurextogetthe correspondingwfrom the embedding PS. (4)Then the embedding worker performs some potential aggregation of original embedding vectors. When this computation î€›nishes, the aggregated embedding vectorwwill be transmitted to the NN worker that issues the pull request. (5)Once the NN worker gets a group of complete inputs for the dense module, it will create a mini-batch and conduct the training computation of the NN according to Algorithm 2. Note that the parameter of the NN always locates in the device RAM of the NN worker, where the NN workers synchronize the gradients by the AllReduce Paradigm. (6)When the iteration of Algorithm 2 is î€›nished, the NN worker will send the gradients of the embedding (ğ¹) back to the embedding worker (also along with the sample ID ğœ‰). (7)The embedding worker will query the buî€ered ID type feature xaccording to the sample IDğœ‰; compute gradientsğ¹ of the embedding parameters and send the gradients to the embedding PS, so that the embedding PS can î€›nally compute the updates according the embedding parameterâ€™s gradients by its SGD optimizer and update the embedding parameters. System implementation goals. To eî€œciently support the hybrid training algorithm, we listed the following system design goals for Persia: Fill the asynchronicity and synchronicity gap.One central functionality of Persia system is to handle the heterogeneity inherited from the hybrid training algorithm. This would request Persia to seamlessly connect the forward- and backward- propagation during the training phaseâ€”the coordination of embeddings and gradients transmitted in a large-scale cluster is a unique challenge for Persia. Utilize the heterogeneous clusters eî€œciently.To achieve the actual performance gain, Persia needs to include diî€erent mechanisms to fully utilize the computation resources (e.g., CPUs, GPUs) given diversiî€›ed link connections. This demands an eî€œcient memory management and a group of optimized communication mechanisms. Provide eî€ective fault tolerance.With the large number of machines that training requires, eî€ective fault tolerance is necessary. Furthermore, the hybrid algorithm would request more complex mechanisms to manage the heterogeneous computation and communication, which poses additional challenges for fault tolerance. We enumerate the implementation details to fulî€›ll the design goals. 4.2.1 Fill the Async/Sync Gap. To î€›ll the gap between synchronous and asynchronous updates, both embedding and NN workers implement some buî€ering mechanisms. NN worker buî€er mechanism.Since we adopt a GPU-pull based schema for the hybrid training procedure between the NN worker and the embedding worker, each NN worker will locally maintain an input sample hash-map keyed on the sample IDğœ‰ and valued on tuples of Non-ID type featurexand labely. In the forward propagation, once a NN worker receives the NonID type feature and label, it will î€›rst insert the key-value pairî€î€î€‘î€‘ key : ğœ‰, value :x, yto the input sample hash-map; and then send the request of the embedding vector to the embedding worker. Later, when the embedding vectorwarrives in the NN worker from an embedding worker, the NN worker will pop theî€î€î€‘î€‘ key-value pairkey : ğœ‰, value :x, yfrom the input sample hash-map, consumew,x,yin the mini-batch for the SGD computation within the GPU. In the backward propagation, once the computation is done, the NN worker will use the sample IDğœ‰ to locate the embedding worker, and then send the gradient of the embedding ğ¹to this embedding worker. Embedding worker buî€ering mechanism.To support the pull request from NN worker, an embedding worker also needs to locally maintain a ID type feature hash-map that is keyed on sample IDğœ‰and valued on ID type featurex. In the forward propagation, when the embedding worker receives a new ID type feature, it will generate a unique ID for this sample, store the key-î€î€‘ value pairkey : ğœ‰, value : xin the ID type feature hash-map. Then the embedding worker willgetthe corresponding embedding parameterwfrom embedding PS and return it to the NN worker. In the backward propagation, once the embedding worker receives the gradient of the embedding vectorğ¹from a NN worker, the embedding worker will î€›nd the corresponding ID type feature x. Note that this is implemented as a local search in the ID type feature hash-mapâ€”the sample IDğœ‰serves as the key to retrievex. Later, the embedding worker canputthe gradient of the embedding parameters back to the embedding PS. 4.2.2 Persia Memory Management. Memory management is an important component for Persia to eî€œciently utilize the hybrid infrastructure, especially for the embedding PS which is responsible for maintaining trillions of embedding parameters. Comprehensively, the embedding PS works like a standard PS, which is not signiî€›cantly diî€erent from a distributed key-value store. As we illustrated in Figure 5, when retrieving the embedding parameters, an embedding worker î€›rst runs an identical global hashing function to locate the embedding PS node that stores the parameters; once the request arrives in the PS node, the parameter can be acquired in an LRU cache as we explain below. LRU cache implementation.Persia leverages LRU cacheto maintain the embedding parameters in RAM. We use array-list and hash-map to implement the LRU. Instead of a doubly linked list where the pointer stores a memory address, we adopt an arraylist design where the pointer stores the index of the pre- or postentrance in the array; similarly, the hash-mapâ€™s value also stores the corresponding embedding parameterâ€™s index in the array instead of the memory address. Besides the pre- and post- indices, each item in the array also includes two î€›elds: the embedding vector Figure 5: Memory management of Persiaâ€™s embedding PS node. The design is based on a LRU cache implemented by hash-map and array-list. and the optimizer states corresponding to this embedding vector. There are two advantages by utilizing an array-based linked list: i) this mechanism avoids frequent allocation and deallocation of fragmented memory blocksâ€”this cost is not negligible when each linked list may contain billions of entrances; ii) since pointer (that stores the memory address) does not exist in the data structure, serialization and deserialization become a straightforward memory copy, which is far more convenient and eî€œcientâ€”this is helpful for periodic saving and loading checkpoints for fault tolerance and model sharing. Note that to fully utilize the CPU cores and RAM in the embedding PS node, we utilize multiple threads in the LRU implementation. Each thread manages a subset of the local hashmap and the corresponding array-list; when there is a request of getorput, the corresponding thread will lock its hash-map and array-list until the execution is completed. 4.2.3 Communication Optimization. To fully utilize the computation power in the heterogeneous cluster, Persia implements a range of system optimizations to decrease the communication overhead. Optimized communication among NN workers.The optimization of AllReduce communication paradigm among NN workers in Persia is the key for hiding communication overhead within the backward computation of the neural network. This functionality is implemented based on Bagua [29], an open-source generalpurposed distributed learning system optimized for data parallelism, also released by Kwai. Currently, Persia utilizes Baguaâ€™s centralized synchronous full-precision communication primitive (equivalent to AllReduce) by default, in an attempt to preserve the accuracy. Persia leverages Bagua for the synchronization among NN workers because additional system optimization enabled by Bagua can be directly adopted, including tensor bucketing, memory î€attening, hierarchical communications, etc. Notice that there are other communication primitives provided by Bagua which could potentially further improve the training throughput; however, it is unclear if these communication primitives would hurt the statistical eî€œciency, we leave this exploration as an interesting future work. Optimized remote procedure call.The point-to-point communication: i) between NN workers and embedding workers, and ii) between embedding workers and embedding PS is implemented by remote procedure call (RPC). Unlike the traditional usage of RPC, where the communication is mainly responsible for transmitting small objects with complex serialization and deserialization mechanism, Persia demands a RPC implementation that is eî€œcient for communicating tensors stored in a large continuous memory space. As so, Persia abandons the protocol buî€er based implementation (adopted by gRPC [4] and bRPC [2] for other learning systems), which introduces signiî€›cant overhead for communicating tensors; instead, Persia adopts a simple but eî€œcient zero-copy serialization and deserialization mechanism targeting for tensors which directly uses memory layout to serialize and deserialize tensors. Further, as we mentioned in Section 4.2.2, since tensors on host RAM are allocated in large pages, the TLB lookup time is also reduced significantly, which accelerates the copying procedure from host RAM to the RAM of the network adapter. Workload balance of embedding PS.The embedding parameter storage is implemented as a sharded PS to support the query and update of the parameter in the embedding layer. The key challenge in embedding PS implementation is about the workload balance of query and update about the embedding parameter. Initially, we adopt a straightforward design by distributing the embedding parameter according to the feature groups. A sub-group of CPU instances are allocated to manage a partition of semantic independent embeddings. We î€›nd that in practice, this would easily lead to congestion in the access of some feature groups during trainingâ€”the access of training data can irregularly lean towards a particular embedding group during the online learning procedure in industrial-scale applications. We solve this issue by adopting an alternative partition of the embedding parameter: the embeddings inside a feature group are î€›rst uniformly shuî€Ÿed and then evenly distributed across embedding PS nodes. We observe that this design eî€ectively diminishes the congestion of the embedding parameter access and keeps a balanced workload for the embedding PS. Communication compression.To reduce the network traî€œc, we adopt both lossless and lossy compression mechanisms for the communication request between embedding and NN workers. The network bandwidth connecting GPU instances is limitedâ€”besides the AllReduce operation that leverages this bandwidth, another noticeable utility is the communication of embedding vectors in the forward propagation (4â—‹in Figure 4) and its gradient in the backward propagation (6â—‹in Figure 4). The communication between GPUs enabled by advanced connections like GPUDirect RDMA is 10Ã—faster than that between GPU and CPU nodes through PCI-e [7], which could leave the communication of embedding activation and its gradients as a bottleneck. Note that although plenty of lossy compression schema has been proposed for distributed learning, one should be cautious of applying them in distributed recommender model trainingâ€”for a commercial recommender system, as mentioned before, even a drop of0.1%accuracy is not aî€ordable [114]. To reduce such network traî€œc, we apply a lossless compression mechanism for the index component and a discreet lossy compression mechanism for the value component. Lossless compression.For the index component of the embedding, instead of representing a batch of samples as a list of vectors, where each vector containing all IDs (represented byint64) of a sample, we represent a batch as a hash-map, where the key is unique IDs in the whole batch, and the value corresponding to each unique ID is the indices of the samples in the batch containing this ID. Since the batch size is relatively small (â‰¤ 65535), the indices can be represented usinguint16instead ofint64without losing any information. Lossy compression.For the value component, We adopt a discreet fp32tofp16compression. Notice that a uniform mapping from fp32tofp16would harm the statistic eî€œciency signiî€›cantly, so we deî€›nes an nonuniform mapping method: supposeâˆ¥Â·âˆ¥represents theğ¿norm of a vector,ğœ…represents a relatively large constant scalar. In the compression side, eachfp32vector blockvis î€›rst scaled byand then converted to afp16block vector. In the decompression side, the compressed block vectorËœvcommunicated asfp16is î€›rst converted back to afp32vector and then divided 4.2.4 Fault Tolerance. Failure can happen frequently considering the large number of nodes that participate in the hybrid training. Towards this end, Persia implements a group of fault tolerance mechanisms to handle diî€erent failures in the cluster. Two observations are interesting: â€¢The infrequent loss of parameter update of the embedding layer is usually negligible for convergence, while the responsive time of the embedding parameter query would be important for the end-to-end training time; â€¢Any drop of the model synchronization by the NN worker that runs the dense synchronous training algorithm is vital for convergence. Based on these observations, Persia implements the following mechanisms for diî€erent components to handle diî€erent system failures. Since thedata loadershould be able to run on top of any other popular distributed storage systems (e.g., HDFS), Persia relies on their own recovery schema once an instance failure happens. Notice that Persia mainly considers the online training setting, where no shuî€Ÿing schema is required by Persiaâ€™s data loader. Theembedding PSshould be responsive during the hybrid training execution. For this purpose, the embedding PS node will put in-memory LRU cache (introduced in Section 4.2.2) in a shared memory spaceâ€”by this fashion, once a process-level failure happens, the process can automatically restart and attach to the consistent shared-memory space without inî€uencing any other instances of the embedding PS. Additionally, embedding PS nodes will periodically save the in-memory copy of the embedding parameter shard, with the advance of our LRU implementation, check-pointing is very eî€œcient (also see Section 4.2.2). Theembedding workerhas no fault recovery schemaâ€”once a failure happens, the local buî€er of the ID type feature hash-map will be simply abandoned without any recovery attempts. By contrast, the dense module cannot aî€ord any drop of model synchronization. As so, theNN workerwould also periodically save the synchronized model as the checkpoint with the same frequency as the embedding PS. Once a failure of GPU instances happens, all the GPU instances will abandon their local copy of the model, load the latest checkpoint, and continue the execution of the dense synchronous training algorithm. In this section, we show the convergence rate of the proposed hybrid algorithm adopted by Persia. In short, the proposed hybrid algorithm converges and admits a similar convergence rate (or total complexity equivalently) to the standard synchronous algorithm. That is to say that the hybrid algorithm takes a signiî€›cant beneî€›t from asynchronicity in the system eî€œciency but sacriî€›ces very little for the convergence rate. To show the detailed convergence rate, we î€›rst provide the essential updating rule of the hybrid algorithm in Persia: whereğ¹andğ¹are notations for gradients ofwand w, short for respectively: whereğ· (ğ‘¡) â©½ ğ‘¡denotes some iterate earlier than the current iterate ğ‘¡. This is because of the asynchronous update for the embedding layer. Since the dense neural network adopts the synchronous update, it always uses the up to date value ofw, while the used value of the embedding layer may be from some early iterate. This is the cost of using asynchronous updating, but we will see that the cost is very minor comparing to our gain in system eî€œciency. To ensure the convergence rate, we still need to make some commonly used assumptions as follows: Assumption 1. We make the following commonly used assumptions for analyzing the stochastic algorithm: â€¢ Existence of global minimum. Assume exists. â€¢ Lipschitzian gradient.The stochastic gradient functionğ¹(Â·; ğœ‰) is diî€erentiable, and ğ¿-Lipschitzian for all ğœ‰:î€î€ â€¢ Bounded variance.The variance of the stochastic gradient is bounded: there exists a constant ğœ â©¾ 0 such thathi â€¢ Bounded staleness.All sparse model update delays are bounded: there exists a constant ğœ â©¾ 0 such that In practiceğœis the number of samples whose emb eddings are retrieved but their gradients are not yet updated into the model parameters. In Persia this value is less than 5 for most cases. Remark 1. The assumptions ofexistence,bounded variance, andbounded stalenessare commonly used ones. Thebounded stalenessassumption is due to the asynchronous update forw. In practice,ğ‘¡ âˆ’ğ· (ğ‘¡) â©½ ğœmeans that any element inwis updated up toğœtimes b etween reading it and writing it within training the same sample. In Persia this value ofğœis less than 5 for most scenarios. Thefrequencyassumption reî€ects the intrinsic property of the recommendation system. For each sampleğœ‰, if every ID type feature ğ‘¥has the same probability of being inx, thenğ›¼ =. On the other hand, if an ID type featureğ‘¥is contained by every sampleğœ‰â€™s ID type features x, then ğ›¼ = 1. Then we obtain the following convergence result: Theorem 1. Denoteğ›¼to be the constant such that for each ID the probability of a sample containing it is smaller thanğ›¼. Under Assumption 1, with learning rate ğ›¾ in (2) as Persia admits the following convergence rate: whereâ‰²means â€œsmaller than or equal to up to a constant factorâ€, and ğ¿, ğºare all treated as constants for simplicity. For exampleğ‘â‰² ğ‘ means that there exists a constantğ¶ > 0such thatğ‘â‰¤ ğ¶ğ‘for allğ‘¡. The complete proof is provided in the preprint version of this paper [53]. Note that the î€›rst two terms in(6)are exactly the convergence rate of vanilla SGD [59]. The third (additional) term is caused by the staleness of the asynchronous update in the embedding layer of the model. The staleness upper boundğœis typically proportional to the number of workers. The ID frequency upper boundğ›¼is a value smaller than1. Notice thatğ›¼ = 1leads a convergence rate exactly matching the asynchronous SGD [59]. It indicates that Persia (the hybrid algorithm) guarantees to be no worse than the asynchronous algorithm in convergence rate. Whenğ›¼ â‰ª 1, which is mostly always true in real recommendation systems, the hybrid algorithmâ€”Persiaâ€”admits a very similar convergence rate to the synchronous algorithm, since the third term is dominated by the second term. Therefore, this theorem suggests that the hybrid algorithm follows the same convergence eî€œciency in terms of iterations. In this section, we introduce experiments to evaluate the design and implementation of Persia, focusing on the following questions: â€¢Can Persia provide signiî€›cant boost of end-to-end training time comparing with the state-of-the-art systems? â€¢Is the hybrid training algorithm statistically eî€œcient in terms of convergence w.r.t training iterations? â€¢Can Persia provide high training throughput and scale out well especially for trillion-parameter scale models? Benchmark.We evaluate Persia over three open-source benchmarks and one real-world production microvideo recommendation workî€ow at Kwai: â€¢ Taobao-Ad(open source [10]): predict the advertisement CTR from Taobaoâ€™s website for 8 days with 26 million records. Figure 6: End-to-end training performance of four benchmarks: Taobao-Ad, Avazu-Ad, Criteo-Ad, Kwai-Video (from left to right). â€¢ Avazu-Ad(open source [1]): predict the advertisement CTR of Avazuâ€™s log for 11 days with 32 million records. â€¢ Criteo-Ad(open source [3]): predict the advertisement CTR of Criteoâ€™s traî€œc for 24 days with 44 million records. Note that we also extend this dataset (noted asCriteo-Syn) by synthesizing diî€erent number of random ID features for capacity and scalablity evaluation (see Section 6.3). â€¢ Kwai-Video(conî€›dential production dataset): predict the explicit behavior of Kwaiâ€™s active users about the microvideo recommendation in 7 days with 3 billion records. For the three open source advertisement CTR benchmarks, we include80%of the records as training set and the rest20%of the records as test set, we consider a fully connected feed forward neural network (FFNN) as the deep learning model with î€›ve hidden layer dimensions of4096,2048,1024,512and256. For the Kwai production microvideo recommendation task,85%of the data are included in the training set while the rest15%are considered as the test set, we also use FFNN as the model to predict multiple user behaviors. We report test AUC to evaluate convergence. The model scale of the each benchmark is listed in Table 1 Baseline systems.We consider two state-of-the-art baselines: XDL [38] and PaddlePaddle [9]â€”XDL is a specialized distributed recommendation framework developed by Alibaba; PaddlePaddle is a general purpose deep learning framework from Baidu with a specialHetermode that implements the design of AIBox [115], Baiduâ€™s recommendation training system, according to private communications we had with members of the PaddlePaddle development community. Cluster setup.We conduct experiments on two clusters. Most of the training is conducted over heterogeneous clusters inside Kwaiâ€™s production data centerâ€”for Persia, we include up to64Nvidia V100 GPUs, and100CPU instances (each with52cores and480GB RAM). The instances are connected by a network with the bandwidth of 100 Gbps. The baseline systems (XDL and PaddlePaddle) are equipped with the same amount of computation resources for each individual setting. We further conduct larger-scale scalability experiments over Google cloud platform, for both capacity and public repro ducibility reasonswith a heterogeneous cluster including: â€¢ 8 a2-highgpu-8ginstances (each with 8 Nvidia A100 GPUs) as NN workers; â€¢ 100 c2-standard-30instances (each with 30vCPUs, 120GB RAM) as embedding workers; â€¢ 30 m2-ultramem-416instances (each with 416vCPUs, 12TB RAM) as embedding PS. We î€›rst compare the end-to-end training time that each system needs to achieve a given AUC, using the three open source benchmarks over a heterogeneous cluster with8GPU workers. We report the results of both synchronous and asynchronous modes of XDL, we conduct a careful trial of diî€erent modes in PaddlePaddle and report the result under the optimal setting. Figure 6 illustrates signiî€›cant performance improvements from Persia: e.g., for the Taobao-Adbenchmark, Persia is7.12Ã—and8.4Ã—faster than that of the synchronous and asynchronous modes of XDL, and1.71Ã— faster than PaddlePaddleâ€“same level of speedup also appears in the Avazu-Ad and Criteo-Ad benchmark. On our productionKwai-Videodataset, XDL fails to run the task to convergence in100hoursâ€”we compare the training sample throughput and î€›nd that Persiaâ€™s throughput is19.31Ã—of XDL). For PaddlePaddle, we fail to run the training task since PaddlePaddle does not support some deep learning operators (e.g., batch normalization) required in our model. The hybrid algorithm is one of the main reasons behind Persiaâ€™s scalability. Here, we evaluate its convergence behavior and leave it scalability and performance to the next section. Figure 7 illustrates the convergence behaviors of diî€erent systems. We see that the hybrid algorithm shows almost identical convergence when comparing with the fully synchronous mode, converging to comparable AUC as illustrated in Table 2 (but as we will see in the next section, the hybrid algorithm is much faster compared with the fully synchronous algorithm). We see that test AUC gap between the hybrid mode and synchronous mode is always less than0.1%in the three open-source benchmarks, and less than0.001%in the productionKwai-videobenchmark; by contrast, the gap between the asynchronous mode and the synchronous mode is much higher (from0.5%to1.0%); further, as we allow more aggressive asynchronicity in PaddlePaddle, the gap is more signiî€›cant. We want to emphasize that even0.1%decrease of accuracy for the production recommender model might lead to a loss of revenue at the scale of a million dollarâ€”this is NOT acceptable. We î€›rst evaluate the scalability of Persia over clusters with diî€erent scales of GPUs in terms of training sample throughput, as illustrated in Figure 8. We see that Persia, with a hybrid algorithm, achieves much higher throughput compared to all other systems. On three open source benchmarks, Persia reaches nearly linear speedup with signiî€›cantly higher throughput comparing with XDL and PaddlePaddle. For theKwai-videobenchmark, Persia achieves 3.8Ã—higher throughput compared with the fully synchronous algorithm. Note that the fully asynchronous algorithm can achieve faster throughput than the hybrid algorithm; however, as illustrated in the previous section, it incurs a lower AUC compared with the hybrid algorithm. We further investigate the capacity of Persia for training different scales of trillion-parameter models in Google cloud platform. Figure 9-left illustrates that the throughput of Persia when varying the number of entrances in the embedding table and î€›xing the embedding output dimension to 128â€”this is corresponding to the model scale ofCriteo-Synin Table 1. We see that Persia shows stable training throughput when increasing the model size even up to 100 trillion parameters. This can be viewed as concrete evidences that Persia has the capacity to scale out to the largest recommender model that is never reported before. Figure 9-right shows that for the 100 trillion-parameter model, Persia also achieves2.6Ã—higher throughput than the fully synchronous mode; on the other hand, asynchronous mode introduces further speedup (1.2Ã—faster than that of the hybrid algorithm)â€”this generally would introduce statistical ineî€œciency for convergence, which is not revealed by the capacity evaluation. We brieî€y discuss the relevant recommender systems and system relaxations for distributed large scale learning. Detailed discussion can be found in more comprehensive surveys (e.g., [110] for recommender system, and [59] for distributed deep learning). Recommender systems are critical tools to enhance user experience and promote sales and services [110], where deep learning approaches have shown great advance recently and been deployed ubiquitously by tech companies such as Alibaba [121], Amazon [70], Baidu [114], Facebook [63], Google [24,118], Kuaishou [103], Pinterest [58], Netî€ix [30], etc. Generally, deep learning based approaches take the sparse ID type features to generate dense vectors through an embedding layer, and then feed the embedding activations into the proceeding neural components to expose the correlation and interaction. Diversiî€›ed features are included to power recommender systems [111], such as sequential information (e.g., session based user id [79,85,86]), text [109,117,125], image [95,98], audio [87,96], video [20,46], social network [27,33], etc. Additionally, various neural components have been explored for recommender systems, including multi-layer perceptron (MLP) [105,106], convolutional neural network (CNN) [72,85], recurrent neural network (RNN) [101,102], Autoencoder [71,124], and deep reinforcement learning [117,119]. Furthermore, multiple relevant goals for recommendation can be learned simultaneously by multi-task learning [24, 63, 107, 116, 121, 122]. Due to the massive scale of both training data and recommender models, distributed learning techniques are applied to train industrialscale recommender systems [63,70,114,118,121]. The heterogeneity in computation naturally leads to the idea of utilizing heterogeneous distributed computation resources to handle such training procedure [38,45,61,63,66,70,100,114,115]. For example, XDL [38] adopts the advanced model server to replace classic parameter server to reduce the traî€œc between CPU nodes and GPU nodes; Baidu proposes a hierarchical PS architecture [114] to implement a sophisticated caching schema to reduce the communication overhead; HET [61] adopts an advanced cache mechanism at the GPU worker side to leverage the skewness of embeddings by caching Figure 8: Scalability and performance of four benchmarks: Taobao-Ad, Avazu-Ad, Criteo-Ad, Kwai-Video (from left to right). Figure 9: Capacity test of Persia for Criteo-Syn benchmark the over Google cloud platform. frequently updated entrances in the workerâ€™s limited local memory. However, as far as we know, there is a lack of a carefully designed distributed training solution to support the heterogeneity inherited from the training data (sparse vs. dense) and gradient decent based optimization (memory bounded vs. computation bounded) over mixed hardwares and infrastructures (CPU instances vs. GPU instances). Distributed deep learning can be categorized as data parallelism and model parallelism. In data parallel distributed deep learning, two main categories of systems are designedâ€”parameter server [26, 35,40,47,112] and AllReduce [21,39,73,113]. In a PS architecture, models are stored in a single node or distributively in multiple nodes; during the training phase, workers periodically fetch the model from PS, conduct forward/backward propagation, and push the gradients to the PS, while the PS aggregates the gradients and updates the parameters. With an AllReduce paradigm, all workers collaborate with their neighbors for model/gradient exchanges. Diî€erent strategies are proposed to speed up the expensive parameter/gradient exchange phase. To reduce communication volumes, lossy communication compression methods are introduced, such as quantization [13,16,99,108], sparsiî€›cation [14,88,91,97], sketching [36], and error compensation [82]). In an attempt to get rid of the latency bottleneck, decentralized communication approaches are proposed [44,49,54,56,81]. Additionally, local SGD is discussed to optimize for the number of communication rounds during training [31,57,77,90]. To remove the synchronization barrier, asynchronous update methods are proposed [52,64,68,76,120,123]. There are also approaches that combines multiple strategies listed above [15,17,44,56,80]. On the other hand, researches about model parallelism attempt to study how to allocate model parameters and training computation across compute units in a cluster to maximize training throughput and minimize communication overheads. Optimizations are proposed for both operation partitioning approach [37,74,75,92] and pipeline parallel approach [32,34,84]. Recently, there are also approaches that combine both data and model parallelism [51, 67, 70]. General purpose deep learning systems have been one of the main driving forces behind the rapid advancement of machine learning techniques with the increasing scalability and performance for distributed deep learning. Popular options include TensorFlow [11], PyTorch [8], MXNet [22], PaddlePaddle [9], MindSpore [6], etc. Extensions and modiî€›cations have been made based on these general purpose learning systems for eî€œcient distributed learning (e.g., Horovod [73], BytePS [41], Bagua [29], Megatron [75], ZeRO [69], SageMaker [42], etc.). However, even including these extensions, the current general purpose deep learning systems do not consider the challenges about handling the heterogeneity over a hybrid infrastructure. Thus, it is diî€œcult to directly use these general purpose learning systems for the training tasks of industrial-scale recommender systems over a heterogeneous cluster. We introduce Persia, a distributed system to support eî€œcient and scalable training of recommender models at the scale of 100 trillion parameters. We archive the statistical and hardware eî€œciency by a careful co-design of both the distributed training algorithm and the distributed training system. Our proposed hybrid distributed training algorithm introduces elaborate system relaxations for efî€›cient utilization of heterogeneous clusters, while converging as fast as vanilla SGD. We implement a wide range of system optimizations to overcome the unique challenges raised by the hybrid algorithmic design. We evaluate Persia using both publicly available benchmark tasks and production tasks at Kwai. We show that Persia leads to up to7.12Ã—speedup compared to alternative stateof-the-art approaches; additionally, Persia can also scale out to 100-trillion-parameter model on Google cloud platform. Persia has been released as an open source project on github with concrete instructions about setup over Google cloud platformâ€” we hope everyone from both academia and industry would î€›nd it easy to train 100-trillion-parameter scale deep learning recommender models.