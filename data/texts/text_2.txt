Citation recommendation is intended to assist researchers in the process of searching for relevant papers to cite by recommending appropriate citations for a given input text. Existing test collections for this task are noisy and unreliable since they are built automatically from parsed PDF papers. In this paper, we present our ongoing eî€ort at creating a publicly available, manually annotated test collection for citation recommendation. We also conduct a series of experiments to evaluate the eî€ectiveness of content-based baseline models on the test collection, providing results for future work to improve upon. Our test collection and code to replicate experiments are available at https://github.com/boudinî€/acm-cr citation recommendation, test collection, digital libraries ACM Reference Format: Florian Boudin. 2021. ACM-CR: A Manually Annotated Test Collection for Citation Recommendation. In The ACM/IEEE Joint Conference on Digital Libraries (JCDL), Sept. 27â€“30, 2021, Online. ACM, New York, NY, USA, 2 pages. Citing has always been an integral part of academic research, whether it is for backing up claims or for referring to previous work of relevance. Yet, citing properly is becoming increasingly diî€œcult and time-consuming as the volume of published research continues to grow exponentially [5]. Researchers are under constant pressure to publish their î€›ndings, but have less and less time to browse the literature to retrieve relevant papers to cite. This has motivated an active line of research on citation recommendation systems (see [4] for a recent survey), whose goal is to relieve the researchers from performing this task by recommending appropriate citations for a given text. More precisely, citation recommendation is the task of î€›nding relevant citations for a given citation context, that is, a text passage (e.g. a sentence or a paragraph) within a document (See Figure 1). Citation recommendation, also referred to as context-aware citation recommendation or local citation recommendation in previous work, is often viewed as a retrieval task where, given a citation context (query), the task is to retrieve the most relevant citations (documents) from a collection of scientiî€›c texts. The beneî€›ts of doing so are two-fold: 1) well-known retrieval models can be readily applied to the task at hand, and 2) the eî€ectiveness of citation recommendation systems can be evaluated oî€Ÿine through test collections. Figure 1: Illustration of the citation recommendation task. Existing test collections for citation recommendation are built automatically by extracting citation contexts (queries) and cited reference(s) (relevant judgments) from a collection of scientiî€›c papers. As a result, they are known to be quite noisy and unreliable due to errors in citation parsing and/or PDF to text conversion [4]. Other reported problems include muddled citation contexts (roughly deî€›ned as î€›xed-size windows of characters around citations), and incomplete metadata information of papers. In this work, we focus on addressing these problems and present our ongoing eî€ort to create ACM-CR, a new publicly available, manually annotated test collection for citation recommendation. As a î€›rst step, we assembled a sizeable collection of documents by collecting bibliographic records of scientiî€›c papers (BTEX entries) from the ACM Digital Library. Our document collection currently holds 114,882 records of scientiî€›c papers on topics related to Information Retrieval (IR) and adjacent research î€›elds (e.g. machine learning, databases or data mining).Each record carries a Digital Object Identiî€›er (DOI), and most of them (91%) provide paper abstracts which are the primary units of indexing in scientiî€›c literature search engines [6]. To create queries and relevance judgments, we collected 50 openaccess scientiî€›c papers published at top-tier IR conferences in 2020, from which we manually extracted citation contexts and cited references. For citation contexts, we extracted full paragraphs that include citations from the introduction and related work sections of each paper (other sections being less suitable for the task [7]). We then took a step further and removed end-of-line hyphens, split paragraphs into sentences and anonymized citations that reveal their authorâ€™s identity (i.e. â€œSmith et al. [1] proposedâ€ is reformulated as â€œ[1] proposedâ€). For cited references, we mapped each reference to its DOI by querying Google Scholar. We paid a particular attention to preprint references in case they have been superseded by refereed publications. For those without DOIs but available online, we indicated the URLs to get their PDF î€›les for later use. The papers we collected have 31.8 cited references on average, half of which occur in our document collection. Annotating a single paper took about an hour on average, all annotations being encoded in XML format. Table 1 presents some statistics about the test collection. Overall, we extracted 341 citation contexts (paragraphs) from which 269 can be used as queries for evaluation because they have cited references that appear in our document collection (underlined in Table 1). It should be noted that our citation contexts are on average twice as long as the 400 characters used in previous work, and thus we might expect better retrieval results. On a lower level of granularity, we annotated 837 out of the 1,800 sentences that make up the 341 citation contexts, each of which has 2 cited references on average. Again, a smaller portion of these annotated sentences (552) have cited references that appear in our document collection and can be used as queries. Here, the idea is to provide two diî€erent types of queries (paragraphs and sentences) to investigate the eî€ect of citation context length on retrieval performance. Table 1: Statistics of the test collection. The average number of tokens, characters and citations per context are reported. We report a series of experiments to evaluate the eî€ectiveness of content-based citation recommendation models on the introduced test collection. It serves two main purposes: 1) to perform an initial sanity check on the test collection, and 2) to provide baseline results for future work to improve upon. The î€›rst model we consider is BM25, a standard ad-hoc retrieval model that is commonly used as baseline in citation recommendation. Speciî€›cally, we use the implementation of BM25 from the Anserini open-source IR toolkit [9] with the default parameters. For the second model, we adopt a twostage neural document ranking approach inspired by [8], and use SciBERT [2] to re-rank the top-20 documents retrieved by BM25. We apply the uncased modelwithout î€›ne tuning and re-rank documents against citation contexts by computing the cosine similarity between their hidden representations. We evaluate eî€ectiveness of the models in terms of recall and nDCG at the top 10 recommendations as recommended in [4]. We use the Studentâ€™s paired t-test to assess statistical signiî€›cance of our retrieval results at ğ‘ < 0.05. Results are presented in Table 2. We observe noticeably higher scores for the BM25 model in contrast to prior work (e.g.â‰ˆ +10% Table 2: Retrieval eî€ectiveness of content-based citation recommendation models. â€  indicates signiî€›cance over BM25. in comparison to [3]), which we attribute to the high quality of the manually extracted citation contexts. Re-ranking using SciBERT increases the retrieval eî€ectiveness, with statistically signiî€›cant improvements in three out of four cases. The impact of re-ranking is more important on sentences, which seems reasonable since short queries are prone to vocabulary mismatch issues. In this paper, we described our progress in creating ACM-CR, the î€›rst manually annotated test collection for citation recommendation. We hope that this resource will provide a useful benchmark for future studies, and help us gain better insights on the eî€ectiveness of citation recommendation models. For future work, we plan to collect and annotate more papers, while also exploring two directions for improving our test collection. The î€›rst one will be to obtain the full-text of all the open-access articles in our document collection, and to construct the underlying citation graph on which many citation recommendation models rely on. The second direction concerns the incomplete nature of the extracted relevance judgments, i.e. that do not include uncited, yet relevant papers. Here, we will investigate how co-citation instances can be used to overcome the sparseness of the relevance judgments. This work was supported by the French National Research Agency (ANR) through the DELICES project (ANR-19-CE38-0005-01).