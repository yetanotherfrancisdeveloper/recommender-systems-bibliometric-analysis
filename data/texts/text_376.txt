In this paper, we consider controllability as a means to satisfy dynamic preferences of users, enabling them to control recommendations such that their current preference is met. While deep models have shown improved performance for collaborative î€›ltering, they are generally not amenable to î€›ne grained control by a user, leading to the development of methods like deep language critiquing. We propose an alternate view, where instead of keyphrase based critiques, a user is provided â€˜knobsâ€™ in a disentangled latent space, with each knob corresponding to an item aspect. Disentanglement here refers to a latent space where generative factors (here, a preference towards an item category like genre) are captured independently in their respective dimensions, thereby enabling predictable manipulations, otherwise not possible in an entangled space. We propose using a (semi-)supervised disentanglement objective for this purpose, as well as multiple metrics to evaluate the controllability and the degree of personalization of controlled recommendations. We show that by updating the disentangled latent space based on user feedback, and by exploiting the generative nature of the recommender, controlled and personalized recommendations can be produced. Through experiments on two widely used collaborative î€›ltering datasets, we demonstrate that a controllable recommender can be trained with a slight reduction in recommender performance, provided enough supervision is provided. The recommendations produced by these models appear to both conform to a userâ€™s current preference and remain personalized. Collaborative Filtering, Representation Learning, Recommendation, Disentangled Representation Learning, Controllable Recommendation, Deep Critiquing Auto-encoder based architectures have shown impressive performance in collaborative î€›ltering with implicit feedback [19,25,30]. However, these methods may fail to model short term or dynamic preferences of a user. Subsequently, methods to tackle this explicitly problem have been proposed, for example, through conversations via a conversational recommender system [13], or critiquing recommendations using language or keyphrases [18,23,35,37]. In contrast to keyphrases, typically mined from reviews or descriptions (for instance), this work considers building controllable or critique-able recommenders using attribute data for items, which can be used to construct preference distributions at a user level. This is in turn used in the critiquing process or to express a short-term preference, allowing a user to exert control over a recommender in a meaningful manner. For instance, a user who typically watches Action movies, or (as a consequence) exclusively receives Action recommendations, can now explicitly request personalized movies of other genres, like Animation. Controllability can be achieved by utilizing the generative nature of certain Deep Generative Recommenders [18,23,35,37]. Such models have an encoder which produces a user-latent representation, which is fed to the decoder to predict items for that user. A manipulation of this representation, followed by decoding step using this, should alter recommendations produced by the model. However, since the latent spaces of these models are typically entangled, the outputs produced through such manipulations are likely to be unpredictable or random, making them unusable for this task. We propose using a disentangled latent space which, in contrast, can be manipulated predictably. A representation is disentangled w.r.t known ground truth variables or generative factors (ex. genres, availability, context, etc), if there is only one latent dimension in the representation that is inî€uenced when this ground truth variable is changed [3, 12, 20, 21]. Prior work tackles critiquing/controllability by modeling a latent space where keyphrases are co-embedded with user/item embeddings. By â€˜zero-ingâ€™ out a certain keyphrase, the corresponding keyphrase embedding is used to update the user representation, producing critiqued recommendations. Our work in contrast doesnâ€™t utilize a co-embedding space, and instead the latent space is directly manipulated. This means no addition optimization (like in [18,22]) is required to incorporate (multi-step) critiques. Furthermore, this also allows for â€˜positiveâ€™ critiques as well as â€˜softâ€™ control (gradual, non-binary) instead of only binary critiques. We propose using supervision to obtain a mapping of a particular factor/aspect to a latent dimension. Since supervision signals might not be available for all users, we experiment with settings where limited data is provided. In addition, while there have been metrics previously proposed for evaluating controllability/critiquing [32,35,37], these metrics donâ€™t explicitly account for the personalization of post-critique recommendations. We propose multiple metrics to evaluate the personalization of critiqued recommendations, measuring both binary critiques as well as â€˜softâ€™, gradual controllability. To summarize, our contributions are as follows: â€¢We propose using (semi-)supervised disentanglement to learn disentangled representations for controllable, personalized recommendations. Through experiments on two large scale collaborative î€›ltering datasets using 2 types of signals as â€˜knobsâ€™, we show the proposed models produce controllable recommendations, at the cost of a slight reduction in overall recommendation performance. In addition, we experiment Figure 1: Deep Controllable Recommendations with (semi)supervised disentanglement: A userâ€™s interactions are captured in x, used to infer the user representation z. Given a latent dimension ğ‘— (the â€˜knobâ€™), corresponding to a known factor (green items), only the values corresponding to the ğ‘—th dimension of zare replaced to produce {Â¯z}. Each of these are then decoded to produce recommendations. The manipulation here pushes the green items higher in the ranked list, leading to controllable recommendations. with diî€erent levels of supervision, and show controllability can be achieved with limited data. â€¢We focus on retaining recommendation performance while achieving eî€ective controllability, and propose several metrics to extensively study (a) the degree of personalization of controlled recommendations (b) whether control is achieved in isolation (i.e if only one factor changes at a time) and (c) the eî€ect of disentanglement on controllability. Using these metrics, we demonstrate that such models are amenable to user control, and the controlled recommendations appear to be personalized to an extent. There have been several works that use Deep Learning for Recommendation [11,31,33,38]. A common theme in several deep models is the use of auto-encoder architectures like the Collaborative Denoising Autoencoder (CDAE) [36] or MultVAE [19]. The latter uses the Variational Autoencoder framework [5,17] for recommendation. Shenbin et al. [30]propose the state-of-the-art RecVAE. Deep Recommender systems however are (typically) black-box models which are diî€œcult to interpret, compared to content-based methods [38]. The use of deep generative recommenders allow for disentangled representations, regaining some interpretability [9,24,34]. Most such models use the VAE framework, where the objective is to reconstruct the input with high probability, with an additional loss term constraining the latent space. By imposing additional constraints, disentangled representations can be learnt. These methods are typically unsupervised and therefore come with limitations [20]: they were found to be very sensitive to hyperparameters, reliably learning unsupervised disentangled representations is a very challenging task. There have been several works that use disentanglement in recommendation: Ma et al. [25]assume that disentanglement is generated by user behaviours on â€˜macroâ€™ and â€˜microâ€™ levels, and show that their models outperform non-disentangled baselines. Ma et al. [26]apply disentanglement to the sequential recommendation task, while Wang et al. [34]disentangle diverse user-intents using graph based collaborative î€›ltering; Cui et al. [9]propose DGCF, which have â€˜implicitâ€™ (unknown) and â€˜explicitâ€™ (known) signals that are disentangled using an RNN based model with a two-step method since some computations are non-diî€erentiable. Wang et al. [32]propose using weakly-supervised disentanglement objective on pairs of items. This allows for attribute-based item retrieval, such that items diî€er more/less on the provided attribute. In contrast to prior work, we use semi-supervised disentanglement, while focusing on using disentanglement for controllability. We focus on critiquing for deep recommenders (for a full overview of earlier work in critiquing, see Chapter 13 of [29]). The following paragraphs detail approaches that leverage language or keyphrases for critiquing, followed by other approaches like ours which does not utilize any language/keyphrase. We end this sub-section with a discussion about evaluation metrics. Recent work has focused on using language or keyphrase based critiquing [18,23,35,37]. Wu et al. [35]propose the Deep Language based Critiquing (DLC) paradigm, where (subjective) keyphrases are treated both as explanations as well as a means for a user to critique recommendations. Here, critiquing is achieved by rejecting or â€˜zeroâ€™-ing out keyphrases which are co-embedded with users. Luo et al. [22]adapt the CE-NCF for multi-step critiquing; [18] use a ranking optimization instead of pairwise re-scoring to rerank items; [23] use a VAE-based model to achieve better training stability and lower computational complexity; [37] propose using Keyphrase Activation Vectors instead of using a second â€™headâ€™ used in [35], and adapt it for â€˜positiveâ€™ critiques. Antognini et al. [2] propose T-RECS, which infers keyphrases from the intersection of user proî€›les and an item. [1] show that a model under a weaksupervision scheme matches or beats recommendation/explanation and critiquing performance while being much faster. We use supervised disentanglement in the user-latent space instead of a separate embedding space, while not requiring any language data (at the cost of no explanations). Prior work focuses on binary signals ex. reject/accept, while our work considers â€˜softâ€™ critiques, allowing for a gradual tuning. Note that while we experiment only with unit critiques, compound and/or multi-step critiques is also possible within our proposed models, which we leave for future work. Wang et al. [32]propose an orthogonal task, where items are retrieved on a â€˜gradientâ€™ given an attribute, focusing on gradual change, similar to our work. Our work focuses on recommendation performance whereas [32] focus on gradient item retrieval. Cen et al. [7]propose ComiRec for sequential recommenders, where control is used to balance diversity and recommendation accuracy, from the perspective of a designer. The method presented in [25] is closest to us, where representations are î€›rst altered, followed by a nearest neighbours search over items using a beam search. In contrast, we utilize the generative nature of the model instead of performing nearest neighbour search, in addition to using semisupervised disentanglement. The authors are aware of concurrent work [28] that is close to ours. Since the paper/code was unavailable at the time of experiments/ publication, we leave the evaluation of [28] for future work. Metrics for critiquing/controllability. Wu et al. [35]propose the Falling MAP for negative critiques. Given a user and a keyphrase (here, genre/tag) this metric measures the diî€erence in MAP values computed on the set of items that have the critiqued keyphrase. If this value is positive, items of the critiqued keyphrase â€˜fallsâ€™ down the ranking list, which is desirable. Similarly [37] propose a follow up metric, which accounts for positive critiques as well, comparing the normalized diî€erence of average ratings of items before / after the critique. We note that these metrics donâ€™t explicitly model the personalization of post-critique recommendations. In contrast, the proposed metrics explicitly consider this, by measuring performance on against items for which a userâ€™s preference is known. We present disentanglement of the latent space using semi-supervision as a means to control in Section 3.1. Adapting disentanglement representation learning for this task is explored in detail in Section 3.2. We train a model using a (semi-)supervised disentanglement loss, producing disentangled user representations. During inference, representations for all users are inferred (possibly oî€Ÿine). In the foreground, the representations are manipulated on the î€y by increasing/decreasing a â€˜knobâ€™, allowing for granular change. In the background, the mapping of a knob to a latent dimension is utilized to modify the userâ€™s representation, which is fed to the decoder to produce controlled recommendations. The following paragraphs motivate the use of disentanglement and semi-supervision to build such a recommender. As mentioned before, the entangled nature of a typical deep recommender renders meaningful manipulation impossible. This is because (a) there is no guarantee that manipulations produce changes in a single generative factor, and (b) it is not apparent which dimension to alter given a factor. Even if we assume (b), predictability is not guaranteed, ex. manipulating the Thriller dimension may increase the number of Thriller movies, but it may inadvertently increase the number of Children movies, which is undesirable. We propose using supervised disentanglement to tackle these issues. Disentanglement can isolate a generative factor to a single dimension, and altering this dimension (assuming perfect disentanglement) space should produce a change only in this generative factor. Using (semi)-supervised disentanglement addresses two issues: (a) unsupervised disentanglement has several shortcomings [20], which semi-supervision may alleviate; and (b) a particular dimension is now â€˜tiedâ€™ to a generative factor, ensuring that the correct dimension is being altered. These recommendations should ideally be personalized, since they were trained with a reconstruction loss. In essence, a recommender system can be tuned on the î€y to express user preferences for the current session. This process is illustrated in Figure 1. In this paper we opt for preference distributions i.e tag/genre distributions, as generative factors or â€˜knobsâ€™ (see Section 5.1.2), since they are widely available. However, preference distributions are not independent and can also be very noisy [4,10], in contrast to independent and exact generative factors used in disentanglement literature e.g dSprites[27]. Consequently, they may be diî€œcult to learn and/or disentangle. The following section discusses particulars about the models. In this section, two models, theğ›½-VAE [12,19] and theğ›½-TCVAE [8], are adapted for the recommendation task using a Multinomial loss [19], resulting in recommenders that can produce disentangled representations. Furthermore, this objective is further supplemented with a semi-supervision loss. 3.2.1 Unsupervised Disentanglement. The VAE [17] optimizes the marginal (log-)likelihood of the observed dataxin expectation over the whole distribution of latent factorsz. It assumes that the data is supported on a low dimensional manifold in a high dimensional space. The assumption of a latent code allows us to express theâˆ« marginal distribution as follows:ğ‘(x) =ğ‘(x|z)ğ‘ (z). Since this quantity is intractable, the Evidence Lower Bound is optimized instead [14]: log ğ‘(x) â‰¥ L= E[E[log ğ‘(x|z)]âˆ’ğ¾ğ¿[ğ‘(z|x) ||ğ‘ (z)]] whereğ‘(z|x)is a variational approximation of the posterior distribution, which is typically a Gaussian distribution.ğ‘ (z), the prior, is usually an isotropic Gaussian:ğ‘ (z) = N (0, 1). Several methods modify this objective to encourage disentanglement: 3.2.2ğ›½-VAE. Theğ›½-VAE[6,12] modiî€›es the VAE loss i.e Equation 1 by adding a ğ›½-multiplier to the KL term: L= E[E[log ğ‘(x|z)] âˆ’ ğ›½.ğ¾ğ¿[ğ‘(z|x) ||ğ‘ (z)]] (2) Disentanglement is encouraged by settingğ›½ >1, which further forces the posterior distributionğ‘(z|x)to be close to the isotropic prior for everyx(as opposed to only on average), imposing statistical independence. However, this involves a trade-oî€ between reconstruction quality and disentanglement [6,12]. While Liang et al. [19]useğ›½ <1 for better recommender performance, we opt forğ›½ >=1 to encourage disentanglement, and achieve similar/better recommender performance by removing the Dropout layer, following Shenbin et al. [30]. An annealing strategy is used for ğ›½, following [6,19]. The resulting model, which uses a Multinomial loss (see Section 5.3.2), is termed Mult-ğ›½-VAE . 3.2.3ğ›½-TCVAE. Chen et al. [8]show that by imposing a constraint on only a part of the KL divergence term, reconstruction and disentanglement increases compared to the ğ›½-VAE model. The KL term is decomposed into 3 terms (a) an index-code Mutual Information (MI) term, (b) dimension-wise KL, and (c) a total correlation (TC) term. The index-code MI captures the information between the data and latent variables, based on the empirical data distribution; the dimension-wise KL prevents deviation of the posterior from the prior dimensions; and the TC term encourages statistical independence of the posterior distribution leading to disentanglement. The key idea is that theğ›½-VAE constrains all 3 terms, which might harm performance, while only the TC term is penalized in this model. The loss function of the TC-VAE is as follows: ğ¿= E[E[log ğ‘(x|z)] âˆ’ ğ›¼ğ¼(z; ğ‘›) whereğ›¼,ğ›¾andğ›½are multipliers for the index code MI, the dimension wise KL and the TC term respectively. Following Chen et al. [8],ğ›¾ = ğ›¼ =1 is used,ğ›½is a hyperparameter. The î€›nal model, which we term Mult-TCVAE , uses a Multinomial loss (see Section 5.3.2). 3.2.4 Semi-supervised Disentanglement. The distributionğ‘(z|x) can be constrained so that a given dimension ofğ‘(z)is associated with a ground truth attribute. That is, given a representation (for instance, the mean vector from the posterior) for a data point, ğ âˆˆ R, and the set of known attributesa âˆˆ Rfor that data point, withğ´ â‰¤ ğ·, theğ‘—th dimension ofğis predictive of theğ‘—th dimension ofa, 1â‰¤ ğ‘— â‰¤ ğ´. This is achieved by adding a semisupervised loss ğ‘…: Lis either Equation 2 or 3. The binary cross entropy loss is used for ğ‘…[21]: ğ‘…(ğ, a) =alog(ğœ (ğ)) + (1 âˆ’ a) log(1 âˆ’ ğœ (ğ whereğœ (.)is the logistic function. Note that the remaining dimensions ofz, those that are not supervised, are not constrained by the semi-supervision loss. In this section we propose an evaluation methodology and a set of metrics that investigates the extent of controllability and personalization of a recommender, described in Section 4.1. We also brieî€y describe existing metrics for evaluating disentanglement in Section 4.2. For ease of discussion, this section assumes a single factorğ‘”, s.t given a userğ‘¢, and a factorğ‘”, the set of all items with factorğ‘”is I, items encountered/rated by a user isI, and items with factor ğ‘”that userğ‘¢has encountered isI= Iâˆ© I(subscripts for zare dropped for brevity in the following descriptions). Note that Table 1: Metrics for Controllability, listed with the inputs and holdouts used in its computation. ğ‘”is the genre being manipulated, while ğ‘”is another genre an item can have multiple factors e.g a movie can be both Action and Adventure. Given a user representationz, the dimension corresponding toğ‘” is obtained by using a mapping functionğ‘˜, s.tğ‘˜ (ğ‘”)is the dimension corresponding toğ‘”inz, withğ‘˜ (ğ‘”) âˆˆ {1, . . . ğ¾}, withğ¾ â‰¤ ğ·, where D is the dimensionality of the latent space. In addition, we assume that manipulations of each dimension are inâˆˆ [0,1](0 is minimum). A manipulation entails the replacement of theğ‘˜ (ğ‘”)dimension with a value indicating the position of a â€˜knobâ€™, producingÂ¯z. In addition, the top-n items ofÂ¯xis denoted bytop(Â¯x), and the number of items of a factorğ‘”in the top-n list is denoted byCount(top(Â¯x), ğ‘”). Finally, theğ‘–element of a vectorais denoted by[b], with[b]=0 indicating that the ğ‘–dimension is set to 0. 4.1.1 Desiderata. For evaluating controllability, there are a list of desiderata: (1)Increasing the values of theğ‘˜ (ğ‘”)dimension should push items ofğ‘”higher. In other words, given[Â¯z]> [Â¯z] and assumingÂ¯zproducesÂ¯xandÂ¯zproducesÂ¯xrespectively, then: for small values ofğ‘›. A natural consequence of this is irrelevant itemsIğ‘”should be pushed down or be replaced with relevant items, i.e.Count(top(Â¯x), ğ‘”)should decrease. Put another way,ğ‘˜ (ğ‘”)should only control items belonging to ğ‘”, and not control items of other factors. (2)In addition, it is crucial to ensure that these recommendations are personalized i.etop(Â¯x)should have items of the requested genre, but only those items that the user might like, i.e. items in I. We assume a ranking metricR-MET, computed against a held out set of items.R-METcan be any ranking metric, e.g. NDCG, assuming the relevance of all items except the ones in the holdout set to zero. We consider î€›ve metrics in total, with the twoğ›¿metrics, ğ›¿and ğ›¿, measuring the ranking changes of items of interest versus other items; and three correlation metrics measuring the gradual change observed as a dimension is manipulated:Corr, and (Corr,Corr). Theğ›¿metrics measures the change induced when a user prioritizes items of only one factor, reî€ecting a critique or a short term preference i.e â€˜show me Animation movies onlyâ€™. The correlation metrics on the other hand can capture exploratory behaviours or soft preferences (â€˜maybe I will try at Horror moviesâ€™), and measures to what extent the recommender is interactive and predictable. These metrics are summarized in Table 1 along with their inputs and holdout sets. Note that all the metrics are averaged across genres and then users. 4.1.2ğ›¿metrics. Similar toğ¹ -ğ‘€ğ´ğ‘ƒ[35] and PostCritRatingDiî€ [37], ğ›¿measures the change in the ranking as a knob is set to its maximum value. Note, however, that only items not belonging toğ‘”that the user likes are fed into the model, with the intent to observe if manipulating theğ‘˜ (ğ‘”)dimension increases the positions of the held-out items of genreğ‘”that a user likes. As a consequence, the model has to infer the correct items ofğ‘”to recommend, without knowledge of the users preference towards items ofğ‘”. Furthermore, in contrast to prior metrics, this evaluation is personalized i.e the metric is computed againstIand not I. The process to compute the metric is detailed below. First, before the manipulation, items of other genres that the user has liked,Iâˆ’Iis used to inferx.R-METis computed on this, usingIas the holdout set, measuring the baseline ranking score of personalized items belonging toğ‘”. Second,[Â¯z]is set to max indicating a high preference, and decoded to produceÂ¯x. This is used to computeR-METon the same holdout, expressing the ranking score of personalized items belonging toğ‘”after manipulation. The diî€erence between the two metrics is ğ›¿: The higher theğ›¿, the better the ability to produce personalized and controlled recommendations. The second metric,ğ›¿, is a variation ofğ›¿. While recommending items of genreğ‘”before items of other genres is important, it is desirable to recommend items that the user might like, over irrelevant items ofğ‘”.ğ›¿is similar toğ›¿, except the holdout set are items ofğ‘”not rated by the user,Iâˆ’ I. Ifğ›¿is high, items that the user might not like are also being promoted, which may be undesirable. While these two metrics reî€ect the extent of controllability, granular metrics, reî€ecting gradual change is presented in the following section. 4.1.3 Correlation Metrics. The following metrics compute the correlation between changes in ranking metricR-METand the values of the manipulated dimension. The correlation measure used in the experiments is the Pearsonâ€™s correlation, we leave non-linear variants to future work. Corrquantiî€›es the correlation of gradual manipulations (as opposed to setting it to the max value as withğ›¿) withR-MET usingIas the holdout set.CorrandCorrmeasure the same correlation with diî€erent holdout sets and identical inputs. CorrusesIas the hold out, measuring the eî€ect of manipulation against items ofğ‘”, whereasCorrusesIwhich measures the eî€ect of manipulation against items of a random control factorğ‘”. The input set for the last 2 metrics has neither items ofğ‘”norğ‘”. In the ideal case,Corrshould be positive i.e controllingğ‘˜ (ğ‘”)positively inî€uences items ofğ‘”, butCorrshould be zero (no change) or negative (decrease, either due to replacement or demotion), meaning items of another factor are not/negatively inî€uenced. It is our hypothesis that keeping items of irrelevant genres low in the ranking is harder if the genre being controlled is highly correlated with another genre in the data (e.g many Action movies are also tagged Adventure). To measure this, we use twoğ‘”:ğ‘” andğ‘”, whereğ‘”is the least co-occurring factor withğ‘” andğ‘”conversely frequently co-occurs withğ‘”producing EasyCorr, EasyCorr, Diî€Corrand Diî€Corr. In addition to controllability, disentanglement of the user representations can also be evaluated since the ground truth generative factors are available. Several metrics have been proposed to evaluate disentanglement, such as the Mutual Information Gap (MIG) [8], theğ›½-VAE metric [12] or the FactorVAE metric [15]. We evaluate disentanglement with the Mutual Information Gap (MIG) [8], due to ease of implementation, wide applicability and the unbiased nature of the metric for all hyperparameter settings [20]. Given a generative factor, the empirical mutual information (M.I) between the values of the ground truth generative factors and each dimension of encoded samples from the data is computed. The MIG of this generative factor is then the diî€erence between the highest and second highest MI values. This quantity is averaged across all generative factors to obtain the MIG score. A MIG of 1 for a particular generative factor implies that one latent dimension has MI=1, with others having MI=0. The datasets, along with the requisite processing steps and generative factors for supervision are described in Section 5.1. The research questions are described in Section 5.2, followed by speciî€›cs of models in Section 5.3. In our experiments we use the Million Songs Dataset (MSD)[4] and the Movielens-20M (ML-20M) [10], two widely used collaborative î€›ltering datasets. The steps for preparing the dataset for training are outlined in Section 5.1.1. Preference distributions are constructed as signals for supervision and for evaluating disentanglement/controllability, which is described in Section 5.1.2. 5.1.1 Dataset processing. Each dataset is processed according to the steps outlined in [19]. The users are split into test, validation and train sets. The test/validation set size for ML-20M is 10,000 users and for MSD is 50,000 users, with 20% of items held-out. The models is trained with the entire train history, and evaluated with the 20% held out set. The data is binarized by keeping ratings of four or higher and only users who have interacted with at least 5 movies are kept. 5.1.2 Generative Factors. The generative factors considered for both datasets are preference distributions computed for each user using genres (MSD/ML-20M) or tags (MSD only). The number of users, items, generative factors are reported in Table 2. For a userğ‘¢, Genreis computed by calculating the proportion of movies that belong to a genre (or tag), divided by the total number of movies that the user has watched, capturing the (global) preferences of a user towards a genre/tag. Since there are 522362 tags in MSD with many repeats, only the most frequent are picked and grouped Table 2: Datasets used, along with signals being considered. The last column reports the MIG score, computed on the ground truth labels against itself using 10,000 users sampled from the test set together before computing the distribution. The dimensions of the signals for ML-20M is 19, and for MSD (Genre) is 21, and for MSD(Tag) is 30, with the supervision constraining only the î€›rst few dimensions. This section details the experimental setup employed in the paper. The evaluation metric for recommender performance throughout this paper is NDCG@100, following [19,30] i.eR-METis NDCG for all experiments. Controllability for models with 0 supervision cannot be evaluated sinceğ‘˜ (.)is unavailable. In addition, the manipulations (which replaces[Â¯z]) are produced by using the Inverse CDF of the probability distribution used during training [17]. 5.2.1RQ1How much supervision is needed for achieving controllability? How does controllability vary across datasets and models? To investigate this, we train Mult-ğ›½-VAE and Mult-ğ›½-VAE with varying levels of supervision:{0%,1%,10%,50%,100%}. For each combination of model, signal, level of supervision, we compute the î€›ve metrics outlined earlier. To summarize,ğ›¿evaluates the performance gain when a user sets a knob to its maximum setting, Corrmeasures the gradual change via a correlation of manipulations andR-MET; the other metrics contrast the controllability of a genre against two control genres. The holdout sets used to compute the metrics above are constructed for 100 users per genre (metrics remain constant beyond 100), while ensuring that there are at least 10/5 items in the input/holdout sets described in Table 1. The number of steps taken in the latent space is 50 for computing the correlation metrics. 5.2.2RQ2To what extent does disentanglement aî€›ect controllability of models? The relationship between disentanglement and controllability is explored in this section, where we measure if models with disentanglement (ğ›½ >=1) perform better than models without (ğ›½ =0). Note thatğ›½ =0 implies no disentanglement, but for the Mult-TCVAE , only the TC constraint is set to 0, i.e which meansğ›½ =0,ğ›¾ = ğ›¼ =1. In addition, we investigate if models that have higher disentanglement (MIG) scores perform better based on recommendation/ controllability metrics. MIG is computed using code from [20], modiî€›ed for use in pytorch. Since the exact M.I is intractable, the empirical M.I is computed by discretizing the values, following [8,20]. The number of bins for the discretizer is set to 20, and it is computed for on 10,000 samples from the test set. 5.2.3RQ3What eî€›ect does introducing controllability into a model have on recommendation performance? This question deals with the relevance of both controlled and â€˜defaultâ€™ recommendations. This is paramount, otherwise recommendations can be irrelevant. As such, NDCG@100 is computed for all models, and model selection is done on the basis of recommendation performance on the validation set. We remind the readers thatğ›¿has to be interpreted with ğ›¿. That is, ifğ›¿is low andğ›¿is high, recommended items may be personalized; however, ifğ›¿is higher, the models do exhibit controllability, but the recommended items might not be all personalized. Note that these metrics might be limited since a user might like these items regardless, or, such recommendations might be explicitly sought by the user. Performance for models with no disentanglement (ğ›½ =0) and no supervision (%=0) are also reported. This section details speciî€›cs of models being used, along with hyperparameters: 5.3.1 Variational Distribution. Both Mult-ğ›½-VAE and Mult-TCVAE use a isotropic Gaussian distribution for a prior (ğ‘ (z) = N (0, 1)), and a Gaussian distribution for the variational posterior ğ‘: where ğ(x) and ğˆ(x) are outputs of the encoder network. 5.3.2 Multinomial loss. We use the Multinomial log-likelihood for ğ‘, which has been shown to perform well over other losses like the Gaussian/Logistic losses [19]: The î€›nal loss used for both Mult-ğ›½-VAE and Mult-TCVAE methods are given in Equation 4, whereLis either Equation 2 for the Mult-ğ›½-VAE or Equation 3 for the Mult-TCVAE , and second term is the binary cross entropy loss (Equation 5). In addition, ğ›¾=0 for models with no supervision (%=0), andğ›¾=1 otherwise (%>0). To obtain recommendations, items are sorted in the descending order of the likelihood predicted by the decoder. 5.3.3 Model Hyperparameters. All models use the same neural network with a 3 layer encoder, with dimensions|I| â†’600â†’ 600â†’200 and a single layer decoder which was found optimal for a variety of settings [30]. The outputs of the encoder are two 200 dimensional real vectors representing the mean/log-variance vectors.All models were trained with Adam [16] with a learning rate of 0.001 and a batch size of 500 for 200 epochs for ML-20M and 100 epochs for MSD. For both, we triedğ›½ âˆˆ {1,2.5,5,10,100}, with the â€˜bestâ€™ model selected using NDCG@100 computed on the validation set. For the Mult-TCVAE , we set ğ›¼ = ğ›¾ = 1. Table 3: Results for ML-20M: MIG measures disentanglement, ğ›¿ Diî€Corrand Diî€Corrcontrasting control of one genre against another. NDCG and ğ›¿ measure personalization of non- and controlled recommendations respectively. Values in braces are standard errors. Table 4: Results for MSD (Genre): Models for this dataset in particular struggle to distinguish between genres (very high CorrCtrl scores) The results of all experiments on ML-20M is reported in Table 3, on MSD (Tag) in Table 4 and î€›nally on MSD (Genre) in Table 5. The results indicate that while increasing supervision generally tends to increase controllability, too much supervision can sometimes harm controllability, or alternatively increase controllability at the cost of more non-personalized recommendations (Section 6.1). The degree of supervision required for controllability varies across models/datasets/signals. In addition, adding a disentanglement objective helps across all metrics, but there appears to be no conclusive trend between degree of disentanglement and controllability (Section 6.2). Finally, controlled recommendations seem to be more personalized for ML-20M compared to MSD, where more non-personalized items and Corr measure control, with EasyCorr, EasyCorr, get recommended (Section 6.3). We discuss each result in detail in the sections that follow. Surprisingly, apparent control over recommendations seems to be achieved with as little as 10% of data for both datasets, as measured by highCorrandğ›¿. In addition, these values increase as supervision is increased to 50% for all data and models. Supervision beyond that, however, can produce mixed results; for instance, Multğ›½-VAE trained on 100% of ML-20M scores worse on ğ›¿and Corr. In contrast, for MSD(Genre) and MSD(Tag) (with the exception of Mult-ğ›½-VAE for 100%),ğ›¿increases with supervision. These trends also hold for theğ›½ =0 models. Therefore, in most cases, supervision increases apparent controllability. Given a model which scores high onğ›¿(orCorr), î€›ner grained RandCorrmetrics can be analyzed, to check if changing a knob inadvertently changes other genres. Increasing supervision for ML-20M makes models score better onDiî€Corr(i.e more negative), while only slightly decreasingDiî€Corr, indicating that supervision can help with similar genres in particular. While supervision initially improvesEasyCorr, 100% supervision actually worsens it slightly from 50%. Therefore, for ML-20M, supervision can help the model avoid confusing similar genres, at the (slight) cost of controlling dissimilar genres, especially if fully supervised. For MSD (Genre), we î€›rst note that values ofEasyCorrand Diî€Corrare positive for this dataset, indicating that other genres are also being manipulated. Increasing supervision increases EasyCorrandDiî€Corri.e help control, whileEasyCorr andDiî€Corrtend to become worse, indicating confusion among other genres. WhileEasyCorrandDiî€Corrbecomes worse with 100% supervision,EasyCorrandDiî€Corrimprove, indicating that full supervision can help with confusion. In conclusion, for MSD (Genre), increasing supervision generally improves control, at the cost of other genres being also recommended, which improves with 100% supervision for MSD(Genre). In MSD (Tag), supervision generally increasesEasyCorrand Diî€Corr. For the Mult-TCVAE , increasing supervision to 100% makes the model score better onDiî€Corr, but worse onEasyCorr. In contrast, for increasing supervision to 100% for the Mult-ğ›½-VAE makes the model score worse on bothDiî€CorrandEasyCorr. For the MSD (Tag) dataset, therefore, while supervision increases controllability, too much can cause confusion between tags. In summary, while an increase in supervision generally increases controllability across datasets and models, and additionally reduces confusion between factors, too much supervision can harm performance by causing other genres to also be recommended alongside the one being controlled. This section discusses if disentanglement (ğ›½ >=1) models are necessary, and if increased disentanglement as measured by the MIG score contributes to controllability. 6.2.1 Comparison with baselines. Models without disentanglement ğ›½ =0, generally score worse in most respects, compared to models with disentanglement. For the ML-20M dataset,ğ›½ =0 models have comparable controllability scores compared with models with disentanglement. However, this comes at a drastic reduction in recommender performance i.e the best NDCG (0.2904) among the baselines is worse than the lowest NDCG among models with disentanglement (0.3675). This is also seen for the MSD (Genre) datasets, although the performance drop is smaller. For both the MSD datasets, however, the gap observed in the controllability performance of entangled/disentangled models is much greater than the gap in the ML-20M dataset, indicating that disentanglement is necessary for controllability in these datasets. Overall, models with disentanglement perform better than models without disentanglement. The next section compares the degree of disentanglement with controllability. 6.2.2 Does increased disentanglement help? For the ML-20M dataset, increased disentanglement results in lower recommendation (NDCG) performance. ML-20M models with high MIG scores perform better onğ›¿, although this doesnâ€™t seem to be necessary for Mult-TCVAE with %=50, which scores high on many metrics despite having a low MIG score. For the MSD (Genre) models, a jump in MIG is accompanied by large increases inğ›¿andğ›¿. However, as the values of MIG scores here are either high or near zero, itâ€™s diî€œcult to make concrete conclusions about the precise relationship between disentanglement scores and controllability. We note, however, that Mult-TCVAE , which achieves better disentanglement [8] in general, seems to perform better on the ML-20M and MSD (Tag) dataset while achieving the highest MIG scores, while Mult-ğ›½VAE performs better and achieves the highest MIG score for MSG (Genre). We î€›rst note that as the amount of supervision, and consequently, controllability increases, average recommender performance (NDCG) decreases. However, we argue that the beneî€›ts might outweigh the cost, as this drop is relatively small while providing control for users (or practitioners). While NDCG measures performance of non-controlled recommendations, the degree of personalization of controlled recommendations can be measured by comparingğ›¿ and ğ›¿. The results vary depending on the model or dataset. For instance, sinceğ›¿is much higher thanğ›¿for Mult-ğ›½-VAE with 100% supervision on the ML-20M, we can conclude that it recommends personalized items instead of random items that a user might like. However, the opposite is true for Mult-TCVAE , asğ›¿andğ›¿ are at a similar level, which means that the items recommended on controlling a genre might not be as personalized to the user. Mult-TCVAE produces the highestğ›¿scores for the ML-20M and MSD(Tag) datasets, while Mult-ğ›½-VAE produces the highestğ›¿ scores for MSD (Genre). In addition, whileğ›¿is higher (more items of that genre are being pushed to the top),ğ›¿is also very high in MSD (Genre), which mean recommendations are controllable, but might not be personalized. However, as data sparsity increases,ğ›¿ might be overestimated because of missing relevance judgements, as evidenced by highğ›¿for almost all models trained on MSD. In addition to incompleteness, this evaluation also does not account for the exploratory/interactive/serendipitous nature of controllable recommendations. In summary, controlled recommendations appear to be personalized, as evidenced by high scores ofğ›¿, showing that users can exert control and expect personalized recommendations to an extent. However, other items of the given genre might also be recommended, which is especially true for the MSD dataset. We showed that controllable recommendations can be achieved by leveraging the generative nature of recommenders. (Semi-)supervised disentanglement is used both to tie a generative factor to a known dimension, and to enforce disentanglement. Consequently, manipulating a dimension produces controlled recommendations, allowing a user to express short-term/dynamic preferences, or to explore these recommendations in an interactive manner. We experimented with genre/tag distributions as supervision signals on two datasets. We proposed metrics to measure the extent of controllability in addition to the degree of personalization of controlled recommendations. Using these metrics, we showed that a user can control recommendations, while retaining personalization to an extent. We showed that such control comes only at a slight reduction in recommendation performance, but might require diî€erent degrees of supervision depending on the data, model or signals used. While the experiments here detail controlling only a single dimension, multiple dimensions can be manipulated allowing for greater or more nuanced control, which we leave for future work. In addition, the dimensionality of the supervision signal is limited by the dimensionality of the latent space, which limits the applicability to large sets of knobs. Evaluating the personalization of controlled recommendations presents a challenge as outlined in the previous section, which we plan to pursue in the future. In addition, this method can be used to â€˜boostâ€™ recommendations of items belonging to a particular category, for instance, if items of category X are being under-recommended (possibly due biases), they can be boosted to compensate for it. The authors would like to thank Mohammad Aliannejadi, Wilker Aziz, Maurits Bleeker, Jin Huang, Antonis Krasakis, Anna Sepliarskaia, Georgios Sidiropoulos and Svitlana Vakulenko for helpful comments and feedback. The authors also thank the feedback received by reviewers during prior review processes. This research was supported by the NWO Innovational Research Incentives Scheme Vidi (016.Vidi.189.039), the NWO Smart Culture - Big Data / Digital Humanities (314-99-301), and the H2020-EU.3.4. - SOCIETAL CHALLENGES - Smart, Green And Integrated Transport (814961). All content represents the opinion of the authors, which is not necessarily shared or endorsed by their respective employers and/or sponsors.