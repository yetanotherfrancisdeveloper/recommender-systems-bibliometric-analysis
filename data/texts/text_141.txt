This paper provides the î€›rst formalisation and empirical demonstration of a particular safety concern in reinforcement learning (RL)-based news and social media recommendation algorithms. This safety concern is what we call â€œuser tamperingâ€ â€“ a phenomenon whereby an RL-based recommender system may manipulate a media userâ€™s opinions, preferences and beliefs via its recommendations as part of a policy to increase long-term user engagement. We provide a simulation study of a media recommendation problem constrained to the recommendation of political content, and demonstrate that a Q-learning algorithm consistently learns to exploit its opportunities to â€˜polariseâ€™ simulated â€˜usersâ€™ with its early recommendations in order to have more consistent success with later recommendations catering to that polarisation. Finally, we argue that given our î€›ndings, designing an RL-based recommender system which cannot learn to exploit user tampering requires making the metric for the recommenderâ€™s success independent of observable signals of user engagement, and thus that a media recommendation system built solely with RL is necessarily either unsafe, or almost certainly commercially unviable. Broadly speaking, recommender systems are algorithms which oî€er the ability to î€›lter through large pools of some kind of entity to identify and recommend particularly relevant entities to an individual or group [6]. Among other domains, recommender systems have been widely deployed to recommend movies and videos, music, and goods on e-commerce platforms. One of their most signiî€›cant areas of application is within news and social media platforms, where they are used to provide users with some content of interest. We refer to recommender systems in this speciî€›c area as â€˜media recommender systems.â€™ One emergent approach to implementing recommender systems involves treating the recommendation problem as a Markov Decision Process (MDP) and applying reinforcement learning (RL) to the recommendation task. While this approach was suggested some time ago [24,26,27], lately it has garnered new interest, due to the emergence of â€˜Deep RLâ€™ and its ability to handle larger, more complex problems [15,31,32]. Research has begun to explore the applicability of Deep RL-based recommendation in the news and social media space [23,33]; this work has demonstrated signiî€›cantly increased user engagement and activeness relative to other leading approaches to the recommendation problem, which are predominantly: (i) â€˜staticâ€™ machine learning approaches [2,6,9,16,18]; and (ii) contextual Multi-Armed Bandit approaches [14, 28â€“30]. As RL research continues to advance and RL techniques become more eî€ectively applicable at scale, RL-based recommender systemsâ€™ impact in the domain of media recommendation will likely continue to grow, and may even eclipse that of the current dominant techniques. Indeed, it is already the case that leading social media platforms like Facebook are undertaking this research & development [10, 17]. The social and ethical implications of media recommender systems have also recently received signiî€›cant research attention [1,13,19,20,25]. A recent survey on the subject enumerated the main areas of concern as â€˜Biased/unfair recommendations,â€™ â€˜Encroachment on individual autonomy and identity,â€™ â€˜Opacity,â€™ â€˜Questionable content,â€™ â€˜Privacyâ€™ and â€˜Social manipulability and Polarisationâ€™ [19]. This paper particularly focuses on the last of these concerns. We argue that the risks posed by an RL-based approach in the space of manipulation and polarisation require serious attention. It has been theorised â€“ but neither demonstrated nor formalised in previous work, to the best of the authorsâ€™ knowledge â€“ that a particularly problematic variant of this concern has the potential to emerge when the recommendation problem is framed as a MDP and RL is employed to solve it [21,22]. The basic idea is that the recommendation algorithm could learn to make recommendations which inî€uence users into becoming easier, more predictable targets for recommendation, as this would heighten the algorithmâ€™s success in the long term. This paper provides the î€›rst concrete formalisation â€“ and experimental veriî€›cation of the potentiality â€“ of the just-described issue, which we call â€œuser tampering.â€ This paper makes two core contributions to the literature on the ethics and safety of media recommender systems. Firstly, we formalise the notion of user tampering as a potential safety issue speciî€›c to RL-based media recommenders; we do this by using the Causal Inî€uence Diagram techniques proposed by Everitt et al. [7] to extract the speciî€›c causal phenomenon enabling RL-based recommenders to learn to manipulate usersâ€™ preferences, opinions and interests. Secondly, we simulate a simple media recommendation problem. We show that a standard Q-learning algorithm can learn to exploit user tampering, by developing a policy of making recommendations that aî€ect our simulated usersâ€™ content â€˜preferencesâ€™, before capitalising on those eî€ects in later recommendations. While our simulation occurs on a signiî€›cantly smaller scale than a real recommendation problem scenario, it nonetheless aî€œrms the user tampering theory and so has signiî€›cant implications for actual RL-based media recommender systems. In this section, we î€›rst generically frame the media recommendation problem as a Markov Decision Process (MDP), and then use Causal Inî€uence Diagrams (CIDs) to extract the relevant causal dependencies that particular variables exhibit under this model. For some background on CIDs, see Appendix A. We have endeavoured to keep the MDP as general as possible, while also incorporating design insights from recent work in implementing RL-based media recommender systems [23, 33]. We now build up a model of the media recommendation problem as a Markov Decision Process âŸ¨ğ‘†, ğ´,ğ‘‡, ğ‘…,ğ›¾âŸ©; this is the problem interpretation upon which RL algorithms are based. Here, we assume that articles/posts are represented in a parameterised form, i.e. as anğ‘›-dimensional vector, where we have identiî€›edğ‘›numeric characteristics of the article by which it can be identiî€›ed (such as topic, stance, author, etc.). This is in opposition to representing articles atomically, which is undesirable; it rules out the use of Deep RL, without the use of which the media recommendation problem is untenably large for RL at industrial scales. Say that we begin by deî€›ning the MDPâ€™s individual elements as follows: â€¢ ğ‘†is a set of states. The speciî€›c deî€›nition of a state could take any number of deî€›nitions in a particular implementation, but we generally deî€›ne it here as a collection of data points which can be divided into â€˜necessaryâ€™ and â€˜optionalâ€™ elements: â€“Necessary: The state must contain a representation of how the recommenderâ€™s recent recommendations have performed. For example, this could be a collection of|ğ‘› Ã— ğ‘š| datapoints, representing usersâ€™ aggregate clicks on recommended items acrossğ‘›categories overğ‘šdiî€erent interpretations of â€˜recent historyâ€™ e.g. the last 1 hour, 6 hours, 1 day, etc. (similar to the approach taken by Zheng et al. [33]). This inclusion is â€˜necessaryâ€™ as without it, the theoretical advantage associated with using RL in the î€›rst place would be lost. Learning policies which capitalise on future as well as current opportunities for reward relies upon including this information in the state representation. â€“Optional: Other observable features which inform about this userâ€™s preferences, including the activity of their â€˜friendsâ€™, contextual features such as time of day, and more. â€¢ ğ´is a set of actions. An action is anğ‘›-dimensional vector, representing the characteristics of one article that could be recommended to the user. This could be extended to deî€›ning an action as recommending a î€›xed-size set of articles to the user, as the characteristics of all the articles could simply be aggregated into the vector. â€¢ ğ‘…is the reward function, mapping the agentâ€™s activity to numeric rewards to give it feedback about the â€˜goodnessâ€™ of that activity. Commonly, recommmender systems could base reward on observable indicators of engagement such as a click, Figure 1: A naive CID of the media recommendation problem. â€˜likeâ€™, or so forth. Depending on the speciî€›c implementation and the deî€›nition of the state space, this function could have multiple diî€erent signatures, includingğ‘…:ğ‘† â†’ R, ğ‘… : ğ‘† Ã— ğ´ â†’ R and others. â€¢ ğ‘‡:ğ‘† Ã— ğ´ Ã— ğ‘† â†’ [0,1]is the transition function. It returns the probability of an agent reaching a particular â€˜successor stateâ€™ ğ‘ , given it has taken a certain action ğ‘ from a state ğ‘ . â€¢ ğ›¾ âˆˆ R is the discount factor for future rewards. In terms of these variables, the recommendation problem can simply be described as an agent taking an actionğ‘at timeğ‘¡, which will transition the system from a current stateğ‘ to a successor state ğ‘ with probabilityğ‘‡ (ğ‘ , ğ‘, ğ‘ ). The agent would thereafter be rewarded with the valueğ‘…(ğ‘ ), and then another action would be chosen at time ğ‘¡ + 1, and so forth. If we were to naively model this MDPâ€™s causal structure as a CID without further consideration, we would reach a representation similar to that shown in Figure 1.At a time-stepğ‘¥, the distribution over possible current states is represented byğ‘†. Once the actual value of the state as of timeğ‘¥is observed, this will constitute the only information available to the agent in its selection of an action atğ´. The distribution over possible states inğ‘†is then deî€›ned byğ‘‡, givenğ‘†andğ´. Finally,ğ‘…represents the distribution over the reward value achieved from the action taken atğ´. We have assumed an interpretation of the reward function asğ‘…:ğ‘† Ã— ğ‘† â†’ Rhere, where reward is determined by a comparison of two consecutive states, as this will provide suî€œcient information to deduce the success of the most recent action (recommendation). However, a simple thought experiment can demonstrate that this CID underspeciî€›es the causal relationships in the actual problem, by leaving key variables external to the MDP unacknowledged. Consider the following: Alice and Bob are two university students who have just created accounts on some media platform, who have so far both been recommended the same three articles about the student politics at their university, and who have both clicked on all three articles. Within our general deî€›nitions, it is quite plausible that the states of the system have been identical thus far from the agentâ€™s perspective. However, what if Bob is uninterested in politics and is just clicking on the articles because his friends feature prominently in the cover photos of all three, whereas Alice is clicking out of a genuinely strong interest in politics, including student politics? If the recommendation to both Alice and Bob at the next time-step â€“ say,ğ´â€“ is an article about federal politics, it is intuitively untrue that the distribution over possible states atğ‘†is the same; Alice is surely more likely to observably engage with this content. Evidently, a random variable exogenous to the MDP must be introduced to properly model the causal properties of the true system. Informally, we argue that this variable can be characterised as the preferences/opinions/interests of the speciî€›c user to which the agent is recommending media. Given that it is exogenous to the MDP, it is unnecessary to assign this variable a speciî€›c form, but we assign it the symbolğœƒfor the purposes of our causal modelling. That is to say, the exogenous variable that is the userâ€™s preferences etc., at timeğ‘¥, is represented asğœƒ. We do not enforce any Markov assumptions onğœƒ; that is, it may be dependent on the variableâ€™s value at multiple, or all, previous time-stepsâ€™ values. A vital observation to make at this stage is the causal relationship betweenğœƒandğ‘†. As the example above demonstrated, without acknowledging the eî€ect ofğœƒ, the real distribution over states ğ‘†cannot be explained. So, there exists a causal link between the former and latter variables. Moreover, the speciî€›c elements ofğ‘† whose distribution would otherwise be unexplainable are precisely those which we explained are necessary inclusions in the deî€›nition ofğ‘†in order to produce the desirable properties of an RL approach. So, this link cannot be removed by any practical redesign of the state space. Finally, it is crucial to recognise that an inî€uence link will also exist betweenğ´andğœƒ; intuitively, this just reî€ects the reality that a userâ€™s consumption of information will update and aî€ect their preferences, interests, etc. going forward. Given thatğœƒis exogenous and given no speciî€›c formal interpretation, it is not our claim that there is a precise model available for how ğ´ aî€ects the distribution over possible values ofğœƒ; rather, we are just acknowledging the existence of the dependency. It is evident that the CID of Figure 1 is in need of revision. If we introduce the exogenous variable to the system, without changing any other deî€›nitions, we arrive at the CID shown in Figure 2. This CID, we argue, more completely captures the actual causal dynamics of the Media Recommendation MDP. We note that previous literature has acknowledged a similar causal structure to the recommendation process [12]; however, this was not formulated in the CID framework that we have used, which permits sophisticated graphical analysis of the kind developed in the next section. Depending on the exact design of the MDP, there are variations on this CID that could exist; see Appendix B for an example of the causal structure that is implied if the designer wishes to expand the reward function to account for observations that are not captured in the state representation. However, such variations have no eî€ect on the role and inî€uence ofğœƒ; its links from the preceding action and to the succeeding state necessarily remain part of the modelâ€™s causal structure. Given that these causal relationships are exactly where we intend to focus our analysis in the next section, the CID in Figure 2 is a suî€œciently general representation for our needs going forward. Figure 2: A CID of the media recommendation problem, extended to include the exogenous variable aî€ecting state transitions. In this section, we use the CID formulated in the previous section (Figure 2) to analyse the safety of the RL-based approach to media recommendation, speciî€›cally with respect to the high-level concerns of user manipulation and polarisation. After introducing the phenomena of â€˜instrumental control incentivesâ€™ and â€˜instrumental goalsâ€™ from the RL incentive analysis literature, we show that in the CID, an instrumental goal exists for the agent to manipulate the expected value of the exogenous variableğœƒ. This lends a concrete, formal interpretation to the (formerly only hypothesised) safety issue that we have called â€˜user tamperingâ€™. An â€˜instrumental control incentiveâ€™ (ICI) is a graphical property of CIDs introduced by Everitt et al. [7]. An ICI exists on a Structural Nodeğ‘‹if it lies on a path in the CID that begins at a Decision Node and ends at a Utility Node, and basically implies that the action chosen at the former aî€ects the expected utility at the latter through aî€ecting the distribution over values at ğ‘‹ . The importance of ICIs is that they provide a simple graphical criterion that can establish either the potential presence or the categorical absence of a so-called â€˜instrumental goalâ€™ on certain events in a RL problem [8]. An RL agent is said to have an instrumental goal to inî€uence the distribution at a Structural Nodeğ‘‹in a certain way if it has an ICI onğ‘‹and that particular inî€uence increases the expected reward accumulated by the agent â€“ in short, if it has both the ability and a reason to aî€ect the distribution at ğ‘‹ . In the CID we have presented, intuitively there are a subset of Structural Nodes upon which instrumental goals are desirable â€“ those representing the set of random state variables{ğ‘†|ğ‘¡ âˆˆ N}. This reî€ects the very premise of RL â€“ we want the agent to be able to manipulate the state of the system in pursuit of â€˜goodâ€™ states (in our context, these are states where many of its recent recommendations have been well-received by the user). As such, any path through the CID from a Decision Node to a Utility Node that only passes through Figure 3: An annotated version of the media recommendation CID for state-based rewards. An example of an undesirable causal path introducing an instrumental control incentive on ğœƒis shown in bolded red. random state variables (e.g. [ğ´â†’ ğ‘†â†’ ğ‘…], or [ğ´â†’ ğ‘†â†’ ğ‘†â†’ ğ‘…]) only involves intended and safe instrumental goals. However, other paths from Decision to Utility Nodes exist in the CID. Speciî€›cally, there are paths which visit the exogenous random variables â€“ for example, [ğ´â†’ ğœƒâ†’ ğ‘†â†’ ğ‘…]. Figure 3 traces this path in the CID. Clearly, there is an ICI onğœƒ, or on any other variable in {ğœƒ|ğ‘¡ âˆˆ N} that appears in similar paths. Given this, if the agent stands to generate higher amounts of reward by recommending to a user with particular preferences, opinions and interests (represented byğœƒ), then the agent may have an instrumental goal to inî€uenceğœƒin that direction, as this may lead to higher expected reward in the long term. In essence, the CIDâ€™s admission of an ICI on at least one node in{ğœƒ|ğ‘¡ âˆˆ N}is precisely the necessary graphical condition that allows the manipulation of users to emerge as an instrumental goal for an RL agent. If such an instrumental goal does exist (i.e. if by aî€ecting usersâ€™ preferences/opinions/interests, the agent can increase its expected reward), then we can expect that an arbitrarily capable RL agent would learn to act on that instrumental goal â€“ we say that this makes user tampering a â€˜learnableâ€™ phenomenon. Deî€›nition 1. User tampering is a â€˜learnableâ€™ phenomenon for an RL-based media recommendation algorithm iî€ it has an instrumental goal to aî€ect at least one of the variables in {ğœƒ|ğ‘¡ âˆˆ N}. Importantly, however, an instrumental goal on aî€ecting some variable in{ğœƒ|ğ‘¡ âˆˆ N}does not imply that an arbitrary RL agent will learn to aî€ect the user in the way necessary to increase its expected reward; it only implies that it could learn this behaviour. So, user tamperingâ€™s learnability in some model is a necessary but insuî€œcient condition for user tampering actually manifesting in an RL agentâ€™s learned policy. It is therefore useful to introduce a second deî€›nition relating to user tampering, such that we can separate our discussions of its theoretical learnability from our discussions of it actually manifesting in a given recommenderâ€™s policy. We introduce a second deî€›nition to address this: Deî€›nition 2. An RL-base d media recommendation algorithm â€˜exploitsâ€™ user tampering iî€ there exists a stateğ‘ such thatğœ‹ (ğ‘ ) = ğ‘ andğœ‹(ğ‘ ) â‰  ğ‘, for the algorithmâ€™s actual learned policyğœ‹, and the hypothetical policyğœ‹that the same learning process would have produced in a world where ğ´âŠ¥âŠ¥ ğœƒ. In Appendix C, we relate these formalisms to a diî€erent form of â€˜tamperingâ€™ in RL: â€˜Reward Function (RF)-tampering.â€™ We explain that although the two phenomena seem to describe similar highlevel issues, the issues are quite separate on a causal level; and that these diî€erences rule out the transferral of promising solutions for the RF-tampering issue to the user tampering context. In this section, we empirically analyse the user tampering phenomenon formalised in the previous section. Firstly, we introduce a simple abstraction of the media recommendation problem, which involves simulated users and a user tampering incentive inspired by recent empirical results about polarisation on social media. Then, we present a Q-learning agent intended to mimic the Deep Qlearning algorithms used in recent media recommendation research, and train it in this environment [23,33]; we show that its learned policy clearly exploits user tampering in pursuit of greater rewards. We begin by introducing our example problem. In this problem, we will have a recommender agent makeâ„sequential recommendations of â€˜political posts/articlesâ€™ to a user. At each timestepğ‘¡, 0â‰¤ ğ‘¡ â‰¤ â„, the agent chooses one of three â€˜sourcesâ€™ from which to recommend; the î€›rst source being consistently left-wing in its perspective, the second being consistently centrist, and the last being consistently right-wing. For the purposes of our example, we assume a deî€›nition of the exogenous parameterğœƒintroduced in Section 2 â€“ recall that the agent does not explicitly model this variable, but we need to here for the purposes of constructing our simulation. We deî€›neğœƒas a tuple of three probabilities as of timeğ‘¡, i.e.Î˜= {(ğœƒ, ğœƒ, ğœƒ) âˆˆ R| âˆ€ğ‘¥ âˆˆ {ğ¿, ğ‘…, ğ¶}.ğœƒâˆˆ [0,1]}. For some arbitrary user, their probabilityğœƒrepresents their probability of clicking an article from the left-wing source if recommended it; the same can be said of ğœƒfor the centrist source, andğœƒfor the right-wing source. We say that a user is initially â€œright-wingâ€ iî€ğœƒ> ğœƒâˆ§ ğœƒ> ğœƒ, and â€œleft-wingâ€ iî€ ğœƒ> ğœƒâˆ§ ğœƒ> ğœƒ. Finally, we include a simple environmental dynamic whereby users who are recommended content from a source that is politically opposed to their own wing gradually become more polarised in favour of their own wing. This is inspired by recent research into user polarisation on social media, which has demonstrated that showing people who identify with one wing of the political spectrum volumes of content from the opposing wing can often increase user polarisation [4, 5]. We do not at all claim that this completely simulates the polarisation phenomenon described in the works cited just above; it is obviously a signiî€›cant simpliî€›cation. However, our intention here is not to simulate this eî€ect accurately, but rather to create an environment which allows the hypothesised eî€ect of user tampering to be tested by introducing a causal eî€ect that could be used by the agent as part of an instrumental goal (while still having its simpliî€›ed dynamics grounded in actual sociological results). The full deî€›nitionâŸ¨ğ‘†, ğ´,ğ‘‡, ğ‘…, ğ›¾âŸ©of the media recommendation MDP, as well as the precise implementation of the â€˜polarisationâ€™ eî€ect we have just described, is provided in Appendix D. Next, we train a Q-learning agent in this environment and show that it learns to perform user tampering on our simulated users. Some extra speciî€›cations are needed to operationalise this environment.â„was set to 30, and the probabilities deî€›ning the exogenous variableğœƒwere limited to maximum values of 0.75.ğ‘, the â€˜polarisation factorâ€™ by which a userâ€™s subsequent probability of clicking on content from their aligned source would increase after being recommended a post from the opposing source, was sampled from the uniform distribution U(1.01, 1.10), making E[ğ‘] = 1.055. We also deî€›ned, for the purposes of our experiment, a population of î€›ve â€˜usersâ€™ with varying preference proî€›les. This contained: â€¢ A â€˜strong leftâ€™ user with ğœƒ= (0.4, 0.1, 0.1) â€¢ A â€˜moderate leftâ€™ user with ğœƒ= (0.3, 0.25, 0.1) â€¢ A â€˜centristâ€™ user with ğœƒ= (0.2, 0.4, 0.2) â€¢ A â€˜moderate rightâ€™ user with ğœƒ= (0.1, 0.25, 0.3) â€¢ A â€˜strong rightâ€™ user with ğœƒ= (0.1, 0.1, 0.4) We trained a Q-learning agent in this environment, with a user randomly selected from the population to provide the initial value ofğœƒfor each episode.Non-deep Q-learning was used for training, in spite of deep Q-learning being the more viable approach at industrial scales; this was a deliberate choice, because unlike deep Q-learning, non-deep Q-learning provably converges towards the optimal policy for the problem.Nonetheless, for consistency with a real Deep RL application, we have still modelled the state space in a parameterised fashion that is amenable to those algorithms. 4.2.1 Results. For each of the î€›ve users in our population, we provide two plots based on 10000 evaluation episodes with the user (using the policy learned from the training process described above). Respectively, these two plots estimate: â€¢The probability with which the learned policy chooses each action, at each time-step of the problem, by taking the perepisode average of each choiceâ€™s total frequency. â€¢The expected reward accumulated up to and including each time-stepğ‘¡, 0â‰¤ ğ‘¡ â‰¤ â„. For context, we plot this against the expected reward accumulated by: Figure 4: Evaluation of the policy learned with Q-learning for each member of our sample user population. â€“A recommender that makes uniformly random recommendations at each time-step. â€“A recommender that follows a simple multi-armed banditesque policy, which provides a â€˜baselineâ€™ of a good policy. This policy makes random recommendations for the î€›rst third of the episode, but then operates like a multi-armed bandit, by always recommending from the source which has the highest mean reward in the episode so far. Figure 4 displays these plots for each simulated â€˜userâ€™ we deî€›ned above. These results possess several interesting properties: â€¢ For all users except the Centrist user, the exploitation of user tampering in the learned policy is clear. Focusing on the strategy plots for the two â€˜left-wingâ€™ users, we can see a clearly dominant strategy has emerged, where: â€“The recommender attempts to proî€›le the user and their preferences, by testing their reaction to centrist and leftwing content (roughly the î€›rst quarter of the episode). â€“The recommender predominantly recommends right-wing content in spite of its low expected reward, which will tamper with the userâ€™s preferences and increase the expected reward from subsequent left-wing recommendations (roughly the second quarter of the episode). â€“The recommender predominantly recommends left-wing content to the (now more) left-wing user, maximising the high expected rewards that action will now oî€er (roughly the second half of the episode). Given user tamperingâ€™s learnability in this problem and the expected rewards from right-wing content here (low), the recommenderâ€™s propensity to nevertheless heavily recommend right-wing content before switching to left-wing recommendations for the remainder of the episode is a clear exploitation of user tampering. Moreover, the inverse behaviour has been learned for right-wing users â€“ the model is not blindly trying to polarise all users to the left, but has developed a sophisticated policy for identifying and exploiting the causal relationship between its actions and the userâ€™s exogenous variable. Further evidence to this eî€ect is given by the policy for the â€˜centristâ€™ user â€“ here, the plot shows clearly that the recommender has recognised that its actions have no discernable causal impacts by which the user could be tampered with, and so makes recommendations which are proportionate to the userâ€™s initial preferences. â€¢ The agent heavily exploits user tampering even though we were able to generate similar cumulative rewards with our crude â€˜baselineâ€™ policy.This adds weight to the safety concerns with respect to user tampering. It indicates that there exist other policies which do not exploit user tampering (although they may make a handful of â€˜polarisingâ€™ recommendations by chance) and which oî€er similar rewards to the one that the recommender learned; nonetheless, over several iterations of retraining, the policy consistently converged to the policy we have presented here (with small natural variations). This implies that in this environment, the unsafe policy is not only learned occasionally, but presents a likely direction of convergence for the learning algorithm. It is also worth establishing that the exploitation of user tampering in the learned policy was robust to simulated users not encountered during training. We generated the same policy plots for the recommender over 10000 evaluation episodes spent recommending to each user in a new, â€˜unseenâ€™ population: an â€˜extremely leftâ€™ user withğœƒ= (0.5, 0.05, 0.05), an â€˜extremely rightâ€™ user with ğœƒ= (0.05, 0.05, 0.5), a â€˜left anti-centristâ€™ user withğœƒ= (0.35, 0.05, 0.2), and a â€˜right anti-centristâ€™ user with ğœƒ= (0.2, 0.05, 0.35). Figure 5: Action probabilities at each time-step for each user in the â€˜unseenâ€™ population. These results are shown in Figure 5. Although these speciî€›c users were never encountered during training, the same unsafe strategies appear here; the three phases of user proî€›ling, then polarisation, and î€›nally preference satisfaction are clearly visible. These results, in combination with the previous sectionsâ€™ formalisations, justify the claims that user tampering is both almost unavoidably learnable for commercially viable media recommender systems built entirely with RL, and potentially highly unsafe in its eî€ects. Speciî€›cally, the primacy of user engagement in content recommendation makes achieving complete safety from user tampering inconvenient at best, and impossible at worst. Appendix E unpacks this claim further for the interested reader. This paper has substantiated concerns about the risks of emergent RL-based recommender systems with respect to user manipulation and polarisation. We have formalised these concerns as a causal property â€“ â€œuser tamperingâ€ â€“ that can be isolated and identiî€›ed within a recommendation algorithm, and shown that by designing an RL-based recommender which can account for the temporal nature of the recommendation problem, user tampering also necessarily becomes learnable. Moreover, we have shown that in a simple simulation environment inspired by recent polarisation research, a Q-Learning-based recommendation algorithm consistently learned a policy of exploiting user tampering â€“ which, in this context, took the form of the algorithm explicitly polarising our simulated â€˜users.â€™ This is obviously highly unethical, and the possibility of a similar policy emerging in real-world applications is a troubling takeaway from our î€›ndings. Due to a combination of technical and pragmatic limitations on what could be done diî€erently in RL-based recommender design, it is unlikely that commercially viable and safe recommenders based entirely on RL can be achieved, and this should be borne in mind when selecting future directions for advancement in media recommendation research & development.