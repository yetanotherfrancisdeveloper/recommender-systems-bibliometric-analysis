Scientiî€›c publishing heavily relies on the assessment of quality of submitted manuscripts by peer reviewers. Assigning a set of matching reviewers to a submission is a highly complex task which can be performed only by domain experts. We introduce RevASIDE, a reviewer recommendation system that assigns suitable sets of complementing reviewers from a predeî€›ned candidate pool without requiring manually deî€›ned reviewer proî€›les. Here, suitability includes not only reviewersâ€™ expertise, but also their authority in the target domain, their diversity in their areas of expertise and experience, and their interest in the topics of the manuscript. We present three new data sets for the expert search and reviewer set assignment tasks and compare the usefulness of simple text similarity methods to document embeddings for expert search. Furthermore, an quantitative evaluation demonstrates signiî€›cantly better results in reviewer set assignment compared to baselines. A qualitative evaluation also shows their superior perceived quality. reviewer assignment, recommendation system, expertise modelling Peer review is a popular method of ensuring scientiî€›c standards for conferences and journals. It requires the assignment of suitable experts for each submission, which is often done manually [4]. These reviewers then provide objective assessment of the manuscript and recommend accepting or rejecting the submission [6].All of this has to be performed in a tight time frame [3]. The continuously increasing number of submissions as well as the high complexity of the task even for experienced chairs of program committees or journal editors calls for fully automatic methods of expert assignment. Furthermore, it is not suî€œcient to focus on the quality of single reviewers, but a good set of complementing reviewers should be recommended for each manuscript. The reviewer assignment problem tackles the task of retrieving sets of suitable reviewers for manuscripts submitted to a venue. Even though the construction of sets of reviewers î€›tting submitted manuscripts has been studied frequently, most work focuses on construction of sets with the highest possible expertise but does not consider (m)any other aspects. Such other aspects could help reduce reviewersâ€™ work load and increase comprehensiveness of reviews. Additionally, actual human evaluation of the sets and thus a reliable conî€›rmation of results is generally not conducted. Numerous works [3,6,8,9,14,17,22,24] tackle the reviewer assignment problem in diî€erent ways, with slightly diî€erent deî€›nitions for the suitability of reviewers. While expertise of a reviewer with the topic of the manuscript [3,6,8,9,14,22,24] has been dominating in existing work, other features like authority [8,14], research interest [8] and diversity [6,14,17] were considered in some existing work, but not in a holistic way. Additionally, these aspects were deî€›ned heterogeneously in present works. We incorporate the following î€›ve aspects into our deî€›nition of suitability of reviewer sets: expertise of reviewers in general topics and methods of a submission, authority of reviewers in the domain of the manuscript, diversity in terms of reviewers diî€ering in their areas of expertise, interest of reviewers in the topics of the submission and diversity in terms of seniority aspects of the reviewer set. In this work, we embark on î€›nding the best reviewer sets for a submitted scientiî€›c paper from a predeî€›ned candidate pool in terms of these î€›ve aspects. Unlike some existing work [1,18], we explicitly do not require the manual deî€›nition of keywords or bids on manuscripts from reviewer candidates. To achieve this, we make two important contributions: 1) We propose and thoroughly evaluate RevASIDE, a new and completely automated technique for recommending sets of reviewers from a î€›xed set of candidates for single manuscripts. For this we introduce seniority as a completely new aspect and its combination with already established but redeî€›ned features. 2) We publish three diî€erent data sets suitable for expert search as well as reviewer set assignment. While we build on established expert retrieval methods to î€›nd reviewers with high expertise, our method is the î€›rst to incorporate all of the complementary factors authority, diversity, interest of candidates and seniority to solve the reviewer set assignment problem. Our approach consists of two steps. Step 1 identiî€›es topically relevant reviewers based on the similarity of their research direction to the manuscript, utilising expert search methods. Step 2 then assembles sets from these experts and determines the reviewer set that performs best in the î€›ve aspects. To the best of our knowledge, this is the î€›rst work that utilises the expert search task as a preparatory step for the reviewer set assignment task. Retrieval-based approaches for scientiî€›c reviewer assignment treat the manuscript for which reviewers are searched as a query. They determine î€›tting reviewers based on diî€erent aspects, often under additional constraints. Such methods can be divided into ones recommending single reviewers for manuscripts, so-called expert search, and those tackling the assignment of whole reviewer sets. Several papers target the recommendation of single reviewers for manuscripts which contrasts our goal of recommending reviewer sets. We identify and assemble the best î€›tting experts to a suitable set while the following works only handle the expert search task which disregards set eî€ects. Numerous works pursue the expert search task as a matching problem between the query manuscript and expert proî€›les formed by their past publications. Some of them also consider more aspects than textual similarity: MINARET [19] is a recommendation framework based on publications and aî€œliations of experts as well as expanded keywords for manuscripts. After an initial î€›ltering step, it returns ranked list of reviewers. Candidates receive a score based on topical coverage, impact, recency, experience in reviewing and their familiarity with the target venue. Chughtai et al. [4] suggest ontology-based and topic-speciî€›c recommendation of single experts î€›tting a submission. Macdonald and Ounis [15] propose twelve voting techniques to î€›nd suitable experts for query manuscripts. These techniques base on similarity of the reviewer candidates and the manuscripts. We use and extend their methods in Step 1 of our approach. Other works transform single expert î€›nding into a classiî€›cation problem: Yang et al. [23] base their approach on word-semantic relatedness via Wikipedia. Reviewers are ranked with respect to a manuscript by experience in the domain of the submission and their number of papers. Zhao et al. [26] utilise word embeddings of keywords from author proî€›les and manuscripts to propose î€›tting reviewers. Similar to this approach we use embedding methods to abstract from words while searching for reviewer candidates. Reviewer set recommendation can be observed for single papers or multiple/all papers of a venue. The following approaches tackle reviewer set recommendation but consider diî€erent or fewer aspects compared to RevASIDE for estimating the quality of reviewer sets. Ishag et al. [6] incorporate theâ„index of reviewers, citation counts and paper diversity into their approach based on itemset mining. They return reviewer sets î€›tting a query manuscript and estimate the setsâ€™ impact. Contrasting their deî€›nition of diversity which uses the number of diî€erent aî€œliations of authors of a single paper, we deî€›ne diversity as a measure between authors to estimate the actual topical diî€erences in reviewer sets. Maleszka et al. [17] tackle the reviewer set assignment problem for one manuscript at a time by focusing on diversity aspects in expertise, the co-authorship graph and style of reviewers. They begin the set recommendation process with a single reviewer determined by another method. Zhang et al. [24] utilise a multi-label classiî€›er for the construction of reviewer sets. The approach bases on predicted research labels for manuscripts and predicts reviewers with similar labels. Set-based eî€ects are ignored which contrasts our approach. Works tackling the reviewer set recommendation for multiple papers can be divided in ones relying on manual inputs such as bidding by reviewers and fully automated ones. Some of the papers incorporating manual inputs, contrasting our fully automated method, are the following: The Toronto Paper Matching System (TPMS) [3] conducts automatic reviewer assignment for all manuscripts submitted to a conference by using either word count representation or LDA topics, but can also incorporate reviewersâ€™ bids on submissions. TPMS supports some constraints: papers must be reviewed by three reviewers, and reviewers are assigned not more than a given limit of papers. Reviewers for manuscripts are determined based on expertise extracted from their publications. TPMS is applied, for example, by the SIGMOD research track [1], where reviewers upload a representative set of their publications. Papagelis et al. [18] present a system which incorporates reviewersâ€™ interests in terms of paper topics, their bids on papers, conî€icts of interests and overall workload balance for the reviewer assignment task. It can either assign reviewer sets automatically if the bidding is completed or the PC chair can manually adjust the sets. The following works are fully automated recommendation approaches intended to work with multiple manuscripts: Liu et al. [14] recommendğ‘›reviewers for each manuscript which are dependent on each other. They model reviewersâ€™ expertise, authority and diversity as a graph which they traverse with random walk with restart. The number of co-authorships is modelled as authority which contrasts our deî€›nition of authority. Kou et al. [9] introduce an assignment system for sets ofğ‘›reviewers which bases on the topic distributions of reviewers and the manuscripts computed with the Author-Topic Model. They deî€›ne expertise of reviewer sets in certain topics as the maximum expertise for the topic found in the set; our deî€›nition of expertise deviates. Jin et al [8] assume reviewers have a certain relevance in a topic which is determined by their publications and usage of the Author-Topic Model. Additionally, authority in form of citations and research interest of researchers are important factors. Here, the number of reviewers per paper and the maximum number of papers a reviewer is assigned to can be predeî€›ned. Amongst others we also observe these factors but deî€›ne them diî€erently. Yang et al. [22] utilise LDA to represent manuscripts as well as past publications of reviewer candidates. They then use a discrete optimisation model which focuses on expertise to assign reviewers to all manuscripts. Likewise, we also incorporate LDA in our approach but we additionally consider more aspects beyond expertise. In our work, we assess the appropriateness of a reviewer set with respect to a submission based on the following seven aspects: Aspect 1Reviewers in a reviewer set cannot have conî€icts of interests: they can be neither authors of the submission nor prior coauthors of its authors [18]. This aspect aims at ensuring unbiased and objective candidates. While we (as well as others [18]) regard this aspect quite vigorously, less restrictive variants (e.g. disallowing co-authorships in the three years prior to the submission) are also feasible. Aspect 2Reviewers cannot be co-authors of any other reviewer in the set. Reviewers having disjoint publications enforces a broader spectrum of diî€erent backgrounds. This could produce broader reviews [17] which is a desirable property in peer review [1]. Aspect 3Reviewers need to be experienced in the area of the manuscript [9]. The topic of the paper should be relevant for them and î€›t their research proî€›le. Not only the content but also the number of papers in the area of a submission contributes to our understanding of experience. This aspect ensures deep reviews, another desirable feature of assessments [1]. Aspect 4Reviewers need to hold authority in the research area of the submission. Reviews of the papers have to be credible, reviewers should be well recognised in the target domain [14]. Authority can be assessed, for example, by an area-dependentâ„index and citation counts of candidates. Aspect 5Reviewers need to be diverse in their area of expertise. Typically, as many topics as possible of a submission should be assessed to create a comprehensive review [1]. Reviewers that are proî€›cient in diî€erent topics from each other support this goal as the candidates in a set have unique perspectives formed by their diî€erent experiences and backgrounds [17]. Aspect 6Reviewers need to be currently interested in the topics of the manuscripts so they accept the reviewing request [8] and are not asked to review topics they no longer work in. Scientiî€›c progress makes it impossible to be up to date in all areas they were formerly interested in. Thus, time-aware suggestion should weigh recent works of reviewers much higher than older publications. Aspect 7Reviewers of a manuscript should not solely consist of senior researchers, but they need to be diverse with respect to the amount of their experience. Senior researchers provide vast reviewing experience and a global vision but they should be handled as a sparse resource as they are asked to review many submissions. Junior researchers are ambitious and resilient while not having that much experience. Usually, they are less frequently asked to review and more of an unexhausted resource. Reviewing load needs to be distributed between senior and junior researchers such that the lower load for senior researchers and incorporation of newer researchers beneî€›ts the overall quality of reviews. Additionally, junior researchers could provide new and refreshing perspectives while the reviewing activity might also beneî€›t their own development. Breaking up well-established reviewer constellations with new candidates could also avoid research cliques [3]. RevASIDE is a system for assigning sets ofReviewers utilising Authority,Seniority,Interest,Diversity andExpertise of reviewers to î€›nd the most suitable reviewer set out of a î€›xed set of candidates, the reviewer candidate poolğ‘…ğ¶ğ‘ƒ, for a given manuscriptğ‘€. Our approach is composed of two steps: in Step 1, suitable reviewers are identiî€›ed from the pool of reviewer candidates; in Step 2, they are assembled to the most suitable set for the manuscript. Figure 1 depicts the schematic overview of our approach. Step 1 handles the left part of Figure 1. We represent publications as tf-idf vectors or ones constructed with BERT [5] or Doc2Vec [12], which allows to depict semantics of documents instead of single tokens. This enables capturing similarity of concepts of papers. Letğ‘€be the manuscript for which a reviewer set should be computed. We ignore any reviewers for which a conî€ict of interest with the authors ofğ‘€exists (Aspect 1). For the remaining reviewers from the reviewer candidate poolğ‘…ğ¶ğ‘ƒ, letğ‘ƒ (ğ‘…)be the set of publications written by reviewerğ‘…. The similarity between a publicationğ‘ƒand a manuscriptğ‘€is given byğ‘ ğ‘–ğ‘š(ğ‘ƒ, ğ‘€); the utilised similarity measure Figure 1: Schematic overview of our approach. The left part depicts the expert search task, the right part depicts the set of reviewers assignment task. Table 1: Voting techniques V T and accompanying formulas for reviewer ğ‘… and manuscript ğ‘€. ğ‘†ğ‘ˆ ğ‘€ğ‘ ğ‘–ğ‘š(ğ‘ƒ, ğ‘€) ğ‘†ğ‘ˆ ğ‘€ğ‘ ğ‘–ğ‘š(ğ‘ƒ, ğ‘€)î€€î€ ğµğ‘œğ‘Ÿğ‘‘ğ‘ğ¹ğ‘¢ğ‘ ğ‘’(|ğ‘ƒ (ğ‘…)| âˆ’ ğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘ƒ, ğ‘€))Ã ğ‘’ğ‘¥ğ‘|ğ‘ƒ (ğ‘…)| âˆ—ğ‘’ can be changed between the two steps. In our experiments, we will use the cosine similarity of the corresponding vectors. We then sort ğ‘…â€™s papers in descending order by their similarity to the manuscript ğ‘€and denote byğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘ƒ, ğ‘…, ğ‘€)the rank of a certain publicationğ‘ƒ of reviewerğ‘…in this order. Similarly, we sort all publications in the collection in descending order by their similarity to manuscriptğ‘€ and denote byğ‘Ÿğ‘ğ‘›ğ‘˜(ğ‘ƒ, ğ‘€)the rank of a publicationğ‘ƒin this order. To obtain a ranked listğ‘…ğ¿of reviewers, we apply a number of voting techniques (VTs) that score reviewer candidates with respect to a manuscript. These voting techniques base on the ones applied by Macdonald and Ounis [15] for expert search. Table 1 shows the exact formulas for the 13 voting techniques considered in our approach. Higher scores signal better î€›t of a reviewer to the given manuscript.ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ computes the number of papers of a reviewer with a similarity to the query manuscript not smaller than a thresholdğ›¿; note that the method was introduced without such a threshold in [15], which corresponds toğ›¿ =0 in our deî€›nition. ğ‘†ğ‘ˆ ğ‘€sums up the similarities of the papers of a reviewer with the query manuscript,ğ´ğ‘‰ ğºuses this score and normalizes it by the total number of papers of the reviewer.ğ‘€ğ‘ğ‘multiplies the ğ‘†ğ‘ˆ ğ‘€score by the number of papers of the reviewer.ğ‘†ğ‘ˆ ğ‘€sums the similarities of theğ‘›papers of the reviewer most similar to the manuscript.ğ‘€ğ¼ ğ‘returns the smallest similarity of the reviewerâ€™s paper with the manuscript,ğ‘€ğ´ğ‘‹is deî€›ned analogously.ğ‘…ğ‘…sums up the reciprocal ranks of the reviewerâ€™s papers in the ordered list of all papers. We additionally introduceğ‘šğ‘…ğ‘…which normalizes this score by the number of papers written by the reviewer.ğµğ‘œğ‘Ÿğ‘‘ğ‘ğ¹ğ‘¢ğ‘ ğ‘’ utilises Borda-fuse as score. The three voting techniquesğ‘’ğ‘¥ğ‘, ğ‘’ğ‘¥ğ‘andğ‘’ğ‘¥ğ‘are deî€›ned as their non-exponential forms but instead of using similarities, they apply the exponential function on similarities. For a î€›xed voting technique, this step generates a ranked listğ‘…ğ¿ of reviewers, i.e. experts, î€›tting the manuscript in question. Step 2 handles the right part of Figure 1, i.e. the actual formation of reviewer sets for manuscriptğ‘€based on the ranked listğ‘…ğ¿of reviewers generated in Step 1. We denote the topğ‘˜reviewers from ğ‘…ğ¿byğ‘…ğ¿; ifğ‘˜ = |ğ‘…ğ¿|, the î€›rst step becomes irrelevant. A smaller ğ‘˜restricts the observed candidates in the second step drastically and is especially useful to improve runtime. We now represent documents by term-based vectors weighted with tf-idf and by topic-based vectors computed with LDA [2]; this allows us to capture concrete terms as well as general topics of publications of reviewer candidates and the submission.Additionally, these document vector representations allow us to easily weight and combine vectors of publications without destroying their expressiveness as each vector dimension represents a single token or topic which can be present in a document to a certain extent. This starkly contrasts BERT or Doc2Vec embeddings, where single dimensions do not have a comprehensible semantics but instead the combination of all dimensions represents a document entirely. These tf-idf and LDA vectors can be constructed either on all parts of manuscripts or only on the technical sections, which consist of the methodology as well as the evaluation. For each reviewerğ‘…this step considers the setğ‘Ÿ(ğ‘…, ğ‘€)of her publications whose similarity to manuscriptğ‘€is not lower than a thresholdğ‘¡; i.e.ğ‘Ÿ(ğ‘…, ğ‘€) = {ğ‘ƒ |ğ‘ƒ âˆˆ ğ‘ƒ (ğ‘…) âˆ§ ğ‘ ğ‘–ğ‘š(ğ‘ƒ, ğ‘€) â‰¥ ğ‘¡}, with ğ‘¡ âˆˆ [0,1]. The threshold is utilised to deî€›ne the selectivity of the research area relevant for the submission. Ifğ‘¡ =0 all papers of a reviewer are included, a value closer to 1 restricts the number of papers taken into account in the second step. We assume similarities lie in [0, 1]. Letğ‘Ÿğ‘’ğ‘(ğ‘ƒ, ğ‘‰ )be the representation of publicationğ‘ƒas a vector of typeğ‘‰ âˆˆ {ğ¿,ğ‘‡ }withğ¿representing LDA vectors andğ‘‡ representing tf-idf vectors. Both document vector representations (DVs) can be used to computeğ‘Ÿ(ğ‘…, ğ‘€), e.g. using the cosine of the corresponding vectors as similarity function. Lastly, letğ‘ƒ=be the length normalized aggregation vector of typeğ‘‰that combines all information on relevant publications of a reviewer ğ‘… with respect to ğ‘€. We now consider all possible candidate reviewer sets of a predeî€›ned size (for example 3) and assess, for each candidate setğ‘…, its suitability with respect to the aspects deî€›ned in Section 3. We prohibit reviewers in a setğ‘…to be co-authors of each other (Aspect 2); sets that include such reviewers are not considered further, they are assigned a î€›nalğ‘ ğ‘ğ‘œğ‘Ÿğ‘’of 0. In addition, we observe î€›ve diî€erent quantiî€›able aspects for suitability for each such setğ‘…of reviewer candidates. These reviewers are taken fromğ‘…ğ¿produced in Step 1. Scores for all aspects are normalised to[0,1]with 1 being the best and 0 being the worst possible value. 4.2.1 Expertise E. Expertise describes the relevance of the reviewers in a set to the manuscript (Aspect 3). Reviewers should have solid knowledge with terms and topics of the manuscript substantiated by numerous publications. Particularly, the submission should be similar to publications written by the reviewers [14] and their number of such papers should be high. Contrasting Liu et al.â€™s work [14] we use the number of co-authorships of reviewer candidates not as an indicator of authority but rather as an indicator of expertise. These conditions are measured by the following scores: These scores are then linearly combined to the î€›nal expertise score, with ğœ–âˆˆ [0, 1] weighting parameters and ğœ–+ ğœ–+ ğœ–= 1: ğ¸(ğ‘…, ğ‘€, ğ‘¡) = ğœ–ğ¸(ğ‘…, ğ‘€, ğ‘¡) + ğœ–ğ¸(ğ‘…, ğ‘€, ğ‘¡) + ğœ–ğ¸(ğ‘…, ğ‘€, ğ‘¡) 4.2.2 Authority A. Reviewers should hold authority in the area the manuscript belongs to (Aspect 4). We propose two scores to measure authority: the averageâ„index of reviewers [14]â„(ğ‘…, ğ‘€, ğ‘¡ ) calculated on papers relevant to the manuscriptğ‘Ÿ(ğ‘…, ğ‘€)(measured byğ´), and the average number of their obtained citations on these papers (measured by ğ´): ğ´(ğ‘…, ğ‘€, ğ‘¡) =|ğ‘…| Â· maxÃğ‘ (ğ‘ƒ) withğ‘ (ğ‘ƒ)being the number of citations a paperğ‘ƒhas obtained. These scores are then linearly combined to the î€›nal authority score, with ğ›¼ âˆˆ [0, 1] a weighting parameter: ğ´(ğ‘…, ğ‘€, ğ‘¡) = ğ›¼ğ´(ğ‘…, ğ‘€, ğ‘¡) + (1 âˆ’ ğ›¼)ğ´(ğ‘…, ğ‘€, ğ‘¡) 4.2.3 Diversity D. We deî€›ne diversity as a measure to ensure that the expertise of reviewers is distributed to areas as disjunct as possible (Aspect 5). This allows for reviews to cover multiple aspects of the manuscript. The corresponding score rewards if topics in which reviewers are proî€›cient overlap as little as possible [14]: ğ· (ğ‘…, ğ‘€, ğ‘¡) = 1 âˆ’|ğ‘…| Â· (|ğ‘…| âˆ’ 1)/2 4.2.4 Interest I. As research objectives of scientists change over time, interest measures the î€›t of reviewers and the manuscript with respect to their temporal development (Aspect 6). Interest of reviewers denotes their willingness to review submissions from certain areas [8]. These interests change over time. If a reviewer was involved in a topic several years ago but then changed her focus, she probably no longer follows the rapid developments in the former research area. Thus she might not be willing or even able to review current submissions from this area. To represent the time-aware proî€›les of reviewers, we combine the publications of reviewers with regards to their age to a length-normalized vector where recent papers are weighted stronger than older ones. This measure works on topical representations of documents: with ğ‘(ğ‘ƒ) describing the age of a publication ğ‘ƒ in years. 4.2.5 Seniority S. In terms of seniority, reviewer sets are desirable which do not solely consist of senior researchers (Aspect 7). In the recommended group of candidates, at least one senior researcher should be contained who is familiar with the methodology of the paper [1] (measured byğ‘†). Further it is desirable to have a diverse group in terms of seniority, the set should include at least one junior researcher (measured byğ‘†). These requisitions are modelled in the following equations: ğ‘†(ğ‘…, ğ‘€, ğ‘¡) = ğ‘šğ‘–ğ‘›ğ‘šğ‘ğ‘¥ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’(ğ‘…, ğ‘€, ğ‘¡)ğ‘ğ‘¢ğ‘ğ‘›ğ‘¡ğ‘–ğ‘™ğ‘’ğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’(ğ‘…, ğ‘€, ğ‘¡), 1 withğ‘Ÿğ‘ğ‘›ğ‘”ğ‘’(ğ‘…, ğ‘€, ğ‘¡) =1+ ğ‘šğ‘ğ‘¥ğ‘(ğ‘ƒ) âˆ’ ğ‘šğ‘–ğ‘›ğ‘(ğ‘ƒ) denoting the temporal range in which reviewerğ‘…has published on topics relevant toğ‘€. These scores are then linearly combined to the î€›nal seniority score, with ğœ âˆˆ [0, 1] a weighting parameter: ğ‘† (ğ‘…, ğ‘€, ğ‘¡) = ğœğ‘†(ğ‘…, ğ‘€, ğ‘¡) + (1 âˆ’ ğœ)ğ‘†(ğ‘…, ğ‘€, ğ‘¡) 4.2.6 Final Equation. We combine all of these î€›ve quantiî€›able aspects to obtain a single scoreğ‘†ğ¶for each reviewer sets. Good reviewer sets will have high values in all aspects; we thus multiply the per-aspect scores: ğ‘†ğ¶ (ğ‘…, ğ‘€, ğ‘¡) =ğ´(ğ‘…, ğ‘€, ğ‘¡) Â· ğ‘† (ğ‘…, ğ‘€, ğ‘¡) Â· ğ¼ (ğ‘…, ğ‘€, ğ‘¡) The candidate reviewer setğ‘…achieving the highestğ‘†ğ¶is the most suitable one and recommended for the manuscript as result of Step 2. We will denote this result as ğ‘…in the experimental evaluation. To evaluate our proposed reviewer set recommendation approach, we develop three novel evaluation data sets. We consider manuscripts from three diî€erent workshops and conferences of diî€erent size and thematic focus that took place in 2017, namely MOL, BTW, and ECIR. As it is practically impossible to obtain all papers submitted to a conference, we use all accepted papers as an approximation instead. Note that this might lead to non-representative topic distributions of manuscripts and unrealistically low number of manuscripts to be reviewed. Additional fuzziness is introduced since we do not distinguish between long, short and demo papers as program committees are oftentimes published in a merged form. We built three diî€erent data setsbased on data from dblp [13] which was merged with abstracts, citations and references from the AMiner part of the Open Academic Graph [20,21]where available as well as full texts of accepted manuscripts. Information from AMiner was joined with dblp data (based on matching DOIs where available, or on matching paper titles, author names and publication years otherwise); this allowed to focus on publications from computer science or adjacent domains and to build rather precise reviewer proî€›les due to dblpâ€™s author disambiguation eî€orts, compared to using reviewer names only. Full texts of accepted manuscripts are not included in the AMiner data set but stem from pdfs collected by hand which were converted to text î€›les using Science Parse. Information on program committees was either taken from conference web sites or conference proceedings. Reviewer names were manually mapped to dblp authors.For each reviewer, we set up a list of her publications identiî€›ed by their dblp keys. Here, only papers up to 2016 were taken into consideration, corresponding to a reviewer selection process in early 2017. For each of the papers the data set contains its publication year, the paper length, the CORE rankof the venue it was published in, the number of citations it accumulated and the averageâ„index of its authors. The concatenated title and abstract (where available) of papers needed to consist of at least three terms to be considered for the data set. Citing papers which are not contained in dblp were omitted. Thus, the number of incoming links might not necessarily represent the number of citations which publications received in the real world. This inî€uences the number of citations and the average â„ index. For each manuscript of our three test conferences the data sets contain a pool of possible reviewers. It consists of all members of the program committee, but excludes those with obvious conî€icts of interest accessible by (former) co-authorships of authors of the manuscripts and reviewers. For each of the papers published by possible reviewers, our data sets also contain tf-idf, Doc2Vec [12], LDA [2] and BERT [5] vector representations of its title and abstract where available. For submitted manuscripts, these four kinds of document representation are contained for the full text as well as only the research sections of the paper (which consist of all sections excluding the abstract, introduction, related work, conclusion, references and acknowledgements). The textual content of the papers is not contained. We consider only English documents for the construction of our data sets. We calculated the document frequencies of words for tf-idf on unstemmed titles of all publications contained in dblp up to 2016 concatenated with abstracts from AMiner where available which were written in English. In total we used 2,940,996 documents. The î€›nal tf-idf vectors are calculated for unstemmed textual data available in the respective data sets including all papers of reviewers and submitted manuscripts. For the construction of BERT [5] vectors, we used the base pretrained uncased model.Since the BERT implementation used is only able to process input vectors of at most 512 tokens, documents were cut at punctuation marks or after half of the tokens if sentences were still too long. A sliding window was used to always input two consecutive sentences to maintain as much context as possible. The model consists of overall twelve hidden layers each having 768 features. The last four layers from these twelve layers were concatenated for each token and averaged over all tokens to receive vectors of length 4 layersÃ—768 features = 3072 dimensions for each publication. [10] Weights for Doc2Vec [12] are trained on the English Wikipedia corpus from 1st February 2020. We refrained from using Doc2Vec on a stemmed corpus as this preprocessing is no prerequisite for achieving good results [12]. We trained two Doc2Vec models, one distributed bag of words (DBOW) and one distributed memory (DM) model, so that resulting vectors consist of 300 dimensions each. This size was proposed by Lau and Baldwin [11] for general-purpose applications.[10] For LDA [2] we again used the 2,940,996 documents which we already utilised for the computation of the document frequency in tfidf. This procedure ensured the computed topics were from the area of computer science. The number of topics was set to 100 resulting in the same number of dimensions for vector representations of manuscripts and publications.[10] MOLâ€™17. The data set contains 12 manuscripts in English language which were accepted at Me eting on the Mathematics of Language â€™17, 22 program committee members and their papers in dblp. We excluded extended abstracts. No distinction between diî€erent paper types and program committees was made. On average each manuscript has 21 possible reviewers which do not have conî€icts of interests. This data set represents a small biannual international conference with a diî€erent focus than the other two data sets. BTWâ€™17. The data set contains 36 manuscripts in English language which were accepted at Datenbanksysteme fÃ¼r Business, Technologie und Web â€™17 (the German database conference), 56 program committee members and their papers in dblp. We again excluded extended abstracts. No distinction between diî€erent paper types was made but the program committees members are split in scientiî€›c, industry and demo paper committee. On average each manuscript has 47.78 possible reviewers which do not have conî€icts of interests. This data set represents a medium sized biannual national conference with several lesser-known reviewers. ECIRâ€™17. The data set contains 80 manuscripts in English language which were accepted at European Conference on Information Retrieval â€™17, 151 program committee members and their papers in dblp. A distinction between full-paper meta-reviewers, full-paper program committee, short paper program committee and demonstration reviewers was made. On average each manuscript has 141.35 possible reviewers which do not have conî€icts of interests. This data set represents a medium to large annual European conference attributed with CORE rank A and mostly well-known reviewers. In our experiments, we solely focus on sets consisting of three reviewers even though our approach is applicable for diî€erent numbers of reviewers per manuscript as well. This number was chosen as a widespread norm [3] to reduce the dimensionality of further evaluation steps. We evaluate our approach on the three introduced data sets MOLâ€™17, BTWâ€™17 and ECIRâ€™17 where we disregard the diî€erent manuscript and committee types. By observing the performance of our approach in venues of diî€erent sizes, we strive to make assumptions on its general applicability. We use Cosine similarity as similarity measure. This ensures similarity values in[0,1]for Step 2 as tf-idf and LDA document vector representations hold non-negative values for all dimensions. For the voting techniques of the algorithm we run tests withğ‘› âˆˆ {5,10} and ğ›¿ âˆˆ {0, .25, .5, .9}. For all signiî€›cance tests, we use ağ‘-value of .05. We evaluate the normal distribution of values using Kolmogorov-Smirnov tests and test the homogeneity of variances with Leveneâ€™s tests. All depicted values are rounded on four decimal places. Considering the overall challenges and goals of RevASIDE, we investigate the following six hypotheses: ğ»Step 1 is useful for the expert search task. ğ»Usage of more advanced document vector representations leads to signiî€›cantly better overall results for Step 1 compared to more basic ones. ğ»Utilisation of diî€erent document vector representations, voting techniques, cutoî€ valuesğ‘˜of the result listğ‘…ğ¿, content types and thresholdsğ‘¡leads to signiî€›cantly diî€erent overall RevASIDE scores and values for the î€›ve quantiî€›able aspects in Step 2. ğ»Utilisation of the full texts of manuscripts leads to worse overall results than restriction of the manuscriptsâ€™ content to the technical sections in Step 2. ğ»The conduction of Step 1 is proî€›table for Step 2. ğ»Results of Step 2 are conî€›rmed by human assessment, thus RevASIDE is useful for the reviewer set assignment task. In this part of the evaluation we intend to assess hypothesesğ»of Step 1 being useful for the expert search task andğ»of utilisation of more advanced DVs producing better results. We randomly selected 20 manuscripts from each of the BTWâ€™17 and ECIRâ€™17 data sets. The manuscripts are represented by their full texts, the proî€›les of reviewers are represented by their papersâ€™ titles and abstracts where available. To create a ground-truth of relevant reviewers, the top 10 reviewer candidates are computed with all 13 (17 with variants) voting techniques and combined. The resulting pools of reviewers for each manuscript from the BTWâ€™17 data set contained 48.35 entries on average and 101.5 entries on average for manuscripts from ECIRâ€™17. In the former case, about all possible reviewers were contained in the respective lists contrasting the ECIRâ€™17 lists which contain a lower percentage of possible reviewers. Unfortunately, a more extensive manual evaluation with more manuscripts would not be feasible. The manuscriptsâ€™ title and abstract as well as the potential reviewers and a link to their dblp proî€›le were presented to an independent senior researcher in the î€›eld who evaluated the reviewers in terms of appropriateness for the given manuscript. For the manual evaluation of relevance, only papers up to 2016 of reviewers were considered. The expert was not aware which method retrieved which reviewers. If the expert observed missing relevant reviewers, they were also included in the ground-truth. In BTWâ€™17, each paper has 10.05 relevant reviewers on average (min=5, max=14, median=10, standard deviation=2.762). In ECIRâ€™17, each paper has 27.2 relevant reviewers on average (min=3, max=55, median=25, standard deviation=13.5671). On average, a reviewer from the program committee is relevant for 3.5893 manuscripts for BTWâ€™17 and 3.1813 manuscripts for ECIRâ€™17. We report result quality with three established metrics, examining the î€›rst 10 retrieved reviewers of each method. Precision@10 measures the fraction of the top-10 recommended reviewers that were actually relevant. Non-interpolated mean average precision@10 (MAP) averages the precision at ranks where a relevant reviewer appears, using a precision of 0 for each relevant reviewer not appearing in the result list. Normalized cumulative discounted gain (nDCG) [7] aggregates relevance of all reviewers appearing in the result, but with a logarithmic discount for later ranks; this follows the intuition that later ranks are less important to a user than earlier ranks. In addition, it normalizes this aggregation by the cumulative discounted gain achieved by an ideal ranking where all relevant reviewers appear in front, thus showing how close the result is to an optimal result and allowing to compare across diî€erent queries with diî€erent numbers of relevant results. The upper part of Table 2 shows result quality for all combinations of document vector representation and voting technique for the twenty manuscripts from BTWâ€™17.ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ is exactly the same for each document vector representation as this voting technique solely considers the number of papers of reviewer candidates and not their similarity with query manuscripts. The lower part of Table 2 shows the same for the twenty manuscripts from ECIRâ€™17. In BTWâ€™17, each paper has 2.7801 relevant reviewers per combination of VT and DV on average, in ECIRâ€™17 this value is signiî€›cantly (Mann-Whitneyğ‘ˆtest) higher (3.4838). These assessments lead to the assumption of the VTs and DVs presented here being useful for the expert search task and therefore verifying ğ». We found signiî€›cant (Kruskal-Wallisğ»tests) diî€erences between the four DVs for several voting techniques, but not for all of them (see rightmost column of Table 2). The more advanced document vector representations Doc2Vec and especially BERT did not achieve better results than tf-idf. The best voting techniques seem to depend on the data set and the utilised document vector representation. BERT performs worse than both tf-idf and the Doc2Vec models. Usage of tf-idf and DM achieves comparable results for the best performing VTs for BTWâ€™17; for ECIRâ€™17, tf-idf and DBOW with their respective best VTs result in similar values. BERT seems to generalise the concepts of papers too much such that the VTs cannot clearly distinguish between relevant and non-relevant reviewers. This is underlined by the fact that three versions ofğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ generate the same values for MAP, P@10 as well as nDCG. Tf-idf has high selectivity and is able to identify experts versed in the exact same techniques described in a manuscript. Hence, hypothesisğ»of more sophisticated VRs being more suitable than basic VRs is rejected. For the ECIRâ€™17 data set, P@10 and nDCG are higher than for BTWâ€™17. This might be caused by ECIRâ€™17 having higher overall numbers of reviewers as well as more relevant reviewers per manuscript. This disadvantages the smaller BTWâ€™17 data set. The evaluation of Step 2 of our algorithm consists of a quantitative and a qualitative evaluation. In Equation 1 we setğœ–= ğœ–= ğœ–= and ğ›¼ = ğœ = .5. As a î€›rst baselineğµ, the three highest ranked reviewers in the ranked listğ‘…ğ¿for each VT and DV are considered as a reviewer set for a manuscript. Such an approach is common in reviewer set recommendation [14,24]. Our second baselineğµchooses three random reviewers fromğ‘…ğ¿. Our third baselineğµchooses three random reviewers from the whole program committee, excluding only those with a conî€ict of interest. For the latter, we cap values of ğ¸, ğ´and ğ´at 1. We experiment with cutoî€sğ‘˜of reviewers inğ‘…ğ¿to generate ğ‘…ğ¿at position 10 and 20 after Step 1 and without cutoî€, i.e. all reviewers without conî€icts of interests for the manuscripts were utilised as a comparison to evaluate the usefulness of Step 1. If we do not restrict the number of candidate reviewers, i.e.|ğ‘…ğ¿| = ğ‘˜, the voting technique used in Step 1 (which determines the reviewer candidates considered in Step 2) becomes irrelevant for Step 2 but still inî€uences the creation of the baselines. We also experiment with diî€erent thresholds ğ‘¡ âˆˆ {0, .25, .5, .9}. Table 2: Mean average precision@10 (MAP), precision@10 (P@10) and nDCG@10 (nDCG) for all combinations of voting techniques (VT) and document vector representations of manuscripts from BTWâ€™17 (upper half) and ECIRâ€™17 (lower half). Best combination in BTWâ€™17: tf-idf + ğ‘†ğ‘ˆ ğ‘€ (short ğ‘). Best combinations in ECIRâ€™17: tf-idf + ğ‘€ğ‘ğ‘ (short ğ‘’ (short ğ‘’). Column sig diî€ gives information on whether or not MAP (m), P@10 and nDCG (n) signiî€›cantly diî€er between the diî€erent DVs. If âœ“, all three measures are signiî€›cantly diî€erent. We divide the manuscripts in non-technical and research sections to better estimate their true content. Non-technical sections include abstract, introduction, related work, conclusion, acknowledgements and references. Research sections are all other parts. We compare the eî€ect of using the full text in Step 2 to using only the content of research sections. Proî€›les of reviewers are represented by their papersâ€™ titles and abstracts where available which are similar enough (threshold ğ‘¡) to the query manuscript. 6.3.1 î€antitative Evaluation. In this part of the evaluation we focus on understanding the inî€uence of the diî€erent factors of our approach and prepare the qualitative evaluation by identifying the combinations achieving the highest scores. In this context we intend to assess hypothesesğ»andğ»as well asğ»which observes the usefulness of Step 1. In these experiments, for each combination of document vector representation in Step 1, voting technique, cutoî€ of relevant reviewers utilised in Step 2, similarity thresholdğ‘¡in Step 2 as well as used Table 3: Signiî€›cant diî€erences between the groups in ğ‘†ğ¶ as well as the î€›ve quantiî€›able aspects by data sets MOLâ€™17 (m), BTWâ€™17 (b) and ECIRâ€™17 (e). content type (CT) in Step 2 we observe the following result types (RT): the three baselines (ğµ,ğµ,ğµ) and the best result returned by RevASIDE (ğ‘…). We test for signiî€›cant diî€erences between groups of experiments to determine which factors really inî€uence the overall score ğ‘†ğ¶(as computed by Equation 1) and the î€›ve quantiî€›able aspects introduced in Sections 4.2.1 to 4.2.5. Kruskal-Wallisğ»tests are used for the following experiments since in most of our observed cases, data is not normally distributed in the diî€erent groups or variances are not homogeneous. Table 3 indicates between which groups of experiments we found signiî€›cant diî€erences in the scores or the î€›ve quantiî€›able aspects. We observe 1,632 (4 DVsÃ—17 VTsÃ—3 cutoî€sğ‘˜Ã—2 CTsÃ—4ğ‘¡in Step 2) experimental setups per data set. Experiments were grouped by document vector type such that there were four groups of experiments, ones using tf-idf in the î€›rst step, ones using Doc2Vec DM, ones using Doc2Vec DBOW and ones using BERT document vector representations. Grouping by VT in Step 1 results in 17 diî€erent groups of experiments. When experiments are grouped by the number of observed candidates ğ‘˜three diî€erent groups result. When grouping by content type, two groups of experiments result, ones which utilise the full text in Step 2 and ones utilising only the research sections of the query manuscript. Grouping by the threshold value ğ‘¡ in Step 2 results in four diî€erent groups. Lastly, grouping by RT produces four groups containing experiments of types ğµ, ğµ, ğµand ğ‘…. DV does inî€uence some aspects signiî€›cantly but overall, the scores of the ECIRâ€™17 data set are not signiî€›cantly inî€uenced by it. VT signiî€›cantly inî€uences the î€›ve aspects for all data sets as well as the score for the two smaller ones. The content type which is utilised in Step 2 is signiî€›cantly inî€uential for values for all data sets except for authority, diversity, and seniority. These values are not calculated by directly utilising the query manuscript and therefore are not inî€uenced by the content type. The cutoî€ valueğ‘˜ which is chosen forğ‘…ğ¿, the threshold valueğ‘¡as well the result types signiî€›cantly inî€uence the results in all three data sets. From these observations we derive the overall validity of hypothesisğ». Table 4 shows the best combinations of DV, VT, cutoî€ values, content type and threshold, measured in terms of the highest overall average scores forğ‘…and the three baselinesğµ,ğµandğµfor each of the three data sets.ğ‘†ğ¶is calculated with Equation 1 and can take take values between 0 and 1 with 1 being the best. As it is multiplicative, a score of.05 can be reached if e.g. values of all quantiî€›able aspects ğ´, ğ‘†, ğ¼, ğ·, and ğ¸ are around .55. ğ‘…achieves the highestğ‘†ğ¶results for each data set. This, together with the signiî€›cant diî€erences between result types observed in the previous experiment (see Table 3), leads to the conclusion that RevASIDE produces signiî€›cantly higher averageğ‘†ğ¶scores than the baselines. This applies to all three diî€erent sized data sets which highlights the general applicability of our approach. Utilising full texts of query manuscripts yields better results than only taking the research sections into account. This leads to the rejection of hypothesis ğ». The restriction ofğ‘…ğ¿toğ‘˜ =10 leads to the best average scores for MOLâ€™17 and ECIRâ€™17; for BTWâ€™17, no restriction ofğ‘…ğ¿leads to the highest average scores (not depicted in the table). This indicates that the reduction of the number of considered reviewers for Step 2 (and therefore the entirety of Step 1) is a major factor in small and large data sets. It also decreases the overall computation time which in general veriî€›esğ». MOLâ€™17 as well as ECIRâ€™17 represent relatively focused areas while BTWâ€™17 is more diverse. For focused data sets it suî€œces to regard the few most relevant reviewers to compose a suitable set but for a diverse conference, it seems more reviewers need to be considered. When grouping all 1,632 experiments by voting technique and threshold, the highest average scores for MOLâ€™17 are achieved byğ‘’ğ‘¥ğ‘and .5; for BTWâ€™17ğ‘†ğ‘ˆ ğ‘€and .25; and for ECIRâ€™17ğ‘†ğ‘ˆ ğ‘€and .9. BERT is the DV which on average performs best for each data set. They outperform the other VTs and thresholds on average but do not appear as a combination in Table 4 under the overall best conî€›gurations. Remarkably, the best results forğ‘…in MOLâ€™17 as well as BTWâ€™17 were achieved by the same combination of DV, VT, CT,ğ‘˜ as well asğ‘¡. The combination of BERT withğ‘€ğ¼ ğ‘orğ‘šğ‘…ğ‘…did not achieve any good results in our manual evaluation of Step 1 but did prove to be useful in Step 2. The highest scores for MOLâ€™17 (.0516), BTWâ€™17 (.0462) as well as ECIRâ€™17 (.0399) for the best performing combinations from Step 1 of our approach (ğ‘: tf-idf + SUM,ğ‘’: tf-idf +ğ‘€ğ‘ğ‘,ğ‘’: DBOW +ğ‘‰ğ‘œğ‘¡ğ‘’ğ‘ ) are independent of DV and VT as they are achieved by|ğ‘…ğ¿| = |ğ‘…ğ¿|. The thresholdğ‘¡is set to .5. These results cannot surpass the best conî€›gurations from Table 4 for the same data but also do not signiî€›cantly diî€er from them. For BTWâ€™17 as well as ECIRâ€™17, we found no signiî€›cant correlation between the scores produced by the twelve (eleven asğ‘=ğ‘) best conî€›gurations from Table 4 forğ‘…and the number of relevant reviewers per manuscript for the forty manuscripts observed in the evaluation of Step 1 with Kendallâ€™s ğœ. We want to point to the fact that some of the DVs and VTs present in Table 4 achieve low results in the evaluation of Step 1 (for BTWâ€™17 .0168 to .171 in MAP, .08 to .34 in P@10 and .0622 to .3677 in nDCG; for ECIRâ€™17 .0264 to .1116 in MAP, .135 to .45 in P@10 and .1353 to .4748 in nDCG). This hints at possible problems with aspects with opposing objectives which will be regarded in depth in the following qualitative evaluation. 6.3.2 î€alitative Evaluation. In this part of the evaluation we assess hypothesisğ»which covers the manual assessment of the sets resulting from Step 2 and RevASIDEâ€™s overall usefulness. In our î€›rst qualitative evaluation of Step 2, we examine the eleven (asğ‘= ğ‘) conî€›gurations which performed best for the diî€erent result types from the three data sets (see Table 4) in the Table 4: Conî€›guration (conf), DV, VT, ğ‘…ğ¿cutoî€ value ğ‘˜, utilised content typ e and threshold ğ‘¡ resulting in the highest average scores and corresponding values for ğ´, ğ‘†, ğ¼ , ğ· as well as ğ¸ per data set and result type. Table 5: Average positions (pos) sets computed by the diî€erent conî€›gurations (conf) were ordered to in the qualitative evaluation as well as the average number of relevant reviewers (#rel) and the average position of entries from the diî€erent RTs per set. quantitative evaluation. For the forty (twenty from ECIRâ€™17 and twenty from BTWâ€™17) documents which were used in the î€›rst manual evaluation, we compute lists of four reviewer sets for all conî€›gurations, consisting of one reviewer set produced by each of the three baselines as well asğ‘…. We present the lists to an expert who then ranks the four entries according to suitability for the query manuscript from 1 (best) to 4 (worst), with the option of ties if two or more entries are equally suitable. Table 5 shows the average ranks of the result types in the evaluated lists for the two data sets, their average number of relevant reviewers per conî€›guration and the average positions that entries from a speciî€›c RT achieved. For BTWâ€™17, the combination achieving the best results isğ‘ (BERT,ğ‘€ğ¼ ğ‘,ğ‘˜= 10,ğ‘¡= .25) and (surprisingly)ğµ. For ECIRâ€™17, the combination achieving the best results came from conî€›gurationğ‘ (BERT, ğ‘šğ‘…ğ‘…, ğ‘˜ = 20, ğ‘¡ = .9) and ğ‘…. Overall,ğ‘…achieves the best results out of all combinations and data sets.ğµgenerates the best results for BTWâ€™17, but highly depends on the conî€›guration as it also achieves considerably bad results, especially for the ECIRâ€™17 data set. Although the results are greatly inî€uenced by the conî€›guration,ğ‘…performs consistently well in general. The combination of conî€›guration and result type achieving the highest number of mean relevant reviewers per data set is not the one achieving the best results in terms of positions, e.g. ECIRâ€™17 +ğ‘+ğµ. This leads to the conclusion that it is not suî€œcient to consider only topical relevance in determining the most suitable combination. In both data sets, the RT achieving the best average positions is ğ‘…. As data was not normally distributed in the diî€erent groups for both data sets, we used Kruskal-Wallisğ»tests on positions of the four RT for the two data sets, which resulted in signiî€›cant diî€erences. We conducted Mann-Whitneyğ‘ˆtests on the positions ofğ‘… and each of the three baselines resulting from all conî€›gurations together on the respective data sets. In the BTWâ€™17 data set,ğ‘… performed signiî€›cantly better thanğµbut no signiî€›cant diî€erences were found when compared to the two other baselines. In the ECIRâ€™17 data set,ğ‘…performed signiî€›cantly better than all three baselines. In a second manual evaluation of Step 2, we examined the best combinations from Step 1 (ğ‘,ğ‘’andğ‘’) with CT = full, k = 20, t = .5 as the best performing combinations from Step 2 performed bad in Step 1. Table 6 was constructed exactly as described previously for Table 5. For both data sets, the best performing RT isğ‘…. It achieves the best average position for all conî€›gurations together and the combination resulting in the best position isğ‘’withğ‘…. For ECIRâ€™17, ğ‘…also produces the best overall position for ğ‘’. To better understand the impact of the î€›ve aspects, a human assessor also evaluated the quality of the results with the best combinations from Step 1 with respect to each aspect, assigning a value between 0 and 1 for each aspect. Table 7 depicts average scores according to Equation 1 for combinations of the three best methods from Step 1 with all result types, manually assessed average values for the î€›ve aspects and â€œmanualâ€ scores computed by multiplying the per-aspect values. In this evaluation, we wanted to compare the manually constructed scores to the automatic ones and evaluate possible eî€ects of opposing aspects. We observe vast diî€erences in the manual scoresğ‘šğ‘†ğ¶and the computedğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘ , in almost all cases ğµachieves the highestğ‘šğ‘†ğ¶. As we have already seen in Tables 5 and 6,ğ‘…generally achieves the best average positions for sets of reviewers. This discrepancy further underlines the suitability of our approach. RevASIDE produces reviewer sets based on calculated aspects which are preferable in a manual evaluation to the sets from ğµwhich achieved the highestğ‘šğ‘†ğ¶in the manual assessment of aspects. We found a positive correlation of aspectsğ‘šğ´andğ‘šğ¼(.597 for BTWâ€™17, .802 for ECIRâ€™17) which is signiî€›cant with Pearsonâ€™s correlation coeî€œcient for both data sets. A higher authority might be equivalent to a higher number of papers, especially in the last seven years which might increase the probability of one of these papers being from the area of the manuscript and thus signals reviewersâ€™ interest. Also for both data sets, the negative correlation betweenğ‘šğ¼andğ‘šğ·(-.589 for BTWâ€™17, -.72 for ECIRâ€™17) is significant with Pearsonâ€™s correlation coeî€œcient. If reviewers in a set are very interested in a manuscript, it seems likely that the set is not as diverse. In BTWâ€™17,ğ‘šğ´is signiî€›cantly correlated withğ‘šğ‘† (-.789), in ECIRâ€™17 this negative correlation is not signiî€›cant with Pearsonâ€™s correlation coeî€œcient. This observation can be explained as sets having high authority normally consist solely of researchers with high seniority. We found opposing objectives coded into the aspects which might have lead to methods from Table 4 achieving low results in Step 1 but being useful in Step 2. In general average positions of sets from the diî€erent RTs are highly dependent on the conî€›guration in BTWâ€™17 and ECIRâ€™17 for the best performing conî€›gurations in Step 2 but the overall best results are achieved independent of conî€›guration byğ‘…. From these observations we conclude thatğ‘…and thereby RevASIDE is a well-performing solution of the reviewer set assignment problem which is generally applicable. Thus, hypothesis ğ»is veriî€›ed. In this paper we proposed and evaluated RevASIDE, a method for assigning complementing reviewer sets for submissions from î€›xed candidate pools. Our approach incorporates authority, seniority, interests of researchers, diversity of the reviewer set as well as candidatesâ€™ expertise. Additionally, we presented three new data sets suitable for reviewer set recommendation. In this context we examine the expert search as well as the reviewer set assignment tasks and show RevASIDEâ€™s general applicability: for the î€›rst task we revaluated expert voting techniques utilising diî€erent document representations. We veriî€›ed the general usefulness of Step 1 for the expert search (addressed with hypothesisğ») and reviewer set recommendation task (addressed with hypothesisğ»). Additionally, we have shown the suitability of simple textual similarity methods utilising tf-idf compared to more advanced techniques using BERT, which in terms rejected hypothesisğ». For the second task RevASIDE produces signiî€›cantly higher overall scores for reviewer set assignment compared to three baselines in an quantitative evaluation which shows the approachâ€™s usefulness. In a qualitative evaluation we observed that sets assembled by our system are generally signiî€›cantly more suitable recommendations compared to our three baselines. We were able to conî€›rm the results from the quantitative evaluation and thus veriî€›ed ğ». Possible extensions might include weighting the diî€erent quantiî€›able aspects deî€›ned in Step 2 of the approach and incorporating the venue which reviewers are recommended for. The number of assigned reviewers could be varied for each submission to take into account papers with broad content. Future work will focus on recommending suitable reviewer sets for whole venues. Here, the optimisation problem of single manuscripts is extended to include all manuscripts and several constraints such as individually diî€ering maximal numbers of papers per reviewer come into consideration. Such an approach should also consider fairness [16] of the recommended reviewer sets. It would be interesting to observe gaps in the expertise displayed by the program committee in terms of î€›t with submitted manuscripts together with suggesting new reviewers matching the missing criteria. Another feasible extension might be the recommendation of a program committee based on former and recent conferences and anticipated submissions. Here, topical development between years is important. Furthermore, explainability [25] of the recommended reviewer sets should be a priority. In our case, radar charts could be used for example to visualise the values which the sets achieved in the diî€erent quantiî€›able aspects.