University of Technology SydneyUniversity of Technology Sydney Yicong.Li@student.uts.edu.auHongxu.Chen@uts.edu.au zhenchao.sun@mail.sdu.edu.cncathylilin@whut.edu.cn Hypergraphs have been becoming a popular choice to model complex, non-pairwise, and higher-order interactions for recommender system. However, compared with traditional graph-based methods, the constructed hypergraphs are usually much sparser, which leads to a dilemma when balancing the beneî€›ts of hypergraphs and the modelling diî€œculty. Moreover, existing sequential hypergraph recommendation overlooks the temporal modelling among user relationships, which neglects rich social signal from the recommendation data. To tackle the above shortcomings of the existing hypergraph-based sequential recommendations, we propose a novel architecture named Hyperbolic Hypergraph representation learning method forSequentialRecommendation (HSeqRec) with pre-training phase. Speciî€›cally, we design three self-supervised tasks to obtain the pre-training item embeddings to feed or fuse into the following recommendation architecture (with two ways to use the pre-trained embeddings). In the recommendation phase, we learn multi-scale item embeddings via a hierarchical structure to capture multiple time-span information. To alleviate the negative impact of sparse hypergraphs, we utilize a hyperbolic space-based hypergraph convolutional neural network to learn the dynamic item embeddings. Also, we design an item enhancement module to capture dynamic social information at each timestamp to improve eî€ectiveness. Extensive experiments are conducted on two realworld datasets to prove the eî€ectiveness and high performance of the model. Sequential Recommendation, Hypergraph, Hyperbolic Space, Selfsupervised Learning ACM Reference Format: Yicong Li, Hongxu Chen, Xiangguo Sun, Zhenchao Sun, Lin Li, Lizhen Cui, Philip S. Yu, and Guandong Xu. 2021. Hyperbolic Hypergraphs for Sequential Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3482351 Graph-based approaches have been widely used and achieved great improvement for next-item recommender systems. However, most existing literature [2,12,15,18,23,29,32,35,45] treat the dynamic time-dependent user-item interactions as a temporal bipartite graph and learn their latent representations for action. Though the graphbased graph modelling can capture the beneî€›cial î€›rst-order (i.e., user-item interactions) and second-order (i.e., co-purchasing) interactions for recommendation, the higher-order signals among users and items are usually neglected by existing works due to the limitations of traditional graph modelling. With noting the shortcoming, recent works [4,24,25,29,30,36,43] resorted hypergraphs to make up and developed hypergraph-based modelling approaches for sequential recommender systems. The basic idea is illustrated in Figure 1, when using traditional graphs to model user-item dynamic relationships evolution, the learned information from Figure 1.(a) to 1.(b) is monotonous due to the simple pair-wise data structure. In contrast, hypergraphs are able to capture high-order dynamic relationships (buy items at the same time, similar user groups, etc.) thanks to the non-pairwise data structure. That is, unlike traditional graphs, an edge in the hypergraph (a.k.a hyperedge) is allowed to connect more than two nodes, promising to capture multi-scale signals for the recommendation. Nevertheless, all these existing works ignored a critical issue of such hypergraph-based approaches, which isthe sparsity issue is becoming more severe in hypergraph based recommender systems. As analysed in [42], the recommendation benchmark Figure 1: A comparison of sequential graph construction and hyp ergraph construction. Single-colored area in (c) and (d) denote hyperedges. Amazon dataset is sparse and exhibits long-tailed distribution, in which many users have limited interactions to the items, while only a small number of users interact with many items. In such a case, the constructed hypergraphs from the original dataset will be much sparser, resulting in insuî€œcient training samples for action. To be speciî€›c, if we construct hypergraphs based on the original simple user-item bipartite graph, the number of items (nodes) does not change, but the number of hyperedges is dramatically shrunken compared to the number of original links between users and items, which is obvious in Figure 1. Therefore, when constructing hypergraphs, the challenging sparsity issue and long-tailed data distribution are becoming even more severe. The second limitation of existing hypergraph-based sequential recommendation lies inlacking exploitation of hidden hyperedges among users, which we believe will be beneî€›cial to understand the hidden but insightful behaviours among users (e.g., common interests groups, co-purchasing groups, users who have similar buying patterns, etc.). Taking the group recommendation task [31,41] as an example, it focuses on a group of usersâ€™ preference, which means users in the same group may tend to have a similar preference, or at least they have more common interest compared to the rest of the world. Inspired from the idea, we are curious about the possibility of exploring and leveraging hypergraphs constructed from users side to improve the overall recommendation performance. For example, in Figure 1(c) and (d), from the timestampğ‘¡toğ‘¡, they are the evolution of sequential hypergraph constructions. It is obvious that in the timestampğ‘¡, the userğ‘¢is of co-purchasing relationship withğ‘¢, and the hyperedges ofğ‘¢ andğ‘¢are connected. In this case, the itemsğ‘–,ğ‘–,ğ‘–andğ‘–are likely to be more similar to each other than toğ‘–andğ‘–. The same is also in the timestampğ‘¡. Therefore, the aim is to capture the dynamic items diversiî€›cation through dynamic hypergraphs constructions according to usersâ€™ relationships to enhance the model. Intuitively, like traditional graph-based methods, this try may gain improvement on recommendation. Figure 2: An example of yearly purchase behaviour. To holistically solve the above issues, we propose a novel architecture namedHyperbolicHypergraph representation learning forSequentialRecommendation (HSeqRec). Speciî€›cally, we propose to î€›rstly pre-train the model with self-supervised learning on three well-sophisticated tasks. It is worth noting that compared with the prior works paying more attention to chronologically model each userâ€™s buying history, one of our contributions is to introduce a hyperedge prediction task, which explicitly models the usersâ€™ historical records as hyperedges at each timestamp and explores the potential relationships between linked hyperedges (users who have similar buying history) [37,48]. Besides, we also investigate hyperbolic embedding spaces [46] and manage to map the sparse data points to the hyperboloid manifold directly. The rationale is that hyperbolic space has a stronger ability than Euclidean space to accommodate networks with long-tailed distributions and sparse structures [1,3,13,14,21], which is also veriî€›ed in our experiments. In addition, to exploit hidden but useful user-side information in sequential recommendation settings, we propose to construct an induced hypergraph for each user to model his behaviour pattern in group-level. Intuitively, a userâ€™s behaviour pattern is not only reî€ected in his historical records but can also be excavated from other users who have similar behaviour pattern. This is particularly helpful to remit the "cold start" problem. Speciî€›cally, for the target userğ‘¢, we î€›rst use a hyperedge to collect historical items taken by ğ‘¢at the timestampğ‘¡. Then we î€›nd the other hyperedges from other users who share the overlapping items withğ‘¢at the timestampğ‘¡. All these hyperedges construct an induced hypergraph to describe the userğ‘¢â€™s behaviour pattern from the group view at the timestampğ‘¡. Although constructing hypergraphs from original simple useritem bipartite graphs seems a better choice, it is diî€œcult to decide the time granularity to which the length of item sequence for each hyperedge is split. After analysing the items in the dataset, we î€›nd the purchase history is not only in a chronological sequence way, but with a periodic regularity, like seasonal evolution. As shown in Figure 2, the user bought a Christmas tree and some decorations in both 2018 and 2019, and she also bought two dresses in the summer of 2018 and 2019. Therefore, rather than model each userâ€™s items in a chronological way, we design three views for hypergraph construction, namely yearly, quarterly and monthly views, and manage to hierarchically learn latent item representations. Moreover, we also î€›nd that when constructing quarterly and yearly hypergraphs, the hypergraphs are no longer as sparse as original single-time grained hypergraphs. In this way, the hierarchical time-span method for hypergraph construction also alleviates the above mentioned sparse problem. In summary, the contributions of this work are as follows. â€¢We propose three self-supervised learning tasks as the pretraining phase. To our best knowledge, we are the î€›rst to Figure 3: Overall architecture HSeqRec contains three modules, multi-scale embeddings via dynamic hierarchical hyperbolic hypergraphs, item enhancement via similar user groups, and learning user preference for sequential recommendation. The pre-training module could be as initial embedding for HSeqRec (HSeqRec-init); and it also could fuse into HSeqRec (HSeqRec-fuse). propose a hypergraph link prediction as a pre-training task to do data augmentation. â€¢We explore hidden insightful behaviours among users by constructing hypergraphs related to users at each timestamp to enhance the recommendation model. â€¢Instead of traditional chronological sequential modelling of data, we argue periodical regularity and model usersâ€™ interest as a hierarchical structure for improving recommendation. â€¢We propose a novelHyperbolicHypergraph representation learning method for Sequential Recommendation (HSeqRec) to well-model long-tail data in sequential recommendation task. â€¢Extensive experiments are conducted on benchmark datasets. Our model outperforms the SOTA sequential recommendations, which shows the eî€ectiveness of our model. The section presents the deî€›nition and the problem formulation. Definition 1.Hypergraph.AssumingG= (V, E)denotes a hypergraph with a nodes setVand a hyperedges setE, a hyperedge ğ‘’âˆˆ Econnects multipleğ‘›nodes (ğ‘› â‰¥ 2). The nodes set conne cted by a hyperedge ğ‘’is a hypernode ğ‘£âŠ‚ V. Problem 1.Sequential Recommendation.In the sequential recommendation task, the userâ€™s related items are in chronological order. Given the user setU = {ğ‘¢, ğ‘¢, ...,ğ‘¢}and the item setI = {ğ‘–, ğ‘–, ..., ğ‘–}, each user has sequential itemsS= {ğ‘ , ğ‘ , ..., ğ‘ }, whereğ‘ âˆˆ Idenotes theğ‘¡historical item of userğ‘¢. The sequential recommendation problem is to predict the nextğ‘items {ğ‘ , ..., ğ‘ }associated to each userğ‘¢, according to sequential historical itemsS= {S, S, ..., S}. In this paper,ğ‘ = 1. For each timestampğ‘¡, a hypergraphGis constructed according to the current user setUand the item setI. Each userâ€™s historical items within the timestamp ğ‘¡ are connected by a hyperedge. Figure 3 is an overview of our proposed sequential recommendation system. Technically, we î€›rst generate yearly, quarterly, and monthly hypergraph snapshots as views from usersâ€™ historical records. Here each hyperedge in the hypergraph refers to one userâ€™s historical items. With these hypergraphs, we design a novel hypergraph convolutional network in the hyperbolic space to learn multi-scale item embeddings. To enhance the item embedding, we further explore userâ€™s behaviour pattern via similar usersâ€™ behaviours. In the end, we leverage the Transformer to learn usersâ€™ preference and predict the next item via multi-layer perception (MLP). Moreover, before theHSeqRec, we design three self-supervised tasks, as shown in Figure 4, to pre-train item representations. The pretrained item representations could be as initial embeddings for HSeqRec, namedHSeqRec-init; and they could also be fused into HSeqRec, named HSeqRec-fuse. Before we start, we need the initial item features to support the downstream model. Inspired by the self-supervised learning framework [48], which can learn item representations without annotations, we propose three novel pre-training tasks (as shown in Figure 4) to learn the pre-trained item representations: â€¢The î€›rst task is to predict whether a masked item sequence is derived from a userâ€™s historical records. The rationale is that in the real world, one item usually cannot change a userâ€™s long-term preference. Therefore, we randomly mask diî€erent items for a target userâ€™s historical sequence and then let these sequencesâ€™ representations as close as possible. â€¢The second task is to predict whether a sub-sequence is consistent with a userâ€™s short-term interest. The motivation is that the consecutive items taken in a short time usually contain a userâ€™s short-term interest, and these items might share similar functions. If we mask part of them, the userâ€™s short-term interest should be still observable from the rest items. Therefore, we î€›rst mask a sub-sequence with length 2 and then wish the masked sequence be close to the complete sequence. â€¢The third task is to predict whether two users have the same behaviour patterns. Intuitively, usersâ€™ behaviour patterns can be reî€ected in their historical records. If two users share many historical items, they are more likely to have the same preference, hobbies or even tastes. To this end, we use a hyperedge to connect one userâ€™s historical items and then let a pair of hyperedges be as close as possible if they share the overlapping items. Lethandhbeing embeddings of two sequences, we adopt contrastive loss [37] for each task: hereğ‘†is the sampled negative sequences of the userğ‘¢.his the embedding of the negative sequence,ğ‘ ğ‘–ğ‘š(h, h)is deî€›ned Figure 5: Multi-scale Embeddings via Dynamic Hierarchical Hyperbolic Hypergraphs. as:whereğœis the hyper-parameter. The sequence embeddingshandhare both calculated by Transformer network with two multi-head attention and feed forward blocks. We combine the above three tasks together with optimized weights {0.1,1,1}. With the above initial features, we then design a dynamic hypergraph neural network for learning the multi-scale item embeddings in hyperbolic space, as shown in Figure 5. 3.2.1 Hierarchical Time-span Hypergraph Construction. To model the complex dependencies among the sequential items, we propose a hierarchical architecture to learn the monthly, quarterly and yearly relationship among sequential items. The motivation is that a userâ€™s behaviour patterns are not just suggested in anteroposterior items of the item sequences but also reî€ected via the seasonal and periodical variance. For example, users prefer to buy Christmasrelated products before Christmas, and purchase T-shirts in summer and coat in winter. Inspired by [29], we utilize hypergraphs instead of traditional graphs to model diî€erent sub-sequences of each user and we also use multi-scale time-spans views (monthly, quarterly, and yearly) to learn the semantics of items. To tackle the sparsity and the long-tail distribution of these hypergraphs, we learn the graph representations in hyperbolic space because it is perfectly suitable for long-tail structures. Speciî€›cally, when we deal with the monthly item representations, we î€›rst construct a monthly hypergraph where each hyperedge connects userâ€™s items within one month. Then we use hyperbolic space-based hypergraph graph neural network (introduced in 3.2.2) to learn the dynamic item embeddings in each month. Following this approach, we can also learn quarterly and yearly item representations. 3.2.2 Hyperbolic Space-based Hypergraph Convolutional Network. Compared with traditional graphs that mainly rely on pairwise useritem interactions, hypergraphs can model much higher relations in user-item interactions and promise to fuse item context to remit the sparsity problem. For example, if userğ‘¢bought î€ower and wedding dress andğ‘¢bought î€ower and a vase, we can use a hyperedge to connect the î€ower and the wedding dress, and use another hyperedge to connect the î€ower and the vase. In this way, the î€owerâ€™s semantic forğ‘¢is about the wedding but forğ‘¢is about decoration. However, if we use a traditional graph, the graph is hard to reveal diî€erent item semantics directly. In addition, the user-item record usually follows the long-tail distribution and is sparse. The traditional Euclidean space usually cannot capture this structure leading to representation distortion, while hyperbolic space is beneî€›cial to deal with the issue. [1] Based on the above motivations, we propose a hypergraph neural network to model the items evolution in hyperbolic space. Specifically, we î€›rst transform the initial item features from Euclidean space to hyperbolic spaceH, and then we feed the initial hyperbolic item embeddings to learn item embeddings. For the hyper-âˆš bolic space, we seto := {ğ‘, 0, 0, ..., 0} âˆˆ Has the north pole in H, whereâˆ’1/ğ‘is the negative curvature of hyperbolic model. As analyzed in [1], the initial item features in hyperbolic space can be deduced from Euclidean space as follows: h= exp0, h wherehandhare the initial hyperbolic embedding and the initial Euclidean embedding, respectively. With the initial features in hyperbolic space, we further transform the features via a linear function in hyperbolic space: wherexis the hyperbolic hidden embedding of itemğ‘–in the ğ¿-th layer after transformation,ğ‘Šandbare the weight and bias, respectively.his itemğ‘–â€™s hyperbolic embedding in the last layer. Whenğ¿ = 1,h= h. Then, the item embeddings can be aggregated from the neighbouring nodes via the following convolutional operation in hyperbolic space: whereyis the hyperbolic hidden embedding of itemğ‘–in theğ¿-th layer after aggregation,N(.)denotes the neighbors soN(ğ‘–)is node ğ‘–â€™s neighbors andN(ğ‘–)is nodeğ‘–â€™s neighbors whereğ‘–is on the same hyperedge asğ‘–. The nodeğ‘—â€™s hyperbolic embedding is transformed to Euclidean embedding viağ‘™ğ‘œğ‘”(.), so the Euclidean-based sum and add operations are available.ğ‘’ğ‘¥ğ‘(.)aims to transform the Euclidean-based embedding to hyperbolic embedding. According to [1], choosingxas the north pole is the best Euclidean approximation at this step.ğ‘€is the projecting weight deî€›ned as follows: ğ‘€(ğ‘€ğ¿ğ‘ƒ (ğ‘™ğ‘œğ‘”(x)||ğ‘™ğ‘œğ‘”(x))) With the above node featuresâ€™ aggregation, we then use an activation function to generate the hidden embedding on Layerğ¿: wherehis the nodeğ‘–â€™s hyperbolic embedding atğ¿-th layer, ğ‘…ğ‘’ğ¿ğ‘ˆis theğ‘…ğ‘’ğ¿ğ‘ˆoperation in the hyperbolic space. Through ğ‘™ğ‘œğ‘”(.), we transform the hyperbolic item embeddinghto Euclidean-space based embedding h, and feed into the next module. 3.2.3 Mix Layer. With item embeddings for each month, quarter and year, we then design a mix layer to fuse them together. Speciî€›cally, to fuse monthly embeddings and yearly embeddings, we use a two-layer neural network to calculate their correlations as follows: whereâ„represents the mixture of monthly and yearly item embeddings at timestampğ‘¡.â„andâ„mean the monthly and yearly item embeddings respectively.ğ‘¡andğ‘¡denote the monthly and yearly timestamp, respectively.ğ‘Šandbare weight matrix and bias.âŠ™means element-wise product. Similarly, the mixed embeddings with the month and the quarter can be deî€›ned as â„, which is calculated by: â„= â„= ğ‘…ğ‘’ğ¿ğ‘ˆ (ğ‘Šâ„+ b) âŠ™ â„(8) whereâ„is the dynamic item embeddings at timestampğ‘¡ learned by this hierarchical architecture. Most existing hypergraph-based sequential recommender systems [29] ignore the hidden relationships among users, like social relationships, co-purchasing relationship, etc. However, these relationships are highly informative to capture the hidden usersâ€™ behaviours. For example, a userâ€™s shopping behaviour may be aî€ected by his friend circle, making the user tend to buy similar products with his friends. In light of this, we utilize user groups to enhance the former learned dynamic item embeddings. Speciî€›cally, we î€›rst î€›nd similar users for each user at each timestampğ‘¡. The similar users are those who take the same products at the same timestampğ‘¡. Using the current userâ€™s shopping records atğ‘¡and his similar usersâ€™ records atğ‘¡, we build an induced hypergraph to model his behaviour pattern in group-level. Each hyperedge connects each userâ€™s items. Then we leverage our proposed hyperbolic space-based hypergraph convolutional network to learn item embeddings in group-level. To enhance the item representations, we mix the above embeddingshwith previous embeddingshin Section 3.2.1. The mix operation is deî€›ned as follows: where his the item ğ‘–â€™s embedding for user ğ‘¢ at timestamp ğ‘¡. In this section, we leverage learned item embeddings to î€›nd usersâ€™ preference and then present the sequential recommendation model. 3.4.1 User Preference Learning. For each userğ‘¢, we use the Transformer [26] to model the dynamic item embeddings during the whole timestamps, mathematically, h= ğ‘‡ğ‘Ÿğ‘ğ‘›ğ‘  ğ‘“ ğ‘œğ‘Ÿğ‘šğ‘’ğ‘Ÿ (h, h, ..., h), ğ‘– âˆˆ S(10) wherehis the embedding of userğ‘¢â€™s item sequence, which can describeğ‘¢â€™s preference. The input of Transformer are the dynamic item embeddings learned in Equation 9. In this way, the userâ€™s sequential items contain both the dynamic hierarchical information and the userâ€™s potential group information. 3.4.2 The Complete Model. With user preference representation h, we use a two-layer Multilayer Perceptron (MLP) to calculate the rating score of the next item, mathematically, whereğ‘Ÿis the rating score of userğ‘¢and itemğ‘–at timestamp ğ‘¡,his the embedding of itemğ‘–at timestampğ‘¡. We train the recommendation by Bayesian Pairwise Loss [20], which aims to maximize the diî€erence between the rating scores of the positive item ğ‘– and negative sample ğ‘—:îƒ• where(ğ‘¢, ğ‘–, ğ‘¡, ğ‘—) âˆˆ Ddenotes the positive pair (u,i,t) and the negative pair (u,j,t) from the training setD.ğœis the sigmoid function,ğ›¼ is the weight of L2 regularization term ||ğ›¿||. 4.1.1 Dataset. We evaluate our method on two real-world datasets, named AMT and Goodreads. The statistics are shown in Table 1. â€¢ AMT.It is the subset of the public Amazon dataset [17], which contains consumersâ€™ buying record and reviews in 29 categories. In our experiment, we choose three categories, Automotive, Musical Instruments and Toys and Games, to form the dataset and remove users bought less than 5 items. The time of AMT dataset is range from 2014 to 2018. The î€›rst two items in 2018 are considered as valid data and test data, respectively. Generally, the distribution of the items among users is long-tail distribution [42]. â€¢ Goodreads.It [27,28] is collected from goodreads website, which is an online book community website. We choose user-book (item) interactions between 2013 and 2015 as the dataset. Last two interactions are valid and test data, respectively. From Table 1, it is evident that Goodreads is much denser than AMT dataset. 4.1.2 Baselines. â€¢ GRU4Rec.[5,6] It is a well-known sequential-based recommendation model that utilizes GRU to sequentially model usersâ€™ interactions to achieve top-N recommendation. â€¢ SASRec.[12] It is a self-attention based sequential approach for next item recommendation, which could capture each userâ€™s both long-term sequential relations through PointWise Feed-Forward Network and short-term interactions with items through an attention mechanism. â€¢ BERT4Rec.[23] This method employs deep bidirectional self-attention to sequentially model the user behaviours in two directions through Cloze task. â€¢ SRGNN.[35] The method utilizes graph neural networks to model the session sequences and obtain the complex item transitions for each session. â€¢ HGN.[15] It proposes a hierarchical gating network with the Bayesian Personalized Ranking in order to capture userâ€™s both long- and short-term interest. â€¢ HyperRec.[29] It uses sequential hypergraphs to model dynamic item embedding sequentially and fuses with static item embeddings as item representations. For each user, item representations are fed into Transformer network to obtain the next item recommendation. 4.1.3 Evaluation. Our proposed method focuses on recommending next item, and therefore we use Topğ¾Hit Ratio (HR@ğ¾) and Topğ¾Normalized Discounted Cumulative Gain (NDCG@ğ¾) as our evaluation metrics. We chooseğ¾ = {1, 5, 10, 20}in the baseline comparison experiment. Our baseline HyperRec [29] randomly selects 100 negative samples for each positive user-item pair. However, we think it is insuî€œcient to reî€ect our modelâ€™s eî€ect accurately. Moreover, the baseline Bert4Rec and SRGNNâ€™s evaluation speed is too slow to test all the data. Therefore, in our experiment, we randomly choose{100, 500}negative samples and rank{101, 501} items to calculate the HR@ğ¾ and NDCG@ğ¾ scores. 4.1.4 Parameter Details. We implement GRU4Rec, SASRec, BERT4Rec and SRGNN from the RecBole python package [47]. For other methods, we use the public code provided by each paper. For all the methods, the feature dimension size is 100. For AMT dataset, all baselinesâ€™ training batch size is set 512. We use the early stop function in RecBole whose condition is not updating NDCG@10 for 10 epochs. The hidden size is 100, and the dropout probability is 0.5, as the same as our proposed methods. Moreover, we choose BPR as the loss of GRU4Rec, SASRec, BERT4Rec and SRGNN to keep identical with our methods. For those baselinesâ€™ code authors provided, we employ the given default parameters, but the learning rate is 0.001 for all baselines. For Goodreads dataset, some baselinesâ€™ training time is so long with training batch size 512, so we change it to 4096 if the baseline does not run out of CUDA memory. Other parameters are the same as training AMT dataset. To evaluate our proposed model, we design three strategies to initialize it: 1)HSeqRec, one-hot item embeddings as input without pre-training tasks, 2)HSeqRec-init, pre-trained item embeddings as input and 3)HSeqRec-fuse, a combination of the î€›rst two, one-hot item embeddings as input, and pre-training item embeddings fused with dynamic item embeddings and group-level item embeddings. For our proposed methods HSeqRec, HSeqRec-init and HSeqRec-fuse, the parameter settings are the same. The layer of hyperbolic hypergraph convolutional network is 2, and we choose the hyperboloid model as hyperbolic geometry with the negative curvature -1. Moreover, since the Goodreads dataset is too dense, we sample1/100users to î€›nd similar user groups. To simplify our proposed methods, we fuse the item embeddings learned via similar usersâ€™ group with the dynamic item embeddings within each batch rather than for each user. In Transformer, we set the number of heads and blocks, 2 and 1, respectively. The epoch of hyperbolic hypergraph convolutional network is 300. The maximum sequence is set to 50 for all the above methods. Other parameters are the same as the baselinesâ€™. 4.2.1 Overall Performance Analysis Between Diî€›erent Baselines. As shown in Table 2, we evaluate our proposed model with 6 state-ofthe-art sequential recommendation baselines. Our proposed model HSeqRec,HSeqRec-init andHSeqRec-fuse could outperform all of them in both HR@ğ¾and NDCG@ğ¾in 100 and 500 negative sampling experiments. For AMT dataset, our best model improves the best baseline 31.61% and 16.94% at NDCG@1 for negative sampling 100 and 500, respectively. In terms of Goodreads dataset, our best model outperforms the best baseline 10.43% and 13.71% at NDCG@1 for negative sampling 100 and 500, respectively. From the last column (Improvement), the advantages also could see in other evaluation metrics, especially in top 1 and 5 ranking. In recommendation task, it is signiî€›cant to have better recommendation in top ranking, because proper recommendation in higher ranking means more eî€ectiveness of the recommendation model. 4.2.2 Eî€›ectiveness on Pre-train Features. In Table 2, for AMT dataset, the proposedHSeqRec-fuse achieves the best performance and improves 5.41% and 5.33% than the NDCG@1 ofHSeqRecwith 100 and 500 negative samples. While for the Goodreads dataset, HSeqRecachieves the best performance on NDCG@1 and better thanHSeqRec-fuse by 2.74% and 7.74% in negative sampling of 100 and 500, respectively. From the results, we could get the conclusion that when training sparser dataset, to do data augmentation as pre-training could help improve model. However, in terms of denser dataset, it is useless or even worse to do pre-training while our proposed model without pre-training could still achieve quite well performance. Therefore, unlike traditional recommendation models which blindly pursue pre-training to improve the model, AMT100 Goodreads100 Table 2: Comparison with baselines. The last column is the improvement of the best proposed method than the best baseline. HSeqRec(Hie Table 3: Impact of Diî€erent Modules (NDCG@K). Â¬ is removing the following module and keeping the others. The last column means the decrease rate of each row compared with HSeqRec on NDCG@10. our experiment shows that sometimes pre-training cannot really help improvement. Moreover, from the columnsHSeqRec-init andHSeqRecfuse of Table 2, we î€›nd that the results are of small diî€erences in two datasets. It means that no matter how to use the pre-training embedding in the model, it is almost the same. Therefore, the conclusion is that pre-training does play a part in the model. We perform ablation test on AMT dataset to study diî€erent modulesâ€™ eî€ects. The evaluation metric is NDCG@{1,5,10,20}, and the number of negative test sampling is 100. In Table 3, theÂ¬represents removing the following module and remaining other modules. For example,HSeqRec Â¬U means the proposed model without Item Enhancement via Similar User Groups module in Section 3.3. In the next row, Hie means the hierarchical module, so removing the hierarchical module represents deleting the quarterly and yearly dynamic item embeddings and only feeding monthly dynamic ones into the recommendation. HB means hyperbolic space, and therefore that row meansHSeqRecabandons hyperbolic space and train in Euclidean space.HSeqRec(Hie) changes the order of fusion time-span in hierarchical architecture, that is, it fuses quarterly dynamic item embeddings î€›rstly into monthly dynamic item embeddings, and then yearly item embeddings fuse into them. As shown in the Table 3, the proposed modelHSeqRecachieves the best performance. The Item Enhancement via Similar User Groups module plays the least important role in the whole model, because of the sparsity of AMT dataset. Moreover, using the hierarchical module to learn dynamic item embeddings and hyperbolic space-based item embeddings learning both could improve the model. However, if we change the hierarchical time-span fusion order, the results will be a little bit worse. That may be because if yearly dynamic item embeddings fuse in the last step in hierarchical architecture, the î€›nal fused item embeddings would contain more yearly information and less quarterly information. However, yearly information may be too large as a time interval, and the quarterly one may be apposite. Therefore, that is why we choose to fuse yearly information î€›rst and then quarterly, and the results also prove our rationality. In Table 4, we evaluate diî€erent pre-training tasksâ€™ inî€uence on the performance of the AMT dataset with 100 negative samples. Minus afterHSeqRec-fuse denotes only considering the following pre-training tasks.ğ‘€means the masking random items task,ğ‘† represents the masking subsequence task, andğ»is the hyperedge link prediction task. HSeqRec-fuse-SH andHSeqRec-fuse-MH obtain better performance, compared withHSeqRec-fuse-MS, which proves the eî€ectiveness of our proposed hyperedge prediction task. We can also get this conclusion by the last row,HSeqRec-fuse-H shows the highest performance compared withHSeqRec-fuse-M and HSeqRec-fuse-S. Therefore, this experiment shows our proposed hyperedge link prediction taskâ€™s eî€ectiveness, which is more helpful than the existing pre-training methods merely model each userâ€™s buying history in a chronological way. Table 4: Impact of diî€erent pre-training tasks (NDCG@K). âˆ’ denotes only considering the following pre-training tasks. In the Figure 6, we analysis diî€erent dimensions of items inî€uence on the proposed model and the baselines, we randomly choose three baselines to compare with our proposed models. The negative test sampling is 100, and the evaluation metric is NDCG@1. As shown in Figure 6a, our proposed modelsHSeqRecand HSeqRec-fuse could always outperform GRU4Rec, SASRec and HyperRec when the dimension is {20, 40, 60, 100}. Moreover, when the number of dimensions is larger in the î€›gure, the pre-training phase is more useful to the recommendation performance. It is because the pre-training data augmentation can learn more information about the sparse dataset when the number of dimensions is larger. In Figure 6b, we compare the NDCG@1 on models training. GRU4Rec is early stopped because of not updating NDCG@1 for 10 epochs, so the line is not complete. In general, our proposed two methods always achieve the best performance, especially the HSeqRec-fuse, whose NDCG@1 is still updating after 25 epochs. Our model contains four parts, item features extraction via selfsupervised learning tasks, learning multi-scale embeddings via dynamic hierarchical hyperbolic hypergraphs, item embeddings enhancement via similar users groups and learning user preference for sequential recommendation. Therefore, we will analyse the time complexity of four separate parts, respectively. For the self-supervised learning phase, we have three tasks. The î€›rst task, for each user, we should randomly select two diî€erent items to mask, so the time complexity isğ‘‚ (ğ‘ˆ Ã—ğ¼Ã—(ğ¼âˆ’1)), where ğ‘ˆis the number of users, andğ¼is each userâ€™s related items. Since ğ¼â‰ª ğ‘ˆ, the time complexity could approximately beğ‘‚ (ğ‘ˆ ). The second task is that we randomly mask a subsequence for each user, and the length is 2, so the time complexity is aboutğ‘‚ (ğ‘ˆ Ã— ğ¼) â‰ˆ ğ‘‚ (ğ‘ˆ ). For the last task, we should î€›nd each userâ€™s neighbours and two-hop neighbours, and then build the hyperedge connecting each userâ€™s items within a timestamp. Therefore, if there areğ‘‡ timestamps, the time complexity isğ‘‚ (ğ‘ˆ Ã—ğ‘‡Ã—NÃ—N), where Nis the average numbers of each userâ€™s neighbours andNis the average number of each userâ€™s two-hop neighbours. Sinceğ‘‡â‰ª ğ‘ˆ , Nâ‰ª ğ‘ˆ , Nâ‰ª ğ‘ˆ, the complexity is approximatelyğ‘‚ (ğ‘ˆ ). The pre-training tasksâ€™ time complexity can be added together, and could approximately ignore the smaller magnitude terms, so the overall time complexity of pre-training isğ‘‚ (ğ‘ˆ ). In practice, it is unnecessary to do the pre-training phase every time. The learned pre-training features could be learned oî€Ÿine and stored. Even if new data comes, it can implement incremental training. Therefore, the time complexity is acceptable. For the multi-scale dynamic hierarchical hyperbolic hypergraphs learning, we have three time-spans views including month, quarter, and year. For monthly hyperbolic hypergraph neural network learning, we consider each userâ€™s items within a month connected by a hyperedge. We perform hyperbolic-based hypergraph convolutional neural network on it, so the time complexity isğ‘‚ (ğ‘‡Ã— ğ¼ Ã— NÃ—ğ¿ Ã— ğ·), whereNis the average number of neighbours for each hyperedge connected to the item, because of the aggregation function in the model. ğ¿ is the number of layers, and ğ· is the embeddingâ€™s dimension. The time complexity is the same as the quarterly and yearly model, but the number of quarterğ‘‡and year ğ‘‡is less thanğ‘‡. To sum up, this moduleâ€™s time complexity is ğ‘‚ ((ğ‘‡+ğ‘‡+ğ‘‡) Ã—ğ¼ Ã—NÃ—ğ¿ Ã—ğ·) â‰ˆ ğ‘‚ (ğ¼), becauseğ‘‡,ğ‘‡,ğ‘‡, ğ¿ are countable, the average number of itemâ€™s neighbours isNâ‰ª ğ¼ and the dimension is set as 100 in our model. Whatâ€™s more, this module can also learn oî€Ÿine due to our step-by-step learning style. In practice, this will signiî€›cantly save the time of the recommendation. The learning strategy is the same as the pre-training phase. If there are new data, this module can also do incremental training. Therefore, the time complexity of this module seems good. For the item embeddings enhancement via similar users groups module, the training method and the hypergraph construction strategy is the same as the last module. However, this module focuses on the relationship between users and will construct multiple hypergraphs for each user. Therefore, the time complexity is ğ‘‚ (ğ‘‡Ã—ğ‘ˆ Ã— NÃ— NÃ—ğ¿ Ã— ğ·), whereğ¼means each userâ€™s related items,Nis the average number of neighbours of each userâ€™s related items andNis our deî€›ned similar userâ€™s related items in section 3.3. Similarly, the approximate time complexity is ğ‘‚ (ğ‘ˆ )becauseNâ‰ª ğ‘ˆ,Nâ‰ª ğ‘ˆ. This module can also train oî€Ÿine in advance, thanks to our step-by-step training strategy. In practice, if there are new users fed into the model, it is convenient to train new usersâ€™ item embeddings. If some existing users are fed into the model again, it is convenient to train incrementally based on the previous version of item embeddings. In the last module, learning user preference for sequential recommendation module, we use the Transformer to obtain each userâ€™s preference embedding. For each user, we feed userâ€™s sequence into the Transformer (self-attention layer), and therefore the time complexity isğ‘‚ (ğ‘ˆ Ã— ğ¼Ã— ğ·) â‰ˆ ğ‘‚ (ğ‘ˆ )because ofğ¼â‰ª ğ‘ˆas claimed before. To train the recommendation, we sample 1 negative training item to maximize the diî€erence between the positive user-item pair score and the negative one. The recommendation partâ€™s time complexity isğ‘‚ (ğ¼Ã— (ğ‘ ğ¸ğº + 1)). Since the user preference embedding learning part and the recommendation part are learned end to end, the whole time complexity of the last module is ğ‘‚ (ğ‘ˆ Ã— ğ¼Ã— ğ· Ã— ğ¼Ã— (ğ‘ ğ¸ğº + 1)) â‰ˆ ğ‘‚ (ğ‘ˆ )because ofğ¼â‰ª ğ‘ˆand ğ‘ ğ¸ğº â‰ª ğ‘ˆ . To sum up, the î€›nal time complexity of the proposed model is the maximum time complexity of the above modules, which is ğ‘šğ‘ğ‘¥ (ğ‘‚ (ğ‘ˆ ), ğ‘‚ (ğ¼ )), so it is acceptable. Early neural recommendations building on typical deep neural networks mostly use recurrent neural networks (RNN) or convolutional neural networks (CNN) to capture the temporal patterns from usersâ€™ historical records. Speciî€›cally, Quadrana et al. [19] design a multi-level RNN structure for learning temporal patterns in a sequence. Although RNN-based methods have their advantages in sequential learning, user-item interactions usually contain noisy information. Only learning these relations is far from achieving more reliable recommendation system. Unlike RNN-based method, Yuan et al. [44] use CNN to learn from user-item sequences because CNN-based methods can not only capture long-term dependencies but also have the character of translation invariance, making the model more stable for various sequential orders. Later, attention is introduced in recommendation models [16,22,23,34]. In particular, Kang et al. [12] treat the user-item sequence as a sentence and model the temporal relations via a transformer model [26]. The transformer takes a self-attention unit to translate sequences as entity embeddings and position embeddings, which can be used to the downstream recommendation system. In the real world, however, users and items contain more nonlinear relations with diî€erent topological structures. To this end, graph-based sequential modelling approaches become more and more popular to model the recommendation data. By modelling graph structure data, graph neural networks (GNN) are introduced recently. As a conceptional extension of linear CNNs, GNN-based method [7,35,39] take the directed graph as input, and capture the interdependence directly on the graph. For example, Wu et al. [35] treat each user or item as a node in the graph, and transform the user-item sequence as a path. With GNN-based model, they can learn both users and items embeddings over the whole graph. Later, more advanced technologies have been introduced such as, self-supervised learning [11,33,37,38,40], group awareness [8,10,31] and so on. In particular, Hwang et al. [9] propose selfsupervised auxiliary learning tasks to predict meta-paths to capture rich-information of a heterogeneous graph, and thereby improve the primary task to do link prediction. Recently, instead of traditional graph, constructing hypergraphs to learn the data structure to do recommendation become a popular approach. Yu et al. [43] design a multi-channel hypergraph convolutional network to enhance social recommendation by exploiting high-order user relations, which shows great improvement. However, it ignores the sequential information for users. Xia et al. [36] model session data as a hypergraph and propose a dual channel hypergraph convolutional network for session-based recommendation. Wang et al. [30] also construct hypergraphs for each session to model the item correlations, and they also introduce a hypergraph attention layer to î€exibly aggregate correlated items in the session and infer next interesting item. Although the above two methods both consider some temporal information, it is within a speciî€›c session, not a whole sequence. Wang et al. [29] propose a novel next-item recommendation framework empowered by sequential hypergraphs to incorporate the short-term item correlations while modeling the dynamics over time and across users. The advantage is to consider sequential information, but all the above three hypergraph-based methods ignore the severe sparsity problem of hypergraphs. In this work, we focus on the sparse problem of most existing hypergraph-based sequential recommendation and on the lacking exploitation of hidden hyperedges among users problem, and proposeHyperbolicHypergraph representation learning method for SequentialRecommendation (HSeqRec) with pre-training phase. Experiments show that our proposed model outperforms the stateof-the-art sequential recommendations and each of the components contributes to the whole architecture. However, on a sparse dataset, the data augmentation improves the recommendation, while on a dense dataset, it is useless and even worse, which proves dense datasets do not need data augmentation to achieve better performance. In future work, we will explore multiple sequential ways to model the sequential recommendation data on graphs. This work is supported by the Australian Research Council (ARC) under Grant No. DP200101374 and LP170100891, and NSF under grants III-1763325, III-1909323, III-2106758, and SaTC-1930941.