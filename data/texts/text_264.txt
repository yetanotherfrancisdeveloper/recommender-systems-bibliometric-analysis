East China Normal University,Shanghai Jiao Tong University, Sequential Recommendation aims to recommend items that a target user will interact with in the near future based on the historically interacted items. While modeling temporal dynamics is crucial for sequential recommendation, most of the existing studies concentrate solely on the user side while overlooking the sequential patterns existing in the counterpart, i.e., the item side. Although a few studies investigate the dynamics involved in the dual sides, the complex user-item interactions are not fully exploited from a global perspective to derive dynamic user and item representations. In this paper, we devise a novel Dynamic Representation Learning model for Sequential Recommendation (DRL-SRe). To better model the user-item interactions for characterizing the dynamics from both sides, the proposed model builds a global user-item interaction graph for each time slice and exploits time-sliced graph neural networks to learn user and item representations. Moreover, to enable the model to capture î€›ne-grained temporal information, we propose an auxiliary temporal prediction task over consecutive time slices based on temporal point process. Comprehensive experiments on three public real-world datasets demonstrate DRL-SRe outperforms the state-of-the-art sequential recommendation models with a large margin. â€¢ Information systems â†’ Recommender systems; Personalization; Temporal data. sequential recommendation, user behavior analysis, graph neural networks, temporal point process ACM Reference Format: Zeyuan Chen, Wei Zhang, Junchi Yan, Gang Wang, Jianyong Wang. 2021. Learning Dual Dynamic Representations on Time-Sliced User-Item Interaction Graphs for Sequential Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3482443 Sequential recommendation [26] aims to recommend items that a target user prefers to interact with in the near future based on the interacted items in the past. It has become a paradigmatic task in recommender systems in recent years. This owes to the fast-developing online services (e.g., e-commerce platforms and streaming media) wherein user sequential behaviors are ubiquitous, and mature information technologies that make the collection of the sequential behaviors become easier. Compared to conventional recommendation settings [29] that form user-item interaction records into a time-independent matrix and model each user-item pair separately, sequential recommendation is more accordant with real situations. It could î€exibly adjust recommendation results with the emergence of usersâ€™ latest behaviors. In the literature of sequential recommendation, there are roughly two branches of studies: session-based recommendation [13] and user identity-aware sequential recommendation [27]. For the former branch, it tends to consider usersâ€™ short behavior sequences in recent history. Usually, user identities are not available for sessionbased recommendation. This is because in short sessions, users may not log onto accounts. As a result, the majority of studies in this regard focus on building models with item sequences [15]. For user identity-aware sequential recommendation, it is accustomed to taking long-term behavior sequences. And user identity (ID) is commonly assumed to be known for model construction. This paper concentrates on the latter branch for leveraging usersâ€™ long-term behaviors, which is more promising to pursue better recommendation performance. Unless otherwise speciî€›ed, we use sequential recommendation to represent user identity-aware sequential recommendation throughout this paper. Sequential recommendation has been extensively studied in recent years [9,14,20,23,27]. Most of the studies share the same spirit that models the change of behavior sequences to characterize the dynamic user interests and learn the corresponding user representations. As a supplement, user IDs are usually mapped to the same embedding space to enhance user representations. Although the temporal dynamics in the user side are largely investigated, item representations are often assumed to be static by existing studies. As such, the sequential patterns involved in the item side are often overlooked. For example, down jackets are bought by more and more users with the coming of winter. Grasping such popularity trends of items is hopeful for deriving better item representations, which would bring a positive eî€ect on sequential recommendation. To model the temporal dynamics in both the user and item sides, only a very few studies have been conducted [24,39,41]. They share a similar workî€ow that two deep sequential models (e.g., recurrent neural networks) are built for the user and item side, Figure 1: Sketch for the time-sliced user-item interaction graph construction. Each dashed box corresponds to a time slice. It is worth noting that the numbers of users/items are not exactly the same for diî€erent time slices since not all users and items occur in each time slice. respectively. The user behavior sequence (consisting of interacted items) and the item interaction sequence (consisting of interacted users) are commonly taken as model input to obtain their dynamic representations corresponding to diî€erent time steps/slices. Although performance improvements could be achieved as compared to the sequential recommendation models only considering user dynamics, the workî€ow still encounters one major limitation: the abundant interactions between users and items happened in the past are often neglected and not explicitly modeled in a global perspective. This will inevitably cause the user and item representations in user behavior sequences and item interaction sequences to be sub-optimal, which further aî€ect the target user and item representations. In this paper, we devise a novel Dynamic Representation Learning model for Sequential Recommendation (DRL-SRe) to tackle the above limitation. It relies on the segmentation of the whole timeline into a speciî€›ed number of equal-length time slices. For each time slice, a global user-item interaction graph is built to take all the users and items occurring in this slice as nodes. Therefore it enables us to distill knowledge from all the user-item interactions to beneî€›t the user and item representation learning. The sketch for the time-sliced user-item interaction graph construction is shown in Figure 1. Given a sequence of the constructed user-item interaction graphs, DRL-SRe develops time-sliced graph neural networks over diî€erent time slices to propagate node representations so as to get user and item representations w.r.t. each time slice. To correlate the timesliced representations belonging to the same user or item, DRL-SRe utilizes two recurrent neural networks (RNNs) for the user side and the item side, respectively. As such, the graph neural networks of diî€erent time slices are integrated into a uniî€›ed model architecture. To alleviate the issue of temporal information loss caused by the timeline segmentation, we further propose an auxiliary temporal prediction task over consecutive time slices based on temporal point process [7]. This empowers DRL-SRe with the ability to perceive the î€›ne-grained temporal information and compensates for the primary sequential recommendation task. In the end, DRL-SRe generates the possibility of the interaction between the target user and item in the near future. We summarize the main contributions of this paper as follows: â€¢To our best knowledge, we are the î€›rst study to propose timesliced graph neural networks for sequential recommendation, which is able to model the abundant and high-order user-item interactions from a whole perspective to obtain better dynamic user and item representations. â€¢In order to enable DRL-SRe to capture î€›ne-grained temporal information, we integrate temporal prediction over consecutive time slices into the training of the proposed model and show that the auxiliary task can obviously promote the sequential recommendation task. â€¢Extensive evaluations on three public real-world datasets have demonstrated the proposed model signiî€›cantly outperforms the state-of-the-art sequential recommendation models and validated the contributions of its crucial components. This section concretely discusses the sequential recommendation studies and the relevant techniques for recommendation, i.e., graph neural networks and temporal point process. Compared to session-based recommendation [15], sequential recommendation studied in this paper considers user IDs and their behavior sequences in a longer time period. The early work [45] proposes to learn usersâ€™ whole feature sequences by recurrent neural networks, wherein user IDs are simply taken as input. On the contrary, Quadrana et al. [27] segmented the whole user behavior sequences into multiple short sessions and adopted a hierarchical recurrent neural network to integrate these sessions. To discriminate the short-term user dynamics and long-term user preference, the studies [9,19] î€›rst learn long- and short-term representations and then fuse them for prediction. Beyond the above schemes for handling sequences, some recent studies investigate advanced mechanisms to obtain behavior sequence representations [14], such as self-attention mechanisms [33], or apply sequential recommendation to speciî€›c scenarios where additional information (e.g., spatial information) is utilized [20]. However, these studies do not consider the temporal dynamics in the item side. Among the studies for sequential recommendation, [24,39,41] are most relevant to this study. The pioneering model RRN [39] utilizes two RNNs to learn the temporal dynamics of the user side and the item side, respectively. The output representations of the last time step for the two RNNs are coupled to denote user preference and item features for computing the interaction probability. DEEMS [41] shares a similar spirit of applying two RNNs to get user and item representations, and provides a new perspective of loss function construction. It not only adopts a user-centered sequential recommendation loss, but also introduces item-centered information dissemination loss to form an enriched loss function. Since the above two methods loosely couple the two sequences and model them independently, SCoRe [24] is proposed to leverage the past user-item interactions to facilitate the representation learning. Nevertheless, the considered user-item interactions by SCoRe are limited to the direct neighbor nodes of a target user-item pair, thus lacking an eî€ective manner to globally utilize all user-item interactions in a time slice. It is worth noting that there are several studies [2,16,18] that learn user and item representations in a continuous time fashion. Nevertheless, they are conî€›ned to the setting where the next useritem interaction time is required to be known in advance to derive real-time user and item representations, which is entirely diî€erent from this study. Moreover, each user-item interaction in the history could only be modeled once under this situation, thus lacking an eî€ective mechanism to capture high-order relations between users and items through repeated propagation over user-item interactions. Graph Neural Networks.Graph Neural Networks (GNNs) [46] have been undergoing rapid development currently and observing many applications in recommender systems. Because user-item interactions are one kind of edges, some studies [11,37] convert the commonly used user-item rating matrix into a user-item interaction graph and develop GNNs to act upon it. Despite user-item interactions, some other studies design GNNs for modeling user social relations [8,31] and item relations in knowledge graphs [34]. Besides, the scalability issue of GNNs in recommender systems is also treated. For example, node sampling is used in representation propagation in graphs [43]. In light of session-based recommendation where only user dynamics are considered, SR-GNN [42] is the î€›rst GNN-based model that builds an item-item interaction graph for each session. Inspired by this, three recent studies [25,35,38] have put forward to build a global item-item interaction graph over all sessions instead of each session. Moreover, LESSR [3] proposes to preserve the order of interactions when using GNNs for each session. However, very few studies have investigated GNNs for the studied sequential recommendation problem where both user and item dynamics should be modeled based on user-item interactions in time slices. Although GLS-GRL [36] models user-item interactions in the sequential scenario, the GNN-based model design serves for learning user group representations and the recommended items are towards user groups but not individuals. In this paper, we develop time-sliced graph neural networks for modeling user-item interactions in multiple graphs to realize sequential recommendation. Note that although a few studies [22,28,40] research on dynamic graph neural networks over graph snapshots/time slices, none of them have been applied to the scenario of sequential recommendation. Temporal Point Process.Temporal point process [7] is an elegant mathematical tool for modeling asynchronous time series. Its highlight is to capture the continuous temporal gaps compared to time discretization methods. The last several years have witnessed its application to user behavior trajectories [4,6,21,32]. Most of these studies adopt sequential models to achieve interaction prediction and time prediction simultaneously. By contrast, this paper diî€ers from them by conducting temporal prediction over the past consecutive time slices, hoping to mitigate the temporal information loss incurred by the time slices. In the domain of recommender systems, a matrixğ‘¹ = {ğ‘Ÿ}is usually deî€›ned to record user-item interactions. We assumeğ‘¢ âˆˆ U andğ‘– âˆˆ I, andğ‘€andğ‘represent the size of user setUand item setI, respectively. The values in the matrix are determined based on whether user feedback is explicit (e.g., rating) or implicit (e.g., click). Here we focus on implicit feedback since it is more common and easily obtained than explicit feedback. As such, we deî€›neğ‘Ÿ=1 if userğ‘¢has interacted with itemğ‘–, andğ‘Ÿ=0 otherwise. To adapt to sequential recommendation, each user-item interaction is associated with a timestampğ‘¡to indicate when the interaction happens. Accordingly, we have a triplet(ğ‘¢, ğ‘–, ğ‘¡)to denote one interaction and deî€›neT = {(ğ‘¢, ğ‘–, ğ‘¡)}to cover all the observed interactions. To facilitate the learning of dual dynamic representations based on graph neural networks, we segment the whole timeline intoğ‘‡ time slices with the equal-length time intervalÎ”ğ‘‡. Therefore we haveT = {T, T, ..., T}whereTcontains all the observed triplets that occur in theğ‘ -th time slice. It is worth noting that some users and items might not have any interaction in a time slice. Under this situation, the corresponding user and item representations would keep unchanged in that time slice. Now we proceed to formulate the sequential recommendation problem as follows: Problem 1 (Seqential Recommendation Problem). For a target userğ‘¢, a candidate itemğ‘–, and all the observed user-item interactionsT, the aim of the sequential recommendation problem is to learn a functionğ‘“that predicts their potential interaction probability in the near future, which is deî€›ned as Overview:Figure 2 depicts the overall architecture of the proposed model DRL-SRe, which takes all the observed user-item interactions as input and outputs the predicted interaction probability for a target user-item pair. Within the model, time-sliced graph neural networks are proposed to learn two sequences of representations for the user and item sides. Each representation in the two sequences corresponds to one time slice, facilitating to characterize the temporal dynamics. Then the user and item representations of the current time slice, together with the static representations gotten from user and item IDs, are fed into the prediction part. For the optimization part, the primary sequential recommendation lossL and the auxiliary temporal prediction lossL, consisting ofLfor itemğ‘–andLfor userğ‘¢, are leveraged to jointly optimize the model end-to-end. In what follows, we organize the model speciî€›cation in a logical fashion. Following the convention of modern deep recommendation methods [44], we map a user ID or an item ID to a dense vectorized representation. This time-independent representation is thus static and used to reveal user long-term interests and item long-term characteristics. Formally, taking userğ‘¢and itemğ‘–as examples, we deî€›ne the following mapping functions: whereğ‘¬âˆˆ Randğ‘¬âˆˆ Rare trainable embedding matrices for users and items, respectively.ğ‘‘denotes the embedding dimension.ğ’is the one-hot encoding for userğ‘¢and it is analogous to ğ’of item ğ‘–. Before delving into the computational formulas of this module, we î€›rst clarify how to build user-item bipartite graphsG = {G, G, ..., G}for each time slice. Taking graphGfor example, we build it based on all the user-item interaction records inT. Assume there areğ‘€users andğ‘items occurring in theğ‘ -th time slice, then we have a node feature matrixğ‘¿âˆˆ Rand an adjacency matrixğ‘¨âˆˆ {0,1}for the graph. An entry in the adjacency matrix takes a value of 1 if the corresponding user-item interaction occurs, and 0 otherwise. We do not take î€›ne-grained edge weights since the ratios of repeated user-item interactions in a time slice are very small or even 0 for the used datasets. However, we should emphasize that our graph could be easily generalized to take î€›ne-grained edge weights if necessary. The module of time-sliced graph neural networks is responsible for learning node representations based on the constructed graphs. By convention, it performs representation propagation along with the edges of graph G, which is deî€›ned as follows: whereË†ğ‘¨= ğ‘«ğ‘¨ğ‘«denotes the normalized adjacency matrix without self loops andğ‘™is the index of the propagation layer. We letğ‘¿= ğ‘¿, consisting of input user and item representations (e.g., ğ’†and ğ’†). After propagating forğ¿layers, we obtain multiple layer-wise representation matrices, i.e.,Ëœğ‘¿= [ğ‘¿;ğ‘¿;. . .;ğ‘¿]. Since the representations of diî€erent layers capture diî€erent semantics, it is urgent to design a reasonable method to combine these representations. We implement î€›ve types of operations and validate user/item-speciî€›c GRUs [5] can lead to good performance in general (see experimental results in Section 5.2.3). To be speciî€›c, we useËœğ‘¿to denote the user-speciî€›c representations inËœğ‘¿and it is analogous toËœğ‘¿. Consequently, we obtain the updated user representationsÂ¯ğ‘¿and item representationsÂ¯ğ‘¿by: whereGRU(Â·)|means returning the hidden representation from the recurrent step ofğ¿ +1.Î˜andÎ˜are the learnable parameters for the respective GRU model. Finally, to model the evolution of the temporal dynamics in the user and item sides, we further correlate the user and item representations across time slices by another two GRUs. This is achieved by collecting time-sliced user representations and item representa- Upon this, we perform recurrent computations to incorporate sequential information into the contextualized representations by: where Î˜and Î˜are the parameters of the above GRU models. Based on the proposed time-sliced graph neural networks, we have two sequences of representations for userğ‘¢and itemğ‘–, i.e., the potential interaction probability for the considered user-item pair, we combine their dynamic representations and static embeddings. Speciî€›cally, we take the output of time-sliced graph neural networks, i.e., the last time-sliced user representationsğ’‰for user ğ‘¢and item representationsğ’‰for itemğ‘–, as the dynamic representations. The representations mapped from IDs are regarded as static representations. After concatenating them, multi-layer perceptrons (MLPs) are adopted to calculate the probability, which is given by: where we use ReLU as the middle-layered activation function.ğœis the sigmoid function to let the value has a range(0,1).âŠ•means the concatenation operation.Î˜represents all the parameters involved in MLPs. For training the model DRL-SRe, we formulate a sequential recommendation loss function w.r.t. the user-item pair(ğ‘¢, ğ‘–)based on the cross-entropy loss, which is given by: As discussed previously, although the timeline segmentation enables to model high-order and complex user-item relations through the time-sliced graph neural networks, the detailed temporal information is inevitably lost, which might cause the representationsğ‘¯ andğ‘¯to be sub-optimal. To mitigate this issue, we introduce an auxiliary temporal prediction task based on temporal point process to compensate for the primary task. Temporal point process takes the conditional intensity function ğœ†(ğ‘¡)as the most important component in capturing the continuoustime temporal dynamics. By convention, we usein a function to indicate it is history-dependent. Within a short time interval [ğ‘¡, ğ‘¡ + ğ‘‘ğ‘¡),ğœ†(ğ‘¡)represents the occurrence rate of an event given the historyğ»and satisî€›es:ğœ†(ğ‘¡)ğ‘‘ğ‘¡ = ğ‘ƒ {ğ‘ âˆˆ [ğ‘¡, ğ‘¡ + ğ‘‘ğ‘¡]|ğ»}. Based on this, the density function is given by: whereğ‘¡is the timestamp of the last event or a starting timestamp. Under the situation of time-sliced user-item interaction graphs, we take the last interaction of a user or an item in a time slice as an event. The corresponding time of the interaction is regarded as the event time. Particularly, we use[ğ‘¡, ğ‘¡, Â· Â· Â· , ğ‘¡]and[ğ‘¡, ğ‘¡, Â· Â· Â· , ğ‘¡] to denote the detailed temporal information for userğ‘¢and itemğ‘– in diî€erent time slices, respectively. Following the study [6] that provides a well-designed conditional intensity function to derive an analytical form of the density function, we deî€›ne the conditional intensity functions for userğ‘¢and item ğ‘– w.r.t. the ğ‘ -th time slice as follows: whereÎ˜= {ğ’˜, ğœ”, ğ‘}andÎ˜= {ğ’˜, ğœ”, ğ‘}are the trainable parameters. Through this manner, both the dynamic representations and temporal information are associated with intensities to characterize the continuous-time gaps over consecutive time slices from the dual sides. The analytical form of the density functions could now be easily obtained. For user ğ‘¢, it is deî€›ned as follows: ğ‘“(ğ‘¡) = ğœ†(ğ‘¡)ğ‘’ğ‘¥ğ‘ (âˆ’ğœ†(ğœ–) ğ‘‘ğœ–) = exp{ğ’˜ğ’‰+ ğœ”(ğ‘¡ âˆ’ ğ‘¡) + ğ‘+exp(ğ’˜ğ’‰+ ğ‘)(10) âˆ’exp(ğ’˜ğ’‰+ ğœ”(ğ‘¡ âˆ’ ğ‘¡) + ğ‘)} . Analogously, its density function could be derived for itemğ‘–. Finally, the loss function of the auxiliary temporal prediction task is formulated as the negative joint log-likelihood of generating temporal sequences: Beneî€›ting from this loss function, learningğ’‰andğ’‰is guided by the î€›ne-grained continuous-time information. In the end, we unify the primary sequential recommendation task and the auxiliary temporal prediction task for jointly training DRLSRe. The hybrid objective function is deî€›ned as follows: whereğ›½is a hyper-parameter to control the relative eî€ect of the auxiliary task. The lossLis easy to be extended to a mini-batch setting where multiple user-item interactions are included. L2 regularization and dropout strategies are commonly employed to alleviate the overî€›tting issue. We use the Adam optimizer to learn the model parameters Î˜ = {ğ‘¬, ğ‘¬, Î˜, Î˜, Î˜}. Since the time-sliced graphs are only constructed for users and items having interactions in corresponding time slices, the number of the edges from all time-sliced graphs are of the same order of magnitude as the edge number of a global user-item interaction graph. Also, the complexity of sparse matrix multiplication depends on the number of edges in the Laplacian matrix. As such, the computation does not cost much. This section î€›rst clariî€›es the experimental setups and then provides comprehensive experimental results, striving to answer the pivotal questions below: Q1.What are the results of the comparison between DRL-SRe and the state-of-the-art sequential recommendation models? Q2.Does the performance of DRL-SRe suî€er from a notable drop when removing any crucial component from the model? Q3.How do alternative designs and hyper-parameter settings of DRL-SRe aî€ect the î€›nal performance? # Interactions 699,254 4,402,067 10,971,024 5.1.1 Datasets. To evaluate the performance of all the models while ensuring reliability, we choose three datasets that are publicly available and with diî€erent origins, which are introduced as follows: Babyis a category-speciî€›c subset extracted from the large public dataset [10] of the e-commerce platform Amazon, ranging from May 1996 to July 2014. This kind of dataset source is used in the relevant dual sequence model [41]. Yelpis a review-based dataset that was released by Yelp in 2019. It contains user ratings on businesses such as restaurants, bars, and spas, etc. We regard the ratings with scores larger than 3 as positive feedback and select their corresponding user-item interactions to build the dataset. Netî€ix[1] is a commonly-used dataset from the Netî€ix contest which contains 100M ratings collected between November 1999 and December 2005. This kind of dataset source is also used in sequential recommendation with dual sides [39]. To ensure the statistical signiî€›cance of the datasets, we remove the users and items with less than 5 interactions. The basic statistics of the three datasets are summarized in Table 1. As shown in the line of â€œTime slicesâ€ in the table, we segment the whole timeline intoğ‘‡time slices with an equal-length time interval â€” shown in the line of â€œTime intervalâ€ â€” for each dataset. We use the user-item interactions coming from the î€›rst time slice to the(ğ‘‡ âˆ’2)-th time slice as the training data. The interactions in the(ğ‘‡ âˆ’1)-th time slice are collected to constitute the validation data. And the interactions in the last time slice are leveraged as the testing data. This data setting is shared by all the adopted sequential recommendation models introduced later, so as to make fair comparisons. It is worth noting that deep sequential recommendation models are usually formulated as predicting the next interacted item. But they could be easily extended to train and predict the next several items [30] that will be interacted with a target user in the next time slice. 5.1.2 Evaluation Protocols. We choose three widely used metrics in recommender systems: (1) HR@k (Hit Ratio@k) is the proportion of recommendation lists that have at least one positive item within top-k positions. (2) NDCG@k (Normalized Discounted Cumulative Gain@k) is a position-aware ranking metric that assigns larger weights to the top positions. As the positive items rank higher, the metric value becomes larger. (3) MRR (Mean Reciprocal Rank) measures the relative position of the top-ranked positive item and takes a value of 1 if the positive item ranks at the î€›rst position. As such, HR@k and MRR mainly focus on the î€›rst positive item, while NDCG@k considers a wider range of positive items. Since it is too time-consuming to iterate over all user-item pairs to generate the complete sorting list and compute the above metrics, we follow the negative sampling strategy which is commonly observed in recommendation studies considering implicit feedback [12,14,24]. Speciî€›cally, for each ground-truth item, we randomly sample 100 negative items for metric computation. Without loss of generality, we show the results w.r.t. HR@10, NDCG@10, and MRR. 5.1.3 Baselines. We choose some representative sequential recommendation models that only consider the temporal dynamics in the user side as follows: o GRU4Rec[13]. This is a pioneering model that successfully applies recurrent neural networks to the sequential recommendation problem. It takes the interacted items for a target user as input to generate sequential recommendations. o Caser[30]. To see how convolutional neural networks (CNNs) behave on the used datasets, we choose Caser that combines CNNs and a latent factor model to learn usersâ€™ sequential and general representations. o SASRec[14]. SASRec is a well-performed model that heavily relies on self-attention mechanisms to identify important items from a userâ€™s behavior history. These important items aî€ect user representations and î€›nally determine the next-item prediction. o TiSASRec[17]. It is recently proposed by the same research group of SASRec and could be regarded as its enhancement. In particular, continuous time interval information between interacted items is encoded to facilitate the self-attention computation. o ARNPP[21]. ARNPP is a joint item and time prediction model that is empowered by temporal point process and attentive recurrent neural networks. Note that social relations utilized in the study [21] are not considered in this paper. o LESSR[3]. This model inherits the beneî€›t of graph neural networks for session-based recommendation and additionally proposes an edge-order preserving aggregation strategy to capture the sequential order of interacted items. We also take the following three well-known dual sequential recommendation models for comparisons: o RRN[39]. As the î€›rst model to address the dynamics in dual sides under the situation of sequential recommendation, it couples two RNNs to model the user temporal patterns and item temporal patterns, respectively. o DEEMS[41]. DEEMS also relies on the architecture of the coupled RNNs and improves the objective function with a novel perspective of building dual prediction tasks. We use DEEMSRNN for its slightly better performance. o SCoRe[24]. This is the state-of-the-art dual sequence recommendation model that considers cross-neighbor relation modeling to better model relations between a target user and a candidate item. An interactive attention mechanism is adopted to gain an overall representation for the user or the item. 5.1.4 Model Implementations. We implement our model by Tensorî€ow and deploy it on a Linux server with GPUs of Nvidia GeForce GTX 2080 Ti (11G memory). The model is learned in a mini-batch fashion with a size of 100. For the adopted Adam optimizer, we set the learning rate to 5e-4 and keep the other hyper-parameters by default. We add L2 regularization to the loss function (Equation 6) by setting the regularization weight to 1e-4. The embedding size of all the relevant models is î€›xed to 16 for ensuring fairness. Table 2: Main results w.r.t. HR@10, NDCG@10, and MRR for sequential recommendation on three datasets. The best and second-best performed methods in each metric are highlighted in â€œboldâ€ and underline, respectively. Improv. denotes the relative improvement over the second-best results. The number of layers used in MLP is set to 3. The source code of this paper is available athttps://github.com/weizhangltt/dualrecommend. 5.2.1 Performance Comparison (Q1). Table 2 presents the overall performance of our model and all the adopted baselines, from which we have the following key observations: â€¢Compared with other models, GRU4Rec achieves poor results on the three datasets. This conforms to the expectation since only using the representation from the last time step of an RNN is insuî€œcient. The reason is that standard RNNs are not good at modeling long-range dependencies and have the forgetting issue. As such, user preference information hidden in behavior sequences could not be eî€ectively distilled. â€¢SASRec outperforms both the RNN-based model GRU4Rec and the CNN-based model Caser. Such improvement might be attributed to the self-attention mechanism, which can assign larger weights to the important interacted items that will aî€ect future user-item interactions. â€¢Compared to other sequential recommendation models that do not consider the dynamics of the item side, TiSASRec achieves better performance in most cases. It makes sense as TiSASRec is an enhancement of SASRec by additionally considering the time interval information between the interacted items in behavior sequences. Thanks to this, attention computation is further promoted. â€¢By comparing ARNPP with GRU4Rec, we could see the beneî€›ts brought by temporal point process. By further investigating the performance of LESSR, we î€›nd it is even comparable with some dual sequence recommendation models in some cases. This demonstrates the power of graph neural networks in modeling user-item interactions. â€¢The three dual sequential recommendation models exhibit the relatively good performance from a whole perspective, demonstrating the positive eî€ect of modeling the dual dynamics. Among them, RRN is not as good as the other two models. This might be caused by the limited ability of RNNs in modeling long-range dependency in a sequence. DEEMS obtains better results than RRN, showing the positive contribution of introducing the dual prediction loss. Thanks to the incorporation of local user-item interactions constrained to one sequence, SCoRe achieves the second-best results in many cases. â€¢Our model DRL-SRe yields consistently better performance than all the baselines. In particular, DRL-SRe improves the secondbest performed models w.r.t. NDCG@10 by 3.39%, 10.22%, and 9.62% on Baby, Yelp, and Netî€ix, respectively. This makes sense because: (1) DRL-SRe can distill knowledge from all the user-item interactions that happened in the past to beneî€›t the dynamic user and item representation learning. (2) The introduction of the auxiliary temporal prediction task over consecutive time slices can signiî€›cantly improve the sequential recommendation task, which is validated in Table 3. Table 4: Results of model variants for DRL-SRe. Improvements over variants are statistically signiî€›cant with p < 0.01. 5.2.2 Ablation Study (Q2). We further conduct an ablation study to validate the contributions of key components in DRL-SRe. Specifically, (1) â€œw/o graphâ€ represents removing graph neural networks and using simple mean-pooling to aggregate the neighbor information for users and items. (2) â€œw/o RNN (time slices)â€ means not using Equation 4 and performing a mean-pooling operation for the user/item representations obtained by time-sliced graph neural networks. (3) â€œw/o concat ID embeddingâ€ simpliî€›es Equation 5 by not concatenating the static user and item embeddings. (4) â€œw/o auxiliary taskâ€ is equivalent to settingğ›½ =0 in Equation 12, meaning not utilizing the auxiliary task. And â€œ- w/ time embeddingâ€ leverages temporal embeddings for discretized time slots as model input instead of using the temporal prediction task. Throughout the result analysis of the ablation study shown in Table 3, we observe that: â€¢â€œw/o graphâ€ suî€ers severe performance degradation. It validates the crucial role of time-sliced user-item interaction graphs for characterizing the dynamics of users and items. This is intuitive since learning from full user-item interactions in a time slice could lead to accurate time-sliced user and item representations. â€¢â€œw/o RNN (time)â€ also sees its signiî€›cant performance drop. This phenomenon reveals that explicitly correlating time-sliced user representations and item representations is indispensable. The reason might be that RNNs could introduce sequential information that each time-sliced representation does not have. â€¢The concatenation of static embeddings has a positive impact by seeing the results of â€œw/o concat ID embeddingâ€. â€¢ â€œw/o auxiliary taskâ€ is obviously inferior to the full model DRL- SRe, verifying that using temporal point process to capture î€›negrained temporal information is advantageous for deriving effective dynamic representations. Moreover, simply feeding time embeddings into model input is not as good as leveraging the temporal prediction task for sequential recommendation. 5.2.3 Analysis of Model Alternatives (Q3). To deeply investigate why the proposed model works, we design several alternatives of the model: (1) â€œw/ global graphâ€ replaces the multiple timesliced graphs with a global user-item interaction graph. (2) â€œw/ last graphâ€ replaces the multiple time-sliced graphs with a last user-item interaction graph. (3) â€œw/ user slicesâ€ only uses the sequences w.r.t. interacted users to do î€›nal prediction, i.e.,ğ’‰âŠ•ğ’†âŠ•ğ’†in Equation 5. (4) â€œw/ item slicesâ€ only adopts the sequences w.r.t. interacted items to perform prediction, i.e., ğ’‰âŠ• ğ’†âŠ• ğ’†in Equation 5. Besides, we investigate some diî€erent operations to fuse layerwise representations from time-sliced graph neural networks, as shown in Equation 3. â€œw/ concatenationâ€ means concatenating the layer-wise representations. â€œw/ last-layer outputâ€ denotes only using the representation of the last layer. â€œw/ mean-poolingâ€ uses mean-pooling to summarize the layer-wise representations. And â€œw/ single GRUâ€ does not diî€erentiate the user side and the item side by only employing a single GRU. From the results shown in Table 4, we î€›nd that: â€¢In the î€›rst part of the table, the performance degradation of â€œw/ global graphâ€ is signiî€›cant. It is intuitive since only using a global user-item interaction graph is unable to capture the temporal dynamics of users and items, although the ability to encode enriched user-item interactions by GNN is maintained. Similarly, â€œw/ last graphâ€ is degraded heavily, which reî€ects the great necessity of modeling long behavior sequences for the studied task. Moreover, by evaluating the results of â€œw/ user slicesâ€ and â€œw/ item slicesâ€, we could conclude that learning from dual dynamics is proî€›table. â€¢In the second part of the table, the four alternatives perform worse than using user/item-speciî€›c GRUs in DRL-SRe. In particular, the comparison between â€œw/ single GRUâ€ and the full model validates that diî€erentiating the dual sides of users and items consistently boosts the recommendation performance. 5.2.4 More Discussions (Q3). This part provides some more discussions about how some hyper-parameters aî€ect DRL-SRe, including hyper-parameter ğ›½ and the propagation layer number of GNNs. Figure 3: Result variation with diî€erent ğ›½ on Yelp (left) and Netî€ix (right). Eî€ect of hyper-parameter ğ›½.To analyze the inî€uence of hyperand illustrate the recommendation performance changing curve of DRL-SRe on the Yelp and Netî€ix datasets in î€›gure 3. We can observe that better results are achieved when settingğ›½in a suitable middle value range and too large values might even hurt the performance. Figure 4: Result variation when increasing the number of propagation layers (shown in x-axis). Eî€ect of layer numbers in GNN.To investigate whether our model can beneî€›t from the multi-layer propagation, we vary the number of GNN layers in the range of{1,2,3,4,5}. Figure 4 depicts the performance trend by HR@10. When increasing the layer number from 1 to 2, DRL-SRe achieves consistently better performance on the three datasets, indicating that introducing neighbor information through representation propagation is proî€›table. However, the continual increase of the layer number even harms the performance. This might be caused by the over-smoothing and overî€›tting issues. This paper studies the sequential recommendation problem by modeling the dual temporal dynamics for both the user and item sides. The novel model DRL-SRe is proposed, which innovatively develops the time-sliced graph neural networks to learn dynamic user and item representations, and introduces the auxiliary temporal prediction task to compensate for the primary sequential recommendation task based on temporal point process. The comprehensive experiments conducted on three public and large datasets validate the superiority of DRL-SRe and the eî€ectiveness of its main components. This work was supported in part by National Natural Science Foundation of China under Grant (No. 62072182) and the Fundamental Research Funds for the Central Universities.