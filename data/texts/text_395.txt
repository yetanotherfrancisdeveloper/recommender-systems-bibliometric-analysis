Personalized outî€›t recommendation has recently been in the spotlight with the rapid growth of the online fashion industry. However, recommending outî€›ts has two signiî€›cant challenges that should be addressed. The î€›rst challenge is that outî€›t recommendation often requires a complex and large model that utilizes visual information, incurring huge memory and time costs. One natural way to mitigate this problem is to compress such a cumbersome model with knowledge distillation (KD) techniques that leverage knowledge from a pretrained teacher model. However, it is hard to apply existing KD approaches in recommender systems (RS) to the outî€›t recommendation because they require the ranking of all possible outî€›ts while the number of outî€›ts grows exponentially to the number of consisting clothing items. Therefore, we propose a new KD framework for outî€›t recommendation, called False Negative Distillation (FND), which exploits false-negative information from the teacher model while not requiring the ranking of all candidates. The second challenge is that the explosive number of outî€›t candidates amplifying the data sparsity problem, often leading to poor outî€›t representation. To tackle this issue, inspired by the recent success of contrastive learning (CL), we introduce a CL framework for outî€›t representation learning with two proposed data augmentation methods. Quantitative and qualitative experiments on outî€›t recommendation datasets demonstrate the eî€ectiveness and soundness of our proposed methods. Personalized Outî€›t Recommendation, Knowledge Distillation, False Negative Distillation, Contrastive Learning Personalized outî€›t recommendation is the task of determining the preference of a user to an input outî€›t that consists of multiple clothing. It has recently attracted attention with the rapid growth of the online fashion industry, and several related studies [22,24,25] have been conducted. However, despite the success of existing works, outî€›t recommendation has two signiî€›cant challenges that should be addressed. First, recommending outî€›ts often requires a complex and large model that involves the utilization of visual information (i.e., images) [24]. Such a large model incurs high latency and memory costs during the inference phase, making it diî€œcult to apply to real-time services [14]. The second challenge is that outî€›t recommendation inevitably suî€ers from the data sparsity problem because the possible pool of outî€›t data grows exponentially to the number of consisting clothing items [22]. The sparsity problem often leads to poor learning of outî€›t representation, which hinders achieving satisfactory recommendation performance [40]. Figure 1: The user-outî€›t space is a representation vector space of the trained teacher model. We deî€›ne the positive boundary as an average distance between a user and its positive outî€›ts. We treat negative outî€›ts inside the positive boundary as false-negative outî€›ts. To address the î€›rst challenge stemming from a large model, one can employ knowledge distillation (KD) techniques that compress a model by transferring knowledge from a large teacher model to a small student model. Accordingly, one may try to apply existing studies [14,18,35] of KD available in recommender systems (RS) to the outî€›t recommendation. However, existing methods leverage predicted ranking of all possible outî€›ts from the teacher model, so they are not applicable in outî€›t recommendation tasks with explosively large pools. Therefore, we propose a novel KD framework named False Negative Distillation (FND) that does not require the ranking of all outî€›t candidates. Similar to most outî€›t recommendation studies [22,24,25], FND utilizes a ranking loss that pulls observed (positive) outî€›ts to a user while pushing unobserved (negative) outî€›ts. As illustrated in Fig. 1, FND claims that unobserved is not the same as true-negative and assumes that negative outî€›ts close enough to the user are false-negative outî€›ts. We show through various experiments that FND is eî€ective, and the assumption is reasonable. The approach for the second challenge is to deal with the problem of poor outî€›t representation. As learning enhanced representation of entities is one of the core components to achieve high performance in deep learning [2], numerous works [3,6,8,27] from diverse domains accomplished this with self-supervised learning (SSL) techniques. There have been several studies [26,41,44] utilizing SSL techniques in RS as well. Among them, more recent works [23,40,43] exploit contrastive learning (CL), especially SimCLR [3], which learns meaningful representations by pulling the diî€erently augmented view of the same data while pushing the others in the batch. Nevertheless, leveraging CL in outî€›t recommendation is relatively unexplored. Hence, as illustrated in Fig. 2, we introduce Figure 2: Example of contrastive learning for outî€›t recommendation. We randomly alter (erase/replace) one item from an outî€›t to generate two diî€erent views. In the case of replace, it substitutes an item with a similar one using our pretrained autoencoder model. In this example, the î€›rst and the second augmentations are erase and replace, respectively. The left outî€›t erased the shoes and replaced the top. The right outî€›t erased the bottom and replaced the bottom. an approach to make use of CL in outî€›t recommendation, along with two proposed data augmentation methods (erase /replace). To demonstrate the eî€ectiveness of our proposed methods, we conduct extensive experiments on outî€›t recommendation datasets. We compare our approaches with state-of-the-art outî€›t recommendation methods with quantitative performance evaluations. In addition, we study the impact of hyper-parameters and the model size with various experiments. We use visualization to show that the intuitive assumption of our FND illustrated in Fig. 1 is sound. We also experiment on the cold-start scenario where users have very few outî€›ts interacted, and the trained model does not have any knowledge of those users. To make appropriate recommendations to cold starters, we introduce two practical strategies that do not require additional training of the model. Our main contributions can be summarized as follows: â€¢We propose a new knowledge distillation framework that can be utilized in outî€›t recommendation tasks without requiring the ranking of all outî€›t candidates in the system. â€¢We propose two novel outî€›t data augmentation methods to leverage contrastive learning in outî€›t recommendation. â€¢We introduce two practical strategies to deal with the coldstart problem. â€¢We demonstrate the eî€ectiveness and soundness of our approaches with comprehensive experiments on fashion outî€›t recommendation datasets. Based on whether the individual preference is neglected or not, existing outî€›t recommendation studies can be classiî€›ed into two categories: non-personalized [4,10,20,32,34,37,38,42] and personalized [12,19,22,24,25] outî€›t recommendation. Lu et al. [25] used pairwise scores, and they employed the weighted hashing technique to tackle the eî€œciency problem. Lin et al. [22] utilized an attention mechanism to estimate the preference score, weighting items in an outî€›t diî€erently. Lu et al. [24] exploited Set Transformer [17], the state-of-the-art model for set-input problems, to capture the high-order interactions among fashion items. They also disentangled each user into multiple anchors to accommodate the variety of preferences. Note that methods based on graph neural networks [4,19,32] or predicting distribution over whole clothing items [42] require the test items to be in the training set. Knowledge distillation is a model-agnostic compression strategy for generating eî€œcient models. Since the early success of KD in image recognition [11,30], KD has been widely accepted in other î€›elds. In recommendation tasks, several works [14,18,35] have employed KD. They rank all items with the teacher model and utilize the items of high rank when training the student model. Tang et al. [35] considered top-ğ¾items as false-negatives and diî€erentiated their relative importance based on their rankings. Lee et al. [18] trained the student to mimic the predicted probabilities of the teacher on the sampled items of high rank. Kang et al. [14] achieved state-ofthe-art performance by transferring both the prediction and latent knowledge of the teacher. Contrastive learning is a framework for obtaining high-quality representations to boost the performance of downstream tasks and was î€›rst introduced in computer vision [3]. CL enhances representations by maximizing agreement between two diî€erently augmented views of the same data. A few works [23,40,43] applied CL to RS, and they showed notable success. In sequential recommendation, Xie et al. [40] used CL by applying three augmentation methods (crop/mask/reorder) to user interaction history. Yao et al. [43] focused on large-scale item recommendations and employed a two-stage augmentation consisting of masking and dropout. Liu et al. [23] utilized CL for graph neural network based RS by randomly removing some edges. We recommend outî€›ts to users based on their preference score. To compute the preference score, we use user embeddings and vector representations of outî€›ts. Due to the set-like nature of fashion outî€›ts, the representation model requires two conditions. First, the outî€›t representation should be invariant to the order of comprising fashion items. Second, the model should be able to process input outî€›ts of any size. To this end, we borrow the architecture from LPAE [24] model, which uses Set Transformer [17] module designed to address these set-input problems. As illustrated in Fig. 3, an outî€›tğ‘œwithğ‘›items is a tuple of fashion item images:ğ‘œ= (ğ‘¥, ğ‘¥, Â· Â· Â· , ğ‘¥) âˆˆ I.Letğ‘“:I â†’ Rbe a Figure 3: A brief architecture of computing the preference score of a user to an outî€›t. First, images comprising the outî€›t are transformed into item features by CNN with fully connected layers. After that, we get an outî€›t representation using Set Transformer. Finally, the cosine similarity between a user embedding and the outî€›t representation is the preference score of the user to the outî€›t. Convolutional Neural Network (CNN) with fully connected layers that encodesğ‘¥into an item feature vectorx= ğ‘“ (ğ‘¥), whereğ‘‘ is a feature dimension. Through Set Transformerğ‘‡:Râ†’ R, we obtain an outî€›t representationo= ğ‘‡ (X)from item features X= [xxÂ· Â· Â· x].Then, we compute the preference score with user embedding uâˆˆ Rfor each user ğ‘¢. 3.1.1 Set Transformer. Having beneî€›ted from the attention mechanism, Set Transformer can eî€ectively reî€ect high-order interactions among items in an outî€›t. As Lee et al. [17] have proved, Set Transformer is an order-free module that always produces the same output regardless of the sequence order. Attention is a map that gives the weighted sum of value vectors V âˆˆ Rwith the weights being determined by each query vector of Q âˆˆ Rand key vectors K âˆˆ R: The multi-head attention utilizes multiple attentions through concatenation to bear more potential relationships: forâ„attention maps, Following previous works [17,24], we useğ‘‘= ğ‘‘/â„andğ‘‘= To apply the attention mechanism to a set, Set Attention Block (SAB) uses self-attention with residual terms: for item features X, whereğœis any row-wise feed-forward layer, andLayerNorm(Â·)is Layer Normalization [1]. Multiple SABs can be stacked to encode higher-order interactions among the items: The î€›nal outputs of the attention blocks are then aggregated by applying another multi-head attention on a learnable seed vector s âˆˆ Ras follows: The obtainedoâˆˆ Ris a single compact vector representation of an outî€›tğ‘œ, holding compatibility relationships among consisting fashion items. 3.1.2 Preference score prediction. Given a userğ‘¢and an outî€›tğ‘œ, our model predicts the preference score of the user to the outî€›t as follows: Large models generally show relatively higher recommendation performance compared to their smaller counterparts. However, employing a small-sized model is necessary to reduce latency and memory costs during the inference phase. Therefore, we propose a novel knowledge distillation framework named False Negative Distillation (FND) that transfers false-negative information extracted from a well-trained large teacher model to a small student model. As illustrated in Fig. 1, in the user-outî€›t space of a trained teacher model, we assume that negative (i.e., unobserved) outî€›ts close enough to the user are false-negative outî€›ts. 3.2.1 Teacher model. Deep learning based recommendation models adopt learning to rank framework via deep metric learning in general. The goal is to maximize the ranking of positive outî€›ts given a predicted preference score. Many existing works [22,25,34], including LPAE [24], use triplet loss [31] or Bayesian personalized ranking (BPR) [29] as an optimization objective. However, they often suî€er from poor local optima, partially because the loss function employs only one negative outî€›t in each update [33]. To address this problem, our model utilizesğ‘-pair loss [33]. Aided by the temperature-scaled cross-entropy,ğ‘-pair loss can take multiple negative outî€›ts into account per positive outî€›t. Let the batchB, size ofğ‘, be a set of pairs(ğ‘¢, ğ‘œ), indicating that the userğ‘¢prefers the outî€›tğ‘œ. Each pair in the batch has a set of negative outî€›ts {ğ‘œ}, sampled in the training step. Note that the negative set mainly contains randomly generated outî€›ts and even can include positive outî€›ts of other users. Our objective for the teacher model is as follows: where ğœ> 0 is a temperature hyper-parameter. 3.2.2 Student model. Once the teacher model is trained, we optimize our student model with the help of additional false-negative information. The ordinaryğ‘-pair loss can be interpreted as â€œpullingâ€ positive outî€›ts to a user while â€œpushingâ€ the negatives, similar to triplet loss with user-anchor. Whenever a given negative outî€›t is determined as a false-negative, we wish to pull it rather than pushing it. Concretely, we determine the false-negativenessğ‘‘ based on the diî€erence between the preference score of a negative outî€›t and the average score of the positives: withğ½a set of indices of positive outî€›ts for a user ğ‘¢, whereğ›¼ >0 is a distillation scaling hyper-parameter andË†ğ‘Ÿdenotes the predicted score from the teacher. The sign ofğ‘‘determines whether the given negative outî€›t is false-negative or not, and the magnitude presents how much the negative should be pushed or pulled. The student model is trained through our proposed FND loss Las follows:î€€î€ â„“(ğ‘–, ğ‘—) = âˆ’ logexpğ‘Ÿ/ğœîƒ•, (13) Since the teacher model is frozen when training the student, ğ‘‘stays constant for eachğ‘–andğ‘—. Note that we use diî€erent validation sets when training the teacher model and the student model. Otherwise, the student model might learn information about the validation set via the teacher, which leads to overî€›tting. By examining the gradient ofL, it can be shown that our objective function pulls negative outî€›ts to the user whenever they are determined as false-negatives byğ‘‘. Supposeğ‘œis an element of the set of negative outî€›ts{ğ‘œ}for(ğ‘¢, ğ‘œ)in the training step. The gradient ofâ„“w.r.t the preference score ofğ‘¢toğ‘œis as follows:î€€î€ The sign of the gradient is the same asğ‘‘sinceğœ>0 and ğ‘ (ğ‘Ÿ) >0 hold. Hence, negative outî€›ts closer than the average of positive outî€›ts are pulled toward the user rather than pushed. To obtain more enhanced outî€›t representations, we propose a novel approach to leverage SimCLR [3] framework in outî€›t recommendation. Speciî€›cally, we suggest two data augmentation methods for outî€›ts. SimCLR learns representations by maximizing the agreement between diî€erently augmented views of the same outî€›t while pushing the others in the batch. Given the batchB = {(ğ‘¢, ğ‘œ)}, each outî€›tğ‘œis augmented twice to create two diî€erent views (ğ‘œ, ğ‘œ), generating 2ğ‘augmented outî€›ts{ğ‘œ}in total. The agreement is measured by cosine similarity between eachî€î€‘ outî€›t representation:ğ‘ = cosğ‘”(o), ğ‘”(o), whereğ‘”(Â·)is a non-linear projection layer. The objective of contrastive learning is as follows:î€€î€ L=12ğ‘[â„“(2ğ‘› âˆ’ 1, 2ğ‘›) + â„“ where ğœ> 0 is a temperature hyper-parameter. To exploit the objective in Eq. 18, we must deî€›ne appropriate data augmentation methods which produce semantically similar outî€›ts with an input outî€›t. As illustrated in Fig. 2, we suggest two augmentation methods suitable for outî€›t recommendation: erase and replace. Both augmentations randomly alter comprising items from an outî€›t while preserving the semantic context. We treat the augmentation set as a hyper-parameter and î€›x them at the beginning of the training. Note that if two identical augmentations are applied, we alter diî€erent items from the input outî€›t to obtain distinct views. 3.3.1 Erase. Randomly erasing components from the input is a common data augmentation method in diverse domains. In sequential recommendation, for example, Xie et al. [40] randomly crop items from user interaction history. In natural language processing, Wu et al. [39] erase or replace randomly selected words in a sentence. In computer vision, DeVries et al. [7] cut out contiguous sections of an input image, inspired by the object occlusion problem. When it comes to outî€›t recommendation, a subset of an outî€›t may imply or even determine the semantic information. Motivated by this, we randomly remove one item from the outî€›t to generate an augmented view. 3.3.2 Replace. Our proposed model computes preference scores based solely on visual information (i.e., images), and this assumption is helpful if access to other metadata is limited. In this situation, the semantic information of an outî€›t is derived from the appearance of consisting items. Accordingly, we claim that the visual similarity of consisting items leads to the semantic similarity of an outî€›t. Based on the claim, we generate an augmented view by randomly replacing one item from the outî€›t with a visually similar item from the same category. To this end, we train a CNN autoencoder model and retrieve similar items through their latent features. The proposed lossesLandLcan be used independently; hence we can take advantage of both methods. Therefore, our î€›nal objective is to minimize the weighted sum of both losses as follows: where ğœ† is a loss weight hyper-parameter. In application services with recommender systems, new users can join the service even after the model is trained and deployed. Such users, or cold starters, have relatively few interactions in general, and the deployed model does not have any prior knowledge of those users. In practice, î€›ne-tuning the model for them is a timeconsuming process; thus, cold starters might starve for the recommendation until the next iteration of deployment. Therefore, it is necessary to have an alternative recommendation method that exploits the already deployed model with no additional training. In personalized outî€›t recommendation, only a few works [24] handled the cold-start problem without î€›ne-tuning the model. Here, we introduce two strategies to compute preference scores of cold starters, analogous to memory-based collaborative î€›ltering. Given a cold starterğ‘¢, we deî€›ne a neighborhoodNfrom the set of non-cold usersUas follows: with a set of indices of positive outî€›ts ğ½for ğ‘¢, whereğ‘ represents the asymmetric similarity fromğ‘¢toğ‘¢.ğ›¿is a similarity threshold, andğ‘–= argmaxğ‘ denotes the index of the most similar user, which ensures at least one neighbor for each ğ‘¢. To compute the preference scoreğ‘Ÿof the cold starterğ‘¢to a given outî€›tğ‘œ, we aggregate the preference scores of neighbors to the outî€›t. Here we use two aggregation strategies: Average and Weighted Average. 3.5.1 Average (avg). A basic aggregation strategy is simply averaging the preference scores: 3.5.2 Weighted Average (w-avg). We further utilize the similarity between the cold starter and its neighbors as aggregation weights using cross-entropy with temperature: whereğœ>0 is a temperature hyper-parameter, and note thatÃ ğ‘ =1 holds. We considered other methods for deriving the aggregation weightsğ‘ fromğ‘ ; however, the suggested method empirically showed the best and stable results, especially in terms of robustness to hyper-parameters. 4.1.1 Datasets. We use datasets collected from the Polyvore website: Polyvore-ğ‘ˆ[25], whereğ‘ˆ âˆˆ {630,519,53,32}denotes the number of users. Polyvore-ğ‘ˆcontains outî€›ts posted by users, each consisting of three categories: top, bottom, shoes. Outî€›ts in Polyvore{630, 53} have a î€›xed number of items: one item for each category. Polyvore-{519, 32} include outî€›ts with a variable number of items (i.e., some outî€›ts may have two tops). We use Polyvore-{630, 519} for most of the experiments and Polyvore-{53, 32} for cold starter tasks. Statistics of the datasets are provided in Table 1. Following previous works [24,25], we deî€›ne user-posted outî€›ts as positive outî€›ts for each user and category-wise random mixtures of items as negative outî€›ts. We also discuss the results of hard negative outî€›ts (i.e., random samples of positive outî€›ts of other users) separately in Sec. 4.4. In the evaluation phase, we set the ratio between positive and negative outî€›ts to 1:10 for each user. We split training, validation, and test sets to 9:2:2, and we further split the validation set into two halves, one for the teacher model and the other for the student model. As [25] aî€œrmed, there are no duplicate items between the training and the test sets for each user. 4.1.2 Evaluation metrics. We evaluate the ranking performance via Area Under the ROC curve (AUC) and Normalized Discounted Cumulative Gain (NDCG), similar to previous works [24,25]. For each user, we rank the test outî€›ts by the predicted preference score of the model. We report the performance averaged over all users. 4.1.3 Considered methods. We compare our methods with the following state-of-the-art non-personalized [10,34,37] and personalized [22,24,25] outî€›t recommendation models. Type-Aware [37] projects pairs of items onto the type-speciî€›c subspaces. Compatibility is then measured in these subspaces and learned through the triplet loss. SCE-Net [34] learns conditional embeddings and their weights using an attention mechanism. Each conditional embedding is implicitly encouraged to encode diî€erent semantic subspaces via the triplet loss. Bi-LSTM [10] considers an outî€›t as a sequence of items and uses a bidirectional LSTM to learn the compatibility. The model is trained by predicting the next and previous items in the sequence through cross-entropy loss. Outî€›tNet [22] consists of two stages to capture both general compatibility and personal taste. The objective of both stages is to maximize the diî€erence between positive and negative scores, similar to BPR. FHN [25] uses pairwise scores to compute outî€›t compatibility and personal preference simultaneously. We train FHN with BPR without the binarization step, following the previous work [24]. LPAE [24] includes two models LPAE-u and LPAE-g, which mainly handles the cold-start problem using multiple anchors for each user. Both models utilize BPR loss, and LPAE-g has additional general anchors to model non-personalized compatibility. For a more fair comparison, we apply temperature scaling when using BPR or cross-entropy loss. 4.1.4 Implementation details. Similar to the previous work [25], we use AlexNet [16] pretrained on ImageNet [5] as a backbone CNN. We deî€›ne two versions of AlexNet to experiment the knowledge distillation. One is AlexNet-large, which is the original AlexNet. Table 2: Performance comparison of diî€erent methods on Polyvore datasets. The other is AlexNet-small, a downsized version of AlexNet that all fully-connected layers are removed and with a global average pooling at the end [21]. Only the teacher model uses AlexNet-large, and the others use AlexNet-small. We set the feature dimension ğ‘‘ =128 for all methods. For simplicity, we setğ›¿to zero, the median of the possible range of cosine similarity. We useğœ= ğœ=0.1, ğœ=0.2,ğœ† =0.2, and the number of headsâ„ =8. We setğ›¼ to 1.25 for Polyvore-630 and 1.5 for Polyvore-519. When it comes to CL, the pair of augmentations are (erase, replace) for Polyvore630 and (erase, erase) for Polyvore-519. SGD with momentum [28] is used to train all methods, and the batch size is set to 32. For each method, we report the test performance with their optimal hyper-parameters searched via the validation set unless otherwise speciî€›ed. As shown in Table 2, our proposed FND outperforms baseline methods under all datasets and metrics. Furthermore, the performance of FND-CL shows the eî€ectiveness of the outî€›t CL framework. Recall that LPAE-u adopts multiple anchors for representing each user, and LPAE-g further leverages non-personalized compatibility. Comparison with LPAE models shows that FND can eî€ectively achieve improved performance without auxiliary parameters and structures. We evaluate the ranking performance of recommending to the cold starters. Concretely, we test a scenario where a model is trained in Polyvore-{630, 519}, and cold starters in Polyvore-{53, 32} desire recommendations. Following the previous work [24], we experiment on the circumstances that each cold starter has only 1 or only 5 interacted outî€›ts. We compare our methods with non-personalized and LPAE methods, which do not require additional training of the model. For LPAE methods, we use the anchor-search [24], which is known to be the most eî€ective strategy in the cold-start case. In FND and FND-CL, we evaluate both avg and w-avg strategies. We conduct experiments 10 times and report the average results in Table 3: Comparison of diî€erent methods on cold starters. For our FND and FND-CL, w/o and w/ â€œ(w)â€ represent the avg and w-avg strategies, respectively. We use AUC as an evaluation metric. Type-Aware [37] 72.79 72.79 77.44 77.44 Table 4: Comparison of diî€erent methods on hard negative outî€›ts. Table 3. The results show that our approaches consistently outperform baseline methods even though the primary purpose of LPAE methods is to deal with the cold-start problem. The w-avg strategy is more eî€ective than the avg strategy in both FND and FND-CL, implying the importance of considering neighbors diî€erently based on similarity rather than treating them equally. We test a more challenging case where negative outî€›ts in evaluation are composed of positive outî€›ts of other users (i.e., hard negatives). We only compare with personalized methods since nonpersonalized methods cannot distinguish users. Following the previous work [25], we set half of the negative outî€›ts to hard negative outî€›ts when training the model. We report the results in Table 4. The results show that LPAE-g, which additionally considers nonpersonalized compatibility, performs poorly in this task compared to LPAE-u. We can see that our FND and FND-CL outperform the baseline methods. Note that the eî€ectiveness of the CL framework is more apparent on hard negative outî€›ts than the results in Table 2. We believe that the functionality of CL to distinguish outî€›ts enables the model to capture more meaningful outî€›t representations, especially in the hard negative setting. Figure 4: Comparison of diî€erent ğ›¼ on Polyvore datasets. Figure 5: Comparison of diî€erent augmentation methods on Polyvore datasets. Augmentation pair XY represents that the î€›rst and the second augmentations are X and Y. We also include the identity function as an augmentation method. I/E/R indicates identity/erase/replace augmentations. We evaluate the performance of FND under variousğ›¼(see Eq. 12). The results are reported in Fig. 4, and we also show the performance ofğ‘-pair and BPR in the same î€›gure. As mentioned in Sec. 3.2.1, ğ‘-pair overcomes the partial shortcoming of BPR by considering multiple negative outî€›ts in each update and thus clearly outperforms BPR. Diî€erent datasets tend to have diî€erent optimalğ›¼, but given adequate value, FND can surpass the performance of a strong ğ‘ -pair. To test the performance of FND-CL for all possible augmentation methods, we put the identity function into the set of augmentations. The results are shown in Fig. 5, and we also report the performance of FND in the same î€›gure. Note that we did not experiment on the (identity, identity) augmentation pair because two augmented views should be diî€erent. Regardless of which augmentation pair we use, FND-CL outperforms FND in almost all cases, and the optimal augmentation pair is diî€erent for each dataset. Concretely, replace and erase augmentation methods tend to be more eî€ective at Polyvore-630 and Polyvore-519, respectively. Therefore, we can see that both erase and replace are meaningful augmentation methods and that the CL framework is eî€ective in outî€›t recommendation. Figure 6: Comparison of diî€erent model sizes on Polyvore datasets. We study the impact of model size on performance, especially in the case of the student model. We consider student models with three diî€erent sizes (i.e., XS, S, M) and the teacher model. XS uses AlexNet-small, and S and M use a downsized version of AlexNet that output dimensions of all fully-connected layers are reduced to 1/4 and 1/2, respectively. Fig. 6 shows the results. It is clear that the larger the size, the better the performance. Moreover, the fact that FND-CL outperforms FND and FND outperformsğ‘-pair is consistent regardless of the size of the model, supporting that our approaches are meaningful. The performance gap between FND andğ‘-pair appears to shrink with the increasing size of the student model. Such a tendency implies that the eî€ectiveness of FND depends on the performance gap between the teacher and the student model, which implicitly emphasizes the importance of utilizing the superior teacher model. On the other hand, we can see that the eî€ectiveness of the CL framework is hardly aî€ected by the size of the model, as expected. Detailed information on inference eî€œciency is measured for each model of diî€erent sizes and reported in Table 5. We conduct experiments using FND for the student model andğ‘-pair for the teacher model. Since FND aî€ects only the training step, FND and ğ‘-pair share the same inference eî€œciency. Note that if CL is added, only the number of parameters increases by about 0.03M. In all inference tests, we use PyTorch with CUDA from Tesla P100 SXM2 GPU and Xeon E5-2690 v4 CPU. From the results, we can see that as the size of the model increases, all metrics that indicate ineî€œciency (i.e., Time, Memory, # Params) also increase. To study the impact of the batch size, we test the performance of FND-CL with diî€erent batch sizes. Note that we use a linear scaling of the learning rate when training with diî€erent batch sizes [9,15]. Fig. 7 shows the results of the experiment. The performance tends to be improved as the batch size increases and appears to converge when the batch size exceeds a certain threshold. We believe that the number of negative samples (i.e., outî€›ts and augmented views) proportional to the batch size is the primary factor of this tendency. Note that except for the number of negative samples, we do not use any other factors signiî€›cantly inî€uenced by the batch size, such as batch normalization [13]. Therefore, we can see from the results in Fig. 7 as well as Fig. 4 that it is important to exploit a Table 5: Model compactness and inference eî€œciency. â€œTimeâ€ denotes model inference time for making recommendations to every user in each dataset, and we report the mean and standard deviation of 10 runs. â€œMemoryâ€ represents GPU memory usage. â€œRatioâ€ indicates the relative parameter size of the student model compared to the teacher model. Polyvore -630 Polyvore -519 Figure 7: Comparison of diî€erent batch sizes on Polyvore datasets. suî€œcient number of negative samples per each update when using the ranking loss. We visualize the user-outî€›t space of the teacher model to support the intuition of FND (see Fig. 1). The visualization usesğ‘¡-SNE [36] and shows three users and their positive and negative outî€›ts from the training set. We focus on the training phase since the approach of FND is to distill knowledge from the teacher model when training the student model. The results are shown in Fig. 8. Recall that negative outî€›ts are randomly generated, and thus a positive outî€›t from the test set can appear as a negative sample by pure chance in the training step. With the help of the teacher model, the student model can treat such samples as false-negatives, denoted as a dashbordered rectangle in the î€›gure. Moreover, other negative outî€›ts close to a user share a similar style with positive outî€›ts, showing the possibility of being false-negatives. Hence, we can conclude that the approach of FND that utilizes false-negative information from the teacher model is reasonable. In this paper, we study how to leverage knowledge distillation (KD) and contrastive learning (CL) framework for personalized outî€›t Figure 8: The ğ‘¡-SNE visualization result of the user-outî€›t space. The â€œXâ€ symbol denotes a user embedding vector distinguished by diî€erent colors. Each rectangle is an outî€›t representation vector. A rectangle with a black border indicates a negative outî€›t. For rectangles with a colored border, they represent the positive outî€›ts of the user corresponding to each color. Among colored rectangles, a dashed border shows a false-negative outî€›t found in the test set. recommendation. We propose a new KD framework named False Negative Distillation (FND) that does not require the ranking of all possible outî€›ts. We also propose two novel data augmentation methods to make use of the CL framework in outî€›t recommendation. Quantitative experiments show that our FND and CL achieve notable success in outî€›t recommendation tasks. In detail, FND outperforms the state-of-the-art methods under fair conditions and achieves improved performance than without using FND in the same model. The outî€›t CL framework also contributes to the recommendation performance by allowing the model to obtain a more meaningful outî€›t representation. We support the soundness of our FND by visualizing the user-outî€›t space of the teacher model. One interesting future work is to apply a contrastive learning framework in a supervised manner by treating each user as a class.