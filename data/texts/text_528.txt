In this paper, we formulate a Collaborative Pure Exploration in Kernel Bandit problem (CoPE-KB), which provides a novel model for multi-agent multi-task decision making under limited communication and general reward functions, and is applicable to many online learning tasks, e.g., recommendation systems and network scheduling. We consider two settings of CoPE-KB, i.e., Fixed-Conî€›dence (FC) and Fixed-Budget (FB), and design two optimal algorithms are equipped with innovative and eî€œcient kernelized estimators to simultaneously achieve computation and communication eî€œciency. Matching upper and lower bounds under both the statistical and communication metrics are established to demonstrate the optimality of our algorithms. The theoretical bounds successfully quantify the inî€uences of task similarities on learning acceleration and only depend on the eî€ective dimension of the kernelized feature space. Our analytical techniques, including data dimension decomposition, linear structured instance transformation and (communication) round-speedup induction, are novel and applicable to other bandit problems. Empirical evaluations are provided to validate our theoretical results and demonstrate the performance superiority of our algorithms. Additional Key Words and Phrases: collaborative pure exploration, kernel bandit, communication round, learning speedup ACM Reference Format: Yihan Du, Wei Chen, Yuko Kuroki, and Longbo Huang. 2021. Collaborative Pure Exploration in Kernel Bandit. In Woodsto ck â€™21: ACM Symposium on Neural Gaze Detection, June 03â€“05, 2021, Woodsto ck, NY . ACM, New York, NY, USA, 44 pages. https://doi.org/10.1145/ 1122445.1122456 Pure exploration [ sequentially chooses options (often called arms) and observes random feedback, with the objective of identifying the best option (arm). This problem î€›nds various applications such as recommendation systems [ and neural architecture search [ cannot be directly applied to many real-world distributed online learning platforms, which often face a large volume of user requests and need to coordinate multiple distributed computing devices to process the requests, e.g., geographically distributed data centers [ information in order to attain globally optimal performance. To handle such distributed pure exploration problem, prior works [ Exploration (CoPE) model, where there are multiple agents that communicate and cooperate in order to identify the best arm with learning speedup. Yet, existing results only investigate the classic multi-armed bandit (MAB) setting [ 3,9,12,18,21,24] is a fundamental online learning problem in multi-armed bandits, where an agent and focus only on the fully-collaborative setting, i.e., the agents aim to solve a common task. However, in many real-world applications such as recommendation systems [31], it is often the case that diî€erent computing devices face diî€erent but correlated recommendation tasks. Moreover, there usually exists some structured dependency of user utilities on the recommended items. In such applications, it is important to develop a more general CoPE model that allows heterogeneous tasks and complex reward structures, and quantitatively investigate how task similarities impact learning acceleration. Motivated by the above facts, we propose a novel Collaborative Pure Exploration in Kernel Bandit (CoPE-KB) problem, which generalizes traditional single-task CoPE problems [20,22,38] to the multi-task setting. It also generalizes the classic MAB model to allow general (linear or nonlinear) reward structures via the powerful kernel representation. Speciî€›cally, each agent is given a set of arms, and the expected reward of each arm is generated by a task-dependent reward function with a low norm in a high-dimensional (possibly inî€›nite-dimensional) Reproducing Kernel Hilbert Space (RKHS) [33,42], by which we can represent real-world nonlinear reward dependency as some linear function in a high-dimensional space, and can go beyond linear rewards as commonly done in the literature, e.g., [10,13,14,35,40]. Each agent sequentially chooses arms to sample and observes noisy outcomes. The agents can broadcast and receive messages to/from others in communication rounds, so that they can exploit the task similarity and collaborate to expedite learning processes. The task of each agent is to î€›nd the best arm that maximizes the expected reward among her arm set. Our CoPE-KB formulation can handle diî€erent tasks in parallel and characterize the dependency of rewards on options, which provides a more general and î€exible model for real-world applications. For example, in distributed recommendation systems [31], diî€erent computing devices can face diî€erent tasks, and it is ineî€œcient to learn the reward of each option individually. Instead, CoPE-KB enables us to directly learn the relationship between option features and user utilities, and exploit the similarity of such relationship among diî€erent tasks to accelerate learning. There are also many other applications, such as clinical trials [41], where we conduct multiple clinical trials in parallel and utilize the common useful information to accelerate drug development, and neural architecture search [17], where we simultaneously run diî€erent tests of neural architectures under diî€erent environmental setups to expedite search processes. We consider two important pure exploration settings under the CoPE-KB model, i.e., Fixed-Conî€›dence (FC), where agents aim to minimize the number of used samples under a given conî€›dence, and Fixed-Budget (FB), where the goal is to minimize the error probability under a given sample budget. Note that due to the high dimension (possibly inî€›nite) of the RKHS, it is highly non-trivial to simplify the burdensome computation and communication in the RKHS, and to derive theoretical bounds only dependent on the eî€ective dimension of the kernelized feature space. To tackle the above challenges, we adopt eî€œcient kernelized estimators and design novel algorithmsCoopKernelFCandCoopKernelFBfor the FC and FB settings, respectively, which only costPoly(ğ‘›ğ‘‰ )computation and communication complexity instead of Poly(dim(H))as in [10,43], whereğ‘›is the number of arms,ğ‘‰is the number of agents, andHis the high-dimensional RKHS. We also establish matching upper and lower bounds in terms of sampling and communication complexity to demonstrate the optimality of our algorithms (within logarithmic factors). Our work distinguishes itself from prior CoPE works, e.g., [20,22,38], in the following aspects: (i) Prior works [20,22, 38] only consider the classic MAB setting, while we adopt a high-dimensional RKHS to allow more general real-world reward dependency on option features. (ii) Unlike [20,22,38] which restrict tasks (given arm sets and rewards) among agents to be the same, we allow diî€erent tasks for diî€erent agents, and explicitly quantify how task similarities impact learning acceleration. (iii) In lower bound analysis, prior works [20,38] mainly focus on a 2-armed case, whereas we derive a novel lower bound analysis for general multi-armed cases with high-dimensional linear reward structures. Moreover, when reducing CoPE-KB to prior CoPE with classic MAB setting (all agents are solving the same classic MAB task) [20, 38], our lower and upper bounds also match the existing state-of-the-art results in [38]. The contributions of this paper are summarized as follows: â€¢We formulate a novel Collaborative Pure Exploration in Kernel Bandit (CoPE-KB) problem, which models distributed multi-task decision making problems with general reward functions, and î€›nds applications in many real-world online learning tasks, and study two settings of CoPE-KB, i.e., CoPE-KB with î€›xed-conî€›dence (FC) and CoPE-KB with î€›xed-budget (FB). â€¢For CoPE-KB with î€›xed-conî€›dence (FC), we propose an algorithm cient kernelized estimator to signiî€›cantly reduce computation and communication complexity from existing and Î” â€¢For CoPE-KB with î€›xed-budget (FB), we design an eî€œcient algorithmî€ rounds is also established to validate the communication optimality of factors). Here inX to RKHS (deî€›ned in Section 5.1.1). â€¢Our results explicitly quantify the impacts of task similarities on learning acceleration. Our novel analytical techniques, including data dimension decomposition, linear structured instance transformation and roundspeedup induction, can be of independent interests and are applicable to other bandit problems. Due to space limit, we defer all the proofs to Appendix. 2 RELATED WORK This work falls in the literature of multi-armed bandits [ of research, i.e., collaborative pure exploration and kernel bandit. Collaborative Pure Exploration (CoPE). considers the classic MAB and fully-collaborative settings, and designs î€›xed-conî€›dence algorithms based on majority vote with upper bound analysis. Tao et al single-agent î€›xed-conî€›dence algorithms, and completes the analysis of round-speedup lower bounds. Karpov et al [22]extend the formulation of [ î€›xed-budget algorithms with tight round-speedup upper and lower bounds, which give a strong separation between best arm identiî€›cation and the extended best MAB and fully-collaborative settings in the above works [ communication eî€œciency due to the high-dimensional reward structures. Collaborative Regret Minimization. regret minimization objective. Bistritz and Leshem bandit with collisions motivated by cognitive radio networks, where multiple players simultaneously choose arms from the same set and receive no reward if more than one player choose the same arm (i.e., a collision happens). Bubeck and Budzinski Poly(dim(H))to onlyPoly(ğ‘›ğ‘‰ ). We derive matching upper and lower bounds of sample complexity log ğ›¿)and communication roundsğ‘‚ (log Î”). HereğœŒis the problem hardness (deî€›ned in Section 4.2), is the minimum reward gap. expâˆ’ğ‘›ğ‘‰and communication roundsğ‘‚ (log(ğœ” (ËœX))). A matching lower bound of communication [6], Bubeck et al. [7]investigate a variant multi-player bandit problem where players cannot communicate but have access to shared randomness, and they propose algorithms that achieve nearly optimal regrets without collisions. Chakraborty et al. [11]introduce another distributed bandit problem, where each agent decides either to pull an arm or to broadcast a message in order to maximize the total reward. Korda et al. [25], Szorenyi et al. [36]adapt bandit algorithms to peer-to-peer networks, where the peers pick arms from the same set and can only communicate with a few random others along network links. The above works consider diî€erent learning objectives and communication protocols from ours, and do not involve the challenges of simultaneously handling multiple diî€erent tasks and analyzing the relationship between communication rounds and learning speedup. Kernel Bandit.There are a number of works for kernel bandit with the regret minimization objective. Srinivas et al. [35]study the Gaussian process bandit problem with RKHS, which is the Bayesian version of kernel bandits, and designs an Upper Conî€›dence Bound (UCB) style algorithm. Chowdhury and Gopalan[13]further improve the regret results of [35] by constructing tighter kernelized conî€›dence intervals. Valko et al. [40]consider kernel bandit from the frequentist perspective and provides an alternative regret analysis based on eî€ective dimension. Deshmukh et al. [14], Krause and Ong[26]study the multi-task kernel bandits, where the kernel function of RKHS is constituted by two compositions from task similarities and arm features. Dubey et al. [16]investigate the multi-agent kernel bandit with a local communication protocol, with the learning objective being to reduce the average regret suî€ered by per agent. For kernel bandit with the pure exploration objective, there are only two works [10,43] to our best knowledge. Camilleri et al. [10]design a single-agent algorithm which uses a robust inverse propensity score estimator to reduce the sample complexity incurred by rounding procedures. Zhu et al. [43]propose a variant of [10] which applies neural networks to approximate nonlinear reward functions. All of these works consider either regret minimization or single-agent setting, which largely diî€ers from our problem, and they do not investigate the distributed decision making and (communication) round-speedup trade-oî€. Thus, their algorithms and analysis cannot be applied to solve our CoPE-KB problem. 3 COLLABORATIVE PURE EXPLORATION IN KERNEL BANDIT (COPE-KB) In this section, we present the formal formulation of the Collaborative Pure Exploration in Kernel Bandit (CoPE-KB), and discuss the two important settings under CoPE-KB that will be investigated. Agents and rewards.There areğ‘‰agents[ğ‘‰ ] = {1, . . . ,ğ‘‰ }, who collaborate to solve diî€erent but possibly related instances (tasks) of the pure exploration in kernel bandit (PE-KB) problem. For each agentğ‘£ âˆˆ [ğ‘‰ ], she is given a set of ğ‘›armsX= {ğ‘¥, . . . , ğ‘¥} âŠ† R, whereğ‘‘is the dimension of arm feature vectors. The expected reward of each armğ‘¥ âˆˆ Xisğ‘“(ğ‘¥), whereğ‘“:Xâ†¦â†’ Ris an unknown reward function. LetX = âˆªX. Following the literature in kernel bandits [14,26,35,40], we assume that for anyğ‘£ âˆˆ [ğ‘‰ ],ğ‘“has a bounded norm in a Reproducing Kernel Hilbert Space (RKHS) speciî€›ed by kernelğ¾:X Ã— X â†¦â†’ R(see below for more details). At each timestepğ‘¡, each agentğ‘£pulls an armğ‘¥âˆˆ Xand observes a random rewardğ‘¦= ğ‘“ (ğ‘¥) + ğœ‚, whereğœ‚is an independent and zero-mean 1-sub-Gaussian noise (without loss of generality).We assume that the best armsğ‘¥= argmaxğ‘“(ğ‘¥)are unique for all ğ‘£ âˆˆ [ğ‘‰ ], which is a common assumption in the pure exploration literature, e.g., [3, 12, 18, 24]. Multi-Task Kernel Composition.We assume that the functionsğ‘“are parametric functionals of a global function ğ¹ : X Ã— Z â†¦â†’ R, which satisî€›es that, for each agent ğ‘£ âˆˆ [ğ‘‰ ], there exists a task feature vector ğ‘§âˆˆ Z such that HereXandZdenote the arm feature space and task feature space, respectively. Eq.(1)allows tasks to be diî€erent for agents, whereas prior CoPE works [20, 22, 38] restrict the tasks (Xand ğ‘“) to be the same for all agents ğ‘£ âˆˆ [ğ‘‰ ]. Denote has a bounded norm in a global RKHS ğœ™ :ËœX â†¦â†’ H satisî€›es that for any ğ‘§, ğ‘§ whereğ¾ measures the similarity of functions and we have that diî€erent, then learning. We give a simple 2-agent (2-task) illustrating example in Figure 1. Agent 1 is given Items 1,2 with the expected rewardsğœ‡ ğœ‡, ğœ‡, respectively, denoted by ğœ™ (Ëœğ‘¥) = ğœ™ ( information on the second dimension of ğœƒ Note that the RKHS and any direct operation on ofğœ™ (Ëœğ‘¥)and explicit expression of the estimate of impracticable. In this paper, all our algorithms only query the kernel function analysis, which is diî€erent from existing works, e.g., [ 43]. Communication. tion protocol in existing CoPE works [ allow these munication rounds, in which each agent can broadcast and receive messages from others. While we do not restrict the exact length of a message, for practical implementation it should be bounded by number of arms for each agent, and we consider the number of bits for representing a real number as a constant. In the CoPE-KB problem, our goal is to design computation and communication eî€œcient algorithms to coordinate diî€erent agents to simultaneously complete multiple tasks in collaboration and characterize how the take similarities impact the learning speedup. Fixed-Conî€›dence and Fixed-Budget. (FC) and the other with î€›xed-budget (FB). Speciî€›cally, in the FC setting, given a conî€›dence parameter agents aim to identify used by each agent. In the FB setting, on the other hand, the agents are given an overall ËœX = X Ã— ZandËœğ‘¥ = (ğ‘¥, ğ‘§). As a standard assumption in kernel bandits [14,16,26,35], we assume thatğ¹ and an unknown parameter ğœƒâˆˆ Hsuch that :=ğœƒğœƒâ‰¤ ğµfor some known constantğµ >0.ğ¾:ËœX Ã—ËœX â†¦â†’ Ris a product composite kernel, which is the arm feature kernel that depicts the feature structure of arms, andğ¾is the task feature kernel that ğ‘§=1 for allğ‘£ âˆˆ [ğ‘‰ ],ğ¾(ğ‘§, ğ‘§) =1 for allğ‘§, ğ‘§âˆˆ Z, andğ¾ = ğ¾. On the contrary, if all tasks are rank(ğ¾) = ğ‘‰.ğ¾allows us to characterize the inî€uences of task similarities (1â‰¤ rank(ğ¾) â‰¤ ğ‘‰) on , ğœ‡, respectively, denoted byX= {ğ‘¥, ğ‘¥}. Agent 2 is given Items 2,3 with the expected rewards Ëœğ‘¥) = [0,1,0],ğœ™ (Ëœğ‘¥) = [0,0,1], andğœƒ= [ğœ‡, ğœ‡, ğœ‡]. The two agents can share the learned ğœ™ (Ëœğ‘¥)andğœƒare only used in our theoretical Following the popular communica- ğ‘‰agents to exchange information via comsamples per agent), and aim to use at mostğ‘‡ Â·ğ‘‰samples to identifyğ‘¥for allğ‘£ âˆˆ [ğ‘‰ ]and minimize the error probability. In both FC and FB settings, agents are requested to minimize the number of communication rounds. To evaluate the learning acceleration of our algorithms, following the CoPE literature, e.g., [20,22,38], we also deî€›ne the speedup metric of our algorithms. For a CoPE-KB instanceI, letğ‘‡denote the average number of samples used by each agent in multi-agent algorithmAto identifyğ‘¥for allğ‘£ âˆˆ [ğ‘‰ ], and letğ‘‡denote the average number of samples used by each task for a single-agent algorithmAto sequentially (without communication) identifyğ‘¥for all ğ‘£ âˆˆ [ğ‘‰ ]. Then, the speedup of Aon instance I is formally deî€›ned as It can be seen that 1â‰¤ ğ›½â‰¤ ğ‘‰, whereğ›½=1 for the case where all tasks are diî€erent andğ›½can approachğ‘‰for a fully-collaborative instance. By takingğ‘‡andğ‘‡as the smallest numbers of samples needed to meet the conî€›dence constraint, the deî€›nition of ğ›½can be similarly deî€›ned for error probability results. In particular, when all agentsğ‘£ âˆˆ [ğ‘‰ ]have the same arm setX= X = {ğ’†, . . . , ğ’†}(i.e., standard bases inR) and the same reward functionğ‘“(ğ‘¥) = ğ‘“ (ğ‘¥) = ğ‘¥ğœƒfor anyğ‘¥ âˆˆ X, all agents are solving a common classic MAB task, and then the task featureZ = {1}andğ¾(ğ‘§, ğ‘§) =1 for anyğ‘§, ğ‘§âˆˆ Z. In this case, our CoPE-KB problem reduces to prior CoPE with classic MAB setting [20, 38]. 4 FIXED-CONFIDENCE COPE-KB We start with the î€›xed-conî€›dence (FC) setting and propose theCoopKernelFCalgorithm. We explicitly quantify how task similarities impact learning acceleration, and provide sample complexity and round-speedup lower bounds to demonstrate the optimality of CoopKernelFC. 4.1 Algorithm CoopKernelFC 4.1.1 Algorithm.CoopKernelFChas three key components: (i) maintain alive arm sets for all agents, (ii) perform sampling individually according to the globally optimal sample allocation, and (iii) exchange the distilled observation information to estimate reward gaps and eliminate sub-optimal arms, via eî€œcient kernelized computation and communication schemes. The procedure ofCoopKernelFC(Algorithm 1) for each agentğ‘£is as follows. Agentğ‘£maintains alive arm setsB for allğ‘£âˆˆ [ğ‘‰ ]by successively eliminating sub-optimal arms in each phase. In phaseğ‘¡, she solves a global min-max optimization, which takes into account the objectives and available arm sets of all agents, to obtain the optimal sample allocationğœ†âˆˆ â–³and optimal valueğœŒ(Line 4). Hereâ–³is the collection of all distributions onX.ğœ‰is a regularization parameter such that which ensures the estimation bias for reward gap to be bounded by 2and can be eî€œciently computed by kernelized transformation (speciî€›ed in Section 4.1.2). Then, agentğ‘£usesğœŒto compute the number of required samplesğ‘, which guarantees that the conî€›dence radius of estimation for reward gaps is within 2(Line 5). In algorithmCoopKernelFC, we use a rounding procedure ROUND(ğœ†, ğ‘ ) with approximation parameter ğœ– from [2, 10], which rounds the sample Algorithm 1: Distributed Algorithm CoopKernelFC: for Agent ğ‘£ (ğ‘£ âˆˆ [ğ‘‰ ]) Ëœğ‘ , . . . , Broadcast on arm any allocation ğœ† âˆˆ â–³ By calling a sub-sequence the number of samples overall observation information, she estimates the reward gap ğ‘£âˆˆ [ğ‘‰ ] and discards sub-optimal arms (Lines 13-14). 4.1.2 Computation and Communication Eî€›iciency. Here we explain the eî€œciency of CoPE-KB, due to its high-dimensional reward structures, directly using the empirical mean to estimate rewards will ,ËœX, . . . ,ËœX,ğ¾ (Â·, Â·):ËœXÃ—ËœX â†¦â†’ R,ğµ, rounding procedureROUND(Â·, Â·)with approximation parameterğœ€. âˆˆ [ğ‘‰ ], |B| > 1 do and ğœŒbe the optimal solution and optimal value of maxâˆ¥ğœ™ (Ëœğ‘¥) âˆ’ ğœ™ (Ëœğ‘¥)âˆ¥, whereğœ‰is a regularization parameter that Ëœğ‘ ) â† ROUND(ğœ†, ğ‘); Ëœğ’”be the sub-sequence of (Ëœğ‘ , . . . ,Ëœğ‘ ) which only contains the arms inËœX; // generate the sample Ëœğ’”and observe random rewards ğ’š; {(ğ‘,Â¯ğ‘¦)}, whereğ‘is the number of samples andÂ¯ğ‘¦is the average observed reward , . . . , B; into the integer numbers of samples ğœ… âˆˆ N, such thatÃğœ…= ğ‘ and ROUND(ğœ†, ğ‘), agentğ‘£generates an overall sample sequence(Ëœğ‘ , . . . ,Ëœğ‘ )according toğœ†, and extracts Ëœğ’”that only contains the arms inËœXto sample (Lines 6-8). After sampling, she only communicates cause loose sample complexity, and naively calculating and transmitting inî€›nite-dimensional parameter ğœƒwill incur huge computation and communication costs. As a result, we cannot directly compute and communicate scalar empirical rewards as in prior CoPE with classic MAB works [20, 22, 38]. Computation Eî€œciency. CoopKernelFCuses three eî€œcient kernelized operations, i.e., optimization solver (Line 4), condition for regularization parameterğœ‰(Eq.(3)) and estimator of reward gaps (Line 13). Unlike prior kernel bandit algorithms [10, 43] which explicitly compute ğœ™ (Ëœğ‘¥) and maintain the estimate of ğœƒon the inî€›nite-dimensional RKHS, CoopKernelFConly queries kernel functionğ¾ (Â·, Â·)and signiî€›cantly reduces the computation (memory) costs from Poly(dim(H)) to only Poly(ğ‘›ğ‘‰ ). Below we give the formal expressions of these operations and defer their detailed derivation to Appendix A.1. Kernelized Estimator. We î€›rst introduce the kernelized estimator of reward gaps (Line 13). Following the standard estimation procedure in linear/kernel bandits [10,19,23,43], we consider the following regularized least square estimator of underlying reward parameter ğœƒ Note that this form ofË†ğœƒhasğ‘terms in the summation, which are cumbersome to compute and communicate. Since the samples(Ëœğ‘ , . . . ,Ëœğ‘ )are composed by armsËœğ‘¥, . . . ,Ëœğ‘¥, we merge repetitive computations for same arms in the summation and obtain (for notational simplicity, we combine the subscriptsğ‘£, ğ‘–inËœğ‘¥, ğ‘,Â¯ğ‘¦by using Ëœğ‘¥, ğ‘,Â¯ğ‘¦, respectively) Hereğ‘is the number of samples andÂ¯ğ‘¦is the average observed reward on armËœğ‘¥for anyğ‘– âˆˆ [ğ‘›ğ‘‰ ].Î¦=î±î±î± is the kernel matrix, andÂ¯ğ‘¦= [ğ‘Â¯ğ‘¦, . . . ,ğ‘Â¯ğ‘¦]is the average observations. Equality (a) rearranges the summation according to diî€erent chosen arms, and (b) follows from kernel transformation.î€€î€ Then, by multiplyingğœ™ (Ëœğ‘¥) âˆ’ ğœ™ (Ëœğ‘¥), we obtain the estimator of reward gaps ğ‘“ (Ëœğ‘¥) âˆ’ ğ‘“ (Ëœğ‘¥) as whereğ‘˜(Ëœğ‘¥) = Î¦ğœ™ (Ëœğ‘¥) = [ğ‘ğ¾ (Ëœğ‘¥,Ëœğ‘¥), . . . ,ğ‘ğ¾ (Ëœğ‘¥,Ëœğ‘¥)]for anyËœğ‘¥ âˆˆËœX. This estimator not only transforms heavy operations on the inî€›nite-dimensional RKHS to eî€œcient ones that only query the kernel function, but also merges repetitive computations for same arms (equality (a)) and only requires calculations dependent on ğ‘›ğ‘‰ . Kernelized Optimization Solver/Condition for Regularization Parameter. Now we introduce the optimization solver (Line 4) and condition for regularization parameter ğœ‰(Eq. (3)). For the kernelized optimization solver, we solve the min-max optimization in Line 4 by projected gradient descent,Ã which follows the procedure in [10]. Speciî€›cally, letğ´(ğœ‰, ğœ†) = ğœ‰ğ¼ +ğœ†ğœ™ (Ëœğ‘¥)ğœ™ (Ëœğ‘¥)for anyğœ‰ >0, ğœ† âˆˆ â–³. We deî€›ne function the gradient of â„(ğœ†) is given by which can be eî€œciently calculated by the following kernel transformation where ğ‘˜( For condition Eq. (3) on the regularization parameter ğœ‰ whereğœ† Both the kernelized optimization solver and condition for dimensional RKHS by querying the kernel function, and only cost (Eqs. (7),(8) only contains scalar ğ¾ ( Communication Eî€œciency. tive computations for the same arms and only transmits of transmitting all ğ‘‚ (ğ‘›ğ‘‰ ) bits (Lines 9-10). 4.2 Theoretical performance of CoopKernelFC Deî€›ne the problem hardness of identifying the best arms whereğœ‰ linear/kernel bandit pure exploration [ of samples used by each agent in algorithm CoopKernelFC. The sample complexity and number of communication rounds of CoopKernelFC are as follows. Theorem 1 (Fixed-Confidence Upper Bound). With probability at least 1 correct answers and communication rounds ğ‘‚ (log Î” = [, . . . ,]is the uniform distribution onËœX,ğ‘˜(Ëœğ‘¥) = [ğ¾ (Ëœğ‘¥,Ëœğ‘¥), . . . ,ğ¾ (Ëœğ‘¥,Ëœğ‘¥)]andğ¾= ğ‘samples as in [16]. This signiî€›cantly reduces the communication cost fromğ‘‚ (ğ‘)bits to = minğœ‰.ğœŒis the information-theoretic lower bound of the CoPE-KB problem, which is adapted from Remark 1. ğœŒis comprised of two sources of problem hardness, one due to handling diî€erent tasks and the other due to distinguishing diî€erent arms (We will decompose the sample complexity into these two parts in Corollary 1(c)). We see that the sample complexity ofCoopKernelFCmatches the lower bound (up to logarithmic factors). For fully-collaborative instances where single-agent algorithms [19,23] haveËœğ‘‚ (ğœŒlog ğ›¿)sample complexity, ourCoopKernelFCachieves the maximumğ‘‰-speedup (i.e., enjoysËœğ‘‚ (log ğ›¿)sample complexity) using only logarithmic communication rounds. Interpretation.We further interpret Theorem 1 via standard expressive tools in kernel bandits [14,35,40], i.e., eî€ective dimension and maximum information gain, to characterize the relationship between sample complexity and data structures, and demonstrate how task similarity inî€uences learning performance. To this end, deî€›ne the maximum information gain over all sample allocation ğœ† âˆˆ â–³as Denoteğœ†= argmaxlog detğ¼ +ğœ‰ğ¾andğ›¼â‰¥ Â·Â·Â· â‰¥ ğ›¼the eigenvalues ofğ¾, and deî€›ne the eî€ective dimension of ğ¾asî€šî€› We then have the following corollary. Corollary 1. The per-agent sample complexity of algorithmCoopKernelFC, denoted byğ‘†, can also be bounded as follows: Here ğ‘”(Î”, ğ›¿) = log Î”log+ log log Î”. Remark 2.Corollary 1(a) shows that, our sample complexity can be bounded by the maximum information gain of any sample allocation onËœX, which extends conventional information-gain-based results in regret minimization kernel bandits [13, 16, 35] to the pure exploration setting in the view of experimental (allocation) design. In terms of dimension dependency, it is demonstrated in Corollary 1(b) that our result only depends on the eî€ective dimension of kernel representation, which is the number of principle directions that data projections in RKHS spread. We also provide a fundamental decomposition of sample complexity into two compositions from task similarities and arm features in Corollary 1(c), which shows that the more tasks are similar, the fewer samples we need for accomplishing all tasks. For example, when tasks are the same (fully-collaborative), i.e.,rank(ğ¾) =1, each agent only spends a fraction of samples used by single-agent algorithms [10,43]. Conversely, when the tasks are totally diî€erent, i.e., rank(ğ¾) = ğ‘‰, no advantage can be attained by multi-agent deployments, since the information from neighboring agents is useless for solving local tasks. 4.3 Lower Bound for Fixed-Confidence Seî€ing We now present lower bounds for the sample complexity and a round-speedup for fully-collaborative instances, using a novel measure transformation techniques. The bounds validate the optimality of and communication. Speciî€›cally, Theorems 2 and 3 below formally present our bounds. In the theorems, we refer to a distributed algorithm at least 1 âˆ’ğ›¿. Theorem 2 (Fixed-Confidence Sample Complexity Lower Bound). Consider the î€›xed-conî€›dence collab orative pure exploration in kernel bandit problem with Gaussian noise A must have per-agent sample complexity Î© ( Remark 3. still requires at least logarithmic factors of the optimal sampling. Theorem 3 (Fixed-Confidence Round-Speedup Lower Bound). There exists a fully-collaborative instance of the î€›xed-conî€›dence CoPE-KB problem with multi-armed and linear reward structures, for which given any ğ›¿-correct and ğ›½-speedup distributed algorithm A must utilize communication rounds in expectation. In particular, when expectation. Remark 4. which validates that CoPE with classic MAB setting [ bounds (Theorems 1 and 3) match the state-of-the-art results in [38]. Novel Analysis for Fixed-Conî€›dence Round-Speedup Lower Bound. bound for the FC setting analysis has the following novel aspects. (i) Unlike prior CoPE work [ on a preliminary 2-armed case without considering reward structures, we investigate multi-armed instances with high-dimensional linear reward structures. (ii) We develop a linear structured progress lemma (Lemma 3 in Appendix A.5), which eî€ectively handles the challenges due to diî€erent possible sample allocation on multiple arms and derives the required communication rounds under linear reward structures. (iii) We propose multi-armed measure transformation and linear structured instance transformation lemmas (Lemmas 4,5 in Appendix A.5), which bound the change of probability measures in instance transformation with multiple arms and high-dimensional linear rewards, and serve as basic analytical tools in our proof. 5 FIXED-BUDGET COPE-KB We now turn to the î€›xed-budget (FB) setting and design an eî€œcient algorithm î€›xed-budget round-speedup lower bound to validate its communication optimality. Theorem 2 shows that even if the agents are allowed to share samples without limitation, each agent Theorem 3 exhibits that logarithmic communication rounds are indispensable for achieving the full speedup, Algorithm 2: Distributed Algorithm CoopKernelFB: for Agent ğ‘£ (ğ‘£ âˆˆ [ğ‘‰ ]) Input: Per-agent budget ğ‘‡ ,ËœX, . . . ,ËœX, ğ¾ (Â·, Â·) :ËœX Ã—ËœX â†¦â†’ R, regularization parameter ğœ‰, rounding procedure Ëœğ‘ , . . . ,Ëœğ‘ ) â† ROUND(ğœ†, ğ‘); 5.1 Algorithm CoopKernelFB 5.1.1 Algorithm.CoopKernelFBconsists of three key steps: (i) pre-determine the numbers of phases and samples according to data dimension, (ii) maintain alive arm sets for all agents, plan a globally optimal sample allocation, (iii) communicate observation information and cut down alive arms to a half in the dimension sense. The procedure ofCoopKernelFBis given in Algorithm 2. During initialization, we determine the number of phases ğ‘… and the number of samples for each phase ğ‘ according to the principle dimension ğœ”(ËœX) (Line 1), deî€›ned as: i.e., the principle dimension of data projections inËœSto the RKHS. In each phaseğ‘¡, each agentğ‘£maintains alive arm sets Bfor all agentsğ‘£âˆˆ [ğ‘‰ ], and solves an integrated optimization to obtain a globally optimal sample allocationğœ† (Line 3). Then, she generates a sample sequence(Ëœğ‘ , . . . ,Ëœğ‘ )according toğœ†, and selects the sub-sequenceËœğ’”that only contains her available arms to perform sampling (Lines 4-5). During communication, she only sends and receives the number of samplesğ‘and average observed rewardÂ¯ğ‘¦for each arm to and from other agents (Lines 7-8). Using the shared information, she estimates rewards of alive arms and only selects the best half of them in the dimension sense to enter the next phase (Lines 11-14). 5.1.2 Computation and Communication Eî€›iciency. solver (Eqs. estimate the rewards in Line 11. Moreover, communication costs. 5.2 Theoretical performance of CoopKernelFB We present the error probability of CoopKernelFB in the following theorem, where ğœ† Theorem 4 (Fixed-Budget Upper Bound). Suppose Î©(ğœŒlog(ğœ” ( . With at most probability and communication rounds ğ‘‚ (log(ğœ” ( Remark 5. samples, which matches the sample complexity lower bound (Theorem 2) up to logarithmic factors. In addition, CoopKernelFB rounds, which also matches the round-speedup lower bound (Theorem 5) within double logarithmic factors. Technical Novelty in Error Probability Analysis. multi-agent setting. The single-agent analysis in [ bound. Instead, we establish novel estimate concentrations and high probability events for each arm pair and each agent to handle the distributed environment, and build a connection between the principle dimension problem hardness ğœŒ Interpretation. maximum information gain and eî€ective dimension in kernel bandits [ into two compositions from task similarities and arm features. Corollary 2. The error probability of algorithm CoopKernelFC, denoted by ğ¸ğ‘Ÿğ‘Ÿ , can also be b ounded as follows: (6),(7)) to solve the min-max optimization in Line 3 and employs the kernelized estimator (Eq.(4)) to ËœX))), and the regularization parameterğœ‰>0 satisî€›esğœ‰maxâˆ¥ğœ™ (Ëœğ‘¥) âˆ’ğœ™ (Ëœğ‘¥)âˆ¥â‰¤ Theorem 4 implies that, to guarantee an error probabilityğ›¿,CoopKernelFBonly requiresğ‘‚ (log()) attains the maximumğ‘‰-speedup for fully-collaborative instances with only logarithmic communication Similar to Corollary 1, we can also interpret the error probability result with the standard tools of (a) ğ¸ğ‘Ÿğ‘Ÿ = ğ‘‚expâˆ’ğ‘‡ğ‘‰ Î”ËœÂ·ğ‘›ğ‘‰ log(ğœ” (ËœX)), where Î¥ is the maximum information gain. eî€ective dimension. Remark 6.Corollaries 2(a), 2(b) bound the error probability by maximum information gain and eî€ective dimension, respectively, which capture essential structures of tasks and arm features and only depend on the eî€ective dimension of the feature space of kernel representation. Furthermore, we exhibit how task similarities inî€uence the error probability performance in Corollary 2(c). For example, in the fully-collaborative case whererank(ğ¾) =1, the error probability enjoys an exponential decay factor ofğ‘‰compared to conventional single-agent results [23] (achieves ağ‘‰-speedup). Conversely, when the tasks are totally diî€erent withrank(ğ¾) = ğ‘‰, the error probability degenerates to conventional single-agent results [23], since in this case information sharing brings no beneî€›t. 5.3 Lower Bound for Fixed-Budget Seî€ing In this subsection, we establish a round-speedup lower bound for the FB setting. Theorem 5 (Fixed-Budget Round-Speedup Lower Bound). There exists a fully-collaborative instance of the î€›xedbudget CoPE-KB problem with multi-armed and linear reward structures, for which given anyğ›½ âˆˆ [,ğ‘‰ ], a ğ›½-speedup distributed algorithm A must utilize communication rounds in expectation. In particular, whenğ›½ = ğ‘‰,Amust useÎ©()communication rounds in expectation. Remark 7.Theorem 5 shows that under the FB setting, to achieve the full speedup, agents require at least logarithmic communication rounds with respect to the principle dimension ğœ”(ËœX), which validates the communication optimality ofCoopKernelFB. In the degenerated case when all agents solve the same non-structured pure exploration problem, same as in prior classic MAB setting [20,38], both our upper (Theorem 4) and lower (Theorem 5) bounds match the state-of-the-art results in [38]. Novel Analysis for Fixed-Budget Round-Speedup Lower Bound.Diî€erent from the FC setting, here we borrow the proof idea of prior limited adaptivity work [1] to establish a non-trivial lower bound analysis under Bayesian environments, and perform instance transformation by changing data dimension instead of tuning reward gaps. In our analysis, we employ novel techniques to calculate the information entropy and support size of posterior reward distributions in order to build induction among diî€erent rounds and derive the required communication rounds. 6 EXPERIMENTS In this section, we conduct experiments to validate the empirical performance of our algorithms. In our experiments, we setğ‘‰ =5,ğ‘‘ =4,ğ‘› =6,ğ›¿ =0.005 andğœ™ (Ëœğ‘¥) = ğ¼Ëœğ‘¥for anyËœğ‘¥ âˆˆËœX. The entries ofğœƒform an arithmetic sequence that starts from 0.1 and has the common diî€erenceÎ”, i.e.,ğœƒ= [0.1,0.1+ Î”, . . . ,0.1+ (ğ‘‘ âˆ’1)Î”]. For the FC setting, we vary the gapÎ”âˆˆ [0.1,0.8]to generate diî€erent instances (points), and run 50 independent simulations to plot the average sample complexity with 95% conî€›dence intervals. For the FB setting, we change the budgetğ‘‡ âˆˆ [7000,300000] to obtain diî€erent instances, and perform 100 independent runs to show the error probability across runs. The speciî€›c values of gap Î”and budget ğ‘‡ can be seen in X-axis of the î€›gures. Fixed-Conî€›dence.In the FC setting (Figures 2(a)-2(c)), we compareCoopKernelFCwith î€›ve baselines:CoopKernel-IndAlloc is an ablation variant ofCoopKernelFCwhich individually calculates sample allocations for diî€erent agents.IndRAGE[19], IndALBA[34] andIndPolyALBA[15] are single-agent algorithms, which useğ‘‰copies of single-agentRAGE[19], divides the sample complexity of the best single-agent algorithm achieves the best sample complexity in Figures 2(a),2(b), which demonstrates the eî€ectiveness of our sample allocation and cooperation scheme. Moreover, the empirical results also reî€ect the impacts of task similarities on learning speedup, and keep consistent with our theoretical analysis. Speciî€›cally, in the fully-collaborative case (Figure 2(a)), matchesIndRAGE-ğ‘‰ ofCoopKernelFC decrease of task similarity; in the totally-diî€erent-task ( the single-agent algorithm IndRAGE, since information sharing among agents brings no advantage in this case. Fixed-Budget. is an ablation variant of andIndUniformFB sampling policies, respectively. As shown in Figures 2(d),2(e), our all other algorithms. In addition, these empirical results also validate the inî€uences of task similarities on learning performance, and match our theoretical analysis. Speciî€›cally, as the task similarity decreases in Figures 2(d) to 2(f), the error probability of speedup. 7 CONCLUSION In this paper, we propose a novel Collaborative Pure Exploration in Kernel Bandit (CoPE-KB) problem with FixedConî€›dence (FC) and Fixed-Budget (FB) settings. CoPE-KB aims to coordinate multiple agents to identify best arms with ] andPolyALBA[15] policies to solve theğ‘‰tasks independently.IndRAGE/ğ‘‰is ağ‘‰-speedup baseline, which lies betweenIndRAGE/ğ‘‰andIndRAGE, since it only achieves smaller thanğ‘‰speedup due to the In the FB setting (Figures 2(d)-2(f)), we compareCoopKernelFBwith three baselines:CoopKernelFB-IndAlloc CoopKernelFBgets closer to that of single-agentIndRAGE-FB, due to the slow-down of its learning general reward functions. We design two computation and communication eî€œcient algorithmsCoopKernelFCand CoopKernelFBbased on novel kernelized estimators. Matching upper and lower bounds are established to demonstrate the statistical and communication optimality of our algorithms. Our theoretical results explicitly characterize the impacts of task similarities on learning speedup and avoid heavy dependency on the high dimension of the kernelized feature space. In our analysis, we also develop novel analytical techniques, including data dimension decomposition, linear structured instance transformation and (communication) round-speedup induction, which are applicable to other bandit problems and can be of independent interests.