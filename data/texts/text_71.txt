Novel technologies in automated machine learning ease the complexity of algorithm selection and hyperparameter optimization. Hyperparameters are important for machine learning models as they signiî€›cantly inî€uence the performance of machine learning models. Many optimization techniques have achieved notable success in hyperparameter tuning and surpassed the performance of human experts. However, depending on such techniques as blackbox algorithms can leave machine learning practitioners without insight into the relative importance of diî€erent hyperparameters. In this paper, we consider building the relationship between the performance of the machine learning models and their hyperparameters to discover the trend and gain insights, with empirical results based on six classiî€›ers and 200 datasets. Our results enable users to decide whether it is worth conducting a possibly time-consuming tuning strategy, to focus on the most important hyperparameters, and to choose adequate hyperparameter spaces for tuning. The results of our experiments show that gradient boosting and Adaboost outperform other classiî€›ers across 200 problems. However, they need tuning to boost their performance. Overall, the results obtained from this study provide a quantitative basis to focus efforts toward guided automated hyperparameter optimization and contribute toward the development of better-automated machine learning frameworks. Meta-learning, hyperparameter importance, hyperparameter optimization, classiî€›cation Machine learning has achieved notable success in diî€erent application domains including, image classiî€›cation, object detection, Data Systems Group, University of Tartu self-driving cars, î€›nancial applications, recommendation systems, and medical diagnosis systems [13,19,26,27]. There is no doubt that we are living in the era of big data and, witnessing huge expansion in the amount of data generated every day from diî€erent resources. Such data should be analysed for richer and more robust insights. As the number of data scientists cannot scale with the number of applications, a novel research area has emerged, commonly referred to as Automated Machine Learning (AutoML). AutoML aims to automatically determine the approach that performs best for this particular application [17]. Thereby, AutoML makes state-of-the-art machine learning approaches accessible to non-experts who are interested in applying machine learning but do not have the knowledge and resources needed to build machine learning pipelines. Every machine learning model has a set of hyperparameters, and the most basic task in AutoML is to tune these hyperparameters to achieve the best performance automatically. Hence, tuning hyperparameters becomes a key problem for a machine learning model. Hyperparameter tuning is an optimization problem where the objective function of optimization is a black-box function. Traditional optimization techniques such as grid search suî€er from scalability problems and thus, there has been a recent surge of interest in more eî€œcient optimization techniques [1,2,15]. The considerable success achieved by such techniques has not been accompanied by suî€œcient eî€ort in providing answers to a set of questions such as how important each hyperparameter is to the model? What is the best value for each of these hyperparameters? Which hyperparameter interactions matter? How tunable is the machine learning algorithm? How the answers to these questions relate to the characteristics of the dataset under consideration? Answering these questions will improve the overall eî€œciency of hyperparameter optimization by reducing the search of hyperparameters that are important to the model. Therefore, performing automatic tuning while achieving high precision and high eî€œciency is an open problem that has not been fully addressed in machine learning. In this paper, we aim to î€›ll this gap from a statistical point of view to simplify the tuning process for laying machine learning practitioners, and to optimize decision-making for more advanced processes. More speciî€›cally, we present a methodology to determine the hyperparameters that appear important to the model, in addition to robust defaults and well-performing machine learning algorithms based on empirical performance data derived from experiments across various datasets. We apply our approach on a benchmark study of six popular classiî€›cation algorithms: random forests [3], support vector machines(SVMs) [5], Adaboost [9], Extra Trees [11], decision tree [21] and gradient boosting [10]. More speciî€›cally, we analyze the importance of hyperparameter tuning based on the performance of these algorithms across 200 datasets obtained from the OpenML [33]. To ensure repeatability as one of the main targets of this work, we provide access to the source codes and the detailed results for the experiments of our study. The remainder of this work is organized as follows. In Section 2, we provide a short overview of related work. In Section 3, we cover the background required about functional ANOVA used in our analysis. Section 4 covers details of experiments, including the used datasets, algorithms, hyperparameters, performance measures, and search techniques. Section 5 describes the details of our experiments before we conclude the paper in Section 6. To the best of our knowledge, a very limited number of research papers consider the problem of tunability of machine learning search spaces. Bergstra and Bengio [2] studied the importance of neural networks hyperparameters and concluded that some hyperparameters are important to the model across all datasets, while others are important on some datasets. Their î€›ndings were used as an argument to justify why random search outperforms grid search when setting hyperparameters for neural networks. Mantovani et al. [22] applied diî€erent techniques to tune the hyperparameters of the decision tree algorithm on 102 heterogeneous datasets. The experimental results show that the improvement of the tuned models over all datasets, in most cases, is statistically signiî€›cant. Diî€erent approaches exist for evaluating the importance of hyperparameters. The principle of forward selection has been used to predict a classiî€›er performance based on a subset hyperparameters that is initialized as empty and greedily updated with the most important hyperparameter [16]. In the same vein, Fawcett and Hoos [6] presented ablation analysis, a technique for identifying hyperparameters that mostly contribute to improving the performance after tuning. For each of the considered hyperparameters, the performance improvement is computed by changing its value from a source conî€›guration to a speciî€›c destination conî€›guration speciî€›ed by the tuning strategy (e.g., a user-deî€›ned default conî€›guration and one obtained using automated conî€›guration). Hutter et al. [16] introduced an approach for assessing the importance of a single hyperparameter based on functional ANOVA [29], which is a powerful framework that can detect the importance of both individual hyperparameters and interaction eî€ects between arbitrary subsets of hyperparameters. More speciî€›cally, functional ANOVA decomposes the variance in model performance into adaptive components due to all subsets of the hyperparameters. Estimation of Distribution Algorithms (EDA) [20] used to estimate the best default value for hyperparameters over the performance data by î€›tting a probability distribution to points in the input space and using this probability distribution to sample new points from. Another line of research is based on meta-learning and performance data from similar datasets, and the resulting predictions are used to recommend a particular set of conî€›gurations for the new dataset. These techniques have achieved great success in recommending good hyperparameters [23,28] to warm-start diî€erent optimization techniques [8] or prune search spaces [34]. The main drawback of these techniques is that it is diî€œcult to choose appropriate meta-features. In addition, extracting such meta-features is a time-consuming process. Another meta-learning approach that alleviates meta-features is Multi-task Bayesian optimization [30]. A multi-task model is built on the outcome of classiî€›ers to î€›nd correlations between tasks which can be used to recommend hyperparameters for a task. However, this technique suî€ers from the cubic time complexity. In this study, we apply a technique for quantifying the importance of the hyperparameters of machine learning algorithms. Following Hutter et al. [31], we consider a setting slightly more general by considering a larger number of machine learning algorithms across a large number of datasets. To the best of our knowledge, all the techniques considered in the literature either applied to a limited number of machine learning algorithms and datasets or relied on defaults as a reference point. In order to obtain representative results, we analyze hyperparameter importance across many diî€erent datasets. More speciî€›cally, we employ the 200 datasets from the OpenML [33] to determine the most important hyperparameters of six classiî€›ers along with the best defaults. The functional ANOVA framework is an eî€œcient technique for assessing the importance of hyperparameters of a machine learning algorithm based on the eî€œcient computations of marginal performance. More speciî€›cally, Functional ANOVA speciî€›es the contribution of each hyperparameter to the variance of the machine learning algorithm performance. In the following we give a quick overview on how functional ANOVA is used to eî€œciently compute the importance of all hyperparameters. For more details about the eî€œcient computation of functional ANOVA, we refer the interested reader to the original work done by Hutter et al. [14]. Given an algorithmğ´withğ‘›hyperparameters with domain Î˜, Î˜, ..., Î˜ğ‘›and conî€›guration spaceÎ˜ = Î˜Ã— Î˜... Ã— Î˜ğ‘›. A conî€›guration of algorithmğ´is a vectorğœƒ = âŸ¨ğœƒ, ğœƒ, ..., ğœƒâŸ©, where ğœƒâˆˆ Î˜. Let a partial conî€›guration ofğ´is deî€›ned as a vector ğœƒ= âŸ¨ğœƒ, ğœƒ, ...ğœƒâŸ©withğ‘ˆî€›xed hyperparameters, whereğ‘ˆis a subset of the set of all hyperparametersğ‘of algorithmğ´. The marginal performanceË†ğ‘(ğœƒ)is deî€›ned to be the average performance of algorithm ğ´ for all complete conî€›gurations ğœƒ that have in commonğœƒ. Computing suchË†ğ‘(ğœƒ)is computationally expensive, however it has been shown by Hutter el al. [14] that for tree-based model,Ë†ğ‘(ğœƒ)can be computed by a produce that is linear in the number of leaves in the model. We apply functional ANOVA on each of the six classiî€›ers as follows. First we collect performance dataâŸ¨ğœƒ, ğ‘¦âŸ©for each algorithmğ´withğ¾diî€erent conî€›gurations, whereğ‘¦is the performance of algorithmğ´measured by Area Under the Curve (AUC). Next, we î€›t a random forest model to the performance data and then use functional ANOVA to decompose the variance in performance of the random forestË†ğ‘¦:Î˜Ã— Î˜...Î˜âˆ’ > Rinto additive components that depends on subsets of the hyperparameters ğ‘ : where the componentsË†ğ‘“(ğœƒ) are deî€›ned as follows: whereË†ğ‘“is the mean value of the functionË†ğ‘¦over its domain. The unary functionË†ğ‘“(ğœƒ)captures the importance of hyperparameterğ‘—average over all possible values for the rest of the hyperparameters, whileË†ğ‘“(ğœƒ)captures the interaction eî€ects between all hyperparameters inğ‘ˆ. Functional ANOVA decomposes the varianceğ‘‰in theË†ğ‘¦into the contributions of all possible subsets of hyperparameters ğ‘‰of algorithm ğ´. The importance of a hyperparameter or a set of hyperparameters is captured by the fraction of the variance the hyperparameter or the set of hyperparameters is responsible for; the higher the fraction, the more important the hyperparameter or the set of hyperparameters is to the model. Thus, such a hyperparameter should be tuned in order to achieve a good performance. In this study, we use 200 datasets from the OpenML project [33] from various domains. The datasets containing between 500 to 100,000 data points are well-balanced and have been used in relevant scientiî€›c publications. These criteria ensure that the classiî€›cation tasks are challenging, meaningful, and the results obtained from such tasks are comparable to earlier studies. The datasets are preprocessed in the same way as follows. The missing values are imputed with the mode for categorical features. For numerical features, the imputation is done using mean, median and mode. For SVMs classiî€›er, the input variables are scaled to have unit variance as SVMs is sensitive to the scale of the input variables. We analyze the hyperparameters of six classiî€›ers implemented on scikit-learn [4,24], namely, random forests (RF), support vector machines (SVMs), Adaboost (AB), Extra Trees (ET), decision tree (DT), and gradient boosting (GB). We consider the same hyperparameters and ranges used by Auto-sklearn [7]. Table 1 shows the considered hyperparameters for each of the considered classiî€›ers along with their description, ranges and potential transformation function. We build a knowledge base that contains performance data of 500 diî€erent conî€›gurations for each of the six used classiî€›ers per dataset. Each record in the knowledge base represents a 10-fold cross-validated evaluation of a certain conî€›guration on a dataset, using one of the 6 classiî€›ers evaluated based on AUC score. The conî€›guration for each classiî€›er is obtained by sample points from independent uniform distributions, where the respective support for each parameter is shown in Table 1. Figure 1: Marginal contribution per 200 datasets for Adaboost classiî€›er Our experiments aim to examine the following: (1) importance of each hyperparameter to each of the studied machine learning models in Section 5.1, (2) good defaults for each of the six studied machine learning in Section 5.2, (3) performance and tunability of the studied algorithms in Section 5.3 and Section 5.4, respectively. In this section, we present the experimental results for determining the important hyperparameters of the six classiî€›ers. This analysis is based upon the performance data of six algorithms with 898507 hyperparameters, running over the 200 datasets, using 135548 CPU days to generate. All performance data we used is publicly available. For a given algorithmğ´and a given dataset, we use the performance data obtained for this algorithm from our knowledge base and then functional ANOVAâ€™s random forest is î€›t on the performance data. Next, we return the variance contribution of each hyperparameter, such that high values of variance indicate high importance. In the following, we study the distribution of these variance contributions across 200 datasets to determine that most important hyperparameters based on empirical data. We present the results of each of the six classiî€›ers as a set of six î€›gures. Each î€›gure shows the violin plots of the variance contribution of each hyperparameter across all datasets. The ğ‘¥-axis shows the hyperparameter(s) to be investigated and theğ‘¦-axis shows the marginal contribution of the hyperparameter(s) across all datasets. A high value for the marginal contribution implies that this hyperparameter contributes for large fraction of variance, and therefore, would account for high accuracy-loss if not tuned in a proper way. Adaboost important hyperparameters:Figure 1 shows the hyperparameter importance for Adaboost. The results reveal that most of the variance could be attributed to a small set of hyperparameters. The maximal depth of the Adaboost contributes to the largest variance followed by learning rate. Both of these hyperparameters were signiî€›cantly more important than the others according to the Wilcoxon signed-rank test with more than 95% level of conî€›dence (p-value<0.05). Figure 2: Marginal contribution per 200 datasets for random forest classiî€›er Random forest and Extra Trees important hyperparameters:Figure 2 and Figure 3 show that the most contributed hyperparameters to the variance for random forest and Extra Trees algorithms, respectively. The results show that the most contributed hyperparameters to the variance are the minimum samples per leaf Figure 3: Marginal contribution per 200 datasets for Extra Trees classiî€›er and maximal number of features for determining the split. The results obtained from functional ANOVA aligned with the î€›ndings that concluded that ensembles perform well when the individual Figure 4: Marginal contribution per 200 datasets for SVMs classiî€›er models outperform the random guessing and when the errors obtained by the individual models are uncorrelated [3,35]. The interaction between the minimum samples per leaf and maximal number of features for determining the split is signiî€›cantly more important than the rest of examined interactions between hyperparameters. Based on the Wilcoxon signed-rank test, both hyperparameters were signiî€›cantly more important than the rest of hyperparameters with more than 95% level of conî€›dence (p-value<0.05). SVMs important hyperparameters:Figure 4 shows the analysis results of the SVMs. The results reveal that the most variance could be attributed clearly to the kernel type and gamma hyperparameters. Both of these hyperparameters are signiî€›cantly more important than any other hyperparameters based on the Wilcoxon signed-rank test. Figure 4 shows that the interaction between the kernel function and the gamma is signiî€›cantly more important than the rest of examined interactions between hyperparameters with more than 95% level of conî€›dence (p-value<0.05). Decision tree important hyperparameters:Figure 5 shows the analysis results of the decision tree algorithm. The results reveal that the hyperparameters that contributed the most to the variance are the maximal number of features for determining the split and the minimum samples per leaf. Both of these hyperparameters are statistically more signiî€›cant than any other hyperparameters based on the Wilcoxon signed-rank test with more than 95% level of conî€›dence (p-value<0.05). Figure 5 shows that the interaction between the maximal number of features for determining the split and the minimum samples per leaf are statistically more important than the rest of examined interactions between hyperparameters, followed by the interaction between the minimum number of samples required to be at a leaf node and the number of features to consider when looking for the best split as shown in Figure 5. Gradient Boosting important hyperparameters:Figure 6 shows the analysis results of the gradient boosting algorithm. The results show that the hyperparameter contributing the most to the variance is the learning rate. The interaction between the learning rate and the maximum depth is signiî€›cantly more important than the maximum depth hyperparameter, as shown in Figure 6. Based on the Wilcoxon signed-rank test, learning rate and the interaction between these two hyperparameters were signiî€›cantly more Figure 5: Marginal contribution per 200 datasets for decision tree classiî€›er Figure 6: Marginal contribution per 200 datasets for gradient boosting classiî€›er important than the rest of hyperparameters with more than 95% level of conî€›dence (p-value<0.05). In addition, the hyperparameters that specify the minimum number of data points for making a leaf and the subset of features when trying to î€›nd the best split are statistically more important than others with more than 95% level of conî€›dence (p-value<0.05). We compare the importance of hyperparameters reported in this study for each of Adaboost, and Random forest to the results obtained in [31,32]. The results show that for these two algorithms, the top two hyperparameters obtained in this study comes inline with the reported results in [31, 32]. In this section, we focus on recommending values for hyperparameters that tended to yield good performance. Figure 7 shows the kernel density estimators for the most important hyperparameters for each of the six classiî€›ers included in this study, where the x-axis represents the hyperparameter value and the y-axis represents the probability that this value will be sampled. Figure 7a and Figure 7b show that the hyperparameter represents the minimum number of samples required to be at a leaf node for the random forest and Extra Trees models should be set to a small value, and hence, the Figure 7: Obtained Priors for the hyperparameter found to be most important for each classiî€›ers over 200 datasets. Th x-axis represents the value of the hyperparameter, the y-axis represents the probability that this value will be sampled default value of 1 for this hyperparameter is quite good, which is consistent with the î€›ndings in [11]. For Adaboost classiî€›er, the maximum depth should be set to a large value, as shown in Figure 7c. For both types of SVMs, the gamma hyperparameter should be set to be less than 1 to achieve good performance, as shown in Figure 7d and Figure 7e. For decision tree classiî€›er, the number of features to consider when looking for the best split should be set to a large value less than 0.8, as shown in Figure 7f. The learning rate hyperparameter for gradient boosting algorithm has a good default and should be set to a small value to achieve a good performance as shown in Figure 7g. A general measure of the tunability of a machine learning algorithm per dataset can be computed based on the diî€erence between the performance of an overall reference conî€›guration and the performance of the best possible conî€›guration on that dataset. For each algorithm, this gives rise to an empirical distribution of performance diî€erences over datasets, which might be directly summarized to an aggregated tunability measured by using mean, or standard deviation. In this work, we use the standard deviation to assess the tunability of each classiî€›er. Figure 8 shows the tunability results for each algorithm evaluated on 500 conî€›gurations for each of the 200 dataset included in this study. Clearly, some algorithms such as SVMs, gradient boosting and Extra Trees are much more tunable than the others, while decision tree and random forest is the least Figure 8: Tunability results for each algorithm evaluated on 500 conî€›gurations for each of the 200 datasets included in this study. tunable one. This results are consistent with the general knowledge in machine learning and with the research î€›ndings in [25]. In this section, we compare the performance of each algorithm across all datasets. Figure 9 compares the performance of the six algorithms included in this study, we plot the mean rankings of the Figure 9: Mean ranking of six Machine Learning algorithms over 194 datasets. Error bars indicate the 95% conî€›dence interval. Figure 10: The percentage of a given algorithm out of 194 datasets outperforms another algorithm in terms of the best AUC on a problem. Algorithms are ordered from top to bottom on the basis of their overall performance on all datasets. Two algorithms are considered to have same performance on a problem when they have achieved accuracy within 1% of each other. algorithms across all datasets. The rankings show the strength of ensemble-based tree algorithms in generating highly performing models. Gradient boosting is the î€›rst ranked algorithm, followed by Adaboost, while SVMs has got the worst ranking. In order to assess the statistical signiî€›cance of the observed diî€erences in algorithm performance across all problems, we use the Wilcoxon signed-rank test. The results show that the diî€erence in performance between all classiî€›ers is statistically signiî€›cant with more than 95% level of conî€›dence (p-value<0.05) except that between Gradient boosting and Adaboost, and between Extra Trees and Random forest. Given the huge amount of obtained performance data, we try to recommend the top-ranked algorithms across the 200 datasets. However, it is worth mentioning that the top-ranked algorithms may not outperform others for some problems. Furthermore, when simpler algorithms perform on par with a more complex one, it is often preferable to choose the simpler of the two. We investigate the pairwise performance comparison between classiî€›ers by calculating the percentage of datasets for which one algorithm outperforms another, shown in Figure 10. It is clear from the results that there is no single machine model that achieves the highest performance across all datasets. For example, there are 16 datasets for which SVMs performs as well as or better than gradient boosting, despite being the overall worst- and best-ranked algorithms, respectively. In this work, we study the hyperparameters of diî€erent machine learning techniques and the impact of tuning them either jointly, tuning individual parameters or combinations, all based on the general concept of surrogate empirical performance models and ANOVA framework. More speciî€›cally, we identiî€›ed the main important hyperparameters for six machine learning algorithms including Adaboost, random forest, Extra Trees, SVMs, decision tree, and gradient boosting. In addition, we quantify the tunability of such algorithms, which have never been provided before in such a principled manner based on a large number of datasets and based on a wide range of problems, including binary and multiclass classiî€›cation problems. Our results conform with common knowledge and results from literature. We also compared the top important hyperparameters for random forest and Adaboost to those obtained from van Rijn and Hutter [31]. One future direction is to study the hyperparameters for diî€erent machine learning problems, including regression and clustering algorithms. Finally, we aim to employ recent advances in metalearning to identify similar datasets and base the priors only on these to yield dataset-speciî€›c priors for hyperparameter optimization. The research speciî€›ed under the object of the agreement was carried out with the support of the European Regional Development Fund and the programme Mobilitas Pluss (project number: 20142020.4.01.16-0024).