Online recommendation requires handling rapidly changing user preferences. Deep reinforcement learning (DRL) is an eî€ective means of capturing usersâ€™ dynamic interest during interactions with recommender systems. Generally, it is challenging to train a DRL agent, due to large state space (e.g., user-item rating matrix and user proî€›les), action space (e.g., candidate items), and sparse rewards. Existing studies leverage experience replay (ER) to let an agent learn from past experience. However, they adapt poorly to the complex environment of online recommender systems and are ineî€œcient in determining an optimal strategy from past experience. To address these issues, we design a novel state-aware experience replay model, which selectively selects the most relevant, salient experiences, and recommends the agent with the optimal policy for online recommendation. In particular, the model uses locality-sensitive hashing to map high dimensional data into low-dimensional representations and a prioritized reward-driven strategy to replay more valuable experience at a higher chance. Experiments on three online simulation platforms demonstrate our modelâ€™s feasibility and superiority to several existing experience replay methods. Recommender System, Deep Reinforcement Learning, Experience Replay Online recommendation aims to learn usersâ€™ preferences and recommend items dynamically to help users î€›nd desired items in highly dynamic environments [35]. Deep reinforcement learning (DRL) naturally î€›ts online recommendation as it learns policies through interactions with the environment via maximizing a cumulative reward. Besides, DRL has been widely applied to sequential decision-making (e.g. in Atari [22] and AlphaGo [31]) and achieved remarkable progress. Therefore, it is increasing applied for enhancing online recommender systems [2, 4, 37]. DRL-based recommender systems cover three categories of methods: deep Q-learning (DQN), policy gradient, and hybrid methods. DQN aims to î€›nd the best step via maximizing a Q-value over all possible actions. As the representatives, Zheng et al. [40]introduced DRL into recommender systems for news recommendation; Chen et al. [4]introduced a robust reward function to Q-learning, which stabilized the reward in online recommendation. Despite the capability of fast-indexing in selecting a discrete action, Qlearning-based methods conduct the â€œmaximize" operation over the action space (i.e., all available items) and suî€er from the stuck agent problem [8]â€”the â€œmaximize" operation becomes unfeasible when the action space has high dimensionality (e.g., 100,000 items form a 10k-dimensional action space) [3]. Policy-gradient-based methods use the average reward as guideline to mitigate the stuck agent problem [3]. However, they are prone to converge to suboptimality [25]. While both DQN and policy gradient are more suitable for small action and state spaces [19,33] in a recommendation context, hybrid methods [3,8,13,38] has the capability to map large high-dimensional discrete state spaces into low-dimensional continuous spaces via combines the advantages of Q-learning and policy gradient. A typical hybrid method is the actor-critic network [18], which adopts policy gradient on an actor network and Q-learning on a critic network to achieve Nash equilibrium on both networks. Actor-critic networks have been widely applied to DRL-based recommender systems [5, 20]. Existing DRL-based recommendation methods except policygradient-based ones rely heavily on experience replay to learn from previous experience, avoid re-traversal of the state-action space, and stabilize the training on large, sparse state and action spaces [34]. They generally require long training time, thus suî€ering from the training ineî€œciency problem. Further more, in contrast to the larger, diverse pool of continuous actions required in recommendation tasks, existing experience replay methods are mostly designed for games with a small pool of discrete actions. Therefore, a straightforward application of those methods may result in strong biases during the policy learning process [12], thus impeding the generalization of optimal recommendation results. For example, Schaul et al. [28]assume that not every experience is worth replaying and propose a prioritized experience replay (PER) method to replay only the experience with the largest temporal diî€erence error. Sun et al. [32]propose attentive experience replay (AER), which introduces similarity measurement into PER to boost the eî€œciency of î€›nding similar statesâ€™ experience, but attention mechanisms cause ineî€œciency on large sized state and action spaces [17]. We present a novel experience replay structure, Locality-Sensitive Experience Replay (LSER), to address the above challenges. Diî€ering from existing approaches, which apply random or uniform sampling, LSER samples experiences based on expected states. Inspired by collaborative î€›ltering (which measures the similarity between users and items to make recommendations) and AER [32], LSER only replays experience from similar states to improve the sampling eî€œciency. Speciî€›cally, we introduce a ranking mechanism to prioritize replays and promote the higher reward experiences. We further useğœ–-greedy method to avoid replaying high-rears states excessively. Considering the high-dimensionality of vectorized representations of states, We convert similarity measurement for highdimensional data into a hash key matching problem and employ locality-sensitive hashing to transform states into low-dimensional representations. Then, we assign similar vectors the same hash codes (based on the property of locality-sensitive hashing). Such a transformation reduces all the states into low dimension hash keys. In summary, we make the following contributions in this paper: â€¢We propose a novel experience replay method (LSER) for reinforcement-learning-based online recommendation. It employs a similarity measurement to improve training eî€œciency. â€¢LSER replays experience based on the similarity level of the given state and the stored states; the agent thus has a higher chance to learn valuable information than it does with uniform sampling. â€¢The experiments on three platforms, VirtualTB, RecSim and RecoGym, demonstrate the eî€œcacy and superiority of LSER to several state-of-the-art experience replay methods. In this section, we will brieî€y introduce the proposed LSER method with theoretical analysis. The overall structure of using LSER in DRL RS can be found in Figure 1. Online Recommendation aims to î€›nd a solution that best reî€ects real-time interactions between users and the recommender system and apply the solution to the recommendation policy. The system needs to analyze usersâ€™ behaviors and update the recommend policy dynamically. In particular, reinforcement learning-based recommendation learns from interactions through a Markov Decision Process (MDP). Given a recommendation problem consisting of a set of users U = {ğ‘¢, ğ‘¢, Â· Â· Â·ğ‘¢}, a set of itemsI = {ğ‘–, ğ‘–, Â· Â· Â· ğ‘–}and userâ€™s demographic informationD = {ğ‘‘, ğ‘‘, Â· Â· Â· , ğ‘‘}, MDP can be represented as a tuple(S, A, P, R,ğ›¾), whereSdenotes the state space (i.e., the combination of the subsets ofIand its corresponding user information)Adenotes the action space, which represents agentâ€™s selection during recommendation based on the state spaceS,P denotes the set of transition probabilities for state transfer based on the action received,Ris a set of rewards received from users, which are used to evaluate the action taken by the recommender system (each reward is a binary value to indicate whether user has clicked the recommended item or not), andğ›¾is a discount factor ğ›¾ âˆˆ [0, 1] for the trade-oî€ between future and current rewards. Given a userğ‘¢and an initial stateğ‘ observed by the agent (or the recommender system), which includes a subset of item setIand userâ€™s proî€›le informationğ‘‘, a typical recommendation iteration for the user goes as follows: î€›rst, the agent takes an actionğ‘ based on the recommend policyğœ‹under the observed stateğ‘  and receives the corresponding rewardğ‘Ÿâ€”the rewardğ‘Ÿis the numerical representation for userâ€™s behavior such as click through or not; then, the agent generates a new policyğœ‹based on the received rewardğ‘Ÿand determines the new stateğ‘ based on the probability distributionğ‘ (ğ‘ |ğ‘ , ğ‘) âˆˆ P. The cumulative reward (denoted byğ‘Ÿ) afterğ‘˜iterations from the initial state is as follows: DRL-based recommender systems uses a replay buî€er to store and replay old experience for training. Given the large state and action space in a recommender system, not every experiences are worth to replay [6]â€”replaying experience that does not contain useful information will increase the training time signiî€›cantly and introduce extra uncertainty to convergence. Hence, it is reasonable to prioritize replay important experience for DRL recommender systems. The ideal criterion for measuring the importance of a transition in RL is the amount of knowledge learnable from the transition in its current state [11,28]. State-of-the-art methods like AER are unsuitable for recommendation tasks that contain large, higher dimensional state and action spaces as their sampling strategies may not work properly. Thus, we propose a new experience replay method named Locality-sensitive experience replay (LSER) for online recommendation, which uses hashing for dimension reduction when sampling and storing the experiences. We formulate the storage and sampling issue in LSER as a similarity measure problem, where LSER stores similar states into the same buckets and samples similar experiences based on state similarities. A popular way of searching similar high-dimensional vectors in Euclidean space is Locality-Sensitive Hashing (LSH), which follows the idea of Approximate Nearest Neighbor (ANN) while allocating similar items into the same buckets to measure the similarity. However, standard LSH conducts bit-sampling on the Hamming space; it requires time-consuming transformation between the Euclidean space to the Hamming space, liable to lose information. Aiming at measuring the similarity between high-dimensional vectors without losing signiî€›cant information, we propose usingğ‘-stable distribution [23] to conduct dimensionality reduction while preserving the original distance. This converts high-dimensional vectors (states) into low-dimensional representations easier to be handled by the similarity measure. To address possible hash collision (i.e., dissimilar features may be assigned into the same bucket and recognized as similar), we introduce the formal deî€›nition of the collision probability for LSH. Then, we theoretically analyze the collision probability forğ‘-stable distribution to prove that our method has a reasonable boundary for collision probability. Figure 1: The proposed LSER with DDPG. The environment provides the current state ğ‘  DDPG model; the action ğ‘can be obtained by ğ‘= ğœ‹ (ğ‘ ). ğ‘Ÿwill be provided by the user (e.g. click or not). LSER takes ğ‘  the input and encodes it on the projective space. Given the enco ded states, LSER will return the most similar experience for DDPG to update he parameters. After that, this transition â„(ğ‘ ) : (ğ‘  Definition 1 (Collision probability for LSH inğ‘-stable distribution). Given an LSH functionâ„âˆˆ Hand the probability density function (PDF) of the absolute value of theğ‘-stable (ğ‘ âˆˆ [1, 2]) distributionğ‘“(ğ‘¡)inğ¿space, the collision probability for vectorsu and v is represented by: where ğ‘ = âˆ¥u âˆ’ vâˆ¥and ğ‘¤ is a user-deî€›ned î€›xed distance measure. Here, we use a 2-state distribution, i.e., normal distribution for dimension reduction. We randomly initializeğ‘›hyperplanes based on normal distribution on the projective spacePto get the hash representation for a given stateğ‘ , whereğ‘›is the dimension of the state. The hashing representationâ„(ğ‘ )for the given stateğ‘ is calculated as follows:( The collision probability of the above method can be represented as: ğ‘ƒ = Pr [â„(u) = â„(v)] = 1 âˆ’ğ´ğ‘›ğ‘”(u, v)ğœ‹ where ğ´ğ‘›ğ‘”(u, v) = arccos|u âˆ© v|î°(3) Eq.(2) formulates the information loss during the projection, where we use termğ‘’to represent the quantiî€›cation between the real valueğ‘ Â· ğ‘£and hashed results induced fromâ„(v). Since the relative positions in original space are preserved during the hash transformation with an extra measurementğ‘’, the upper bound and lower bound of collision probability boundary in projective space , ğ‘, ğ‘ , ğ‘Ÿ) will be stored. is guarantee to be intact. That means the more dissimilar states will not receive a higher probability to be allocated into the same hash result. Lemma 1. Given an arbitrary hash functionâ„âˆˆ H, the collision probability for a given vector u and v is bounded at b oth ends. Proof.SincePr [â„(u) = â„(v)]monotonically decreases inğ‘for any hash function from the LSH familyH, the collision probability is bounded from above byğ‘ƒğ‘Ÿ [â„(u) = â„(v)]forğ‘ âˆ’ğ‘’ and from below by ğ‘ƒğ‘Ÿ [â„(u) = â„(v)] for ğ‘ + ğ‘’. Then, we have the upper bound: â‰¤ğ‘ƒ +ğ‘’ğ‘¤ğ‘ğ‘“(ğ‘) ğ‘‘ğ‘ â‰¤ ğ‘ƒ +ğ‘’ğ‘ âˆ’ ğ‘’ and the lower bound: We compute the upper bound based on HÃ¶lderâ€™s inequality in ğ¿space: Considering the ğ¿space, we have: We use the similar method in ğ¿to compute the lower bound: and in ğ¿: The collision probabilityğ‘ƒğ‘Ÿ [â„(ğ‘¢) = â„(ğ‘£)]is bounded from both ends as follows: ğ‘ƒ âˆ’ minğ‘ + ğ‘’,2(ğ‘ + ğ‘’), ğ‘ƒ + minğ‘ âˆ’ ğ‘’,2(ğ‘ âˆ’ ğ‘’) Note that, when calculating the lower and upper bounds,ğ‘representsand, respectively. The algorithm of LSER is shown in Algorithm 1. In the following, we demonstrate from two perspectives that LSER can î€›nd the similar states eî€œciency. First, we show the efî€›cacy of LSER with theoretical guarantee, i.e., similar states can be sampled given the current state. We formulate â€˜the sampling of similar statesâ€™ as a neighbor-î€›nding problem in the projective space and provide a theoretical proof of the soundness of LSER. Given a set of statesS, and a queryğ‘, LSER can quickly î€›nd a stateğ‘  âˆˆ Swithin distanceğ‘Ÿor determine thatShas no states within distanceğ‘Ÿ. Based on existing work [15], the LSH family is(ğ‘Ÿ, ğ‘Ÿ, ğ‘, ğ‘)-sensitive, i.e., we can î€›nd a distributionHsuch thatğ‘â‰¥ ğ‘ƒğ‘Ÿ[â„(u) = â„(v)]whenuandvare similar and ğ‘â‰¤ ğ‘ƒğ‘Ÿ[â„(u) = â„(v)] when u and v are dissimilar. Theorem 2. LetHbe(ğ‘Ÿ, ğ‘Ÿ, ğ‘, ğ‘)-sensitive. Supposeğ‘> 1/ğ‘› andğ‘> 1/ğ‘›, whereğ‘›is the size of data points. There exists a solution for the neighbor î€›nding problem in LSER withinğ‘‚ (ğ‘›ğ‘log ğ‘›) query time, and ğ‘‚ (ğ‘›ğ‘) space. Proof.Assumeğ‘Ÿ, ğ‘Ÿ, ğ‘, ğ‘are known,ğœŒ =, and ğ‘˜ =whereğ‘˜is the number of hash functions, and LSH initializes ğ¿ tables. Based on the deî€›nition in [15], we have: ğ‘˜ğ¿ = ğ‘˜ğ‘â‰¤ ğ‘˜ (ğ‘’+ 1) â‰¤ ğ‘˜ (ğ‘›/ğ‘+ 1) = ğ‘‚ (ğ‘›/ğ‘log ğ‘›) The space complexity is calculated asğ‘‚ (ğ¿ğ‘›ğ‘‘)whereğ‘‘is the dimension of stateğ‘ . It can be written asğ‘‚ (ğ‘›/ğ‘ğ‘‘)(by applying ğ¿ = ğ‘›/ğ‘) and further simpliî€›ed into ğ‘‚ (ğ‘›/ğ‘). Then, we prove LSER can î€›nd similar neighbors. Theğ¿table can be classiî€›ed into two categories: similar and dissimilar. Given a stateğ‘ , the similar category gives similar states while the dissimilar category provides dissimilar states. We split the two categories such thatğ¿ = âŒŠğ‘›âŒ‹ +âŒˆğ‘šâŒ‰and its correspondingâŒŠğ‘˜âŒ‹,âŒˆğ‘˜âŒ‰. Given any state ğ‘  âˆˆ Sin the distanceğ‘Ÿ, LSER must be able to î€›nd the most similar states in a high probabilityâ€”the query and the data need to share the same hash-bucket in one of the tables. The probability of their not sharing the same hash-bucket is (1 âˆ’ ğ‘)(1 âˆ’ ğ‘)â‰¤ (1 âˆ’ ğ‘)(1 âˆ’ ğ‘)(5) whereğ›¼ =âŒˆğ‘˜âŒ‰âˆ’ğ‘˜. We have applied the deî€›nitionsğ‘= ğ‘= ğ‘› for step 6 to step 7 andğ‘›ğ‘+ğ‘šğ‘= ğ‘›for step (7) to (8). Finally, we get the probability of LSERâ€™s getting the similar states as follows: Existing experience replay methods in DRL research assume that the recent experience is more informative than older experience. Therefore, they simply replace the oldest experience with the newest experience to update the experience buî€er in DRL-based recomender systems without further optimization. As such, some valuable experience might be discarded, i.e., catastrophic forgetting. In contrast, we design a state-aware reward-driven experience storage strategy, which removes the experience with the lowest rewardâ€”instead of following the First-In-First-Out (FIFO) strategyâ€”when the replay buî€er is full. Formally speaking, a transitionğœ: (ğ‘ , ğ‘, ğ‘ , ğ‘Ÿ) will be stored in the replay buî€er based on the valueâ„(ğœ.ğ‘ ). If the replay buî€er is full, the transition with the same value of â„(ğœ.ğ‘ )but lower reward will be replaced. In practice, an indicatorğ‘šis stored in the transition as well to indicate when the recommendation should terminate. Figure 2: Given a high dimensional space, three random hype-planes are initialized based on normal distribution. Each hype-plane splits the space into two hash areas 0 and 1. The space is split into six hash areas. We can î€›nd that, states are encoded into a binary string e.g.,{111, 101, 011, 001, 000} Sampling strategy is another crucial component of LSER, which determines which experience should be selected for the agent to optimize in LSER. We propose a state-aware reward-driven sampling strategy that only replays the experience with the top-N highest rewards in the same hashing area; this way, the agent can quickly î€›nd the correct direction for optimization. We call our sampling strategy â€˜state-awareâ€™ because we use a hash key to encode the state and replay the experience based on the hash key. Compared with uniform sampling, our strategy has a higher chance to replay the correct experience. Here, we illustrate how to address three related challenges faced by our sampling strategy: exploitation-vsexploration dilemma, bias annealing and non-existence dilemma. Exploitation vs. exploration dilemma. Exploitation and exploration dilemma is a well-known dilemma when training an agent for RL, including LSER. TWhile our reward-driven strategy forces the agent to exploit existing high-rewarding experiences, the agent may converge to a sub-optimal policy instead of the globally optimal one. We use a similar method toğœ–-greedy to achieve a trade-oî€ between exploitation and exploration. LSER î€›rst draws a random probability ğ‘ âˆˆ [0, 1]then uses reward-driven sampling if the probability less than a threshold,ğœ–and random sampling otherwise. The threshold allows LSER to replay low priority experience to fulî€›ll the exploration requirement. Bias annealing. Prioritizing partial experiences among the replay buî€er may introduce inductive bias [28,32]â€”the training process is highly non-stationary (due to changing policies); even a small bias introduced by the sampling strategy may change the solution that the policy converges to. A common solution is to let the priority anneal periodically so that the agent can visit those less-replayed experiences. By using the threshold, our ğœ–-greedy method has the similar eî€ect as annealing on allowing low-priority experiences to be replayed. Non-existence dilemma. When split the projective space into areas to initialize hyperplanes, some areas may not have any data points (esp. when the number of hyperplanes is large), causing the â€˜non-existence dilemmaâ€™. Consequently, when a new transition comes, the algorithm will stop if no experience can be found on â„. We use the similarity measure to overcome this problem. Speciî€›cally, we î€›nd the two hash areas that are most similar to each other (based on currentâ„) and conduct sampling on those two states. We use Jaccard similarity to measure the similarity between hash codesğ´, ğµ. As such, LSER can always replay the relevant experience. We use Deep Deterministic Policy Gradient (DDPG) [19] as the training backbone. We choose an actor-critic network as the agent and train two parts of the actor-critic network simultaneously. The critic network aims to minimize the following loss function: whereğœƒandğœƒare the critic and actor parameters,ğ‘is the size of the mini-batch from the replay buî€er,ğœ“andğœ™are the target critic and target actor network, respectively. We apply the OrnsteinUhlenbeck process in the action space to introduce perturbation; this encourages the agent to explore. The target network will be updated based on the corresponding hyper-parameter ğœ. We conduct experiments on three widely used public simulation platforms: VirtualTB [30], RecSim [14] and RecoGym [27], which mimic online recommendations in real-world applications. VirtualTBis a real-time simulation platform for recommendation, where the agent recommend items based on usersâ€™ dynamic interests. VirtualTB uses a pre-trained generative adversarial imitation learning (GAIL) to generate diî€erent users who have both static interest and dynamic interest. Itâ€™s worth to mention that, the GAIL is pre-trained by using the real-world from Taobao, which is one of the largest online retail platforms in China. Moreover, the interactions between users and items are generated by GAIL as well. Beneî€›t from that, VirualTB can provide a large number of users and the corresponding interactions to simulate the real-world scenario. RecSimis a conî€›gurable platform for authoring simulation environments that naturally supports sequential interaction with users in recommender systems. RecSim diî€ers from VirtualTB in containing diî€erent, simpler tasks but fewer users and items. There are two diî€erent tasks from RecSim, namely interest evolution and long-term satisfaction. The former (interest evolution) encourages the agent to explore and fulî€›ll the userâ€™s interest without further exploitation; the latter (long-term satisfaction) depicts an environment where a user interacts with content characterized by the level of â€˜clickbaitiness.â€™ Generally, clickbaity items lead to more engagement yet lower long-term satisfaction, while non-clickbaity items Algorithm 1: LSH memory by using dictionary input : Transition for storage ğœ : (ğ‘ , ğ‘,ğ‘š, ğ‘ , ğ‘Ÿ), = encode(ğ‘ ); have the opposite eî€ect. The challenge lies in balancing the two to achieve a long-term optimal trade-oî€ under the partially observable dynamics of the system, where satisfaction is a latent variable that can only be inferred from the increase/decrease in engagement. RecoGymis a small Open AI gym-based platform, where users have no long-term goals. Diî€erent from RecSim and VirtualTB, RecoGym is designed for computational advertising. Similar with RecSim, RecoGym uses the click or not to represent the reward signal. Moreover, similar with RecSim, users in those two environments do not contain any dynamic interests. Considering RecoGym and RecSim have limited data points and do not consider usersâ€™ dynamic interests, we select VirtualTB as the main platform for evaluations. Our model is implemented in Pytorch [26] and all experiments are conducted on a server with two Intel Xeon CPU E5-2697 v2 CPUs with 6 NVIDIA TITAN X Pascal GPUs, 2 NVIDIA TITAN RTX and 768 GB memory. We use two two-hidden-layer neural networks with 128 hidden unit as the actor network and the critic network, respectively.ğœ,ğ›¾, andğ‘are set to 0.001, 0.99 and 1ğ‘’, respectively, during experiments. The evaluation metrics are environment-speciî€›c. For VirtualTB and RecoGym, click-through rate is used as the main evaluation metric. For RecSim, we use the built-in metric, which is a quality score, as the main evaluation metric. We compare our method with the following baselines. â€¢Prioritized Experience Replay (PER) [28]: an experience replay method for discrete control, which uses TD-error to rank experience and a re-weighting method to conduct the bias annealing. â€¢Dynamic Experience Replay (DER) [21]: an experience replay method designed for imitation learning, where stores both human demonstrations and previous experience. Those experiences are selected randomly without any priority. â€¢Attentive Experience Replay (AER) [32]: an experience replay method that uses attention to calculate the similarly for boosting sample eî€œciency with PER. â€¢Selective Experience Replay (SER) [16]: an experience replay method for lifelong machine learning, which employs LSTM as the experience buî€er and selectively stores experience. â€¢Hindsight Experience Replay (HER) [1]: an experience replay method that replays two experience (one successful, one unsuccessful) each time. For AER, PER, SER and HER, We use the same training strategy as LSER. For DER, we use its original structure to run experiments without human demonstrations. The size of the replay buî€er is set to1, 000, 000for VirtualTB and10, 000for RecSim and RecoGym. The number of episodes for our experiments is set to90, 000for VirtualTB and1, 000for RecSim and RecoGym. Note that only PER, AER and SER contains a prioritize operation to rank or store the experience. Results for the three platforms (Fig 3) demonstrate our method (LSER) outperformed the baselines: LSER yields signiî€›cant improvements on VirtualTB, which is a large and sparse environment; while AER, DER, PER and SER î€›nd a correct policy within around50, 000 episodes, ours takes around30, 000episodes; HER does not perform well because it introduces too much failed experience and has a slow learning process; DER introduces the human demonstration Figure 3: Result comparison with four baseline methods on VirtualTB, RecSim and RecoGym. The experiments are repeated î€›ve times, and mean values are reported. 95% conî€›dence intervals are shown. (a) is the result for VirtualTB; (b) is the result for long-term satisfaction in RecSim; (c) is the result for interest evolution in RecSim; (d) is the result for RecoGym; (e) is the result for ğœ– study; (f) is the ablation study to show the eî€ectiveness for each component into the vanilla ER which is hard to acquire for recommendation task. Applying PER to DDPG slightly outperforms applying DER to DDPG, which is consistent with observations by previous work [24, 32]. As PER was originally designed for Deep Q-learning, it uses the high TD-error to indicate the high informative experience for the value-network. When applying PER into DDPG, which is an actor-critic based algorithm, the sampled experience is also used to update the policy network. Those experiences with high TD-error normally diverge far away from the current policy and harm the updates of the policy-network. In contrast, LSER selects experience according to the similary with the current state. This preference for on-distribution states tends to discard experiences that contain old states and stabilize the training process of the policy network. AER does not perform as well as PER in VirtualTB because it heavily relies on the attention mechanism to calculate the similarity score between states. LSERâ€™sğœ–-greedy method can enforce agent to do more exploration when userâ€™s interest shift. All methods gained similar results on RecSim and RecoGym because all methods can iterate all possible combinations of states and actions. Fig. 3b, 3c and 3d show that LSER is slightly better and more stable than the baselines on RecSim and RecoGym. Since the two platforms are quite small, similarity matching andğœ–-greedy do not signiî€›cantly improve performance. We report the running time of the selected experience replay methods in Table 1 to evaluate the eî€œciency of LSER. LSER outperforms all While performing poorly on RecSim and RecoGym, it is faster than most of the baselines. In comparison, LSER introduces extra running time in small environments (e.g, RecSim and RecoGym) than large environments. For VirutalTB, AER takes much longer time than all other methods, due to attention calculation [17]. We further investigate the eî€ect of LSERâ€™s store and sampling strategy by replacing our store strategy with the normal strategy and our sampling strategy with random sampling. The results of our ablation study are shown in Fig. 3f, where LSER-P denotes LSER with the replaced store strategy and LSER-S denotes the LSER with the replaced sampling strategy. We found the sample strategy played the most critical role in achieving good performance, as LSER-S underperformed LSER signiî€›cantly. The store strategy also contributed to the better performance. LSER-P was less stable Table 1: Comparison of running time for DER, PER, SER, AER and LSER coupling with DDPG in three diî€erent environments when running the experiments in 90, 000 episodes (indicated by a wider error bar). but outperformed LSER atâˆ¼ 30, 000 episodes, due to occurrence of sub-optimal policies. In our method, the number of hyperplanes is critical to determine the length of the result hash-bits of a given state. Longer hashbits can provide more accurate similarity measurement result but low eî€œciency, while shorter hash-bits can increase eî€œciency but decrease the accuracy. Itâ€™s a trade-oî€ which needs a middle-point to balance between eî€œciency and accuracy. We want to answer the following question:â€œDoes increase the hyperplanes always boost the recommendation performance?â€ and î€›nd out the optimal number. We report the experimental results in VirtualTB, where we evaluate the eî€ect by varying number hyperplanes in LSER (shown in Fig 4). The results on the other two platforms show the similar pattern. The performance gradually increases with more hyperplanes, Figure 4: Performance Comparison of diî€erent number of hyperplanes but it levels oî€ or even drops when number of hyperplanes reaches 20. Fig 3a shows LSER suî€ers instability after reaching the î€›rst peak at episodeâˆ¼ 50, 000. Diî€erent from the other methods, LSER can quickly reach the optimal policy but suî€ers î€uctuation. That indicatesğœ–-greedy tends to lead the agent towards learning from low-priority experience after the optimal policy is reached. We alleviate the issue by adjusting the value ofğœ–. Here, we triedğœ– = {0, 0.9, 0.99, 1}to determine the best choice of theğœ–on VirtualTB. The results are shown in Fig. 3e, whereğœ– = 1corresponds to greedy sampling whileğœ– = 0refers to randomly sampling. Besides, we provide an intervention strategy to stabilize the training processâ€”the agent will stop exploration once the test reward is higher than a reward thresholdğ‘‡. This strategy allows the agent to î€›nd an nearoptimal policy at an early stage. We examined the performance under ğ‘‡=0.95, which delivers a better training process. Zhao et al. [38]î€›rst introduced DRL to recommender systems Zhao et al. [38]. They use DQN to embed user and item information for news recommendaion, where Vanilla ER is used to help the agent learn from past experience. And until present, most methods use only vanilla ER, which uniformly samples experiences from the replay buî€er. Among them, Zhao et al. [39]apply DQN to online recommendation and RNN to generate state embeddings; Chen et al. [4]point out that DQN receives unstable rewards in dynamic environments such as online recommendation and may harm the agent; Chen et al. [3]found that traditional methods like DQN become intractable when the state becomes higher-dimensional; DPG addresses the intractability by mapping high-dimensional discrete state into low-dimensional continuous state [5, 36]. Intuitively, some instances are more important than others; so a better experience replay strategy is to sampling experiences according to how much current agent can learn from each of them. While such a measure is not directly accessible, proxies propose to retain experiences in the replay buî€er or to sample experiences from the buî€er. Replay strategies reply on optimization objectives. In simple continuous control tasks,experience replay should contain experiences that are not close to the current policy to prevent î€›tting to local minima, and the best replay distribution is in between an on-policy distribution and uniform distribution [7]. However, they De Bruin et al. [7]also note that such a heuristic is unsuitable for complex tasks where policies are updated for many iterations. In DRL problems, when the rewards are sparse, the agent can learn from failed experiences by replacing the original goals with states in reproduced artiî€›cial successful trajectories [1] For complex control tasks, PER [28] measures the importance of experiences using the TD-error and designs a customized importance sampling strategy to avoid the eî€ect of bias. Based on that, Ref-ER [24] actively enforces the similarity between policy and the experience in the replay buî€er, considering on-policy transitions are more useful for training the current policy. AER [32] is an experience replay method that combines the advantages from PER and Ref-ER. It uses attention score to indicate state similarity and replays those experiences awarded high similarity with high priority. All aforementioned work focuses on optimizing the sampling strategy, aiming to select the salient and relevant agentâ€™s experiences in replay buî€er eî€ectively. Selective experience replay (SER) [16], in contrast, aims to optimize the storing process to ensure only valuable experience will be stored. The main idea is to use an Long-short term memory (LSTM) network to store only useful experience. In this paper, we propose state-aware reward-driven experience replay (LSER) to address the sub-optimality and training instability issues with reinforcement learning for online recommender systems. Instead of focusing on improving the sample eî€œciency for discrete tasks, LSER considers online recommendation as a continuous task; it then uses locality-sensitive hashing to determine state similarity and reward for eî€œcient experience replay. Our evaluation of LSER against several state-of-the-art experience-replay methods on three benchmarks (VirtualTB, RecSim and RecoGym) demonstrate LSERâ€™s feasibility and superior performance. In the future, we will explore new solutions for improving stability, such as better optimizers to help the agent get rid of saddle points, new algorithms to stabilize the training for DDPG, and trust region policy optimization to increase training stability [29]. Moreover, more advance reinforcement learning algorithms could be used to replace the DDPG such as soft actor-critic (SAC) [10] or Twin Delayed Deep Deterministic (TD3) [9].