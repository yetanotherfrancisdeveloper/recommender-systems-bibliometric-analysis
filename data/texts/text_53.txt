{bencheng.ybc,pengjie.wpj,victorlanger.zk,kuang-chih.lee,xiyu.xj,bozheng}@alibaba-inc.com,lwsaviola@163.com Embedding learning for categorical features is crucial for the deep learning-based recommendation models (DLRMs). Each feature value is mapped to an embedding vector via an embedding learning process. Conventional methods conî€›gure a î€›xed and uniform embedding size to all feature values from the same feature î€›eld. However, such a conî€›guration is not only sub-optimal for embedding learning but also memory costly. Existing methods that attempt to resolve these problems, either rule-based or neural architecture search (NAS)-based, need extensive eî€orts on the human design or network training. They are also not î€exible in embedding size selection or in warm-start-based applications. In this paper, we propose a novel and eî€ective embedding size selection scheme. Speciî€›cally, we design an Adaptively-Masked Twins-based Layer (AMTL) behind the standard embedding layer. AMTL generates a mask vector to mask the undesired dimensions for each embedding vector. The mask vector brings î€exibility in selecting the dimensions and the proposed layer can be easily added to either untrained or trained DLRMs. Extensive experimental evaluations show that the proposed scheme outperforms competitive baselines on all the benchmark tasks, and is also memory-eî€œcient, saving 60% memory usage without compromising any performance metrics. â€¢ Information systems â†’ Recommender systems. Recently, deep learning-based recommendation models (DLRMs) have been widely adopted in many web-scale applications such as recommender systems [2,5,9â€“11,16]. One of the main parts of DLRMs is the embedding layer, which exploits the categorical features. A standard embedding layer maps the categorical feature Figure 1: Comparison among existing methods and ours. to an embedding space [2,5,16]. Speciî€›cally, given a feature î€›eldğ¹ and let its vocabulary size be|ğ¹ |, each feature valueğ‘“âˆˆ ğ¹is mapped to an embedding vector by an embedding matrixğ‘Š âˆˆ R, where ğ· is a predeî€›ned embedding dimension. However, the above standard method can lead to two problems. First, in real applications, diî€erent feature values in the same feature î€›eld can have signiî€›cantly diî€erent frequencies. For highfrequency feature values, it is necessary to use a suî€œciently large embedding dimension to express rich information. Meanwhile, assigning too large embedding dimensions to low-frequency feature values is prone to over-î€›tting issues. Therefore, a î€›xed and uniform embedding dimension for all the feature values in a feature î€›eld can undermine eî€ective embedding learning for diî€erent feature values. Second, storing the embedding matrix with a î€›xed and uniform dimension may result in a huge memory cost [14,17,18]. A î€exible dimension assignment is needed to reduce the memory cost. There are some existing works trying to learn unî€›xed and nonuniform embedding dimensions for diî€erent feature values. They can be primarily divided into two categories. (1) Rule-based methods adopt human-deî€›ned rules, typically according to the feature frequencies, to give diî€erent embedding dimensions to diî€erent feature values [4] (see Fig. 1 (b) for an example). The problem with this category of methods is that they heavily rely on human knowledge and human labor. The resulting rough dimension selection for groups of feature values can often lead to poor performance (see Section 3.2). (2) Neural architecture search (NAS)-based methods use NAS techniques to search from several candidate embedding dimensions to î€›nd a suitable one for each feature value [8,12,18,19] (see Fig 1 (c) for an example). These methods require careful design of the search space and training-searching strategies. The search space is usually limited to a restricted set of discrete dimensions. Besides, both categories of methods mentioned above require training (i.e., embedding learning) from scratch. However, in real applications, there may exist some embedding matrices already trained with a huge amount of data. Such embedding matrices can be utilized for warm starting (see Section 3.4). Unfortunately, existing methods are not friendly to accommodate such a warm start mechanism. In this paper, we propose a novel and eî€ective method to select proper embedding dimensions for diî€erent feature values. The basic idea is to add an Adaptively-Masked Twins-based Layer (AMTL) on top of the embedding layer. Such a layer can adaptively learn a mask vector to mask the undesired dimension of the embedding vector for each feature value. The masked embedding vectors can be taken as the vectors with adaptive dimensions and are fed into the subsequent processes in DLRMs. This method exhibits some nice properties. First, it is eî€ective for embedding learning because the embedding dimension of diî€erent feature values can be learned and adjusted in continuous integer space with suî€œcient î€exibility without human interaction or speciî€›c NAS design (see Section 3.2). Second, it is eî€œcient since a memory-eî€œcient model can be built by adjusting the embedding dimension (see Section 3.3). Third, the parameters of the embedding matrix can be eî€œciently trained with the warm start mechanism (see Section 3.4). We summarize our contributions as follows: (1) We propose a novel embedding dimension selection method that completely removes the necessity of human rules or NAS architectures to facilitate adaptive dimension learning. (2) The proposed method (AMTL) can be easily applied in trained DLRMs to facilitate a warm start. The twins-based architecture successfully tackles the sample unbalance problem. (3) Extensive experimental results demonstrate that the proposed method outperforms strong baseline methods. The nice properties of AMTL helped us reduce memory cost by up-to 60% without compromising any performance metrics, and can further improve the performance by the warm start mechanism. We î€›rst recall a standard embedding layer which can be expressed asğ‘’= ğ‘Šğ‘£whereğ‘£is a one-hot vector for the feature value ğ‘“,ğ‘Š âˆˆ Rrefers to the embedding table andğ‘’âˆˆ Ris the embedding vector ofğ‘“. Then we deî€›ne a mask vectorğ‘šâˆˆ {0, 1} for ğ‘“. This mask vector should satisfy whereğ‘˜âˆˆ [0, ğ· âˆ’ 1]is a learnable integer parameter which is inî€uenced by the frequency ofğ‘“. Then, to allow diî€erentğ‘“can adjust its embedding dimension, the basic idea is that we can use the mask vector ğ‘što mask the embedding vector ğ‘’, i.e.,^ğ‘’= ğ‘šâŠ™ ğ‘’ whereâŠ™represents the element-wise multiply. Since the value whose index is larger thanğ‘˜in^ğ‘’is zero, the masked embedding vector^ğ‘’can be taken as an embedding vector where the embedding dimension is adaptively adjusted by the mask vector, and the î€›rst ğ‘˜+1dimensions (i.e., from 0-th toğ‘˜-th dimension) ofğ‘’is selected. Memory Saving.When storing^ğ‘’, we can simply drop the zero values in^ğ‘’to save memory and when fetching the stored vector, we can simply re-pad zero values to recover^ğ‘’. Embedding Usage.When embedding vectors are assigned with diî€erent dimensions, all of the existing methods [4,8,12,18,19] have to design an additional layer to unify these vectors to a same length to î€›t the following uniform MLP layers in DLRMs. Unlike these methods, our method does not need any additional layers since the masked embedding vectors^ğ‘’have the same length by zeros paddings, and can be directly fed into the following layers. Why select î€›rst ğ‘˜+ 1 dimensions?In this paper, We take the strategy about selecting î€›rstğ‘˜+ 1dimensions (i.e., from 0-th to ğ‘˜-th dimension) ofğ‘’as an example and others (e.g., selecting last ğ‘˜+ 1dimensions) are also allowed. One should keep in mind is that the select strategy should follow some rules. In other words, randomly selectingğ‘˜+ 1dimensions ofğ‘’is not a good strategy because we can hardly directly drop and recover these zeros values in^ğ‘’to save memory due to the random distribution of zeros values in^ğ‘’, and it also prevents the model to characterize each feature value since the same feature value may be mapped to diî€erent embedding vectors due to the random selection. Adaptively-Masked Twins-based Layer (AMTL) is designed to generate a mask vectorğ‘šin Eq 1 for each feature valueğ‘“. The framework of AMTL is shown in Fig 2. 2.2.1 Input and Output.Input:Sinceğ‘šis required to be adjusted by the feature frequency, to allow AMTL to have such frequency knowledge, we take the frequency attribute (e.g., the appear times in history, the frequency rank in this feature î€›eld and so no) ofğ‘“ as the input of AMTL. The input sample is denoted asğ‘ âˆˆ ğ‘…andğ‘§ is the input dimension.Output:The output of AMTL is a one-hot vector (called selection vector) to represent ğ‘˜in Eq 1. 2.2.2 Architecture. We propose a twins-based (i.e., two branches) architecture and each branch is an Adaptively-Masked Layer (AML). Note parameters of the two branches are not sharing. Both of AML is a multilayer perceptron:â„= ğœ (ğ‘Šâ„+ ğ‘)whereâ„is the frequency vector,ğ‘Šandğ‘are the parameters of theğ‘™-th layer,ğœ is an activation function andâ„âˆˆ Ris the output of the last layer. The motivation of such twins design is that if we only take a single branch (i.e., AML), the parameters update of AML will be dominated by the high-frequency feature values due to the unbalanced problem. Speciî€›cally, since high-frequency feature values appear more times in samples, the major part of the input sample of AML represents high-frequency vectors. Then, the parameters of AML may be heavily inî€uenced by the high-frequency vectors and AML may blindly select large embedding dimensions. Hence we design twins-based architecture to address this problem where the two branches (i.e., h-AML and l-AML) are used for high- and low- frequency samples respectively. In this way, the parameters of the l-AML will not be dominated by high-frequency samples and can give an unbiased decision. Weighted Sum.However, one challenge is that we can hardly give a threshold to diî€erentiate the high- and low- frequency samples to feed diî€erent samples to diî€erent branches. Hence, we propose a soft decision strategy. Speciî€›cally, we deî€›ne the frequency value ofğ‘“asğ‘which refers to the present times ofğ‘“in history, and feed the input sampleğ‘ into h-AML and l-AML respectively. A weighted sum is applied on theğ¿-th outputs (i.e.,â„âˆˆ R and â„âˆˆ R) of h-AML and l-AML, i.e., whereğ›¼âˆˆ [0, 1]is the weight score which is inî€uenced byğ‘. ğ‘›ğ‘œğ‘Ÿğ‘šoperation normalizesğ‘to a standard normal distribution which allowsğ›¼is distributed smoothly around1/2. Otherwise, all ğ›¼may be close to one without theğ‘›ğ‘œğ‘Ÿğ‘šoperation. In this way, for the samples with high-frequency, the correspondingâ„ is dominated byâ„due to a largeğ›¼. Then the parameters of h-AML are mainly updated during back-propagation and vice versa. Hence, AMTL can adjust the gradients of h-AML and l-AML to address the unbalanced problem. Note here we only give an example to calculate the weight valueğ›¼, other ways are also allowed as long as the produced ğ›¼has similar properties. Then we apply softmax function on â„, i.e., whereğ‘œâˆˆ Rrefers the probability to select diî€erent embedding dimension ofğ‘“, andğ‘œis theğ‘-th element ofğ‘œ. The selection vector can be obtained by Then the corresponding mask vector can be generated by whereğ‘€ âˆˆ Ris a pre-deî€›ned mask matrix andğ‘€= 1when ğ‘— â‰¤ ğ‘–otherwiseğ‘€= 0. Then the masked embedding^ğ‘’can be obtained byğ‘š. Note in practice, we usually apply diî€erent AMTLs on diî€erent important feature î€›elds (e.g., User ID and Item ID) and the parameters of these layers are not sharing for the purpose of î€›eld awareness. 2.2.3 Relaxation. However, the problem is that the learning process of AMTL is non-diî€erentiable due to the discrete process in Eq 5. It means the parameters of AMTL cannot be directly optimized by a stochastic gradient descent (SGD). To address this problem, we relaxğ‘¡to a continuous space by temperated softmax [6,7,13]. Concretely, the ğ‘-th element of ğ‘¡can be approximated as ğ‘¡â‰ˆ^ğ‘¡= ğ‘’ğ‘¥ğ‘ (â„/ğ‘‡ )/Ã(ğ‘’ğ‘¥ğ‘ (â„ whereğ‘‡is the temperature hyper-parameter. Whenğ‘‡ â†’ 0, this approximation becomes exact. Since^ğ‘¡is a continuous vector with diî€erentiable process, SGD can be naturally applied. Hence, instead of learning the discrete vector ğ‘¡, we learn^ğ‘¡to approximate ğ‘¡. However, there exists an information gap between training and inference phases when using temperature softmax. Speciî€›cally, we use the vector^ğ‘¡for training. While in inference, we only use the discrete vectorğ‘¡. To close this gap, inspired by the idea StraightThrough Estimator (STE) [1], we rewrite ğ‘¡as where stop_gradient is used to prevent the gradient from backpropagation through it. Since the forward pass is not aî€ected by stop_gradient,eğ‘¡= ğ‘¡during this phase. For the back-propagation, it avoids the non-diî€erentiable process by stop_gradient. Data Sets.(1) MovieLensis a user review data about movies and is collected from MovieLens website. There are a total of 1,000,209 records. (2) IJCAI-AACis collected from a sponsored search in Ecommerce. There are a total of 478,138 records. (3) Taobao Dataset is an industrial dataset which is constructed from Taobao. There are a total of 50 billion around records. Baselines.We consider diî€erent kinds of state-of-the-art embedding methods as baselines (1) Standard: traditional Fixed-based Embedding (FBE). (2) Rule-Based: MDE [4] divides diî€erent feature values into several blocks by their frequency, and assigns diî€erent embedding dimensions for diî€erent blocks by rules. (3) NAS-Based: AutoEmb [19] adopts NAS to select embedding dimensions among some candidate embedding dimensions for diî€erent feature values. Setting.The maximal embedding dimension and the DLRM boneskeleton of all methods are set the same. The dimension selection strategy is applied to the feature î€›elds which are related to the user and item property (e.g., User ID and Item ID). For AutoEmb, the candidate dimension list is set smoothly by following the original paper [19]. The temperature ğ‘‡ in Eq 7 is set by grid search. Here, we compare our method AMTL with baselines on clickthrough rate (CTR) prediction tasks and take the AUC [3] score as the metric. Note a slightly higher AUC at0.1%-levelis regarded as signiî€›cant for the CTR task [15,20]. As shown in Table 1, we can conclude that: (1) Compared with FBE, AMTL can archive better performance in all datasets. It shows that adopting an unî€›xed embedding dimension can improve the model performance. (2) Compared with the rule-based method (i.e., MDE), AMTL outperforms MDE. Besides, MDE only obtains similar performance with FBE. It indicates a rough human rule on dimension selection cannot always guarantee an improvement. (3) For the NAS-based method (AutoEmb), AMTL also archives better performance. It demonstrates that AMTL adopts a more suitable scheme i.e., selecting a dimension from a continuous integer space. Here, we compare the memory cost of diî€erent methods. Since the memory size of the embedding matrix is in direct proportion to the dimension [14,17], for simplicity, the averaged dimension (i.e., Avg(Dim)) are reported. We take the feature î€›eld "User ID" in Taobao as an example (others can have similar conclusions), and itsâ€™ maximal dimension is set as 300. Table 2 shows the results. We also show the Avg(Dim) ratio compared with FBE. We can î€›nd that (1) Compared with FBE, all the dimension selection methods can save memory size by reducing the embedding to a suitable dimension. (2) Since AMTL allows a more î€exible dimension selection in a continuous integer space, it reduces memory cost more signiî€›cantly by around 60 %. Here, we conduct experiments to evaluate the warn start on the dataset Taobao which is close to the industrial and real system. There are two kinds of parameters in DLRM, i.e., the parameters of embedding matrix and hidden layers. In the warm start setting, for existing dimension selection methods, we initialize the parameters of hidden layers by loading the parameters from the online model in Taobao, and the parameters of embedding matrix are randomly initialized due to the inability on the warm start of embedding matrix. While, since AMTL and FBE can warm start the parameters both of the embedding matrix and the hidden layers, we load both of them from the online model for these two methods. The results are shown in Table 3. The results of random start are also provided. We can î€›nd that compared with MDE and AutoEmb, AMTL can perform better in the warm start setting. Speciî€›cally, compared with the best baseline AutoEmb, the gain of AMTL is 1.6% in the warm start manner, which is 5Ã—times larger than the gain in the random start manner. Furthermore, due to the inability for the warm start of the embedding matrix, MDE and AutoEmb even perform worse than the standard full embedding. It demonstrates that the dimension selection scheme designed in AMTL is a more wise and î€exible way in real application systems. Here we analyze whether AMTL can give suitable dimensions for diî€erent feature values. Speciî€›cally, we divide the feature value into 7 groups (i.e.,ğº, ğ‘– âˆˆ {0, 1, 2, ..., 6}) by frequency and the average frequency of diî€erent groups is increased fromğºtoğº. Note there are too many feature î€›elds in diî€erent datasets, we take the feature î€›eld "User ID" in Taobao as an example, and others have similar results. The averaged embedding dimension in diî€erent groups is reported in Fig 3 (a). It shows that when the frequency increases (i.e., fromğºtoğº), the selected average dimension is increased. Figure 3: Dimension section of AMTL and AML. It indicates AMTL can assign suitable embedding dimensions to diî€erent frequency feature values adaptively. Here, we conduct an ablation study on the twins-based architecture and STE. Due to the limited space, the results on IJCAI-AAC are reported and similar conclusions can be found from other datasets. Evolution on twins-based architecture.We compared the AUC score between AMTL and AML (only a single branch) on CTR task. From Table 4, although both AML and AMTL perform better than FBE, AMTL archives a higher performance. It indicates twins-based architecture plays an important role in feature learning. Besides, similar to Section 3.5, we also visualize the dimension selection of AML in Fig 3 (b). We can î€›nd that AML only successfully gives suitable dimensions in high-frequency groups (i.e.,ğºtoğº). In low-frequency groups, due to the unbalanced problem, it blindly gives high dimensions for low-frequency values. And the lower the frequency, the worse it is. Evolution on STE.Here, we analyze the eî€ectiveness of STE, and implement a variant of AMTL without STE, denoted as AMTL-nSTE. From Table 4, compared with AMTL-nSTE, AMTL can archive better performance. It demonstrates the usefulness to bridge the information gap between training and inference phases by STE. In this section, we conduct experiments to report the time cost per epoch of diî€erent methods on IJCAI-AAC (similar conclusions can be found in other datasets). As shown in Table 5, we î€›nd that compared with FBE, the dimension selection methods need more time to train the model per epoch due to the dimension selection processes. During inference, we can directly look up the learned embedding table which has adaptive dimensions without the process of dimension selection to save time. Traditional embedding learning methods usually adopt a î€›xed dimension for all features which may cause problems in space complexity and performance. To address this problem, we propose a novel dimension selection method called AMTL which produces a mask vector to mask the undesired dimensions for diî€erent feature values. Experimental results show that the proposed method can archive the best performance on all tasks especially in the case of embedding warm start, give a suitable dimension for diî€erent features and save memories at the same time.