School of Software Engineering, Xiâ€™anCollege of Artiî€›cial Intelligence, Figure 1: Screenshots of GeneAnnotator. (a) The user interface. The relationship between the subject, i.e. the red-masked car, and the object, i.e. the blue-masked car in the image region is deî€›ned by a directed graph, and described by editing or selecting from the recommended candidate relationships listed on the GUI. (b) To avoid trivial and irrelevant relationships, a â€œregionâ€ of interest, i.e. the yellow bounding box, could be set during the annotation. (c) Nearby cars can be annotated as a â€œclusterâ€ in GeneAnnotator, sharing the same relationship with other cars or roads. (d) The real-time visualized scene graph according to annotations in (a). In this manuscript, we introduce a semi-automatic scene graph annotation tool for images, the GeneAnnotator. This software allows human annotators to describe the existing relationships between participators in the visual scene in the form of directed graphs, College of Artiî€›cial Intelligence, Xiâ€™an Jiaotong University liuyh@mail.xjtu.edu.cn hence enabling the learning and reasoning on visual relationships, e.g., image captioning, VQA and scene graph generation, etc. The annotations for certain image datasets could either be merged in a single VG150 data-format î€›le to support most existing models for scene graph learning or transformed into a separated annotation î€›le for each single image to build customized datasets. Moreover, GeneAnnotator provides a rule-based relationship recommending algorithm to reduce the heavy annotation workload. With GeneAnnotator, we propose Traî€œc Genome, a comprehensive scene graph dataset with 1000 diverse traî€œc images, which in return validates the eî€ectiveness of the proposed software for scene graph annotation. The project source code, with usage examples and sample data is available at https://github.com/Milomilo0320/A-Semi-automaticAnnotation-Software-for-Scene-Graph, under the Apache opensource license. â€¢ Applied computing â†’ Document metadata; Annotation. scene graph, traî€œc knowledge representation, visual relationship description, autonomous driving dataset ACM Reference Format: Zhixuan Zhang, Chi Zhang, Zhenning Niu, Le Wang, and Yuehu Liu. 2021. GeneAnnotator: A Semi-automatic Annotation Tool for Visual Scene Graph. In MM â€™21, October 20â€“24, 2021, Chengdu, China. ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/1122445.1122456 Scene graph is a topological structured data representation of visual content. It is usually represented by a directed graph, the nodes of which represent the instances and the edges represent the relationship between instances. Speciî€›cally, for a relationship, the starting node is called the subject, and the ending node is called the object. In recent years, scene graph has been applied to semantic image retrieval, image captioning, visual question answering and achieved reasonable performance. Large-scale scene graph datasets with precise manually annotated are the prerequisites for related research. The open-source annotation tool is relatively uncommon at present. Existing datasets cannot organize data into a uniî€›ed data framework. It is not conducive to customize datasets. In order to î€›ll these gaps, we propose the GeneAnnotator, a semi-automatic annotation software for scene graph generation by Python. It has the following major features:(1) Friendly user interactivity.Figure 1 shows the GUI of GeneAnnotator. Users can visually view the contents annotated and scene graph in real time. (2) Diverse information.In addition to the relationships between instances, it also allows users to annotate diverse information such as regions, clusters and attributes.(3) Semi-automatic annotation.GeneAnnotator can recommend the relationship according to the recommendation algorithm and rule system, which greatly improves the annotation eî€œciency.(4) Highly customized.Users are free to deî€›ne the underlying data, including object categories, relationship lists and so on. It is helpful for researchers to build customize datasets.(5) Applying easily.The output dataset î€›les are organized in VG150â€™s data format and can be ready to most existing models for scene graph learning. At the same time, we provide Traî€œc Genome, a scene graph dataset with 1000 traî€œc images built by GeneAnnotator. Trafî€›c Genome contains information about the location, labels, relationships, attributes and other information of the elements in traî€œc scenes. It is one of the earliest open-source traî€œc scene graph datasets. In comparison to other related datasets, such as Visual Genome[2] and VG150[6], objects and relationships in Traî€œc Genome are denser and the attribute coverage is highest. In this section, we present the system architecture of GeneAnnotator, as illustrated in Figure 2. We î€›rst detail the functionalities and concepts in core modules and then introduce the annotation workî€ow. Main Annotation Module is the most basic module. Its main functions include data input and output, data encoding, relationship annotation and scene graph visualization. This module allows users to freely deî€›ne the underlying data, i.e. object lists, object categories, object hierarchies, relationship lists, relationship categories, and relationship hierarchies. Object hierarchy and relationship hierarchy[8]:objects and relationships are hierarchical and instances connect to others within or across diî€erent layers to form a scene graph. Auxiliary Function Module allows researchers to annotate regions, clusters and attributes. Region:In order to make the scene graph directly express the visual perception of images, we suggest user located each visually meaningful region with a bounding box like Figure 1(b). The region is helpful to avoid annotating trivial and irrelevant relationships in scenes. Cluster:Users can regard objects nearby and attributes as a "cluster". For example, nearby cars in Figure 1(c) were annotated as a â€œclusterâ€. The practice has proved that clusters can make the annotation more eî€œcient. Attributes:Attributes are an additional description of objects that can provide î€›ner-grained nodes for the scene graph to better distinguish diî€erent instances. In GeneAnnotator, â€œorientationâ€ attributes are constructed for the research content. The researchers can add the required attributes to software according to their own needs, such as speed, color, size and so on. Semi-automatic Annotation Module recommends the best relationship for each pair of instances according to the recommendation algorithm and rule system. Recommendation algorithm:Based on Simple Tag-based Recommendation algorithm, <subject, relationship, object> is represented as(ğ‘¢, ğ‘–, ğ‘).ğ‘¢indicates a pair of <subject, object>, which has relationshipğ‘–and attributeğ‘.ğ‘is a vector, each element in which is a visual attribute of the pair ofğ‘¢, like â€œwhether the proî€›le touchâ€ or â€œwhether the diî€erent size of bounding boxesâ€. The equation of interest between a pair of <subject, object>ğ‘¢and their relationship ğ‘– is given by: whereğ‘›indicates the number of times thatğ‘¢has attribute ğ‘.ğ‘›indicates the number of times thatğ‘¢with attributeğ‘has relationships ğ‘–. The prior database consists of the annotated relationships. The parameters in Equation (1) can be quickly updated and relationships are recommended without training. Figure 2: System Architecture. In GeneAnnotator, there are three core modules: Main Annotation Module, Semi-automatic Annotation Module and Auxiliary Function Module. Each module is independent of one another. Removing one of the modules does not aî€ect the use of the whole software, which makes it easy for developers to update and maintain. Rule system:The recommendation algorithm is based on a large number of prior information. In the stage of start in the sparse data, self-encoding rule are used to guess a relationship. The default rules in GeneAnnotator is to judge the relationship according to the contact and position of two instances. The workî€ow of building a scene graph dataset is shown in the workî€ow layer in Figure 2. First of all, the conî€›guration î€›les of the original image and other underlying data are read into GeneAnnotator. GUI shows the image and user on-demand region annotation and cluster annotation. Then the Semi-automatic Annotation Module recommends relationships, which users can modify or add to. The annotated relationships will be stored in the prior database to enrich the recommendation algorithm. Finally, the output î€›les are the scene graph dataset. Traî€œc Genome is a traî€œc scene graph dataset comprised of 1000 traî€œc scenes. The original images are selected from Cityscapes[1], which is a pixel-level semantic segmented dataset. We oî€er two diî€erent formats of Traî€œc Genome. One format is following the structure of VG150, which covers all the scene graphs (1000 images). According to related work, most of the existing scene graph learning models use the database and metadata proposed in VG150. Therefore, î€›les in this format can be directly used in existing models. However, it does not support users to modify or add relationships to a speciî€›ed scene graph. Thus, we designed another format for per-image, containing image instances map, clusters annotations and other more detailed information. It is convenient to modify the speciî€›ed scene graph. We also provide code to convert between the two formats. In Traî€œc Genome, there are 34 semantic object classes (including "unlabeled") and 51 relationships. In comparison to related datasets, objects and relationships in Traî€œc Genome are denser and the attribute coverage is highest. As shown in Table 1, there is a comparison among Traî€œc Genome, Visual Genome and VG150. Object density:In Traî€œc Genome, there are 25,147 annotated instances, of which 19,291 are involved in the scene graph. On average per image, Traî€œc Genome has 19.29 instances were involved in a scene graph, which is very close to the 20.85 in Visual Genome and much larger than the 5.69 in VG150. At the same time, the percentage of instances involved in a scene graph in Traî€œc Genome is about 76.71%, much larger than the 58.65% of Visual Genome and the 53.66% of VG150. It indicates that Traî€œc Genome has richer and î€›ner annotations, allowing more instances in the images to participate in the construction of the scene graph. Relationship density:We annotated 29,192 relationships in Traî€œc Genome. On average, each scene graph has 29.19 relationships, much larger than the 14.17 in Visual Genome and the 5.76 in VG150. We also calculated the average of relationships involved per object. The results show that one object has relationships with another 3.02 objects in Traî€œc Genome. The value in Visual Genome and VG150 is 1.36 and 2.02. It can be regarded as a measure of the sparsity of a scene graph. A denser graph has more edges involved per node. That means Traî€œc Genome has a much denser relationship annotation than the other two datasets. Most of the relationships are focused on spatial relationships (such as â€œin left ofâ€) and area relationships(such as â€œdriving onâ€), 43.85% and 42.04% of the total relationships, respectively. The top 5 common combination of <Object â€“ Relationship - Subject> are : â€œPerson â€“ Walking on - Sidewalkâ€, â€œCar â€“ In front of/In back of Carâ€, â€œCar â€“ driving on - Roadâ€, â€œCar â€“ Parking on - Roadâ€, â€œPerson â€“ Walking on - Roadâ€. Attribute coverage:Traî€œc Genome annotated oriented attribute for each instance, i.e. "forward", "leftward", "rightward" and "backward". The attribute coverage is 100%, which is the highest of the three datasets. We use Traî€œc Genome to experiment and evaluate the following scene graph generation models: â€¢IMP[6]: The model solves the scene graph inference problem using standard RNNs and learns to iteratively improves its predictions via message passing. â€¢Motif[7]: Regularly appearing substructures in scene graphs make object labels are highly predictive of relation labels. â€¢VC-Tree[5]: Proposing a method to compose dynamic tree structures that place the objects in an image into a visual context. â€¢VCTree-TDE[4]: Building a causal graph and making biased training with the graph. â€¢EBM-Loss[3]: The additional constraint in the learning framework acts as an inductive bias and allows models to learn eî€œciently from a small number of labels. Since Traî€œc Genome was given the labels of ground-truth, we only evaluated the performance of predicate classiî€›cation. Models successfully run with Traî€œc Genome. As shown in Table 2, the predicate classiî€›cation results of Traî€œc Genome are very close to VG150. It even performs better in mR@100. By evaluating Trafî€›c Genome in scene graph generation models, GeneAnnotator is proved to be a valuable tool for scene graph annotation. Table 2: The predicate classiî€›cation results on diî€erent models. GeneAnnotator is released under the license of Apache 2.0 and its source code is openly available at: https://github.com/Milomilo0320/ASemi-automatic-Annotation-Software-for-Scene-Graph. In addition, Traî€œc Genome will be made available at https://github.com/Milomi lo0320/Traî€œc-Scene-Graph-1000. Contributions from the opensource community are welcome, via the GitHub issues/pull request mechanisms. In this paper, we have developed GeneAnnotator, a semi-automatic annotation software for scene graph generation by Python. It is one of the earliest open-source scene graph annotation tools. With modular designs, GeneAnnotator is easy-to-use and easy-to-extend. Furthermore, we provide Traî€œc Genome, a scene graph dataset with 1000 traî€œc images built by GeneAnnotator. By evaluating Traî€œc Genome in scene graph generation models, GeneAnnotator is proved to be a valuable tool for scene graph annotation. This work was supported by the National Key Research and Development Program of China, No. 2018AAA0102504.