1 INTRODUCTION Oï¬€-policy evalu ati on (OPE) is the met hod that attempts to estimate the performance of decision making policies using historical data generated by diï¬€erent policies without conducting costly online A/B tests [1, 2]. Accurate OPE is essential in domains such as healthcare, marketing or recommender systems to avoid deploying poor performing policies, as such policies may hart human lives or destroy the user experience. Thus, many OPE methods with t heoretical backgrounds have been proposed, including Direct Method (DM), Inverse Probability Weighting (IPW), and Doubly Robust (DR). One emerging challenge with this trend is that a suitable estimator can be diï¬€erent for each application setting. For example, DM has low variance but has a large bias, and thus, performs better in small sample settings. On the other hand, IPW has a low bias but has a large variance, and thus reveals better performance in large sample sett ings. It is often unknown for practitioners which estimator to use for their speciï¬c applications and purposes. To ï¬nd out a suitable estimator among many candidates, we use a data-driven estimato r selection procedure for oï¬€-policy policy performance estimators as a p ractical solution. As proof of concept, we use our procedure to select the best estimator to evaluate coupon treatment policies on a real-world online content delivery ser vice. In the experiment, we ï¬rst observe that a suitable estimator might change with diï¬€erent deï¬nitions o f the outcome variable, and thus the accurate estimator selection is critical in real-world applications of OPE. T hen, we demonstrate that, by utilizing our estimator selection procedu re, we can easily ï¬nd ou t suitab le estimators for each purpo se. We believe that our estimator selection procedure and case study help practitio ners identify the best OPE method for their environments. 2 SET UP AND METHOD We denote ğ‘‹ âˆˆ X as a context vector and ğ‘‡ âˆˆ T = {0, 1} as a binary treatment assignment indicator individual user ğ‘– receives the treatment, ğ‘‡ denoted as (ğ‘Œ (0), ğ‘Œ (1)) for each individual. ğ‘Œ (0) is a potential outcome associated with ğ‘‡ = 0, and ğ‘Œ (1) is associated with ğ‘‡ = 1. No te that each individual receives only one treatment, and only a potential outcome for the received treatment is observed. We c an represent the observed outco me as: ğ‘Œ = ğ‘‡ğ‘Œ (1) + (1 âˆ’ ğ‘‡ )ğ‘Œ (0). Table 1. Relative RMSE of oï¬€-policy estimators when ğ‘Œ is con-Table 2. Relative RMSE of oï¬€-policy estimators when ğ‘Œ is revtent consumption indicatorenue from users Note: Dâ†’ ğœ‹is a case where we attempt to estimate the performance of ğœ‹using log data generated by ğœ‹. In contrast, Dâ†’ ğœ‹is a case where we attempt to estimate the performance of ğœ‹using lo g data ge nerated by ğœ‹. The bold fonts represent the best oï¬€-policy estimator among DM, IP W, and DR for each setting (lower value is better). A policy automatically assigns t reatments to users aiming to maximize the out come. We denote a policy as a function that maps a context vector to one of the possible treatments, i.e., ğœ‹ : X â†’ T . Then, the performance of a policy is deï¬ned as ğ‘‰ (ğœ‹) = E[ğ‘Œ (ğœ‹ (ğ‘‹ ))]. The goal of OPE is to estimate ğ‘‰ (ğœ‹ ) for a given new policy ğœ‹ using log data D = {(ğ‘‹,ğ‘‡, ğ‘Œ)}collected by an old policy diï¬€erent from ğœ‹. Our strategy to select the suitable estimator is to use two sou rces of logged bandit feed back collected by two diï¬€erent behavior policies. We denote log data generated by ğœ‹and ğœ‹as D= {(ğ‘‹,ğ‘‡, ğ‘Œ)}and D= {(ğ‘‹,ğ‘‡, ğ‘Œ)}, respectively. To evaluate the performance of an estimato rË†ğ‘‰ , we ï¬rst estimate policy performances of ğœ‹and ğœ‹by Ë†ğ‘‰ (ğœ‹; D) andË†ğ‘‰ (ğœ‹; D). Then, we use on-policy estimates as the ground-truth policy performances, i.e., ğ‘‰ (ğœ‹) â‰ˆ Ë†ğ‘‰ (ğœ‹; D) = ğ‘›Ãğ‘Œand ğ‘‰ (ğœ‹) â‰ˆË†ğ‘‰ (ğœ‹; D) = ğ‘›Ãğ‘Œ. Finally, we compare the oï¬€-policy estimates Ë†ğ‘‰ (ğœ‹; D) andË†ğ‘‰ (ğœ‹; D) with their ground-truths (on-policy estimates) ğ‘‰ (ğœ‹) and ğ‘‰ (ğœ‹) to evaluate the estimation accuracy of an estimatorË†ğ‘‰ . We evaluate the estimation accuracy of an estimatorË†ğ‘‰ by the relative root mean-squared-r error deï¬ned asğ¾Ã()(same for ğœ‹) where ğ‘˜ denotes a diï¬€erent subsample of logged bandit feedback made by the sample splitting or bootstrap sampling. By applying the above procedure to several candidate estimators, we can select the estimator having the best estimation accuracy among candidates in a data-driven manner. 3 A CASE STUDY To show the usefulness of our procedure, we constructed Dand Dby randomly assigning two diï¬€erent policies (ğœ‹and ğœ‹) to users on our content delivery platform. Here, ğ‘‹ is a userâ€™s context vector, ğ‘‡ is a coupon assignment indicator, and ğ‘Œ is either a userâ€™s content consumption indicator (binary) or the revenue from each user (continuous). We report the estimator selection results for each deï¬nition of ğ‘Œ in Table 1 and 2. We u sed DM, IPW, and DR as candidate oï¬€-policy estimators. The tables show that diï¬€erent estimators should be used for each setting and purpose. This is because the prediction accuracy of the outcome regressor used in DM and DR can be diï¬€erent for each deï¬nition of ğ‘Œ . We conclude from the results that we should use DM when we want to maximize the usersâ€™ content consumption probability. In contrast, we use IPW or DR when we consider the revenue from users as the outcome. After the successful empirical veriï¬cation, our data-driven estimator selection method has been used t o decide which estimators to use to create coupon allocation policies on our platform.