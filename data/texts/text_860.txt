Our goal is to identify inhibitors and catalysts for productive longterm scientiï¬c software development. The inhibitors and catalysts could take the form of processes, tools, techniques, environmental factors (like working co nditions) and software artifacts (such as user manuals, unit tests, design documents and code). The effort (time) invested in catalysts will pay oï¬€ in the long-term, while inhibitors will take u p resources, and can lower product quality. Developer surveys on inhibitors and catalysts will yield responses as varied as the education and experiential backgrounds of the respondents. Although well-meaning, respo nses will predictably be biased. For instance, developers may be guilty of the sunk cost fallacy, promoting a technology they have invested considerable hours in learning, even if the current costs outweigh the beneï¬ts. Likewise developers may recommend against spending time on proper requirements, not as an indication that requirements are not valuable, only that current practice doesnâ€™t promote requirements [ 2]. Another perceived inhibitor is time spent in meetings. For instance, the lack of visible short-term beneï¬ts renders department retreats unpopular, even though relationship building and strategic decision making may provide signiï¬cant future rewards. Evaluating the usefulness of meetings is diï¬ƒcult. Rather than relying o n preference and perception, as these examples illu strate, we need to measure the long-term impact of development choices to make wise ones. A scientiï¬c approach requires a solid foundation. The building blocks for scientiï¬c discourse are: communicating co ncepts via an unambiguous language, for mulating hypotheses, p lanning data collection, and analyzing models and theories. To start with, we need to classify the software under discussion. Likely dimensions include: general purpose scientiï¬c tools versus special purp ose physical models, scientiï¬c domain, open source versus commercial software, project maturity, project size, and level of safety criticality. We also nee d to be precise about our software quality goals. Qualities such as reliability, sustainability, reproducibility and productivity need precise deï¬nitions. Attempts have been made since the 1970s [7], but the resulting deï¬nitions arenâ€™t usually speciï¬c t o scientiï¬c software (as shown by the confusion between precision and accuracy is the ISO/IEC deï¬nitions [4]). Moreover, the deï¬nitions often focus on measurability, where the ï¬rst priority should be conceptual clarit y, analogous to the unmeasurable, but conceptually clear, deï¬nition of forward error, which req uires knowing the (usually unknown) true answer. For each relevant quality we recommend collecting as many distinct deï¬nitions as possible. Once collected, they can be assessed against the following criteria (b ased on IEEE [3]): completeness, consistency, modiï¬ability, traceability, unambiguity and abstractness. The understanding gained from this systematic survey and analysis can be used to either choose solid deï¬nitions, or propose new ones. In all cases, the deï¬nitions should enable reasoning about quality. Our deï¬nition of long-term productivity [9] provides an example of our vision, and meets our criteria. We deï¬ne productivity as: where ğ‘ƒ is productivity, ğ¼ is the inputs, ğ‘‚ is the outputs, 0 is the time the project started, ğ‘‡ is the time in the future where we want to take stock, ğ» is the total number of hours available by all personnel, ğ¶ represents diï¬€erent classes of users (external as well as internal), ğ‘† is user satisfaction and ğ¾ is eï¬€ective knowledge, and ğ¹ is a weighing function that indicates â€œvalueâ€. Thus productivity is measured in â€œvalue per year.â€ and is a mixture of external and internal value produced. Value should not be equated with money; measuring the productivity of free software development is just as important as for commercial software. While the most straightforward use of such a formula is t o measure productivity of a team, it can also be used in â€œwhat ifâ€ scenarios to assist in planning interventions, i.e. changes intended to improve productivity. Measuring over too short a time-frame will assuredly give warped results. This leads so me to argue that productivity shouldnâ€™t even be measured [5]. Proper science requires measurement. We can only d etermine whether a given intervention is a catalyst or inhibitor by measuring its impact. Let us examine in more details t he consequences of our proposed deï¬nition. First, the time integrals emphasize that productivity is something that happens over time. The most interesting kind of productivity is that of an organization over the span of years. Measuring over too short a time frame is one of the main sources of t echnical debt [6] as it devalues planning, team work, being strategic, etc. Secondly, as Drucker [1] reminds u s, quality is at least as import ant as quantity. Here we use a proxy for quality, namely user satisfaction. It is important to note that unreleased prod ucts and unreleased features induce no user satisfaction. A broken prod uct might be even worse, and produce negative satisfaction. The input ğ» is the number of hours worked by the team, including managers and support staï¬€, as appropriate. To o ptimize productivity, we want to make ğ¼ , and thus ğ» , small. This is the raw input being applied, whether eï¬€ect ive or not. We use user satisfaction (ğ‘†) as a proxy for eï¬€ective quality. How to measure this is left for future study. It can be approximated by measures such as numbers of users, number of citations, number of forks of a repository, number of â€œstarsâ€, surveys of existing users, number of mentions in the issue tracker, and usability experiments. Probably the trickiest part is eï¬€ective knowledge (ğ¾). The idea is that while source code embodies operational knowledge that has the potential to directly lead to user satisfaction, a p roject usually also generates a lot of tacit knowledge about design, including the rationale for various choices. This is the kind of knowledge that is lost when employees leave, and is the most co stly to build and replace. In other words, human-reusable knowledge such as documentation factors in here. The best measure for knowledge is an area for future exploration. Software development typically produces many artifacts, such as requirements, speciï¬cations, user manuals, unit tests, system tests, usability tests, build scripts, API (Ap plication Programming Interface) documentation, READMEs, license d ocuments, process documents, and code. We regard all of these as containing knowledge, albeit encoded in diï¬€erent forms. Furt hermore, it is cr ucial to recognize that the knowledge of a single product is distributed amongst those artifacts. In particular, the various artifacts contain many copies of the same core knowledge â€” by design. To understand the importance of certain artifacts, it makes sense to look at the productivity impact of their presence/absence. For example, long-lived projects will inevitably encounter contributor turnover. How long should it t ake for new contributors to be productive? How much training by peer mentors will it t ake? Could some documentation be written that would shorten this learning perio d and, just as imp ortantly, reduce the time it takes from experienced peo ple? Of course, documentation that is out-of-date could be even worse: a false sense of knowledge that results in even more wasted work that needs repairing. As we gain understanding on measures of value, we can use them to evaluate the state of practice in diï¬€erent research software domains. We can estimate the knowledge ğ¾ embedded in, and the user value ğ‘† derived from, existing artifacts. In particular, we can compare these to the artifacts produced by recommended processes from standard so ftware engineering textbooks. For example, we can test the hypothesis that knowledge duplication between code and requirements, coupled with the fact that requirements get de-synchronized from the code and the tenuous link to user value, is the likely reason for low adoption of requirements in scientiï¬c software development [2]. Nevertheless, documentation remains useful, especially for the very long term. Another means to judge the utility of documentation is to loo k at assurance cases. An assurance case [8] presents an organized and explicit argument for correctness (or whatever other software q uality is deemed important) through a series of sub-arguments and evidence. Assurance cases gives at least one measure of which documentation is relevant and necessary. One way to improve productivity is to waste less on non-productive or counter-productive activities. That code is the most visible artifact that contributes user-value, along with with testing (because quality is an extremely important factor in user-value) explains the inordinate focus on just those artifacts. Furthermore, the deemphasis on documentation, even to the extreme of some metho dologies having none, can feel like p roductivity improvements in the short term! A better approach would be to capture knowledge in ways that keeps it continuously synchronized between the various artifacts where it appears. One promising approach is to generate all artifacts from a single knowledge base [10]. This relies on a solid understanding of the contents of all of the artifacts present in t he software engineering process. Our proof-of-concept shows that this is possible. As the artifacts are now generated, knowledge duplication is not a problem. Even better, the knowledge is synchronized-by-construction. Furthermore, it b ecomes easy to tailor artifacts, documentation as well as code, to diï¬€erent classes of â€œusersâ€. Our positio n is that decisions on processes, tools, techniques and software artifacts should be driven by science, not by personal preference. Decisions should not be based on anecdotal evidence, gut instinct or the path of least resistance. Moreover, decisions should vary depending on the users and the context. In most cases of interest, this means that a longer term view should be adopted. We need to use a scientiï¬c approach b ased on unambiguous deï¬nitions, empirical evidence, hypothesis testing and rigorous p rocesses. By developing an understanding of where input hours are spent, what most contributes to user satisfaction, and how to leverage knowledge produced, we can determine what has the greatest return o n investment. We will be able to recommend software production processes that justify their value because the long-term output beneï¬ts are high compared to the required input resources.