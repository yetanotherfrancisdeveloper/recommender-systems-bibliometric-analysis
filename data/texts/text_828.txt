Software developers often look for solutions to their code-level problems by submitting questions to technical Q&A websites like Stack Overî€ow (SO). They usually include example code segments with questions to describe the programming issues. SO users prefer to reproduce the reported issues using the given code segments when they attempt to answer the questions. Unfortunately, such code segments could not always reproduce the issues due to several unmet challenges (e.g., external library not found) that might prevent questions from receiving prompt and appropriate solutions. A previous study produced a catalog of potential challenges that hinder the reproducibility of issues reported at SO questions. However, it is unknown how the practitioners (i.e., developers) perceive the challenge catalog. Understanding the developersâ€™ perspective is inevitable to introduce interactive tool support that promotes reproducibility. We thus attempt to understand developersâ€™ perspectives by surveying 53 users of SO. In particular, we attempt to â€“ (1) see developersâ€™ viewpoints on the agreement to those challenges, (2) î€›nd the potential impact of those challenges, (3) see how developers address them, and (4) determine and prioritize tool support needs. Survey results show that about 90% of participants agree to the already exposed challenges. However, they report some additional challenges (e.g., error log missing) that might prevent reproducibility. According to the participants, too short code segment and absence of required Class/Interface/Method from code segments severely prevent reproducibility, followed by missing important part of code. To promote reproducibility, participants strongly recommend introducing tool support that interacts with question submitters with suggestions for improving the code segments if the given code segments fail to reproduce the issues. â€¢ Software and its engineering â†’Software repository mining; Software maintenance and evolution; Practitionersâ€™ perspective; Software maintenance tools. Stack Overî€ow, issue reproducibility, code segments, reproducibility challenges, user study ACM Reference Format: Saikat Mondal and Banani Roy. 2021. Reproducibility Challenges and Their Impacts on Technical Q&A Websites: The Practitionersâ€™ Perspectives. In Proceedings of 15th Innovations in Software Engineering Conference (ISEC 2022). ACM, New York, NY, USA, 11 pages. https://doi.org/XX.XXXX/XXXXXXX. XXXXXXX Stack Overî€ow (SO) has emerged as one of the largest and most popular technical question and answer (Q&A) sites. SO is continuously contributing to the body of knowledge in software development [33,36,40]. Millions of software developers interact through SO every month to solve their programming-related problems [11]. About seven thousand questions are submitted in SO every day. Among them, a large number of questions discuss code level problems (e.g., errors, unexpected behaviour) [36]. Such questions often include problematic code segments with the programming issue descriptions. SO users generally prefer to reproduce the issues reported at the questions using the code segments and then submit their solutions [18]. Reproducibility means a complete agreement between the reported and investigated issues [18,41]. Unfortunately, such programming issues could not always be reproduced by other users due to several unmet challenges (e.g., too short code segment) of the code segments [18,19,32]. This phenomenon prevents the questions from getting prompt and appropriate solutions. Mondal et al. [18] show that a question whose code segment could reproduce the reported issue has more than three times higher chance of receiving an acceptable answer (i.e., working solution) than the question whose code segment could not reproduce the issue. Moreover, the median time delay of receiving an accepted answer is double, and the average number of answers is signiî€›cantly less for the questions with irreproducible issues than those of the questions with reproducible issues. â€¢Class/Interface/Method not foundâ€¢Important part of code missingâ€¢External library not foundâ€¢Identiî€›er/Object type not foundâ€¢Too short code snippetâ€¢Database/File/UI dependency â€¢ Outdated code A couple of existing studies [13,43] investigate the challenges of usability and executability of the code segments posted on Q&A sites. For example, a study conducted by Yang et al. [43] analyzes the parsability and compilability of code segments extracted from the accepted answers of SO. However, their analysis was fully automatic that only exposes parse and compile errors (e.g., syntax errors, incompatible types). Horton and Parnin [13] examine the executability of the Python code segments found on the GitHub Gist system. They identify several î€aws (e.g., syntax errors, indentation errors) that prevent the executability of such code segments. However, a simple execution success does not always guarantee the reproducibility of issues [18]. Several studies investigate the quality of SO code segments by measuring their readability [4â€“6,26,30,37], and understandability [17,29,38]. Unfortunately, their capability of reproducing the issues reported at SO questions was not investigated. Thus, their approach also fails to address reproducibility challenges. Mondal et al. [18] î€›rst investigate the reproducibility of issues reported at SO questions related to Java programming language. Their investigation produces a catalog of challenges (see Table 1) that might prevent reproducibility. However, the catalog was not validated by the practitioners (i.e., developers). Thus, it is unknown to us how the developers perceive the reproducibility challenges. Understanding the developersâ€™ perspective is important to introduce eî€œcient tool support to promote reproducibility. This study addresses developersâ€™ perspectives on the reproducibility challenges and their impact by surveying 53 software developers. In particular, we attempt to â€“ (1) validate the challenge catalog (see Table 1) by asking developersâ€™ agreement to those challenges, (2) understand the potential impact of each of those challenges to answer SO questions, (3) see how developers address reproducibility challenges, and (4) î€›nd developersâ€™ recommendations on designing eî€œcient tool support to help question submitters improve their code segments to promote reproducibility. Replication Packagethat contains the survey questionnaire and all the responses is shared in our online appendix [2]. In this section, we start by introducing the issue reproducibility and the challenges that prevent reproducibility. We then proceed to describe related studies. Figure 1: An example [25] question of SO that discusses a programming issue. Reproducibility is often closely related to repeatability and replicability [1]. However, its deî€›nition diî€ers across disciplines. In this study, reproducibility means a complete agreement between the reported and the investigated issues [18,41]. Consider the example question in Fig. 1, where a user was trying to take a personâ€™s full name (e.g., John Doe) as input by invoking thenext()method of JavaScannerclass. However, when the user was printing the name, only the î€›rst name (e.g., John) was being printed, and the last name (e.g., Doe) was getting lost. In particular, the user was not getting the part of the name after space. She included the deî€›nition of the methodprocessName()with the question description, where she was taking and printing the name. If other users also î€›nd the î€›rst part of the name and lose the part after space by invoking processName()method, that means the issue is reproducible. On the contrary, when others fail to regenerate the reported issue using the given method, it suggests the issue is not reproducible. Mondal et al. [18] analyze 400 SO questions related to the Java programming language and attempt to reproduce their issues. However, they could not reproduce about 22% issues due to several unmet challenges of the code segments. Table 1 shows those reproducibility challenges. In this section, we discuss the challenges that prevent reproducibility as follows. Figure 2: An example [21] question of SO whose issue could not be reproduced due to mainly two unmet challenges â€“ (i) class/interface/method not found and (ii) important part of code missing. (1)Class/Interface/Method not found.Developers often submit only the code segments of interest with questions, which are neither complete nor compilable. Such code segments also invoke methods from classes. However, the code segments often miss the deî€›nition of the methods. Let us consider the example question in Fig. 2, where the developer attempts to reset all the variables by the default values. However, the code was not working as expected, and some of the variables were retaining their old values. Unfortunately, this issue could not be reproduced due to several unmet challenges. Especially, deî€›nitions of theGameclass and playGame()method are missed but essential to reproduce the issue. Thus, one developer commented while attempting to answer the question â€“ â€œCan you post more co de? The Game class? The class that contains the restart() method?â€. (2)Important part of code missing.Code segments included with questions often miss such statements (i.e., part of code) without which issues could not be reproduced. Here, â€œan important partâ€ means such part of code that could never be guessed properly. For instance, we could add the deî€›nition ofGameclass and playGame()method in the code example in Fig. 2 and make the code executable. However, we cannot reproduce the issue. We need the part of the code inside the class and the method by which the developer attempted to reset the variables. Figure 3: An example [24] question of SO whose issue could not be reproduced due to mainly three unmet challenges â€“ (i) external library not found, (ii) identiî€›er/object type not found and (iii) too short code snippet. (3)External library not found.Resolving external library dependencies is one of the major challenges to reproduce the issues from the submitted code segments [18]. Java has thousands of external (i.e., third-party) libraries with millions of classes and methods. However, diî€erent libraries contain classes and methods with the same name. Thus, if the code segments miss import statements of the external libraries or question descriptions do not have hints that point to the appropriate libraries, developers face diî€œculty adding the appropriate libraries. As a result, they could not compile/execute the code and fail to reproduce the question issues. Consider the code example shown in Fig. 3, where the import statement for the library that contains classXMLTypewas missing. Thus, the inclusion of the appropriate external library is a major challenge here to reproduce the question issue. (4)Identiî€›er/Object type not found.Code segments use identiî€›ers/objects without declaring them. In some cases, developers infer the type of identiî€›ers/objects by looking at the assigned values or method invocations. Otherwise, they î€›nd diî€œculties in inferring their types that might prevent the reproducibility of issues. Consider the code example in Fig. 3, where the types ofpoxmlandrs are unknown and hard to guess from the submitted code. (5)Too short code snippet.Developers often submit incomplete code segments with their questions. However, code segments are too short to reproduce the question issues in many cases. For example, Fig. 3 shows a question where the question submitter included only one line of code with the issue description. Thus, it is genuinely challenging to guess the missing statements and make the code compilable/executable to reproduce the issue. The challenge â€œtoo short code segmentâ€ could overlap with an â€œimportant part of code missingâ€. However, an important part of code could be missed even when the developers submit a long code. (6)Database/File/UI dependency.Several code segments could not reproduce the issues due to their complex interactions with databases, external î€›les, and UI elements. Consider the example question in Fig. 4, where the developer attempted to create and load an XML doc. Unfortunately, the code got a null exception when the developer attempted to access the doc more than once. Such an issue could not be reproduced due to the external î€›le dependency. However, the doc could not be created because the question lacks important detail (e.g., î€›le path, content) associated with the î€›le. (7)Outdated code.A few code segments contain outdated code (e.g., deprecated class/API) that could prevent issue reproducibility. Figure 4: An example [23] question of SO whose issue could not be reproduced due to mainly two unmet challenges â€“ (i) database/î€›le/UI dependency and (ii) class/interface/method not found. Figure 5: An example [22] question of SO whose issue could not be reproduced due to mainly outdated code challenge. A few studies investigate the outdated code of Stack Overî€ow [27,44]. However, in this study, we consider a code is outdated when the code contains such classes/APIs that are not compatible with JDK-1.8. It is hard to î€›nd an equivalent or alternative class/API for the outdated class/API in some cases. Consider the code segment in Fig. 5, where the classesAudioStreamandAudioPlayer are not being used in JDK-1.8. Thus, the developers face diî€œculties in î€›nding their alternatives while reproducing the question issue. Several studies investigate usability (e.g., parsability, compilability) [43], executability [13] and reproducibility [9,13,18â€“20,28,32,41, 43] challenges of the code segments posted at crowd-sourced developer forums (e.g., GitHub, SO). However, our study î€›rst investigates the practitionersâ€™ perspective on the reproducibility challenges of the programming issues reported at SO questions, their impact to answer questions, and interactive tool design requirements to promote reproducibility. Yang et al. [43] analyze the usability of about 914K Java code segments extracted from the accepted answers of SO. They expose several challenges of their parsability (e.g., syntax error) and compilability (e.g., incompatible types). The authors employ automated tools such as Eclipse JDT and ASTParser to parse and compile the code segments and report the challenges that prevent them from parsing and compiling. However, they do not analyze the challenges of reproducibility. Horton and Parnin [13] investigate the executability of Python code found on the GitHub Gist system. They report the types of execution failures encountered while running Python gists such as import error, syntax error, indentation error. However, a code segmentâ€™s execution success does not always guarantee the reproducibility of an issue reported at the SO question. Reproducibility may require testing and debugging that warrant manual analysis, which was not done by Horton and Parnin [13]. Mondal et al. [18] go beyond code execution and manually investigate the reproducibility of issues reported at 400 questions related to Java programming language using the code segments included with questions. They produce a catalog of challenges that prevent reproducibility of SO question issues. However, it is important to listen to the practitioners whether they agree to the challenges and understand the potential impact of those challenges to answer a question that was not done by any earlier studies. Several researchers investigate the reproducibility challenges of software bugs and security vulnerabilities [8,20,28]. Joorabchi et al. [8] analyze 1,643 irreproducible bug reports and investigate the causes of their irreproducibility. They reveal six root causes, such as environmental diî€erences, insuî€œcient information that prevent bug-reportsâ€™ reproducibility. Rahman et al. [28] conduct a study to understand the irreproducibility of software bugs. They investigate 576 irreproducible bug reports from two popular software systems (e.g., Firefox, Eclipse) and identify 11 challenges (e.g., bug duplication, missing information, ambiguous speciî€›cations) that might prevent bug reproducibility. The authors then survey 13 developers to understand how the developers cope with irreproducible bugs. According to the study î€›ndings, developers either close these bugs or solicit further information. Mu et al. [20] analyze 368 security vulnerabilities to quantify their reproducibility. Their study suggests that individual vulnerability reports are insuî€œcient to reproduce the reported vulnerabilities due to missing information. Besides, many vulnerability reports do not include details of software installation options and conî€›gurations, or the aî€ected operating system (OS) that could hinder reproducibility. Then they survey hackers, researchers, and engineers who have domain expertise in software security. Survey î€›ndings suggest that apart from internet-scale crowd-sourcing and some interesting heuristics, manual eî€orts (e.g., debugging) based on experience are the sole way to retrieve missing information from reports. Our study relates to the above studies in terms of research methodologies and problem aspects. However, our research context diî€ers from theirs since we attempt to understand the practitionersâ€™ perspectives on SO questionsâ€™ reproducibility issues. Tahaei et al. [33] analyze 1,733 privacy-related questions of SO to understand the challenges and confusion that developers face while dealing with privacy-related topics. Ebert et al. [7] investigate the reasons (e.g., missing rationale) and impacts (e.g., merge decision is delayed) of confusion in code reviews. Their study suggests how developers cope with confusion during code reviews. For example, developers attempt to deal with confusion by requesting information, improving the familiarity with existing code, and discussing oî€-line the code review tool. They survey developers to obtain actionable insights for both researchers and tool builders. While our work overlaps with them in terms of methodology, however, our research goals are diî€erent. They attempt to î€›nd reasons and impacts of confusion in code reviews. However, we survey 53 developers to understand (1) the impacts of reproducibility of SO questions issues, (2) how developers plan to modify the code segments to make them capable of reproducing the issues, and (3) the insights for introducing interactive tool supports. Ford et al. [10] deploy a month-long, just-in-time mentorship program to SO to guide the novices with formative feedback of their questions. Such mentorship reduces the negative experience caused by delays in getting answers or adverse feedback. However, human mentorship is costly. Thus, it is hard to sustain such mentorship. Horton and Parnin [14] present DockerizeMe, a system for inferring the dependencies required to execute a Python code snippet without import errors. Terragni et al. [34] propose a technique CSnippEx to automatically convert Java code segments into compilable Java source code î€›les. However, we plan to determine the design requirement to introduce an intelligent and interactive tool based on the developersâ€™ recommendations. Such a tool could analyze SO code segments included with questions related to Java programming problems and suggest users improve the code segments to promote reproducibility. In this study, we aim to understand developersâ€™ perspectives on reproducibility challenges and estimate their impacts to answer SO questions. We also plan to introduce tool support to promote reproducibility by seeking developersâ€™ suggestions on the design of the tool. Following this aim, we guide our study with î€›ve research questions and make î€›ve contributions in this paper as follows. â€¢ (RQ) What do developers consider to be the challenges behind the irreproducibility of issues reported at Stack Overî€ow questions?Mondal et al. [18] produce a catalog of challenges (see Table 1) that might prevent reproducibility. However, developersâ€™ feedback on such empirical î€›ndings is essential to increase conî€›dence in the î€›ndings [28]. To answer RQ, we present four example questions of SO with their issue descriptions and code segments. Participants were asked to reproduce the reported issues using the code segments. Upon failure, we ask for their agreement to the given reproducibility challenges. The given challenges were validated by 53 software developers with an agreement level between 80% and 94%. â€¢ (RQ) What are the perceived impacts of the reproducibility challenges to answer a Stack Overî€ow question?Understanding the impact of each of the challenges is important to determine which challenges must be resolved by question submitters to receive appropriate solutions to their questions. To answer RQ, we present each of the challenges with î€›ve options â€“ (1) not a problem, (2) moderate, (3) severe, (4) blocker, and (5) no opinion. We then ask participants to select one of the î€›ve options. Participants assessed â€œan important part of code missingâ€ as mostly a blocker, and â€œoutdated codeâ€ as mostly not a problem. â€¢ (RQ) How do developers prioritize to address the reproducibility challenges?Prioritization of challenges is important to determine which challenges need to be î€›xed î€›rst within a strict budget (e.g., time constraint). We thus ask participants to specify three reproducibility challenges they would like to prioritize above others. We î€›nd the following challenges in the top three list â€“ (1) an important part of code missing, (2) class/interface/method not found, and (3) too short code snippet. â€¢ (RQ) How do developers plan to modify the code segments to make them capable of reproducing the question issues?Developersâ€™ code editing plan to reproduce the question issues oî€ers more insights into how an intelligent tool could suggest question submitters improve their code segments. We thus attempt to observe participantsâ€™ editing actions to address the reproducibility challenges. We see that participants perform several editing actions such as adding demo classes and methods, declaring and initializing variables, invoking methods, including external and native libraries. In particular, the participants î€›rst attempt to make the code segments compilable/executable to reproduce the issues. On the contrary, several participants do not modify the code segments. They consider the code segments insuî€œcient to make them compilable/executable, or modiî€›cations based on assumptions (without appropriate hints) could deviate the code segments much from the original code. â€¢ (RQ) What are the interactive tool design requirements to address the reproducibility challenges?Questions with reproducible issues have a signiî€›cantly higher chance of receiving acceptable answers with minimum time delay [18]. However, the current question submission system of SO is not capable of addressing reproducibility issues. To introduce tool support, we thus ask participantsâ€™ recommendations on tool design to promote reproducibility. According to them, intelligent tools that (1) analyze the code segments statically to î€›nd the reproducibility challenges and (2) suggest question submitters improving the code examples could help them receiving appropriate solutions to their questions. In the following, we describe the study methodology. First, we survey to understand â€œwhat developers sayâ€ (RQ,RQ, andRQ) about reproducibility challenges and their potential impact to answer questions. Then we see the code modiî€›cation plan to understand â€œwhat developers doâ€ (RQ) to reproduce the issues. Next, we î€›nd â€œwhat developers suggestâ€ (RQ) to receive recommendations that inform the design needs of the interactive tool supports to assist reproducibility of question issues. Finally, we analyze the survey î€›ndings and report them. 3.2.1 Survey. We conduct an online survey aiming at validating and extending the catalog of reproducibility challenges (see Table 1) identiî€›ed by Mondal et al. [18]. Furthermore, we ask the participants to give their viewpoints on the impacts of those challenges and their potential î€›xes. We primarily follow Kitchenham and Pî€eegerâ€™s guidelines for personal opinion surveys [16]. However, we also consider the guidance and ethical issues from the established best practices [12, 31]. Figure 6: Information of the survey participants (SD: Software Survey Design.Our survey includes diî€erent types of questions (e.g., multiple-choice and free-text answers). Before asking questions, we explain the purpose of the survey and our research goals to the participants. We ensure the survey participants that the information they provide must be treated conî€›dentially. We î€›rst piloted the preliminary survey with a small set of practitioners (i.e., 3 participants). We collect their feedback on (1) whether the length of the survey was appropriate and (2) the clarity and understandability of the terms. We then perform minor modiî€›cations to the survey draft based on the received feedback and produce a î€›nal version. We inform the estimated time (i.e., 45â€“50 minutes) required to complete the survey to the participants based on the pilot survey. We exclude the three responses from the pilot survey from the presented results in this paper. Our survey comprises î€›ve parts as follows. (i) Consent and Prerequisite. In this part, we ask the participants to conî€›rm whether they consent to participate in this survey and agree to process their data. We also ask questions to conî€›rm whether they answer SO questions and have experience in Object-Oriented Programming (OOP), especially Java. Otherwise, we did not allow them to participate in our survey. (ii) Participants Information. In this part, we attempt to collect some information about the participants, such as years of experience in OOP and professional software development, their current profession, SO account age, and their question answering experience in SO. (iii) Agreement to the Reproducibility Challenges. In this part, we present four SO questions from the manually analyzed dataset by Mondal et al. [18]. We ask participants to reproduce the question issues using the code segments. Upon failure, we ask their agreement/disagreement with the challenges. Otherwise, we ask for their code editing actions required to perform to reproduce the issues. In both cases, there are options to report additional challenges or editing actions. (iv) Impact of the Reproducibility Challenges. Here, we ask questions to understand the potential impact and severity of each of the reproducibility challenges. (v) Tool Support Needs. Finally, we seek the participantsâ€™ opinions on the needs and design requirements of an intelligent tool to promote reproducibility. We also oî€er a few tool support options and employing a 5-point Likert Scale [15,39] to estimate the participantsâ€™ responses. Recruitment of Survey Participants.We recruit participants in the following two ways. (i) Snowball Approach: We recruit a list of participants from a set of companies worldwide based on personal contacts. We then adopt a snowballing method [3] to encourage the participants to disseminate our survey to some of their colleagues with similar experiences and willing to participate in our survey. In this process, we conî€›rm 48 participants. However, we allow 44 of them to complete our survey. The remaining four did not satisfy our constraints perfectly. (ii) Open Circular: To î€›nd potential participants, we post a description of this study and our research goals in the specialized Facebook groups where professional software developers discuss their programming problems and share software development resources. We also use LinkedIn as a research tool to reach potential participants because it is one of the largest professional social networks in the world. We get 15 participants from this open circular who are willing to participate and satisfy our constraints. Finally, we got nine valid responses from them. Fig. 6 summarizes the participantsâ€™ Java experience and professions. Out of the 53 participants, 34 (about 64%) had Java experience î€›ve years or less (see Fig. 6a). Developers with comparatively less experience are likely to be more actively engaged in Stack Overî€ow. However, we recruit 19 participants who had more than î€›ve years of Java development experience. The professions of the survey participants were mainly software developers (45.3%), and academic practitioners (e.g., faculty member, student, postdoctoral researcher) (37.7%) (see 6b). However, there were four technical leads and î€›ve research engineers. Other details of the participants (e.g., professional software development experience, SO account age, and question answering experience) can be found in our online appendix [2]. 3.2.2 Survey Data Analysis. In total, we received 53 valid survey responses. We then analyze the responses with appropriate tools and techniques based on the question types. For multiple-choice questions, we report the percentage of each option selected. To identify the agreement level of each statement, we analyze the Likert-scale ratings. We use Borda count [42] to rank the reproducibility challenges. Furthermore, we extract comments that our survey participants give on the impacts of the reproducibility challenges and tool design requirements. In this section, we discuss the survey results and answer our research questions. We attempt to î€›nd developersâ€™ agreement on the challenges (see Table 1) that prevent reproducibility. Such agreement on empirical î€›ndings increases conî€›dence in the î€›ndings [28]. Approach.We present four SO questions (see Fig. 2, 3, 4 & 5) from the published dataset by Mondal et al. [18]. We î€›rst ask the participants to reproduce the question issues using the code segments included with the questions. We then ask either they can reproduce the issues using the code segments or not. Upon selection, â€œI cannot reproduceâ€, we present a list of reproducibility challenges that could prevent reproducibility of the issues. We then ask their agreement/disagreement on those reproducibility challenges. Besides, we oî€er an option to mention if they î€›nd additional challenges. On the contrary, upon selection, â€œI can reproduceâ€, participants are asked to mention their agreement/disagreement with a list of editing actions. However, we oî€er an option to mention if they perform additional actions. Our primary focus is to validate the challenge catalog. Thus, we select four questions whose issues could not be reproduced by Mondal et al. [18] and cover all the seven challenges (e.g., Table 1). Here, we ask the following three questions and analyze the participantsâ€™ agreement in contrast to the reproducibility status (i.e., reproducible/irreproducible) and challenges reported by Mondal et al. [18]. Q)Do the participants reproduce the reported issues? (I can reproduce the issue/I cannot reproduce the issue ) Q)Do the participants agree with the reproducibility challenges? (Yes/No) Q)Do the participants î€›nd additional challenges reproducing the issues? (Text) Findings.Table 2 shows the summary of the î€›ndings. For question 1 (Fig. 2), 94.3% of the participants could not reproduce the issue. Only 5.7% of them report that they could reproduce the issue. We found two reproducibility challenges for this code example that prevent reproducibility. They are - Class/Interface/Method not found and Important part of co de missing. We see that 92% of participants also encounter those two challenges. However, only 8% of them disagree with those challenges. We see a similar agreement/disagreement to the reproducibility status and their associated challenges for the questions 2 (Fig. 3) & 3 (Fig. 4). However, in the case of question 4 (Fig. 5), about 26% of developers can reproduce the reported issue, where the challenge was outdated code. The code segment contains classes (e.g.,AudioStream) that are not being used in JDK-1.8. However, some developers could use previous JDK versions to analyze the code or use alternative classes to reproduce the issue. We ask participants to submit the modiî€›ed code segments and î€›nd that several participants compose new code to reproduce the issue. In addition to the given challenges, participants report a few additional challenges they encountered as follows. (1)Error log/stack trace missing.Error log or stack trace contains meaningful insights about program failures. Some issues could not be reproduced without such error reports. (2)System dependency or environment setup is missing. Several programming issues are involved with particular Operating Systems (OS) (e.g., Windows), IDE (e.g., Eclipse), and software versions (e.g., Python 3). Some issues could not be reproduced because question submitters do not specify them. (3)Sample input-output missing.Several issues demand sample input-output (i.e., test cases) while reproducing them. We then analyze the agreement according to the participantsâ€™ Java experience and profession. Table 3 summarizes the agreement. Question No.Reproducibility StatusReproducibility Challenges We î€›rst measure the agreement with reproducibility status and challenges for each of the four example questions and then compute their average. We see that participants with lower experience (e.g., î€›ve years or less) agree 8% more on average with the reproducibility status than those having higher experience (e.g., more than î€›ve years). Conversely, participants with higher experience agree 7% more with the given reproducibility challenges. Participants with higher experience might have the skill to apply diî€erent potential approaches to reproduce the issues. However, low-experienced participants are more enthusiastic about resolving the challenges (e.g., î€›nding external libraries, î€›xing database dependencies). Analysis by profession shows that technical leads and research engineers agree comparatively less with reproducibility status and challenges. They might not be actively involved in programming and have a hectic task schedule. Thus, they could guess the reproducibility status and the potential challenges. However, our investigation î€›nds that they mainly disagree with the reproducibility status and challenge of question 4 (see Fig. 5), where the challenge was â€œoutdated codeâ€. Summary.About 88% of participants (on average) fail to reproduce the issues using the code segments, and 89% of them encounter the given challenges. Only about 11% of participants (on average) claim that they could reproduce the issues, and thus they do not agree to the challenges. According to the agreement/disagreement analysis, at least 77% more participants agree to the reproducibility status and challenges on average. Furthermore, our analysis by experience and profession also shows that the lowest agreement level with reproducibility status and challenges is 70% (on average) which is acceptable. Such agreements support and validate the reproducibility status and challenge catalog (Table 1). We see the practitionersâ€™ agreement to the reproducibility challenge catalog in the previous section (Section 4.1). However, it is important to understand the impact of each challenge to answer a question. Such understanding helps us to determine which challenges need to be addressed early. Approach.To î€›nd the potential impact of the reproducibility challenges, we ask the participants to mark each of the challenges with one of the î€›ve categories: (1) not a problem, (2) moderate, (3) severe, (4) blocker, and (5) no opinion. Here, moderate means irritating, but able to answer appropriately. Severe means wasted much time to reproduce the issue, but able to answer, and blocker means could not reproduce the issue and thus could not answer the question. In particular, we ask a question to the participants as follows. What are the impacts of the reproducibility challenges to answer a question? (not a problem/moderate/severe/blocker/no opinion) Findings.Fig. 7 shows the summary of how the participants assess the impact of each of the reproducibility challenges. We see that majority of the participants (about 36%) perceive the challenge â€œClass/Interface/Method not foundâ€ as moderate. About 30% of them consider it a blocker. However, only 7.5% of participants consider this challenge not a problem, and the remaining 1.9% of them did not give any opinion. â€œAn important part of code missingâ€ is mostly (about 59% of participants) considered a blocker. We î€›nd that â€œToo short code snippetâ€ is also perceived as a blocker by the majority of the participants (about 47%). About 43% of participants perceive the challenge â€œIdentiî€›er/Object type not foundâ€ as severe, whereas 34% of them perceive it as moderate. We then attempt to î€›nd the top challenges in each of the î€›ve categories. As Fig. 7 shows, â€œClass/Interface/Method not foundâ€ is the top in the moderate challenge category. The challenge â€œIdentiî€›er/Object type not foundâ€ is the top in the severe category, whereas â€œImportant part of code missingâ€ is assessed as the top in blocker category. However, among the reproducibility challenges, â€œOutdated codeâ€ is considered â€œnot a problemâ€ by most participants, or many of them did not give any opinion. To reproduce the issue, developers could migrate the outdated code or use alternative classes/APIs in place of obsolete classes/APIs. Thus, this challenge could be estimated mostly as not a problem among the seven challenges. In addition to that, we attempt to get more insights into participantsâ€™ choices of impact. We thus ask their justiî€›cations behind the choices. Although it was optional, we receive a few of their comments (see Table 4). For example, one developer states that without the Class/Method deî€›nition, it is nearly impossible to reproduce the exact issues. Summary.Most of the developers could not reproduce the question issues and thus could not answer if code segments miss important statements. Other challenges (e.g., Class/Interface/Method and Identiî€›er/Object type not found) could irritate developers and kill their valuable time. Such factors could also prevent or delay appropriate solutions to the questions. However, developers do not encounter more diî€œculties in dealing with outdated code. It is not always possible to î€›x all the reproducibility challenges due to a strict budget (e.g., limited time). Thus, it is important to prioritize more severe challenges than the others to î€›x them within a strict budget. Approach.We ask the following question to participants to î€›nd the rank of the reproducibility challenges. Which three challenges the participants would like to prioritize above others? (î€›rst choice/second choice/third choice) We then use the Borda count as proposed by Yamashita and Moonen [42] to analyze and rank the challenges according to their severity. Borda count is a rank-order aggregation technique for ğ‘›candidates. The î€›rst ranked candidates receivedğ‘›points, the secondğ‘› âˆ’1 points, and so on. Since we ask three options to choose, we assign the î€›rst option to 3 points, the second to 2 points, and the third to 1 point. Identiî€›er/Object type not found13 Database/File/UI dependency16 Class/Interface/Method not found87 Important part of code missing101 Figure 8: The rank of reproducibility challenges according to their score. Findings.Fig. 8 shows the challenges with their points. We see that that â€œimportant part of code missingâ€ gets the highest (i.e., 101) and â€œOutdated codeâ€ get the lowest (i.e., 5) point. However, we can now prioritize (e.g., rank) the top three challenges according to their severity â€“ (1) important part of code missing, (2) class/interface/method not found, and (3) too short code snippet. This î€›nding of severity analysis is consistent with impact analysis (section 4.2). Summary.The î€›ndings from severity analysis suggest that question submitters should include the part of code (i.e., statements) that could never be guessed, deî€›ne necessary classes/methods. Moreover, they should not submit too short code segments (e.g., 1/2 lines of code) to receive fast and appropriate solutions. We could get more insights on how to address the reproducibility challenges by observing the developersâ€™ code modiî€›cation plan. Approach.We attempt to see the participantsâ€™ code editing actions to reproduce the question issues. In particular, we want to see how participants mitigate the reproducibility challenges of the code segments. We thus ask participants to submit their modiî€›ed code segments and manually level the editing actions. We shared the modiî€›ed code with the level of actions in our online appendix [2]. Findings.Table 5 shows the summary of the code editing actions to make the code segments capable of reproducing the issues. The participants add demo classes and methods, include import statements for the native and external libraries, create objects, declare identiî€›ers and initialize them, and migrate the outdated code to mitigate the reproducibility challenges. In particular, participants î€›rst attempt to make the code segments compilable/executable by â€œI really think not î€›nding â€˜Class/Interface/Methodâ€™ is a blocker problem, because how can I understand a code without their metho d explained and reproduce. Same case happens in the case of â€˜Too short code snippetâ€™ and â€™Important part of code missingâ€™.â€ â€œWithout Class/Method deî€›nition, itâ€™s nearly impossible to identify the exact error. Besides, it is helpful, if individuals give some sample input and output examples. Dependency on Database: this is kind of blocker to run the sample code. â€ â€œIf the given code part is too small, solution can not be made by depending only few line of code.â€ â€œAs I already answered most of the time we are irritated to help with question without proper playground. ( class not found, Important code missing... etc.).â€ 01 (see Fig. 2)(1) Class/Interface/Method not found,Addition of demo classes and methods (62%), ob- 02 (see Fig. 3)(1) External library not found, (2) Identi-Addition of demo classes and methods (26.5%), in- 03 (see Fig. 4)(1) Class/Interface/Method not found,Addition of demo classes and methods (28%), inresolving the external dependencies. For example, when methods are invoked without deî€›ning them, participants attempt to add their deî€›nitions. Participants modify the codes based on the given code segments, hints from question descriptions or guessing. However, we see that 36% â€“ 68% of participants did not modify the code segments. Intuitively, they assume that the code segment is insufî€›cient to reproduce the issues, and thus they did not attempt to modify the code segments. For example, one developer commented, â€œI did not try to reproduce it, because it needs much guessing and it may diî€er much from the original codeâ€. Summary.Developersâ€™ î€›rst attempt to make the code segments compilable/executable by resolving all possible dependencies. However, developers do not modify the code segments based on guessing when the code segments are insuî€œcient. Questions whose issues could be reproduced have at least three times higher chance to receive acceptable answers than the questions with irreproducible issues. However, the existing question submission system of SO is not intelligent enough to estimate reproducibility. It does not interact with the question submitters even if the code segments fail to reproduce the question issues. To enhance the possibility of receiving an appropriate answer and save developersâ€™ valuable time, we plan to introduce interactive tool support (e.g., browser/IDE plugin) to promote reproducibility. Approach.We seek participantsâ€™ recommendations on tool design requirements. Besides, we oî€er a few tool support options (Table 7) and employ a 5-point Likert scale (i.e., 1â€“5) to estimate the participantsâ€™ consent with the tool options. In particular, we ask two questions as follows. Q)What kind of tool support do the participants need to assist with the reproducibility challenges? (Text) Q)How do the participants agree with our tool support options? (see options from Table 7) Findings.Table 6 shows a few valuable recommendations from participants (complete recommendation list can be found in our online appendix [2]). For example, one participant recommended, â€œA tool that can sense my written code snippet and see what important part might be missing thereâ€. Besides, Table 7 shows our tool options. We oî€er six options, such as a tool that warns users about the severe challenges or challenges that could block reproducibility. According to the participantsâ€™ assessment, three of them are considered very inî€uential (scoreâ©¾4.21), and the remaining three are considered inî€uential (3.41 â©½ score â©½ 4.20). Summary.By analyzing the developersâ€™ recommendations and assessing the given tool options, we could summarize the primary tool design requirements as follows. â€¢Interact with question submitters and suggest including the part of code that could never be guessed and mandatory to reproduce the question issues. â€œThe tool that help me to guess the missing code, library version, missing input data depending on the issue context. It will be great if the tool generates missing information for me depending on the question/issue.â€ â€œA software that can ensure the missing cases and force the user to complete them, Auto code generator.â€ â€œA tool that helps to guess the missing important code depending on the question context.â€ â€œA tool that can sense my written code snippet and see what important part might be missing there.â€ â€œIt will provide suggestions to reproduce after pasting the code in IDE. â€ A tool that warns users about reproducibility challenges that are severe or may block the reproducibility â€¢Suggest adding the deî€›nition of necessary classes/methods and declaration of identiî€›ers/objects. The absence of such deî€›nitions/declarations could prevent reproducibility. â€¢Warn question submitters not to include a too short code segment. Developers could not guess the actual problem from such a code segment. Sometimes developers guess the problem. However, such guessing often goes wrong, and thus the submitted answer may not solve the actual problem. â€¢Recommend to include the external libraries when required. It is a time-consuming task to î€›nd the external libraries without import statements or any hints about the libraries in the question description. Threats to internal validity relate to experimental errors and biases [35]. Our key reproducibility challenges were derived from a qualitative study by Mondal et al. [18], which could be a source of subjective bias. However, the challenges were validated by 53 developers with an agreement level of about 90% on average. Moreover, the diî€erence between agreement and disagreement on reproducibility challenges is quite large (about 77%) and statistically signiî€›cant. Threats to external validity relate to the generalizability of our î€›ndings [35]. We investigate the practitionersâ€™ perspective on reproducibility challenges, their impacts and tool design requirements. However, the challenges were derived by analyzing questions related to Java programming problems. Thus, the impacts and tool design requirements can apply to the statically-typed, compiled programming languages such as C++ and C#. Nonetheless, replication of our study using diî€erent languages (e.g., dynamically-typed languages like Python) may prove fruitful. Our survey participants range from novice to experienced (see Fig. 6a) and constitute mainly software developers but also other related professions (see Fig. 6b). Such diversity in the survey participants oî€ers validity and applicability to the survey î€›ndings. However, any individual bias in the survey responses should be mitigated via a large sample of 53 users. Developers submit thousands of questions to SO daily to resolve their code-level problems. They include an example code segment to support the problem descriptions. Users of SO prefer to reproduce the issues using the code segments and then submit their answers. However, such code segments could not always reproduce the issues due to several unmet challenges that prevent the questions from getting prompt and acceptable solutions. A previous study produces a catalog of challenges (Table 1) that might prevent reproducibility of question issues. However, they are not validated by developers. We survey 53 developers (users of SO) to understand their perspectives on those challenges, impacts of those challenges, how developers address those challenges, and tool design requirements that could mitigate those challenges. Our î€›ndings are fourfold. First, about 90% of developers agree to the reproducibility challenge catalog. Second, missing part of code that could never be guessed but required to reproduce question issue mostly prevent questions from receiving answers. Third, developers perform several editing actions (e.g., addition of demo classes/methods) to make the code segments reproduce the issues. However, they do not try to reproduce the issues and submit answers if the code segments are entirely insuî€œcient to guess any actions. Finally, an intelligent tool that identiî€›es the reproducibility challenges according to their severity could help question submitters improve their code segments and reduce the developersâ€™ workload who attempt to answer questions. Acknowledgement:This research is supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) and by two Canada First Research Excellence Fund (CFREF) grants coordinated by the Global Institute for Food Security (GIFS) and the Global Institute for Water Security (GIWS).