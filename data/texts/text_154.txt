Learning user representations based on historical behaviors lies at the core of modern recommender systems. Recent advances in sequential recommenders have convincingly demonstrated high capability in extracting eî€ective user representations from the given behavior sequences. Despite signiî€›cant progress, we argue that solely modeling the observational behaviors sequences may end up with a brittle and unstable system due to the noisy and sparse nature of user interactions logged. In this paper, we propose to learn accurate and robust user representations, which are required to be less sensitive to (attack on) noisy behaviors and trust more on the indispensable ones, by modeling counterfactual data distribution. Speciî€›cally, given an observed behavior sequence, the proposed CauseRec framework identiî€›es dispensable and indispensable concepts at both the î€›ne-grained item level and the abstract interest level. CauseRec conditionally samples user concept sequences from the counterfactual data distributions by replacing dispensable and indispensable concepts within the original concept sequence. With user representations obtained from the synthesized user sequences, CauseRec performs contrastive user representation learning by contrasting the counterfactual with the observational. We conduct extensive experiments on real-world public recommendation benchmarks and justify the eî€ectiveness of CauseRec with multiaspects model analysis. The results demonstrate that the proposed CauseRec outperforms state-of-the-art sequential recommenders by learning accurate and robust user representations . â€¢ Information systems â†’ Recommender systems. Sequential Recommendation, User Modeling, Contrastive Learning, Counterfactual Representation ACM Reference Format: Shengyu Zhang, Dong Yao, Zhou Zhao, Tat-seng Chua, Fei Wu. 2021. CauseRec: Counterfactual User Sequence Synthesis for Sequential Recommendation. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR â€™21), July 11â€“15, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3404835.3462908 Due to the overwhelming data that people are facing on the Internet, personalized recommendation has become vital for retrieving information and discovering content. Accurately characterizing and representing users plays a vital role in a successful recommendation framework. Since usersâ€™ historical interactions are sequentially dependent and by nature time-evolving, recent advances recommendation, which captures the current and recent preference by exploiting the sequentially modeled user-item interactions. A sequential recommender aims to predict the next item a user might interact with based on the historical interactions. The challenging and open-ended nature of sequence modeling lends itself to a variety of diverse models. Traditional methods mainly exploit Markov chains [15] and factorization machines [22,49] to capture lower-order sequential dependencies. Following these works, the higher-order Markov Chain and RNN (Recurrent Neural Network) [18,21,66] are proposed to model the complex high-order sequential dependencies. More recently, MIND is proposed to transform the historical interactions into multiple interest vectors using the capsule network [51]. ComiRec [5] diî€ers from MIND by leveraging the attention mechanism and introducing a factor to control the balance of recommendation accuracy and diversity. Despite signiî€›cant progress made with these frameworks, there are some challenges demanding further explorations. A vital challenge comes from the noisy nature of implicit feedback. Due to the ubiquitous distractions that may aî€ect the usersâ€™ î€›rst impressions (such as caption bias [39], position bias [24], and sales promotions), there are inconsistencies between usersâ€™ interest and their clicking behaviors, known as the natural noise [45]. Another challenge relates to the deî€›ciency of existing methods in confronting data sparsity problem in recommender systems where users in general Figure 1: An illustration of the proposed contrastive user representation learning by modeling the counterfactual world (below), compared with most traditional approaches that solely model the observational user se quences (above). only interact with a limited number of items compared with the item gallery which can easily reach 100 million in large live systems. Therefore, solely modeling the observational behavior sequences that can be both sparse and noisy may end up with a brittle system that is less satisfactory. To this end, learningaccurateandrobust usersâ€™ user representations is essential for recommender systems. In this paper, we proposeCounterfactualUserSequence Synthesis for Sequential Recommendation, abbreviated asCauseRec. The essence of CauseRec in confronting the data sparsity problem is to model the counterfactual data distribution rather than the observational sparse data distribution where the latter can be a subset of the former one, as shown in Figure 1. We mainly aim to answer the counterfactual question, "what the user representation would be if we intervene on the observed behavior sequence?". Speciî€›cally, given the observed behavior sequence, we identify indispensable/dispensable concepts at both the î€›ne-grained item level and the abstract interest level. A concept indicates a certain aspect of user interest/preference. We perform counterfactual transformations on both the item-level and the interest-level user concept sequences. We obtain counterfactually positive user representation by modifying dispensable concepts, and counterfactually negative user representation by replacing indispensable concepts. To learn accurateandrobustuser representations, we propose to conduct contrastive learning between: 1) the observational and the counterfactual user representations; and 2) the user representations and the target items. Contrast with such out-of-distribution hard negatives potentially makes the learned representationsrobustsince they are less sensitive to dispensable/noisy concepts. Contrast with such out-of-distribution positives potentially makes the learned representationsaccuratesince they will trust more on the indispensable concepts that are better representing userâ€™s interest. We conduct in-depth experiments to validate the eî€ectiveness of the proposed CauseRec architectures on various public recommendation datasets. With a naive deep candidate generation (or matching) architecture as the baseline method, CauseRec outperforms SOTA sequential recommenders for deep candidate generation. We conduct comprehensive model analysis to uncover how diî€erent building blocks and hyper-parameters aî€ect the performance of CauseRec. Case studies further demonstrate that CauseRec can help learn accurate user representations. To summarize, this paper makes the following key contributions: â€¢We propose to model the counterfactual data distribution (besides the observational data distribution) to confront the data sparsity problem for recommendation. â€¢We devise the CauseRec framework which learns accurate and robust user representations with counterfactual transformations on both î€›ne-grained item-level and abstract interestlevel, and with various contrastive objectives. â€¢We conduct extensive experiments and show that with a naive deep candidate generation architecture as the baseline, CauseRec can outperform SOTA sequential recommenders. Sequential recommendation can be traced back to leveraging Markovchain [14,15] and factorization machines [22,49]. To capture longterm and multi-level cascading dependencies, deep learning based techniques (e.g., RNNs [10,21,47,66] and CNNs [55,75]) are incorporated into sequential modeling. DNNs are known to have enticing representation capability and have the natural strength to capture comprehensive relations [76] over diî€erent entities (e.g., items, users, interactions). Recently, there are works that explore advanced techniques, e.g., memory networks [53], attention mechanisms [56,79], and graph neural networks [9,26,31,36,81] for sequential recommendation [6,23,29,54,61,67,72]. Typically, MIND [32] adopts the dynamic routing mechanism to aggregate usersâ€™ behaviors into multiple interest vectors. ComiRec [5] differs from MIND by leveraging the attention mechanism for user representations and proposes a factor for the trade-oî€ between recommendation diversity and accuracy. Diî€erent from the above works that solely model the observational user sequences, we take a step further to model the counterfactual data distributions. By contrasting the user representations of the observation with the counterfactual, we aim to learn user encoders that can better confront out-of-distribution user sequences and learn accurate and robust user representations. A growing number of attempts have been made to exploit the complementary power of self-supervised learning (e.g., contrastive learning) and deep learning, with domains varying from computer vision [12,17,68], natural language generation [7,77], to graph embedding [46]. However, how to consolidate the merits of contrastive learning into recommendation remains largely unexplored in the literature. Recently, Sun et al.[33] adopt noise contrastive estimation [16] to transfer the knowledge from a large natural language corpus to recommendation-speciî€›c content that is sparse on longtail publishers and thus learning eî€ective word representations. CLRec [83] bridges the theoretical gap between contrastive learning objective and traditional recommendation objective, e.g., sampled softmax loss, as well as more advanced inverse propensity weighted (IPW) loss. They show that directly performing contrastive learning can help to reduce exposure bias. CP4Rec [69] and S-Rec [84] integrates Bert structure and contrastive learning objective for user pretraining, which require a î€›ne-tuning stage. Compared with these works, we design model-agnostic and non-intrusive frameworks that help any baseline model learn more eî€ective user representations in an end-to-end manner. Such representations are more accurate and robust by contrasting the original user representation with counterfactually positive samples and counterfactually negatives samples. Causality and counterfactual reasoning have attracted great attentions in various domains [11,78,80]. Previous counterfactual frameworks in recommendation focus on debiasing the learningto-rank problems. A rigorous counterfactual learning framework, i.e., PropDCG [1], is proposed to overcome the distorting eî€ect of presentation bias. The position bias and the clickbait issue are investigated in [2,64] and [62], respectively. The Inverse Propensity Score [52,70] method obtains unbiased estimation by sample re-weighting based on the likelihood of being logged. Another line of works encapsulates the uniform data into recommendation by learning imputation models [73], computing propensity [52], using knowledge distillation [13,37], and directly modeling the uniform data [4,28,38,50]. Diî€erent from these works, we focus on denoising user representation learning and considers the retrospect question, i.e., â€™what the user representation would be if we intervene on the observed behavior sequence?â€™. Technically, we propose several counterfactual transformations based on the identiî€›cation of indispensable/dispensable concepts and devise several contrasting objectives for learning accurate and robust user representations. In the view of sequential recommendation, datasets can be formulated asD = {(ğ‘¥, ğ‘¦)}, whereğ‘¥= {ğ‘¦}denotes a userâ€™s historical behaviors prior to theğ‘¡th behaviorğ‘¦and arranged in a chronological order, andğ‘‡denotes the number of behaviors for the userğ‘¢. The goal of sequential recommendation is to predict the next itemğ‘¦given the historical behaviorsğ‘¥, which can be formulated as modeling the probability of all possible items: We will drop the sub-scripts occasionally and write(ğ‘¥,ğ‘¦)in place of(ğ‘¥, ğ‘¦)for simplicity. LetXdenote the set of all possible click sequences, i.e.ğ‘¥ âˆˆ Xand eachğ‘¦ âˆˆ Yrepresent a clicked item, while Y is the set of all possible items. Since the number of items|Y|can easily reach 100 million, industrial recommender systems consist typically of two phases, i.e., the matching phase and the ranking phase, due to concerns on system latency. The matching (also called deep candidate generation) phase focuses on retrieving Top N candidates for each user, while the ranking phase further sorts the N candidates by typically considering more î€›ne-grained user/item features and incorporating complex modeling architectures. In this paper, we mainly conduct experiments in the matching stage (e.g., comparing with SOTA matching models). The paradigm of a matching model includes a user encoderğ‘“(ğ‘¥) âˆˆ R, which takes the userâ€™s historical behavior sequence as input and output one (or more) dense vector representing the userâ€™s interests, and an item encoderğ‘”(ğ‘¦) âˆˆ R, that represents the items in the same vector space as the user encoder. We denote all the trainable parameters in the system asğœƒ, which includes the parameters inğ‘“andğ‘”. With the learned encoders and the extracted item vectors, i.e.,{ğ‘”(ğ‘¦)}, a k-nearest-neighbor search service, e.g., Faiss [27], will be deployed for Top-N recommendation. Specifically, at serving time, an arbitrary user behavior sequenceğ‘¥will be transformed into a vectorial representationğ‘“(ğ‘¥)and topğ‘ items with the largest matching scores will be retrieved as Top-N candidates. Such matching scores are typically computed as inner productğœ™(ğ‘¥,ğ‘¦) = âŸ¨ğ‘“(ğ‘¥), ğ‘”(ğ‘¦)âŸ©or cosine similarity. In a nutshell, the learning procedure can be formulated as the following maximum likelihood estimation: In the matching phase, it can be infeasible to sum over all possible itemsğ‘¦as in the denominator. Here we adopt the sample softmax objective [3,25]. To demonstrate the eî€ectiveness of the proposed CauseRec architecture, we utilize a naive framework as the baseline. Speciî€›cally, the item encoderğ‘”(ğ‘¦)is a plain lookup embedding matrix where theğ‘›th vector represents the item embedding with item idğ‘›. The user encoderğ‘“(ğ‘¥)aggregates the embeddings of historically interacted items using global average pooling and then transforms the aggregated embedding into the same embedding space as item embeddings using multi-layer perceptrons (MLP): In this section, we give an brief illustration on the intuition and overall schema/pipeline of the CauseRec architecture, which is depicted in Figure 2, and introduce the building blocks in detail. 3.3.1 Overall Schema. The essence of CauseRec is to answer the retrospect question, â€™what the user representation would be if we intervene on the observed behavior sequence?â€™ The counterfactual transformation in CauseRec relates to the â€™intervention on the observed behavior sequence.â€™ For answering â€™what the user representation would be,â€™ we introduce an important inductive bias that makes the intervention work as expected. Speciî€›cally, we î€›rst identify indispensable/dispensable concepts in the historical behavior sequence. An indispensable concept indicates a subset of one behavior sequence that can jointly represent a meaningful aspect of the userâ€™s interest. A dispensable concept indicates a noisy subset that is less meaningful/important in representing an aspect of interest. We introduce the details in Section 3.3.2. Given the identiî€›ed concepts, a representative counterfactual transformation is designed to build out-of-distribution counterfactual user sequences. Here comes the inductive bias, i.e., counterfactual sequences constructed by replacing the dispensable concepts in the original user sequence should still have similar semantics to the original one. Here semantics refer to the characteristics of user interests/preferences. Therefore, replacing indispensable concepts in the original user sequence should incur a preference deviation from the resulted user representation to the original user representation. We denote these resulted user representations as counterfactually negative user representations. We note that such negatives are hard negatives where hard refers to that other dispensable concepts stay the same as the original user sequence and negatives means that the semantics of the user sequence should be diî€erent. In contrast, replacing dispensable concepts in the original user sequence should incur no preference change in representations. We denote these resulted user representations as counterfactually positive user representations. Diî€erent contrastive learning objectives are proposed to learn accurate and robust user representations that are less sensitive to the (attack on the) dispensable/noisy concepts and that trust more on the indispensable concepts that better represent the userâ€™s interest. Details can be found in Section 3.3.5. 3.3.2 Identification of Indispensable/Dispensable concepts. To identify indispensable/dispensable concepts, we propose to î€›rst extract concept proposals and compute the proposal scores. Item-level Concepts.Inspired by instance discrimination [17], a straightforward while workable solution is to treat each item in the behavior sequence as an individual concept since each item has its unique î€›ne-grained characteristics. In this way, we obtain the concept sequenceC = X âˆˆ R, whereX = ğ‘”(ğ‘¥)denotes the vectorial representations of the behavior sequence. In essence, concept scores indicate to what extent these concepts are important to represent the userâ€™s interest. Since there is no groundtruth for one userâ€™s real interest, we use the target itemğ‘¦as the indicator: wherecindicates the representation ofğ‘–th concept inC, andy indicates the representation of the target item.ğœ™is the similarity function, and we empirically use dot product for its eî€ectiveness in the experiment. ğ‘is thus the score for the ğ‘–th concept. Interest-level Concepts.However, such a solution may incur redundancy in concepts since some items may share similar semantics, and might deteriorate the capability of modeling higher-order relationships between items. To this end, besides the item-level concepts, we introduce interest-level concepts by leveraging the attention mechanism [56] to extract interest-level concepts. Formally, X âˆˆ R, we obtain the following attention matrix: whereWâˆˆ RandWâˆˆ Rare trainable transformation matrices. K is thus the number of concepts that is pre-deî€›ned.Ais of shape R. We obtain the concept sequence as the following: Since interest-level concepts and the target item are not naturally embedded in the same space, we compute the concept score by the weighted sum of item-level scores: For both item-level concepts and interest-level concepts, we treat the top half concepts with the highest scores as indispensable concepts and the remaining half concepts as dispensable concepts. This strategy is mainly designed to prevent the number of indispensable or dispensable concepts from being too small. We leave î€›nding more eî€ective solutions as future works as illustrated in Section 5. 3.3.3 Counterfactual Transformation. The proposed counterfactual transformation aims to construct out-of-distribution user sequences by transforming the original user sequence for one user. We here useuser sequenceto generally denote the concept sequence, which can be either the commonly known item-level concept sequence, i.e., the original user behavior sequence, or the interest-level concept sequence. Based on the inductive bias described in Section 3.3.1, we propose to replace the identiî€›ed indispensable/dispensable concepts at the rate ofğ‘Ÿto construct counterfactually negative/positive user sequences, respectively. We note that directly dropping indispensable/dispensable concepts also seems feasible, but replacement has the advantage of not aî€ecting overall sequence length and the relative positions of remaining concepts. We maintain a î€›rst-in-î€›rst-out queue as a concept memory for each level and use dequeued concepts as substitutes. We enqueue the concepts extracted from the current mini-batch. We denote the user sequence with indispensable/dispensable concepts being replaced as counterfactually negative/positive user sequence. 3.3.4 User Encoders. We note that item-level concept representations can be trained along with the item embedding matrix in most recommendation frameworks. Therefore, the above itemlevel concept identiî€›cation and counterfactual transformation processes can be performed without any modiî€›cation on the user encoder in the original baseline model, i.e., a model-agnostic and non-intrusive design. We denote the architecture solely considering item-level concepts as CauseRec-Item. CauseRec-Item obtains counterfactually positive/negative user representations {x}/{x}from counterfactual item-level concept sequences using the original user encoder ğ‘“. We denote the architecture solely considering interest-level concepts as CauseRec-Interest. Diî€erent from CauseRec-Item, interestlevel concepts are constructed with learnable parameters, i.e.,W andWin Equation 6. Therefore, CauseRec-Interest is an intrusive design, and the inputs to the user encoder should be the interestlevel concept sequence rather than the behavior sequence at the item-level. We note that there are no further modiî€›cations, and the architecture of the user encoder can stay the same as in the original baseline model. CauseRec-Interest obtains counterfactually positive/negative user representations from counterfactual interestlevel concept sequence using the original user encoder ğ‘“. We denote the architecture that considers counterfactual transformation on both the item-level concept sequence and the interestlevel concept sequence as CauseRec-H(ierarchical). CauseRec-H is also an intrusive design with interest-level concepts as the inputs of the user encoder. Diî€erent from CauseRec-Interest, CauseRecH further considers counterfactual transformations performed on item-level concepts. The counterfactually transformed item-level sequence will be forwarded to construct interest-level concept sequence using Equation 6-7. We note that counterfactual transformations will not be performed on these two levels simultaneously, which might introduce unnecessary noises. In other words, each counterfactual user representation is constructed with transformation on sequence solely from one level. 3.3.5 Learning Objectives. Besides the original recommendation lossLdescribed near Equation 2, we propose several contrastive learning objectives that are especially designed for learning accurate and robust user representations. Contrast between Counterfactual and Observation.As discussed in Section 3.3.1, arobustuser representation should be less sensitive to (possible attack on) dispensable concepts. Therefore, the user representations learned from counterfactual sequences with indispensable concepts transformed should be intuitively pushed away from the original user representation. Similar in spirit, an accuraterepresentation should trust more on indispensable concepts. Therefore, user representations learned from counterfactual sequences with dispensable concepts transformed should be intuitively pulled closer to the original user representation. Under these intuitions, we derive inspiration from the recent success of contrastive learning in CV [17,68] and NLP [7], we use triplet margin loss to measure the relative similarity between samples: wherexdenotes the original user representation. We set the distance functionğ‘‘as the L2 distance since user representations generated by the same user encoder are in the same embedding space. We empirically set the margin Î”= 1. Contrast between Interest and Items.The above objective considers the user representation side solely, and we further capitalize on the target itemğ‘¦, which also enhances the user representation learning. Formally, given the L2-normalized representation of the target itemËœy and user representationËœx, we have: This objective also has the advantage of preventing the user encoder from learning trivial representations for counterfactual user sequences. We set the marginÎ”=0.5 in the experiment. Finally, the loss for training the whole framework can be written as: During testing/serving, only the backbone model that generates the user representation is needed. The identiî€›cation of indispensable/dispensable concepts and the counterfactual transformation processes are disregarded. Noteworthy, the computation of proposal scores which depends on the target item does not belong to the backbone model and is not required during testing. We conduct experiments on real-world public datasets and mainly aim to answer the following three research questions: â€¢ RQ1: How does CauseRec perform compared to the base model and various SOTA sequential recommenders? â€¢ RQ2: How do the proposed building blocks and diî€erent hyperparameter settings aî€ect CauseRec? â€¢ RQ3: How do user representations beneî€›t from modeling the counterfactual world and contrastive representation learning? To demonstrate the generalization capability on learning usersâ€™ representations of the proposed CauseRec architecture, we employ an evaluation framework [5,35,41] where models should confront unseen user behavior sequences. Speciî€›cally, the users of each dataset are split into training/validation/test subset by the proportion of 8 : 1 : 1. For training sequential recommenders, we incorporate a commonly used setting by viewing each item in the behavior sequence as a potential target item and using behaviors that happen before the target item to generate the userâ€™s representation, as deî€›ned in Section 3.1. For evaluation, only users in the validation/test set are considered, and we choose to generate usersâ€™ representations on the î€›rst 80% behaviors, which are unseen during training. Such a framework can help justify whether models can learn accurate and robust user representations that can generalize well. We mainly focus on the matching phase of recommendation and accordingly choose the datasets, comparison methods, and evaluation metrics. DatasetsWe consider three challenging recommendation datasets, of which the statistics are shown in Table 1. â€¢ Amazon Books.We take Books category from the product review datasets provided by [44], for evaluation. For each user, we keep at most 20 behaviors that are chronologically ordered. â€¢ Yelp2018.Yelp challenge (2018 edition) releases the review data for small businesses (e.g., restaurants). We view these businesses as items and use a 10-core setting [20,65] where each item/user has at least ten interactions. â€¢ Gowalla.A widely used check-in dataset [34] from the Gowalla platform. Similarly, we use the 10-core setting [19]. Comparison MethodsWe mainly consider sequential recommenders for comparison since models are required to confront unseen behaviors for each user. Therefore, factorization-based and graph-based methods are not considered. The compared state-ofthe-art models are listed as the following: â€¢ POP.A naive baseline that always recommends items with the most number of interactions. â€¢ YouTube DNN [8].A successful industrial recommender that generates one userâ€™s representation by pooling the embeddings of historically interacted items. â€¢ GRU4Rec [21].An early attempt to introduce recurrent neural networks into recommendation. â€¢ MIND [32]. The î€›rst framework that extracts multiple interest vectors for one user based on the capsule network. â€¢ ComiRec-DR [5].A recently proposed SOTA framework following MIND to extract diverse interests using dynamic routing and incorporate a controllable aggregation module to balance recommendation diversity and accuracy. â€¢ ComiRec-SA [5].ComiRec-SA diî€ers from ComiRec-DR by using self-attention to model interests. Evaluation MetricsWe employ three broadly used numerical criteria for the matching phase, i.e., Recall, Normalized Discounted Cumulative Gain (NDCG), and Hit Rate. We report metrics computed on the top 20/50 recommended candidates. Higher values indicate better performance for all metrics. Implementation DetailsWe use Adam [30] for optimization with learning rate of 0.003/0.005 for Books/Yelp and Gowalla,ğ›½=0.9, ğ›½=0.99,ğœ– =1Ã—10, weight decay of 1Ã—1ğ‘’âˆ’5. We train CauseRecItem for (maximum) 10 epochs and CauseRec-Interest/CauseRec-H for (maximum) 30 epochs with mini-batch size 1024. All models are with embedding size 64. We set hyper-parametersğœ†= ğœ†=1 and do not tune them with bells and whistles. As illustrated in Section 3.2, the item encoder is a plain embedding lookup matrix, and the user encoder is a three-layer perceptron with hidden size 256. We setğ‘ =8,ğ‘€ =1,ğ‘Ÿ=0.5 for CauseRec-Item/-Interest andğ‘ =16ğ‘€ =2 for CauseRec-H to accommodate transformation on two levels, as illustrated in Section 3.3.4. We setğ¾ =20 for CauseRec-Interest/-H. The comparison results of CauseRec with SOTA sequential recommenders are listed in Table 2. We report three architectures of CauseRec including CauseRec-Item (CauseItem), CauseRec-Interest (CauseIn), and CauseRec-Hierarchical (CauseH), as described in Section 3.3.4. In a nutshell, we observe a clear improvement of these architectures over various comparison methods and across three diî€erent metrics. Notably, CauseRec-H improves the previous SOTA ComiRec-SA/DR by +.0299 (relatively 22.1%) concerning NDCG@50 on the Amazon Books dataset and +.0179 (relatively 8.64%) concerning Recall@20 on the Gowalla dataset. Among the comparison methods, ComiRec mostly yields the best performance by modeling multiple interests for a given user. However, only modeling the noisy historical behaviors might result in diverse but noisy interests that may not accurately represent users, î€›nally leading to inferior results. GRU4Rec achieves comparably good results with ComiRec on the Gowalla dataset. GRU4Rec can eî€ectively model the sequential dependency between items in the behavior sequence. However, it might be more likely to suî€er from the noises due to the strict step-by-step encoding process. In contrast, CauseRec architectures confront the noises within usersâ€™ behaviors by pushing the user representation away from counterfactually negative user representations and pulling it closer to counterfactually positive user representations. Besides, these results demonstrate the generalization capability of CauseRec on confronting out-of-distribution user sequences by modeling the counterfactual world. Among three CauseRec architectures, CauseRec-Item is a modelagnostic and non-intrusive design, which means it can be applied to any other sequential recommender without any modiî€›cation on the original user encoder, and solely functions in the training stage without sacriî€›cing inference eî€œciency. CauseRec-Interest constructs interest-level concepts by grouping items that may belong to a certain interest (e.g.,chocolateandcakebelong to sweets) into one holistic concept. Compared to CauseRec-Item, CauseRec-Interest has the advantage of reducing concept redundancy and modeling higher-order relationships between items, and thus improving CauseRec-Item. To combine the merits of CauseRecInterest and CauseRec-Item, CauseRec-Hierarchical considers both Table 2: Comparison results of three CauseRec architectures with SOTA sequential recommenders designed for the matching phase. CauseItem/CauseIn/CauseH stand for CauseRec -Item/-Interest/-Hierarchical, respectively. The symbol âˆ— indicates the improvements over the strongest baseline (underlined) are statistically signiî€›cant (ğ‘ < 0.05) with one-sample t-tests. Gowalla interest-level and item-level concepts in counterfactual transformation. CauseRec-H achieves the best results, which shows that counterfactual transformation on item-level concepts still yields some unique advantages, such as modeling î€›ne-grained preferences. For example, people might not generally like allsweetsand prefer cake to chocolate. 4.2.1 Ablation Studies. We are interested in the CauseRec-Item architecture due to its strengths of being: easy to implement (modelagnostic), eî€œcient in serving (non-intrusive), and eî€ective. To obtain a better understanding of diî€erent building blocks in CauseRecItem, we consider surgically removing some components and construct the following architectures. The results on Yelp and Gowalla datasets are shown in Table 3. w.o. (without) L.This means we do not consider the contrast between the counterfactual and the observation. The performance drop compared to CauseRec-Item indicates that pushing counterfactually negative user representation away and pulling counterfactually positive user representation closer can potentially help the learned observational user representation to trust more on indispensable items (accurate) and to be immune from dispensable items (robust). w.o. L. Lis deî€›ned in Equation 10. RemovingLmeans we do not consider the contrast between counterfactual user representations and positive/negative target items. According to Table 3, we observe a clear performance drop compared to CauseRecItem. Furthermore, eliminatingLyields poorer performance than eliminatingL. We attribute this to that,Lprevents the user encoder from yielding trivial representations for counterfactual user sequences (item-level and interest-level) by contrasting counterfactual user representations with target items representations. In other words,Lwith possibly trivial counterfactually user representations (without L) might hurt the eî€ectiveness L. ğ‘ƒğ‘œğ‘  Only.CauseRec-Item with only counterfactual transformations on dispensable items and counterfactually positive user representation is denoted asğ‘ƒğ‘œğ‘ . This architecture disregardsLandÃî€î€‘ termmax0,ËœxÂ·Ëœyâˆ’ Î”inL. Not surprisingly,ğ‘ƒğ‘œğ‘  Only architecture achieves inferior results compared with w.o.L architecture, demonstrating the merits of counterfactually negative user representations. Still, this architecture improves the base model, demonstrating the eî€ectiveness of contrasting counterfactual user representations with target items for recommendation. ğ‘ğ‘’ğ‘” Only.CauseRec-Item with only counterfactual transformations on indispensable items is denoted asğ‘ğ‘’ğ‘”. We observe similar results to theğ‘ƒğ‘œğ‘ Only architecture. This again veriî€›es the eî€ectiveness of the contrastive user representation learning framework by modeling the counterfactual distribution. Compared toğ‘ƒğ‘œğ‘ Only architecture, ğ‘ğ‘’ğ‘” Only architecture achieves inferior results. This phenomenon might indicate that making user representation trust more on indispensable concepts can be potentially more important than eliminating the eî€ect of indispensable concepts (i.e., noisy items in CauseRec-Item) on user representation learning. This is reasonable in the sense that the former process might potentially make the user in the embedding space away from all other items, including the dispensable items that are replaced during counterfactual transformation. Base ModelA naive matching baseline described in Section 3.2. All other Architectures yield improvement over the Base Model. Table 3: Ablation studies by constructing diî€erent architectures. We progressively ablate key components in CauseRec-Item, which is a model-agnostic and non-intrusive design. w.o. L w.o. L 4.2.2 Analysis on the number of counterfactual user representations. We ablate the number of counterfactually positive/negative user representations, i.e., M and N, as deî€›ned in Section 3.3.4. As shown in Table 4, we can observe that 1) increasing the number (from ğ‘ = ğ‘€ =1 toğ‘ = ğ‘€ =8) not necessarily leads to better performance. 2) particularly increasing the number of counterfactually negative user representations (fromğ‘ = ğ‘€ =1 toğ‘ =8, ğ‘€ =1) can be useful. This result is reasonable in the sense that such representations can be interpreted as hard negatives since each of the corresponding counterfactual user sequences contains dispensable items staying the same as the original user sequence, and hard negatives are known to be helpful. 3) particularly increasing the number of counterfactually positive user representations (fromğ‘ = ğ‘€ =1 toğ‘ =1, ğ‘€ =8) may introduce noises since each of them contains some randomly sampled items that can be falsely interpreted as "positive". Therefore one counterfactually positive user representation can be enough for learning accurate user embeddings. 4.2.3 Analysis on the Replace Ratio. We are interested in how the replacement ratio, i.e.,ğ‘Ÿ, in counterfactual transformation of CauseRec-Item aî€ects the model performance. Table 5 shows the results by varyingğ‘Ÿ. The best result is achieved withğ‘Ÿ= 0.4/0.5 for the Yelp dataset andğ‘Ÿ=0.5 for the Gowalla dataset. Either too small or too largeğ‘Ÿwill lead to sub-optimal results. We attribute this phenomenon to that smallğ‘Ÿwill aî€ect the capability of counterfactual learning and largeğ‘Ÿwill introduce more noises brought by randomly sampled items for replacement. ğ‘Ÿmakes a tradeoî€ between these two impacts. 4.2.4 Analysis on the number of interest-level concepts. Here we take an analysis on the number of interest-level concepts particularly for CauseRec-Interest architecture. Speciî€›cally, we ablateğ¾ as described in Equation 6. As shown in Table 6, we observe a performance improvement withğ¾increasing (e.g., 4â†’10, 10â†’20). Each interest-level concept can be more coarse-grained whenğ¾ becomes smaller and more î€›nd-grained whenğ¾becomes larger. It can be hard to classify a coarse-grained concept as indispensable or dispensable. It may lead to also coarse-grained or even inaccurate transformations afterward, and thus hurting the performance. Too largeğ¾(e.g., 30) may bring redundancies and noises (more random concepts introduced with a î€›xed replace ratio) to the framework and eventually leads to inferior results. Table 4: Performance analysis on the number of counterfactually positive/negative user representations in CauseRecItem, denoted in ğ‘€/ğ‘ . ğ‘ = 1, ğ‘€ = 1 0.111 0.310 0.541 0.219 0.413 0.559 ğ‘ = 4, ğ‘€ = 4 0.113 0.308 0.542 0.210 0.394 0.541 ğ‘ = 8, ğ‘€ = 8 0.106 0.298 0.524 0.204 0.381 0.521 ğ‘ = 8, ğ‘€ = 1 0.116 0.318 0.558 0.224 0.412 0.560 ğ‘ = 1, ğ‘€ = 8 0.096 0.273 0.483 0.185 0.361 0.498 Table 5: Performance analysis on the replace rate ğ‘Ÿin counterfactual transformation for CauseRec-Item. Table 6: Analysis on the number of constructed interest concepts ğ¾ for CauseRec-Interest. To understand how the learned user representations beneî€›t from the CauseRec framework, we plot randomly selected six users from the Amazon books dataset. We also plot ten corresponding items sampled from the test set for each user. Speciî€›cally, we perform Figure 3: Visualization of randomly sampled users (shown as stars) with their interacted items (shown as points of the same color) from the Amazon Books dataset. We perform the t-SNE transformation on the representations learned by the base model (left) and CauseRec (right). t-SNE transformation on the user/item representations learned by the base model (as shown in Figure 3a) and CauseRec-Item (as shown in Figure 3b). The connectivities of users and test items in the embedding space can help reî€ect whether the model learns accurate and robust user representations. From Figure 3b, we observe that users with their corresponding test items easily form clusters and show small intra-cluster distances and large inter-cluster distances. By jointly comparing the same users (e.g., 590695, and 586100) in Figures 3a and 3b, we can see that CauseRec-Item helps the user encoder learn representations that are closer to their corresponding test items. These results qualitatively demonstrate the eî€ectiveness of CauseRec on learning accurate and robust user representations. We also present a recommendation result from the Amazon Books test datasets in Figure 4. We list the historical behaviors, the top î€›ve books recommended by the base model and CauseRec-Item, and books interacted by the corresponding user in the test set. We mainly visualize the booksâ€™ covers and categories for better clarity. We note that the side information is generally not considered in training matching models (both the base model and CauseRec). As shown in Figure 4, we observe that CauseRec yields more consistent recommendation results to the books in the test set. Supposing historical behaviors consist of noisy ones, and behaviors in the test accurately reî€ect usersâ€™ interest for the current state, CauseRec successfully captures usersâ€™ interests, i.e., Childrenâ€™s Books, and Literature&Fictions. In contrast, the base model is more likely to be aî€ected by noisy behaviors that appear only a few times, such as the Biographies&Memories, and Education&Reference. These results further demonstrate that CauseRec can learn accurate and robust user representations that are less distracted by noisy behaviors. In this work, we propose to model the counterfactual data distribution to confront the sparsity and noise nature of observed user interactions in recommender systems. The proposed CauseRec conditionally sample counterfactually positive and negative user sequences with transformations on the dispensable/indispensable concepts. We propose multiple structures (-item, -interest, -hierarchical) to confront both î€›ne-grained item-level concepts and abstract interest-level concepts. Several contrastive objectives are devised to Figure 4: Case study by visualizing a real-world sample from the Amazon Books testing set. We mainly show the booksâ€™ covers and categories for clarity. contrast the counterfactual with the observational to learn accurate and robust user representations. Among several proposed architectures, CauseRec-Item has the advantage of being non-intrusive, i.e., solely functioning at training while not aî€ecting serving eî€œciency. With a naive matching baseline, CauseRec achieves a considerable improvement over it and SOTA sequential matching recommenders. Extensive experiments help to justify the strengths of CauseRec as being both simple in design and eî€ective in performance. This work can be viewed as an initiative to exploit the joint power of constative learning and counterfactual thinking for recommendation. We believe that such a simple and eî€ective idea can be inspirational to future developments, especially in modelagnostic and non-intrusive designs. CauseRec-Item is compatible with various user encoders within most existing sequential recommenders. We choose a naive baseline to better demonstrate the eî€ectiveness of this work, and we plan to explore its strengths in more models. Another future direction is to whether more eî€ective solutions of identifying indispensable/dispensable concepts exist, including both the computation of concept scores and the determination of indispensable or dispensable for each concept based on the scores. Lastly, we will explore the strengths of CauseRec for the ranking phase of recommendation. Counterfactual transformations designed with various auxiliary features and complex model architectures will open up new research possibilities. The work is supported by the National Key R&D Program of China (No. 2020YFC0832500), NSFC (61625107, 61836002, 62072397), Zhejiang Natural Science Foundation (LR19F020006), and Fundamental Research Funds for the Central Universities (2020QNA5024).