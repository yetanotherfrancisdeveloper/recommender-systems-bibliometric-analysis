In this position paper, we discuss recent applications of simulation approaches for recommender systems tasks. In particular, we describe how they were used to analyze the problem of misinformation spreading and und erstand which data characteristics aï¬€ect the performance of recommendation algorithms more signiï¬cantly. We also present potential lines of future work where simulation methods could advance the work in the recommendation community. CCS Concepts: â€¢ Information systems â†’ Recommender systems; Collaborative ï¬ltering. Additional Key Words and Phrases: evaluation, data characteristics, misinformation, preference sampling ACM Reference Format: Alejandro BellogÃ­n and Yashar Deldjoo. 2021. Simulations for novel problems in recommendation: analyzing misinformation and data characteristics. In SimuRec â€™21: The SimuRec workshop held in conjunction with the 15th ACM Conference on Recommender Systems (RecSys), Sep 27â€“ Oct 01, 2021, Amsterdam, Netherlands (online). ACM, New York, NY, USA, 1 IMPORTANCE OF SAMPLING IN RECOMMENDATION Recent years have witnessed an explosion where simulat io ns have been used in several aspects of Recommender Systems (RS), either inspired from Machine Learning (ML) or Information Retrieval (IR) problems â€“ where simulations have been used for a long time, as in cl ick models [12] or learning to rank [ 2] approaches â€“, or to attack inherent and concrete issues that are prevalent in RS, such as data scarcity and sparsity. In general, sampling in recommendation has been used t o open up (or simulate) evaluation scenarios. For example, in [26] the authors used it to downsample t he observed interactions and generate diï¬€erent scenarios of cold-start. We also found works where sampling is employed to analyze and understand the evaluation procedure, as in [ is used to perform a robustness analysis, in [ optimization, or to directly debias the evaluation, as described in [ However, there is also a growing body of literature that has been using sampling for learning preferences, as in the well-known Bayesian Personalized Ranking (BPR) algorithm [ are then provided to the algorithm. However, it is not obvious how this sampling should be done so that it always work, and several approaches have been investigated which diï¬€er in eï¬€ectiveness and biases â€“ such as p opularity â€“ learned by the method [ is applied to recommendation [ 11, 25]. Recent works that also rely on samplings or simulations are those where reinforcement learning Function generate user u, ratio r while (desNeg > |neg|) OR (desNeu > |neu|) do end userProï¬le(u) â† sample(neg, desNeg) âˆª sample(neu, desNeu); end before have even been applied to recommendation, as in [23], the sampling process is key for a successful translation of the approach from one area to the other. In the rest of this paper, we present two use cases where the authors have recently applied data sampling on recommendation tasks with diï¬€erent goals in each case. We later discuss the main lessons learned in this process and the advantages of using simulations or sampling strat egies. We end the paper w ith several open qu estions or ideas to explore in the future. 2 SUCCESSFUL APPLICATIONS OF SAMPLING In this section, we describe two applications of data sampling on recent works performed by the authors. Section2.1 shows how sampling allowed to analyze the eï¬€ect of misinformation in recommender systems, whereas Section 2.2 describes a sampling strategy that is used to infer the impact of data characteristics in recommendation performance. 2.1 Analyzing misinformation In [21], we analyzed the eï¬€ect that some recommendation algorithms cause on the ampliï¬cation of misinformation. For this, we ï¬rst created a dataset by merging information from fact-checkers and Twitter. Then, to simulate a wide range of situations â€“ not only the one captured at the moment of collect ing the data â€“ we used Algorithm1 to generate user proï¬les with a pre-deï¬ned set of constraints, in particular, the proportion of users who shared some misinformative items. In this way, we would simulate a population where 20% (or 50% or 80%) of users share this type of item. This also gives us more control on the amount o f information received by the algorithms in terms of sparsity or other important dataset statistics. Function data-sampling URM user-item rating matrix ğ‘›â† number of users of the URM; ğœâ† constraint on average number of ratings for users; ğœâ† constraint on maximum number of items; ğ‘› â† 1 ; while ğ‘› â‰¤ ğ‘ do end Output: ğ‘ sub-datasets (ğ‘¢ğ‘Ÿğ‘š end It is interesting to note that simulation has been very recently proposed in [2 8] also to study the societal impact o f recommender systems. By a general event-driven simulation model, the authors analyze some case studies and discuss the implications for reproducibility such a framework may have. 2.2 Understanding data characteristics In [15], and as an extension of [17], we aim t o better understand the impact of an array of characteristics in a dataset with respect to accuracy and fairness (in [ framework using regression models in its core, a methodol ogy originally proposed in [ whether a set of data characteristics (independent variables of the regression model) are related to a given performance metric (the dependent variable). To obtain enough points so that the framework could produce signiï¬cant results, we need to simulate ğ‘ diï¬€erent datasets (ğ‘ = 600 in both ou r papers). These simulated datasets are generated from sampling the original dataset as described in Algorithm characteristics in each case, although satisfying some constraints that allow the dataset to be useful in the analysis â€“ for example, by restricting the maximum number of items (via ğœ these experiments by considering diï¬€erent types of item content [ accuracy, robustness [ 3 DISCUSSION From the applications presented in Section 2 we have learned that simulation, and in particular, data sampling, is complex, even though it might be beneï¬cial â€“ sometimes the only tool to produce the data points needed for an analysis. Its diï¬ƒculty relies on sampling the dat a in a signiï¬cant but fair and realistic way, without introducing new biases, as acknowledged in recent works such as [ â† number of users of the URM; â† number of ratings of the URM ; Random shuï¬„e the rows of the URM; ğ‘›â† ğ‘Ÿğ‘›ğ‘‘ ([100,ğ‘›]); ğ‘›â† ğ‘Ÿğ‘›ğ‘‘ ([100,ğ‘›]); ğ‘¢ğ‘Ÿğ‘šâ† Select io n of ğ‘›, ğ‘›from shuï¬„ed URM; if< ğœor ğ‘›> ğœthen ğ‘› â† ğ‘› + 1; end with imposing additional constraints to obtain simulated datasets with a particular density, number of ratings per user, or number of items. Based on our experience, we foresee a continuous use of simulation techniques, such as data sampling or synthetic data, to experiment with novel evaluation conditions, such as those described before. In particular, consider the limitations of public datasets, with a ï¬xed number of attributes for users and items and a (sometimes) a small number of interactions, these strategies will allow evaluating new conditions with little eï¬€ort â€“ or at least, with less eï¬€ort than that of creating datasets including all the required information. 4 OPEN QUESTIONS Throughout this position paper, we have presented several situations where simulations or sampling str ategies have been successfully applied to recommendation. However, we believe there is room for improvement, and several aspects remain unexplored, both in our works and in t he community, under a more general perspective. First, regarding how to extend our sampling approaches, it would be more realistic if the temporal dimension is incorp orated in the process, so that the interactions follow a comp atible order with the one in the original dataset. When done properly, this would allow generating and studying feedback loops, like those created by reinforcement learning algorithms, but at a higher, more global level. As observed recently, and in agreement with our misinformation analysis [21], some algorithms are more prone to reproduce biases at each feedback loop [29]. Regarding our second application (see Section2.2), adding more variability and ï¬‚exibility in the types o f datasets being generated would allow us to address more complex q uestions. In particular, we envision a deï¬nition of user typ es (or personas) which are then simulated or sampled at diï¬€erent rates, either randomly or controlled via some parameter. Similarly, generating content features realistically would help go beyond collaborative ï¬ltering algorithms and test content-based methods at varying levels of information, quality, and sparsity [19]. For this, recent advances from the Natural Language Processing community could be convenient, where Neural Networks may generate realistic pieces of text in several domains [9, 20]. Besides content attributes, includ ing sensitive attributes in the set of controled (or simulated) information to b e generated will allow to explore fairness [14] analyses in a more comprehensive way than what is being done right now. Finally, an interesting perspective that could be promising is to shift the focus of the application, as mentioned earlier, from the data to the algorithms so that the sampling strategy instead of sampling data should sample algorithms. In this way, the input data would be ï¬xed, and the data points for the regression analysis would be obtained from a wide selection of algorithms, probably with diï¬€erent hyperparameters, to increase their variability. Such an approach would have connectio ns with automatic hyperparameter tuning techniques like Bayes Optimization [8], but it would be applied to solve a diï¬€erent problem and in a more extensive search space, as the type of algorithm will also be part of the sampled variables [7]. From a more general perspective, we believe several open questions need to be considered when doing simulations or, in particular, so me data sampling. An important aspect that should be considered is that of reproducibility [13]. While usually sampling â€“ or simulations in general â€“ are random in nature, this hinders reproducing other peopleâ€™s works. However, by sharing the code â€“ where customizable seeds are included wherever is needed â€“ and/or the generated simulated/sampled datasets or the scripts used, the p otential to reproduce t hese types of works should increase [6]. By addressing the reproducibility problem, a related issue we have also striven to present properly in the past could be solved. Here we refer to present all the t echnical decisions involved in a sampl ing strategy carefully and as detailed as possible. Sometimes pseudocodes or algorithms do not have the granularity level needed to specify implementation details that may greatly impact how the sampling or the simulation is generated. At the same time, although there are some works already addressing this issue (see [ important to understand the potential biases that a particular simul ation may be introducing in the generated data. This may be intentional but, usually, there should be some mechanism to avoid them. Related to this problem, there should exist a deï¬nition of when a simulation can be considered faithful to the or iginal (or intended) data, which we have referred throughout this work as realistic. Without such deï¬nition, we might end up wit h data samples that are too diï¬€erent (or diï¬€erent in speciï¬c, key aspects) from the actual data, hence not satisfying their underlying assumptions or constraints. This also applies w hen generating synthetic data, a task not so popul ar in recommendation because of its diï¬ƒculty, but where some eï¬€orts have been reported in the last years [