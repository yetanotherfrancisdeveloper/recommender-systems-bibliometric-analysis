Virtual support agents have grown in popularity as a way for businesses to provide better and more accessible customer service. Some challenges in this domain include ambiguous user queries as well as changing support topics and user behavior (non-stationarity). We do, however, have access to partial feedback provided by the user (clicks, surveys, and other events) which can be leveraged to improve the user experience. Adaptable learning techniques, like contextual bandits, are a natural î€›t for this problem setting. In this paper, we discuss real-world implementations of contextual bandits (CB) for the Microsoft virtual agent. It includes intent disambiguation based on neural-linear bandits (NLB) and contextual recommendations based on a collection of multi-armed bandits (MAB). Our solutions have been deployed to production and have improved key business metrics of the Microsoft virtual agent, as conî€›rmed by A/B experiments. Results include a relative increase of over 12% in problem resolution rate and relative decrease of over 4% in escalations to a human operator. While our current use cases focus on intent disambiguation and contextual recommendation for support bots, we believe our methods can be extended to other domains. â€¢ Computing methodologies â†’ Reinforcement learning; Neural networks; Learning linear models; Natural language processing; Batch learning; â€¢ Applied computing â†’ Enterprise computing. multi-armed bandits, contextual bandits, chat bots ACM Reference Format: Sandra Sajeev, Jade Huang, Nikos Karampatziakis, Matthew Hall, Sebastian Kochman, and Weizhu Chen. 2021. Contextual Bandit Applications in a Customer Support Bot. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD â€™21), August 14â€“18, 2021, Virtual Event, Singapore. ACM, New York, NY, USA, 9 pages. https://doi.org/ 10.1145/3447548.3467165 Virtual support agents have become ubiquitous in customer service for businesses. The Microsoft virtual agent is a customer support agent with which millions of users engage everyday. A typical session between the virtual agent and the user is shown in î€›gure 1. Challenges that come with operating this system include adapting to changing user behavior and support topics. For instance, bugs associated with the release of new operating systemğ‘Œcan result in users asking more questions aboutğ‘Œrather than previous operating systemğ‘‹. In addition, editors will create new support articles about operating systemğ‘Œto help address common problems; such articles should be suggested by the system when relevant as opposed to outdated content. Another example is that the rise in employees working from home due to a pandemic may result in users asking more questions than before about dual monitor detection or how to connect audio wirelessly. Machine learning, speciî€›cally reinforcement learning, has many opportunities to help optimize virtual support agents and meet these challenges head-on. We drew inspiration from advances in recommendation systems in news [12] and video [6,17]. The industry standard for recommendation systems has recently been switching to CBs [11], a simpliî€›ed single-step reinforcement learning (RL) paradigm which requires exploration but does not require dealing with credit assignment. In addition, CBs are data-eî€œcient, which is helpful in our scenario where exploration of riskier actions can hurt the end-user experience and have negative monetary implications. In the context of the virtual agent, we deal with a partial information setting, meaning we only receive feedback (click, survey response, etc.) for the actions the user took. This can easily be modeled with CBs. Our work presents applications of CBs for two scenarios of the Microsoft virtual agent: intent disambiguation and contextual recommendations. In intent disambiguation, the goal is to provide recommendations that tailor to the user query about a topic. For contextual recommendations, the objective is to provide a set of recommendations to the user as soon as they engage the virtual agent prior to any user query, based on a set of context features. These contextual features can include information such as the website from where the user engages the virtual agent. We have deployed a MAB solution for contextual recommendations and a NLB solution for intent disambiguation, both of which showed positive impact on key performance indicators (KPIs) in online A/B experiments. The rest of the paper is organized as follows. First, we present the problem setting and notation. Next, we describe the methods used, going into detail about the learning paradigms. Then, in the implementation section, we outline how we incorporated MABs for Figure 1: The Microsoft virtual agent. In response to the user query, an intent clariî€›cation dialog presents a slate of topics generated by a policy trained using NLBs. contextual recommendations and NLBs for intent disambiguation. Finally, we present the eî€œcacy of our solutions through results from online experiments in the evaluation section. The application motivating our work is the Microsoft virtual agent â€“ an interactive dialogue system providing î€›rst-line customer support for many Microsoft products. The virtual agent can be accessed via multiple channels â€“ most commonly through the Microsoft support websiteor the Get Help app which comes with Windows 10. In the Microsoft virtual agentâ€™s original architecture, manually conî€›gured rules dictated the behavior of the system. Most of these rules were not important from a business perspective, but rather assumptions made by application developers due to a lack of data. Moreover the original system could not adapt automatically to the diversity of user intents and a changing environment, such as software updates causing new issues or updated support content. RL addresses some of these challenges and is a good î€›t for this problem setting for the following reasons: (1)Microsoft products (including Xbox, Oî€œce, Windows, Skype) have a large customer base, hence the virtual agentâ€™s incoming traî€œc is also signiî€›cant, making RL methods feasible. (2)We have access to multiple feedback signals including click behavior, escalation to a human agent, and responses to survey questions such as â€œDid this solve your problem?â€. Some of these signals are tracked by the product team as their KPIs. At the same time, the application poses interesting challenges for the use of RL. Goal-oriented dialogue systems need to not only understand the userâ€™s original intent, but also be able to carry the state of the dialogue and guide the user towards their goal. While our long-term plan is to be able to have a single RL agent in charge of the whole conversation, the requirements of such an endeavor, including the of number of samples needed to learn non-trivial policies, are substantial. Therefore, we started by applying RL to the virtual agent in isolated components î€›rst, ignoring issues of credit assignment and thus working in the setting of CBs. In this work, we focus on two speciî€›c scenarios within the Microsoft virtual agent. In the following subsections, we describe them in more detail, and then formalize the notation that is shared between both scenarios. The domain of customer support, especially for a large company such as Microsoft, is complex and has a signiî€›cant number of intents. Our intent disambiguation policy is tasked with deciding when the userâ€™s query is clear enough to directly trigger a solution, such as a troubleshooting dialogue, or to ask a clariî€›cation question. The inputs to this policy are the statement of the issue by the user (the query), user context features, and a list of candidate actions. The query can range from a few keywords to long and complex sentences or even paragraphs. The user context includes information like the userâ€™s operating system and its version (e.g., Oî€œce products work on Windows, Mac, iOS, and Android). The candidate actions are a collection of pre-authored dialogue intents or solutions related to the userâ€™s query pulled from the Web. Retrieval of these candidates is currently performed by several strategies. One of them includes a deep learning model similar to the one described in [8]. Another uses Bing Custom Search for customized document retrieval. These retrieval components are currently out-of-scope for RL-based optimization so we will focus on the policy that operates with a small list of retrieved candidates. Given the input, the policy can do one of the following: â€¢Directly trigger a single intent or a solution: this can start a troubleshooting dialogue or display a rich text solution. â€¢Ask a yes/no question: â€œHereâ€™s what I think you are asking about: . ..Is that correct?â€ â€¢Ask a multiple-choice question: â€œWhich one did you mean?â€, followed by titles of two to four intents as well as option â€œNone of the above.â€ â€¢Give up: â€œIâ€™m sorry, I didnâ€™t understand. It helps me when you name the product and brieî€y describe the issue.â€ Figure 1 presents an example of a multiple-choice question action taken by the disambiguation policy. The â€œSettingsâ€ app is a desktop application in Windows 10, where a user can click â€œGet helpâ€ from any Settings page (e.g., Bluetooth, Display etc.) and interact with the Microsoft virtual agent. We will refer to the page the user is coming from as the source page - it is a crucial part of the user context that is available to us. The goal is to provide contextual recommendations, i.e., to recommend a slate (sequence) of solution topics before the user types anything, simply based on the context sent by the app (see î€›gure 2). The baseline experience is a î€›xed mapping from source pages to slates of up to six topics manually picked by human editors. We were motivated by several reasons to leverage feedback data to learn better contextual recommendations and improve upon the Figure 2: Contextual recommendations in Settings in Windows 10, for the source page â€œPrintersâ€. baseline. First, maintaining manually-speciî€›ed dialogues does not scale to the approximately 200 source pages present in the app (most of them served the same î€›xed slate of six topics). Second, a large amount of traî€œc î€ows through the app, which suggests that data-driven techniques could do well here. Third, the available user context includes more signals than just source page, which can help with suggesting relevant solutions in certain situations, such as the device network type (wired, wiî€›) and battery status (charging, discharging). Given the context, the agent can take one of the following actions: (1) Suggest up to six solutions out of about 3000 candidates. (2)Choose to fall back to default behavior of recommending six î€›xed options chosen by editors for the current source page. The feedback signals available to learn from are the same in both scenarios described above: â€¢ Click.When a slate of topics is presented, the option selected by the user is recorded. This signal is censored in scenario 2.1 when the policy decides to directly trigger. While useful, it is not a metric important to the business, so we do not optimize for it alone. â€¢ Survey.After providing a î€›nal answer to the user, the virtual agent asks whether the solution has resolved their problem. The user may respond â€œyesâ€, â€œnoâ€, or decline to answer. The product team tracks this signal, aggregated per user session and averaged, as the most important KPI called Problem Resolution Rate (more details in section 5.1). Hence, this feedback signal is also our main reward signal. â€¢ Escalation.The user can decide at any time to talk to a human agent. This negative reward signal is directly related to the actual cost of running a call center. In general, this metric is monitored but is not being optimized. The contextual recommendation and intent disambiguation scenarios that we focus on in this paper can be modeled as combinatorial CBs. We adapt and expand notation proposed in [19] to the scenarios found in the Microsoft virtual agent. In this setting, an agent interacts with the environment repeatedly as follows: For time step ğ‘¡ = 1, 2, . . .: (1)The world produces a contextğ‘¥âˆˆ X(e.g., the user sends a query ğ‘¥to the system). (2)The agent chooses a slates= (ğ‘, ..., ğ‘) âˆˆ S(ğ‘¥), of variable lengthğ‘™(0 â‰¤ ğ‘™ â‰¤ ğ¿), consisting of actionsğ‘âˆˆ A(ğ‘¥). We callA(ğ‘¥)an item action space andS(ğ‘¥)a slate space. Position ğ‘— in a slate is called a slot. (3)Given the context and slate, the world produces feedback signals, including: â€¢ ğ‘ğ‘™ğ‘–ğ‘ğ‘˜âˆˆ {0, 1}informing of the slot selected by the user. In this work, we assume it is either a one-hot vector or a zero vector (i.e., no more than a single item can be selected). â€¢ ğ‘ ğ‘¢ğ‘Ÿğ‘£ğ‘’ğ‘¦âˆˆ {yes, no, skipped}indicating the userâ€™s assessment of the solution presented by the virtual agent, provided via a survey. Values â€œyesâ€ and â€œnoâ€ mean positive and negative feedback, respectively, while â€œskippedâ€ means the user has not responded to the survey. â€¢ ğ‘’ğ‘ ğ‘ğ‘ğ‘™ğ‘ğ‘¡ğ‘–ğ‘œğ‘›âˆˆ {0, 1}indicates whether the user has decided to escalate the case to a human agent (1) or not (0). (4)The reward for each slotğ‘—in the slatesis denoted byğ‘Ÿ. It is computed as a function of one or more of the feedback signals. Note that the item action spaceA(ğ‘¥)is not a î€›xed set but can vary based on the context. The actions are parametric, with features such as title, type of content, etc. In our case, the majority of actions represent support documents and troubleshooting dialogues, but A(ğ‘¥)may also include the special action null item, denoted as âŠ¥. It gives the user a choice of indicating that none of the other actions is relevant. If selected viağ‘ğ‘™ğ‘–ğ‘ğ‘˜, the system moves on to other suggestions. To gain intuition about this notation, letâ€™s consider some examples: â€¢ The intent disambiguation scenario presented in the î€›gure 1. The query â€œi lost my emailâ€ is the main property of the contextğ‘¥. Topics â€œMicrosoft account recovery formâ€ and â€œRecover email that is still in the deleted items folderâ€ are some actionsğ‘andğ‘. The â€œNone of the aboveâ€ option rendered in the dialog translates to the null itemâŠ¥- it gives the user an opportunity to explore other topics or ask another question. Hence, the slate chosen by the policy at that time step was s = (ğ‘, ğ‘, âŠ¥). The intent disambiguation policy is also allowed, for some contexts, to directly trigger a topic. Such a slate would look likes = (ğ‘), without theâŠ¥action. What kind of slates are allowed in what contexts depends on the function S(ğ‘¥). â€¢ The contextual recommendations scenario presented in the î€›gure 2. The source page â€œPrintersâ€ is the main property of the contextğ‘¥. Ignoring the presented topics and typing a query manually is equivalent to selecting the null itemâŠ¥- hence, the slate can be represented ass = (ğ‘, ğ‘, . . . , ğ‘, âŠ¥). In the contextual recommendations scenario, every slate in S(ğ‘¥), in every contextğ‘¥, must always includeâŠ¥(i.e.: topics are never directly triggered). The reward used by our CB algorithms is always a function of feedback signals. Decoupling them from the reward deî€›nition, while not a standard approach in the RL literature (as in [5,11]), is useful in the learning algorithms we are presenting in later sections. Some feedback signals may be delayed. For example, a long conversation may occur between the actionğ‘and the point at which we ask the user if the proposed solution addressed their problem (to generate the signalğ‘ ğ‘¢ğ‘Ÿğ‘£ğ‘’ğ‘¦). We currently treat all variations that can happen there as part of the environment that generates a noisy version of the reward. This simpliî€›cation is a practical choice and can be removed with better modeling of the interaction between the user and the bot. We divide discussion about the solutions into two main categories: learning from a discrete context (corresponding to the contextual recommendations scenario described in section 2.2) and learning from a rich context (relevant in the intent disambiguation scenario from section 2.1). For each discrete contextğ‘¥âˆˆ X, let there be two MABs corresponding to two feedback signals: clicks and survey responses. At time step ğ‘¡ , each MAB can be described as follows: â€¢We have collected a history of reward successes and failures overğ‘¤total past time steps in each contextğ‘¥separately for each action ğ‘. This ğ‘¤is speciî€›c to each context. â€¢We have a functionğ‘†ğ‘ğ‘šğ‘ğ‘™ğ‘’ (ğ‘ ğ‘¢ğ‘ğ‘ğ‘’ğ‘ ğ‘ ğ‘’ğ‘ , ğ‘“ ğ‘ğ‘–ğ‘™ğ‘¢ğ‘Ÿğ‘’ğ‘ )that samples scores for all actions. â€¢ Actions are ordered by these sampled scores. â€¢ The slate ends at the position at which âŠ¥ is ranked. â€¢successes: the total instances actionğ‘has been clicked for the past ğ‘¤time steps â€¢failures: the total instances an actionğ‘has been observed (ranked above âŠ¥) but not clicked For the survey response bandit and contextğ‘¥, we ignore the cases that the survey is not answered, considering â€¢ successes: total â€œyesâ€ answers â€¢ failures: total â€œnoâ€ answers 3.1.1 Sampling with a Discrete Context. To balance exploitation with exploration, we use a sampling algorithm to produce plausible estimates of the probability of a click and the probability of a â€œyesâ€ survey. We compared both Thompson sampling [16,20] and EwS [13], and qualitatively we found that results in our application looked better with Thompson sampling so we focus on it here. However, EwS is reasonable to use as well. Letğ›¼represent the successes for actionğ‘in contextğ‘¥, given the data until time stepğ‘¡, within windowğ‘¤. Likewise, letğ›½ represent the failures for actionğ‘in contextğ‘¥, given the data until ğ‘¡, within windowğ‘¤. We start with a uniform prior distribution ğµğ‘’ğ‘¡ğ‘(ğ›¼ = 1, ğ›½ = 1)and assume each event is iid. Then the posterior probability isğµğ‘’ğ‘¡ğ‘(ğ›¼+ 1, ğ›½+ 1). In other words, we add one to all success and failure counts. In practice, we track successesğ›¼ and trials ğ‘ , and failures are then computed as ğ›½ = ğ‘ âˆ’ ğ›¼. Each bandit samples an estimated action valueğ‘„ (ğ‘¥, ğ‘)from the posterior distributionğµğ‘’ğ‘¡ğ‘(ğ›¼+ 1, ğ›½+ 1)for each action ğ‘. To represent the î€›nal score for an actionğ‘based on both bandits, we initially combined the scores from the two bandits using a naive product to model the joint probability ğ‘ƒ (ğ‘ğ‘™ğ‘–ğ‘ğ‘˜) Â· ğ‘ƒ (ğ‘ ğ‘¢ğ‘Ÿğ‘£ğ‘’ğ‘¦ = ğ‘¦ğ‘’ğ‘ |ğ‘ğ‘™ğ‘–ğ‘ğ‘˜). Later, having amassed more logged data via online experimentation, we switched to a more î€exible approach of using a context-speciî€›c interpolation scoreğœ†which weights the click score lower if there are suî€œcient survey response trials. log(ğ‘„(ğ‘¥, ğ‘)) â‰œ ğœ†log(ğ‘„(ğ‘¥, ğ‘))+(1âˆ’ğœ†) log(ğ‘„(ğ‘¥, ğ‘)) This combined score is used to rank actions in descending order. The size of the slate visible to the user is determined by the minimum of the position ofâŠ¥in the slate and action space rules dictating the maximum slate length. In other words, the slate that the user actually sees is a subset of the scored actions. All actions with a higher sampled score thanâŠ¥is considered observed by the user. Having assembled the slate, the user then provides feedback signals. As mentioned in 2.4, theğ‘ğ‘™ğ‘–ğ‘ğ‘˜signal is a one-hot vector: 1 for the clicked slot and 0 for the rest. Theğ‘ ğ‘¢ğ‘Ÿğ‘£ğ‘’ğ‘¦signal is considered observed only for the slot that was clicked. Note that the downside of Thompson sampling in comparison to EwS is that it does not produce closed-form probabilities of choosing an action in a given state. Collecting such probabilities is often useful for oî€-policy learning and evaluation methods. This can be mitigated by logging theğ›¼andğ›½values at the time of making a decision and estimating these probabilities oî€Ÿine in a Monte-Carlo fashion. A rich context is deî€›ned by large cardinality of the setX, which makes it diî€œcult to learn the value function for each state. Therefore value function approximation is required for rich contexts, like natural language embeddings and image data. Assume there is a representation functionğœ™that takes in a rich contextğ‘¥ âˆˆ X and actionğ‘ âˆˆ Aand outputs a set of low-dimensional features, ğœ™ (ğ‘¥, ğ‘) âˆˆ R. We assume that the expected reward is linear with respect to these features. In other words, there exists an unknown vectorğ‘¤, which models the linear relationship of the expected rewardE[ğ‘Ÿ |ğ‘¥, ğ‘], for action ğ‘ and a context ğ‘¥. 3.2.1 Neural-Linear Bandit. Correct quantiî€›cation of uncertainty is critical in bandit algorithms because the algorithm is in charge of how data is collected. If the algorithm uses uncertainty estimates that are too wide, it over-explores and unnecessarily tries suboptimal actions. If the uncertainty estimates are too narrow, the algorithm under-explores and runs the risk of not discovering the best action. In this work we use the Neural-Linear Bandit strategy from [15], which compares various methods for quantifying uncertainty with deep learning models. There it was demonstrated that the neurallinear approach is often the best single-model method (i.e., not relying on an ensemble) for uncertainty quantiî€›cation. We have slightly adapted this method for our purposes. The NLB consists of two main parts. First is the representation functionğœ™:XÃ—A â†¦â†’ R which produces a d-dimensional vector of neural features. The functionğœ™is generated by î€›tting a neural network,ğ‘:X Ã— A â†¦â†’ R, which predicts the observed reward from query-answer pairs. The last layer of this network is removed to represent ğœ™ (ğ‘¥, ğ‘). The second part is the bandit function, where we try to estimate theğ‘¤that satisî€›es(2)and the uncertainty around it. This is the simplifying assumption in NLBs: all uncertainty in the model can be approximated just by the uncertainty in the last layer of the network. Due to the assumption that the reward is linear with respect to the neural features, the focus is on creating a posterior distribution for the solution. Note that validating this assumption becomes diî€œcult in practice due to the large dimensions of the neural features. This is a simple least squares problem that we need to solve each time we update the bandit. To do this eî€œciently and support rolling windows of training data we maintain the suî€œcient statistics: which help us rewrite eq.(3)asğµ^ğ‘¤= ğ‘“. We solve the latter with the help of the Cholesky factorizationğ¿ğ¿= ğµwhich we also use for obtaining the uncertainty for new predictions^ğ‘Ÿ (ğ‘¥, ğ‘) = ğœ™ (ğ‘¥, ğ‘)^ğ‘¤. To do this we need a notion of a count of how many times actionğ‘has been tried in contextğ‘¥. For linear bandits this is given by The bandit gives more beneî€›t of doubt to an actionğ‘with small ğ‘(ğ‘). Similarly to the number of times an action has been tried in the case of a MAB,ğ‘(ğ‘)is unit-less even ifğœ™ (ğ‘¥, ğ‘)has units: the matrixğµcancels them. The inverse of the quadratic form is also necessary for other reasons. Ifğœ™were one-hot vectors, thenğµ would be diagonal with the count for actionğ‘in contextğ‘¥on the diagonal. Therefore, the inverse quadratic form is the count. 3.2.2 Sampling with a Rich Context. The NLB policy samples actionsğ‘to generate the slateğ‘ according to a sampling algorithm. The main algorithms we explored for sampling in the rich context are the linear versions of Thompson sampling [3] and EwS [1]. Both sampling algorithms result in stochastic policies. Linear Thompson sampling [3] requires speciî€›cation of a prior covariance matrix forğ‘¤, typically chosen to be a multiple of the identity matrix. However, there is no easy way to set this hyperparameter as it can have a hard-to-predict eî€ect on the learning dynamics. In addition, this algorithm doesnâ€™t provide the probability of sampling actionğ‘in an explicit form which is useful for oî€-policy evaluation. Linear EwS [1] was chosen since it has no requirement of setting a prior. In Linear EwS, the probability of selecting an actionğ‘has an explicit form. Given contextğ‘¥we î€›rst compute the action the model currently prefers and the gaps between the best action and all actions Finally, as noted in [1], Linear EwS samples actionğ‘proportional to exp(âˆ’2ğ‘(ğ‘)ğ‘”(ğ‘)) where ğ‘(ğ‘) is given by (6). 3.3.1 Making Exploration Safer. If a strong baseline already exists, to make exploration safer, we can compare the total value of the sampled slate to the value of the baseline slate. Whichever slate has a higher value is then presented to the user. In our case, we consider the slate value to be the sum of the sampled scores of the actions in the slate. This simple trick can reduce variance in the KPIs, normally expected from exploration, after initial deployment of an automatic learning system. Note that the baseline slate can be still used to collect feedback signals and improve future predictions. 3.3.2 Sampling to Obtain a Slate. One can simply sample for expected rewards from possible actions once to obtain a full slate of actions. If there are per-slot rules dictated by business requirements, they can be applied after the sampling. This section outlines key implementation details about our MAB solution for contextual recommendations and NLBs for intent disambiguation. Both training pipelines were written in Python. All training and evaluation was done asynchronously using Azure Batch. The inference code was written in C# to easily integrate with our partner teamâ€™s product, the Microsoft virtual agent. The intent disambiguation models were trained using PyTorch [14] and converted to ONNX [4] for inference in the production system. ONNX Runtime is used to load the models in .NET. 4.1.1 Problem Formulation. The discrete context in our case is categorical: the source page from where the user is clicking â€œGet Figure 3: Feedback loop for contextual recommendations. helpâ€. In oî€Ÿine analysis, we found that while there are other interesting attributes in the context such as battery value and network type, the source page is the most indicative of what the user may free-type when they choose not to select from the given list of â€œtop solutionsâ€. For example, users clicking â€œGet helpâ€ from the Printers page tend to ask printer-related questions. The candidate content is represented simply by a unique id and no other features. We associate the null item âŠ¥ with the user free-typing. User clicks and surveys are logged and aggregated every four hours using asynchronous jobs over a moving window. This moving window ensures that content that is not performing well and not interacted with will disappear from recommendations eventually. Note that this window is context-speciî€›c, as content performing poorly in one context does not have an eî€ect on another context. At runtime, a user clicks on â€œGet helpâ€ from source pageğ‘¥. First, we gather available candidates that have click and survey counts. We have limitations on the size of our logged data, so we perform an initial sampling of the candidates from over 1000 candidates down to around 25 using Thompson sampling as described in 3.1.1. With this pared-down set of candidates, we sample once more using Thompson sampling to obtain a î€›nal slate of actions. The length of the slate is determined by the minimum of the following: the null itemâ€™s position and the conî€›gured max length of the slate ğ¿(in our caseğ¿ = 7includingâŠ¥). We apply the trick of making exploration safer from 3.3.1 as there is a strong human-authored baseline. 4.1.2 Automatic Action Space Expansion. So far as weâ€™ve described, this experience has a concrete set of candidates per source page from which it draws to recommend to the user. Occasionally, an editor may author new content relevant to a source page which performs well within the context of a source page but outside of the initial recommendation, e.g., the user free-types a query that triggers this content, î€›nds it helpful, and answers â€˜yesâ€™ to the endof-chat survey. Thanks to our work with intent disambiguation, these interactions are also logged. To address this, we have another asynchronous job that runs once a day. This job merges statistics from our intent disambiguation î€ow that occur within the context of a source page with statistics from the initial Settings recommendations î€ow. For each source page, we use the null itemâ€™s score as a success threshold for candidates in consideration from intent disambiguation. We î€›rst î€›lter out candidates with insuî€œcient trials, e.g., 10 trials. For each potential candidateğ‘we then compute a bound on the probability that the expected reward of ğ‘ is larger than the expected reward of âŠ¥. We then î€›lter out candidates whose bound on the probability is less than our conî€›gured allowable false positive probabilityğ‘“ ğ‘. Intuitively, this allowable false positive probability is the maximum probability we allow that an individual new candidate has lower survey response success thanâŠ¥, i.e., the probability of a candidate not being a false positive isâ‰¥ 1 âˆ’ ğ‘“ ğ‘. The qualifying candidatesâ€™ counts are copied over from the intent disambiguation space to the Settings spaceâ€“however, we zero out the click counts as the trials between the two spaces are not comparable. 4.2.1 Problem Formulation. For the intent disambiguation scenario, the context is rich. The input data to the policy contains the text embedding of the user query and candidate title generated in a manner similar to [8]. It also includes a collection of categorical user context features, such as the website from where the user initiated the virtual agent. When the user engages the virtual agent by typing a question, our policy samples from a set of candidates described in 3.3.2 to generate a slate. The slate length is determined by the null item. 4.2.2 Training. To train the NLB policy, we î€›rst featurize the logged data, limiting to sessions that have explicit user survey responses (â€œyesâ€ or â€œnoâ€). We initially deî€›ned the reward to be a function of the survey feedback of only the clicked action (8). In subsequent iterations, we experimented with combining multiple feedback signals for the reward as described in section 5.5. After featurization, we follow the learning procedure for NLBs deî€›ned in 3.2.1. The training objective of the networkğ‘was set to minimize the squared error between the reward of the clicked action and the prediction. This network is used to generateğœ™withğ‘‘ = 2048neural features. Note thatğ‘‘cannot be excessively large as the bandit needs to store ğµor ğ¿which is of size ğ‘‘ Ã— ğ‘‘ to compute eq. (6). We train the bandit in î€›xed periods (e.g., one day). For each î€›xed period, we solve the least squares problem deî€›ned in(3). When training this model in practice, the matrixğµcan be very close to low rank. To be able to have a usable inverse, we used principal component regression and retained enough principal components to capture 99% of the variation in the neural features. The initial NLB policies were î€›xed: trained on a set number of days of data and deployed to production. We later explored autoupdating policies with hourly retraining. The virtual agent product team tracks several KPIs where some were the metrics for which we optimized. Our metrics are reported over user sessions and each user session is treated independently. We do not track an individual userâ€™s behavior over time, e.g. if a user re-engages with the Microsoft virtual agent, that is treated as a separate user session. â€¢ Problem Resolution Rate (PRR):a ratio of user sessions with a positive survey response to all sessions with any survey response. This is the main metric we track while running oî€Ÿine or online experiments. â€¢ Engaged Assisted Support (EAS):fraction of user sessions that end up with escalation to human-assisted support. â€¢ Self Help Success (SHS):fraction of user sessions that ended with assumed success (i.e., an answer is delivered and the user does not engage assisted support or indicate the answer was not useful). â€¢ User Engagement (UE):fraction of user interactions where the user sends at least one message after the virtual agentâ€™s greeting (where the greeting may include a slate with contextual recommendations, in case of help for the Settings app). We can approximate some of these metrics using feedback signals described in sections 2.3 and 2.4. E.g., for a time periodğ‘¡ = 1, ...,ğ‘‡ we can deî€›ne the following approximations: Where is an indicator function. The diî€erence between the product teamâ€™s KPIs and the approximated values is that the real metrics are aggregated across a user session rather than across an individual event within a user session. A single session lasts from the moment the user connects with the virtual agent until the last event that occurs within the conversation. Speciî€›c aggregation depends on the metric - e.g., the user may respond negatively to one survey, but then continue conversation with the virtual agent, and eventually respond to the second survey positively. The product teamâ€™s KPI would indicate this as a single event with success contributing towards PRR, while our simpliî€›ed deî€›nition would consider this as two events - one failure and one success. Our CB algorithms, as currently deî€›ned, are capable of optimizing only the approximated metrics like^ğ‘ƒğ‘…ğ‘…or^ğ¸ğ´ğ‘†. While aware of this problem, we observe that a CB policy optimizing a simpliî€›ed metric will usually cause movement of the real KPI in the same direction, when deployed to production, as the following sections show. Before attempting online experimentation with the NLB policy for intent disambiguation, we î€›rst used oî€Ÿine evaluation to check whether a newly trained policy seemed promising. This helped limit the negative impact to actual users in production traî€œc. Since we made use of EwS sampling, our behavior policy was stochastic and we logged the probabilities. This allows us to run oî€-policy evaluations to estimate the impact of our policy. We use the SNIPS [18] estimator to measure how much more reward our current policy would have provided compared to the logging policy. In our counterfactual evaluation, we were only able to use data where the logged policy and the new policy agreed on the clicked action, rather than the entire slate. The primary metrics we observed were how the logged policy and new policy compared in terms of rewards based on the survey and deî€ection feedback signals. A policy had to achieve higher reward estimates than the logging policy with low variance to be promoted to an online experiment. The requirement of low variance was enacted to reduce mismatch between oî€Ÿine evaluation and online experiments. In the case of contextual recommendations, we do not perform oî€Ÿine evaluation beyond qualitative analysis as described in 3.1.1 and careful monitoring of the auto-updated results. The updated model is deployed automatically without any experiments or human in the loop, although with an automated monitoring system alerting about unusual behavior. We conducted several online A/B experiments to verify the eî€œcacy of both our new contextual recommendation and intent disambiguation policies in comparison to a control policy (i.e., the current deployed production policy) on user traî€œc from the Microsoft virtual agent. Each A/B experiment should be viewed independently, as the control policy and the amount of user traî€œc varies from experiment to experiment. The A/B experiments gave key insights into how well the polices improved KPIs relative to control. Some of these KPIs were only approximated by oî€Ÿine evaluation and others were not captured in oî€Ÿine evaluation, like UE or SHS (see section 5.1). Furthermore, A/B experiments reî€ected the actual environment in which the policy was deployed, which diî€ered in some cases from the training data. For instance, the NLB policies were trained only with data that included the survey signals, "yes" or "no", leaving out data where the user did not answer the survey. Finally, these A/B experiments allowed us to quantify impact, which was especially key for contextual recommendations which did not have formal oî€Ÿine evaluation. The results from these A/B experiments were the deciding factor in whether to promote these new policies to the default experience, thus serving general production traî€œc in the future. When looking at results from online A/B experiments, there were several KPIs that we observed as described in section 5.1. Optimizing for several KPIs was a balancing act. Some of these metrics like PRR were almost directly optimized for as reward signals during training, others like EAS were used as limitations where we sought to prevent harmful movement, and others such as SHS and UE were assumed to be correlated to reward signals. We î€›rst ran an A/B experiment for an automatically updating implementation of our MAB approach based on Thompson sampling that used the naive combined scoreğ‘ƒ (ğ‘ğ‘™ğ‘–ğ‘ğ‘˜) Â· ğ‘ƒ (ğ‘ ğ‘¢ğ‘Ÿğ‘£ğ‘’ğ‘¦ = ğ‘¦ğ‘’ğ‘ |ğ‘ğ‘™ğ‘–ğ‘ğ‘˜) described in section 3.1.1. The control was a human-authored î€›xed mapping from source pages to slate. The experiment showed gains in SHS and UE, which led us to deploy this new policy to all production traî€œc. Afterwards, we set up a small experiment to monitor the autoupdating policyâ€™s performance over time. The MAB policy was set as the champion policy and the old control behavior as the challenger. The challenger only ran with a small amount of traî€œc as to provide minimal impact on performance. Running this monitoring experiment was a crucial decision, as over time the champion policy began to show negative movement in PRR as well as smaller gains in SHS and UE. As PRR is our primary KPI, gains in SHS and UE was not worth a drop in PRR. Motivated in part by the negative results of the monitoring experiment, we worked on implementing a more dynamic interpolation scoring approach described by equation 1. We ran an A/B experiment for a second automatically updating MAB based on this approach. This experiment also showed positive movement in SHS and UE for the treatment compared to the control without any drop in PRR, as shown in table 1. This second policy was then deployed to production, replacing the previously deployed MAB policy. Afterwards, we set up another small experiment to monitor the new auto-updating policyâ€™s performance over time. Over several weeks, the policy began to show improvements in PRR and EAS as well as greater gains in SHS and UE as seen in table 2. It is important to note that a reduction in EAS is a positive improvement, as it is reducing the number of escalations to a human agent and practically, reducing cost. Table 2: Dynamic Thompson Sampling Policy Monitoring A/B Results In the î€›rst A/B experiment we ran with our NLB policy, our initial NLB policy showed improvement over the then-current policy, a deep network trained via policy gradient, where both policies had the same input. The greatest improvement was in PRR, which showed a large increase over our previous policy, as shown in table 3. There was also a small improvement in SHS. We also ran an A/B experiment that compared our NLB policy with a greedy rules-based policy. This rules-based policy was the original solution for intent disambiguation in the Microsoft virtual agent. The experiment results showed that while the NLB policy improved PRR, the greedy rules-based policy had a better EAS score. This is likely because the NLB policy can determine if there is no candidate that matches the userâ€™s intent, and thus shows no result more frequently, leading to escalations. After promoting the new NLB policy to be the default policy for production traî€œc, we experimented with reducing the intent disambiguation slate size from four to three. The objective of reducing slate size was to decrease the likelihood of a user getting distracted by suboptimal candidates in the slate. This change was tested in an A/B experiment against a control of the NLB policy with a slate size of four. The experiment results showed very little movement amongst all KPIs. This gave us the conî€›dence to reduce our slate size to three for all production traî€œc, also helping reduce the action space of our intent disambiguation policy. To study the eî€ect of optimizing diî€erent feedback signals, we ran an experiment with a model optimized for EAS in addition to PRR with the hope of lowering EAS. This A/B experiment shows promising results with improvements in both EAS and PRR as seen in table 5. Interestingly, there was also a drop in SHS. We plan to pursue this method of training in the future. Table 4: Neural-Linear Bandit Policy Optimized for Escalation A/B Results To achieve the objective of adapting to changing user traî€œc, we began experimenting with auto-updating policies. For this experiment, the control was the NLB policy with a slate size of three. The treatment auto-updating NLB policy was initialized with the same parameters as the control aside from a higher exploration rate. The higher exploration rate was chosen to counteract the overî€›tting eî€ects of training with less data. Then, we retrained NLBâ€™s banditlayer hourly on new data from the treatment traî€œc. There were promising improvements in PRR over the two-week experiment. With this experiment, we successfully closed the loop for the intent disambiguation scenario using the auto-updating NLB policy. In recent years, there have been many advances in using RL for recommendation systems. Policy gradient-based methods are one such approach to model recommendation systems, such as REINFORCE [6]. This method is more stable in regards to policy convergence compared to Q-learning techniques. Recent work on slate recommendation like SlateQ [9] allows the decomposition of a slateâ€™s total value as a function of the items. In the SlateQ approach, a Q-Network is trained with a user choice model that approximates the click-through rate. One caveat is that SlateQ makes signiî€›cant assumptions about user behavior through the use of a user choice model. CBs are a lightweight single-step RL method that works well with interactive systems. It is the augmented form of a K-armed bandit problem with contextğ‘¥as formalized in [11]. In addition, CBs oî€er key properties that simplify the problem space. For instance, the reward is only associated with the selected action. CBs enable exploration via sampling algorithms like Thompson sampling, epsilon-greedy, or EwS [11,13,20]. Furthermore, CBs have been successfully utilized in recommendation systems. In [12], MABs modeled personalized news article recommendation with LinUCB sampling. CBs have also been used in search engines for ranking recommendations [19]. Inspired by the application of CBs in traditional recommendation systems, we extend these ideas to be applied in intent disambiguation and contextual recommendations for a virtual support agent. For the implementation of our CB solutions, we extended Decision Service [2], a simple interface for any developer to use RL. To learn more details about the implementation of our RL system and practical lessons, refer to [10]. In this work, we describe two applications of CBs in the Microsoft virtual agent in the areas of intent disambiguation and contextual recommendations. We have demonstrated the eî€œcacy of these solutions through online A/B experiments in the Microsoft virtual agent. The NLB solution for intent disambiguation provided 12.45% improvement in the primary KPI of the Microsoft virtual agent, PRR. Our MAB models for contextual recommendations also increased PRR by 2.36% in addition to increasing SHS and UE, while lowering EAS (escalations to a human agent). We have since deployed these solutions to production and impact millions of users each day. For future work, we are exploring how we can apply the insights from MABs and NLBs to multi-domain support bots as well as other products. We are also exploring real-world applications of episodic RL where it is important to do proper credit assignment. For this we plan to rely on a reduction approach [7] which can operate well with the rest of our existing system. We thank Paul Mineiro and John Langford for providing research advice for the CB scenario, Eslam Kamal and the Microsoft Power Virtual Agents Team for their support with the Intent Disambiguation use case, Mary Buck, Sean Quigley, and the Microsoft Digital Customer Support team for their help with both the Contextual Recommendation and Intent Disambiguation scenarios.