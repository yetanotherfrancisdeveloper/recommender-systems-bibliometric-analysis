Thanks to the mature manufacturing techniques, solid-state drives (SSDs) are highly customizable for applications today, which brings opportunities to further improve their storage performance and resource utilization. However, the SSD eî€œciency is usually determined by many hardware parameters, making it hard for developers to manually tune them and determine the optimal SSD conî€›gurations. In this paper, we present an automated learning-based framework, named LearnedSSD, that utilizes both supervised and unsupervised machine learning (ML) techniques to drive the tuning of hardware conî€›gurations for SSDs. LearnedSSD automatically extracts the unique access patterns of a new workload using its block I/O traces, maps the workload to previously workloads for utilizing the learned experiences, and recommends an optimal SSD conî€›guration based on the validated storage performance. LearnedSSD accelerates the development of new SSD devices by automating the hardware parameter conî€›gurations and reducing the manual eî€orts. We develop LearnedSSD with simple yet eî€ective learning algorithms that can run eî€œciently on multi-core CPUs. Given a target storage workload, our evaluation shows that LearnedSSD can always deliver an optimal SSD conî€›guration for the target workload, and this conî€›guration will not hurt the performance of non-target workloads. Flash-based solid-state drive (SSDs) have become the backbone of modern storage infrastructures in various computing platforms, as they oî€er orders-of-magnitude better performance than hardware-disk drives (HDDs), while their cost is approaching to that of HDDs [8,32,36,54,55,62,68]. Thanks to the development of manufacturing and shrinking process technology [1], the industry has been able to rapidly produce SSD devices with diî€erent hardware conî€›gurations. Although SSD devices are becoming highly customizable to meet the ever-increasing demands on storage performance and capacity for new applications [19,55], identifying optimal device conî€›gurations is on the critical path of SSD development. This is because the SSD hardware conî€›gurations are usually determined by the requirements from applications and customers [20,41], and these conî€›gurations involve many components in the storage controller, such as î€ash chip speciî€›cations, chip layout, block/page sizes, device buî€er sizes, and so on. In order to deliver optimal performance for applications in a new generation of storage device development, storage vendors usually use typical application workloads as their benchmarks to aid them to determine the device conî€›gurations. However, an SSD device has hundreds of parameters in its conî€›gurations, and these parameters usually have dependencies (i.e., the update of one parameter may aî€ect other parameters), making it hard for hardware engineers to tune the device conî€›gurations and identify the optimal ones in a short time. This signiî€›cantly hurts the productivity of new SSD device development [20]. Furthermore, there is an increasing demand for customized storage devices from various computing platforms and applications. This is for two reasons. First, computing platforms always wish to deploy the best-î€›t storage devices for their workloads, such that they can achieve the maximum performance. This is especially true for cloud platforms that require highly customized SSDs to support their cloud services, such as Database-as-a-Service [3] and web services [2]. As applications such as cloud storage services are evolving quickly, we need to revolutionize the conî€›guration tuning procedure to shorten the lifecycle of producing new SSDs. Second, our study shows that storage workloads can be categorized with learning algorithms, which provides the evidence that it is feasible to customize storage devices for a speciî€›c workload type (see Figure 2), especially considering the SSD manufacturing techniques become mature today. However, there is still a long-standing gap between application demands and SSD device conî€›gurations. And our community lacks a framework that can instantly transfer application demands into device conî€›gurations of SSDs. In this paper, we develop an automated framework named LearnedSSD, which exploits both supervised and unsupervised machine learning (ML) techniques to drive the hardware conî€›gurations for new SSDs. Given a storage workload, LearnedSSD will recommend an optimal SSD conî€›guration that delivers optimized storage performance. It leverages the linear regression techniques to expose the device conî€›gurations that have the strongest correlation to the storage performance. To present reasonable device conî€›gurations, we formulate diî€erent types of hardware parameters in the SSD, transfer them into the vectors in the ML model, and utilize learning techniques to explore the optimization space and identify the near-best options, with speciî€›ed constraints such as SSD capacity, interfaces (NVMe or SATA), and î€ash memory types. To reduce the execution time of learning an optimal SSD conî€›guration while ensuring the learning accuracy, we develop pruning algorithms to identify the most important hardware parameters in SSDs. LearnedSSD also maintains a conî€›guration database named ConfDB that stores the learned workloads and SSD conî€›gurations. For a new workload, LearnedSSD will extract its features and compare them with the records in the ConfDB using similarity comparison networks. If LearnedSSD identiî€›es a similar workload in its ConfDB, it will recommend the corresponding SSD conî€›guration directly, such that we can utilize the previously learned experience. Otherwise, LearnedSSD will learn a new SSD conî€›guration for the workload, and add them into its ConfDB for future references. As many storage workloads share similar data access patterns and can be categorized into a general type (see Figure 2), LearnedSSD can assist developers to identify the most critical parameters for a type of storage workloads, and recommend an optimal SSD conî€›guration. Our study with LearnedSSD leads to interesting insights. We summarize them into learning rules that can aid developers to prioritize their optimization strategies when producing new SSDs. For instance, (1) not all SSD parameters are equal, the layout arrangement of î€ash chips is an important factor for storage performance; (2) with diî€erent targets and conî€›guration constraints, the tuning prcedure of device conî€›gurations are diî€erent, and for each parameter, its correlation with SSD performance is also diî€erent; (3) not all parameters are sensitive to storage performance, and some of them can be conî€›gured as the same as commodity SSDs today. To evaluate the eî€œciency of LearnedSSD, we implemented our proposed techniques using PyTorch [72], the scikit-learn tool [5], and a production-level SSD simulator MQSim [70]. We perform experiments with a variety of storage traces. Our experimental results show that LearnedSSD delivers an SSD conî€›guration that can achieve 1.28â€“34.61Ã—performance improvement for a target workload, without hurting the performance of non-target workloads, compared to the conî€›gurations speciî€›ed by the released commodity SSDs. We also show that LearnedSSD can learn a new conî€›guration in 37.3 seconds, and î€›nalize an optimal conî€›guration in 121 iterations on average for a workload, with a multi-core server processor. Overall, we make the following contributions: â€¢We present the î€›rst study of SSD hardware parameters and popular storage workloads with learning in mind, and demonstrate the feasibility of applying the learning-based approach for identifying optimal SSD speciî€›cations. â€¢We formulate the tuning problem of SSD device conî€›gurations using a learning-based approach, and develop an Figure 1. Internal architecture of î€ash-based SSDs. automated learning framework that can eî€œciently recommend optimal SSD conî€›gurations for diî€erent workloads. â€¢We summarize a set of learning rules that can facilitate the hardware conî€›gurations and development of new SSDs, based on our study with LearnedSSD. â€¢We study the eî€œciency of LearnedSSD and show its beneî€›ts in comparison with released commodity SSD settings. SSD has proven to be a revolutionary storage technology. It performs much faster than hard-disk drives (HDDs), while its price is reaching to that of HDDs. The rapidly shrinking process and manufacturing technologies have accelerated the SSD device development and enabled their widespread adoption in a variety of computing platforms, such as data centers [4, 22, 55, 68]. 2.1 SSD Architecture We present the internal system architecture of a typical SSD in Figure 1. An SSD consists of î€›ve major components: a set of î€ash memory packages, an SSD controller having embedded processors like ARM, oî€-chip DRAM (SSD DRAM), î€ash controllers, and the I/O interface that includes SATA and NVMe protocols [23,31,42]. The î€ash packages are organized in a hierarchical manner. Their organization not only determines the storage capacity but also aî€ects the storage performance. Each SSD has multiple channels where each channel can receive and process read/write commands independently. Each channel is shared by multiple î€ash packages. Each package has multiple î€ash chips. Within each chip, there are multiple planes. Each plane includes multiple î€ash blocks, and each block consists of multiple î€ash pages. And the page size varies in diî€erent SSDs. In order to manage the î€ash memory, the SSD controller usually implements the Flash Translation Layer (FTL) in its î€›rmware. The FTL was developed for taking care of the intrinsic properties of SSDs. When a free î€ash page is written once, that page is no longer available for future writes until that page is erased. However, erase operation is performed only at a block granularity. To remove the expensive erase operations from the critical path, writes are issued to free pages that have been erased, which is also called out-of-place update. The SSD controller will perform Garbage Collection (GC) later to clean the stale data. As each î€ash block has limited endurance, it is important for blocks to age uniformly (i.e., wear leveling). Modern SSD controllers employ outof-place write, GC, and wear leveling to overcome these aforementioned shortcomings and maintain indirections for handling the address mapping in their FTL. 2.2 SSD Manufacturing and Parameter Tuning According to the interviews with SSD product managers [21] and our discussions with SSD vendors, î€›nalizing the SSD hardware speciî€›cations or parameters is on the critical path in the SSD design. These speciî€›cations are usually determined by the requirements from applications and customers. And they involve the components of the SSD controller as shown in Figure 1. Without these speciî€›cations, the SSD development cannot proceed to the manufacturing stage. With the conî€›rmation from SSD vendors, there are more than a hundred tunable parameters in a typical SSD. To î€›nalize the SSD speciî€›cations, a straightforward approach is to test and proî€›le application workloads with different hardware conî€›gurations. However, this is not scalable as we target diî€erent application workloads. Given an application workload, it is challenging for developers to test all the combinations of the device parameters. And likewise, give a new SSD speciî€›cation, it requires signiî€›cant manual eî€ort to quantify the eî€ectiveness of the selected speciî€›cations. In this work, we use the learning-based approach to automate the SSD hardware conî€›gurations. 2.3 Software-Deî€›ned Solid-State Drive With the increasing demands on storage performance from applications, we have seen a trend that modern storage systems are embracing software-deî€›ned hardware techniques [4, 22]. This allows upper-level applications to achieve maximum performance beneî€›ts and resource eî€œciency with customized storage devices. For instance, the recent development of software-deî€›ned SSDs [19,37] enables platform operators to customize the number of î€ash channels and chips in an SSD, with the cooperation with SSD vendors. This is especially true for both public and private cloud platforms that require highly customized SSDs to support their cloud services, such as database-as-a-service [3], web services [2], web search [37], and batch data analytics [25]. For these applications, their workloads can be highly classiî€›ed. For instance, we use our proposed learning-based workload characterization approach (see the detailed discussion inÂ§3.3) to study the storage traces from a set of popular application workloads (see Table 1). Our experiments demonstrate that each workload type has its unique characteristics, and I/O traces from the same workload type have similarities in their data access pattern, as shown in Figure 2. This provides the evidence showing that it is feasible to customize storage devices for a speciî€›c category of applications. Figure 2. A clustering of popular storage workloads. Unfortunately, our community lacks a framework that can eî€œciently transfer application demands and characteristics into the hardware conî€›gurations of SSDs. 2.4 Learning-Based Parameter Tuning We have seen a disruptive advancement of machine learning (ML) techniques over the past decade. In general, we can categorize machine learning techniques into two types: supervised learning and unsupervised learning. As for supervised learning, it learns a set of rules with labeled datasets and then generalizes these rules to make predications for new inputs. Typical example algorithms include decision trees [26], support vector machines [69], Bayesian networks [16], and artiî€›cial neural networks [14]. Unlike supervised learning, the unsupervised learning can identify unknown patterns based on unlabeled datasets, such as k-means clustering and principal component analysis (PCA) [27]. that the learning-based method is a promising approach to solve system optimization problems. However, none of them investigated their applications in SSD development, and it is unclear how can we utilize learning-based approaches to overcome the challenges of tuning hardware speciî€›cations of SSDs. In this work, we will use both supervised learning and unsupervised learning to develop LearnedSSD. In this section, we î€›rst discuss the goals of LearnedSSD. And we will present the system architecture of LearnedSSD, and its core components respectively. 3.1 Design Goals The high-level goal of LearnedSSD is to enable the automated tuning of hardware conî€›gurations of SSDs for a speciî€›c application workload with learning techniques. Speciî€›cally, LearnedSSD will achieve the following goals: â€¢It can generate an optimal SSD conî€›guration for a target workload, while this hardware conî€›guration has minimal negative impact on other workloads. Figure 3.System overview of LearnedSSD: LearnedSSD î€›rst learns new workload features with clustering (Â§3.3). If it is similar to workloads in existing clusters in the conî€›guration database ConfDB, LearnedSSD will recommend an optimal conî€›guration stored in the database. If not, LearnedSSD will î€›rst conduct (Â§3.4 and Â§3.5)to identify performance-critical parameters of SSDs. After that, LearnedSSD will conduct automated tuning which consists of three modules:performance regressionwith Gaussian process, validation (Â§3.6). â€¢It can identify an optimal SSD conî€›guration in a short time, without introducing much extra computation overheads. â€¢It can scale to support diverse target workloads as well as diî€erent device constraints. In the following, we will discuss how LearnedSSD achieves these goals and overcomes the challenges in both systems building and conî€›guration learning procedure. 3.2 System Overview We develop an automated framework named LearnedSSD, which exploits both supervised and unsupervised learning techniques to drive the speciî€›cations for new SSD devices. Given a storage workload and design constraints, LearnedSSD will recommend an optimal SSD conî€›guration that can deliver optimized storage performance shortly. To be speciî€›c, we leverage linear regression techniques to expose the device speciî€›cations that have the strongest correlation with the storage eî€œciency. To present reasonable device speciî€›cations, we formulate diî€erent types of hardware parameters in the SSD, transfer them into the vectors of the learning models, and learn the optimal options with speciî€›ed constraints, such as storage capacity and SSD interfaces (NVMe or SATA). To reduce the execution time of learning an optimal SSD conî€›guration while ensuring the learning accuracy, we develop pruning algorithms to identify the most important hardware parameters in SSDs. We will also maintain a conî€›guration database named ConfDB to store the learned workloads and the corresponding SSD speciî€›cations. Therefore, we can utilize the learned experiences for new workloads in LearnedSSD. For a new workload, LearnedSSD will extract its features and compare them with the records in the conî€›guration database ConfDB, as shown in Figure 3. If LearnedSSD identiî€›es similar workloads, it will either recommend a known optimal speciî€›cation, or retune the SSD conî€›gurations with the recorded SSD speciî€›cations, such that we can utilize previous learned experiences. Otherwise, LearnedSSD will learn new SSD speciî€›cations and add the learned conî€›gurations into ConfDB for future references. As many storage workloads can be categorized into a general type (see Figure 2), LearnedSSD can also assist developers to identify the most critical parameters for a type of storage workloads. It is worth noting that LearnedSSD will ensure the recommended SSD speciî€›cations will not hurt the storage performance for other generic workloads. 3.3 Learning-based Workload Clustering Unlike traditional ways of using the read/write ratio, and I/O patterns (e.g., sequential and random read/write) to categorize workloads, which cannot capture the whole picture of workload characteristics, we develop a learning-based clustering approach based on block I/O traces. We chose block I/O traces to learn the characteristics of storage workloads, because this approach does not have system dependencies and not require application semantics. To develop the learning-based workload clustering, we î€›rst partition each I/O trace into small windows. According to our study of diverse workloads, we use 3,000 trace entries in each window by default. This is because fewer entries may lose the unique data access patterns of the trace, and more entries would generate less valid data points in a cluster, they both could hurt the accuracy of the workload clustering. The trace information used to conduct the workload characterization include I/O timestamp, I/O size, device number, block address, and operation types. We convert each window of the I/O traces into a data point, and use Principal Component Analysis [7] to transfer the data points into two dimensions. After that, we use k-means to cluster these data points. After we cluster all the data points of a new workload, we will calculate the distance between the center of the examined data points and the center of an existing cluster. If the distance is below a threshold, we make the conclusion that the majority of the examined data points fall into the existing workload cluster. In other words, the new workload belongs to this cluster. If LearnedSSD cannot identify a similar cluster, LearnedSSD will create a new cluster for the new workload. Our experimental results, as demonstrated in Figure 2, show that our learning-based workload clustering can successfully identify a cluster of storage workloads that belong to the same or similar workload type. To further verify the eî€ectiveness of the learning-based approach for workload clustering, we divide the same workload into the training and validation datasets, and evaluate the clustering results for each workload as listed in Table 1. We observe that 95% of their data points fall into the same workload cluster on average. This provides the evidence showing that our proposed learning-based approach is suî€œcient to identify an appropriate cluster for new workloads. It also drives the LearnedSSD design by oî€ering the insight that it is feasible to develop an SSD which can deliver optimized performance for a category of application workloads. 3.4 Transfer SSD Speciî€›cations into ML Parameters We now discuss how we can transfer the tuning problem of SSD speciî€›cations for a workload category into a ML problem, such that we can automate the tuning procedure with high accuracy. To address this problem, we î€›rst need to formulate the SSD speciî€›cations into ML parameters. However, this is not easy, since we have to ensure the parameter formulation does not lose the meaningful semantic of a speciî€›c SSD hardware parameter, and the ML parameters should be able to represent the characteristics of diî€erent hardware speciî€›cations and their correlations. To model the SSD speciî€›cations via ML parameters, we formulate them into three major parts in the ML model: (1) the performance metrics used as the optimization targets for SSDs; (2) SSD hardware conî€›gurations that can be vectorized as parameters in a ML model; and (3) the conî€›guration constraints (e.g., the SSD capacity) that bound the optimization space of the ML model. We describe each of them as follows. Performance metrics used in the ML model.As for storage performance, LearnedSSD focuses on the storage latency and throughput. To quantify whether a SSD conî€›guration delivers optimized performance or not, we use reference performance as the baseline (e.g., the latency and throughput obtained from a commercial SSDâ€™s conî€›gurations), and the relative performance improvements as the evaluation metrics. We set the performance optimization goal as follows: ğºğ‘œğ‘ğ‘™ (ğ‘ğ‘œğ‘›ğ‘“ ) = (1 âˆ’ ğ›¼) Ã— ğ‘™ğ‘œğ‘”(ğ¿ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦) whereğ›¼is a tunable coeî€œcient factor for balancing the latency and throughput at a proper scale. We setğ›¼ = 0.9 by default in LearnedSSD, based on our study of diî€erent coeî€œcients. In our evaluation (seeÂ§4.5), we will examine the impact of theğ›¼on the learning eî€œciency. As discussed, the ğ¿ğ‘ğ‘¡ğ‘’ğ‘›ğ‘ğ‘¦andğ‘‡â„ğ‘Ÿğ‘œğ‘¢ğ‘”â„ğ‘ğ‘¢ğ‘¡will be given as the reference performance in Formula 1 in the ML model. SSD hardware speciî€›cations as ML parameters.To represent SSD hardware speciî€›cations in the ML models, we transfer them into four types of parameters and use diî€erent ways to set their values. They include continuous, discrete, boolean, and categorical parameters. â€¢Continuous parameter: typical examples of continuous parameter include over-provisioning ratio for GC, and the number of î€ash channels. To set the value of this type of parameters as we run the ML model, we identify a range of possible values it could take in advance, and divide the range uniformly into N small pieces. Therefore, LearnedSSD can take N endpoints as the possible values. For each continuous parameter, we set the range to cover all common values in commodity SSDs for ensuring the learned SSD speciî€›cations are practical. â€¢Discrete parameter: typical examples of discrete parameter include the SSD DRAM capacity, I/O queue depth, and page size. We select all their possible values and store them in a list. Therefore, we can use the list index in the vector of the ML model. Their possible values also cover all the common values. Each discrete parameter follows diî€erent rules as LearnedSSD sets the value at runtime. For instance, the SSD DRAM capacity will take the power of 2 as we increase it; and there are only î€›ve PCIe bandwidth settings, according to the PCIe protocol. ML model to indicate whether a function or feature (e.g., statistic wear leveling, and greedy GC) will be enabled in the SSD or not. With the 0-1 boolean parameter, 0/1 means function enabled/disabled respectively. â€¢Categorical parameter: As for the categorical parameter, we convert it to the dummy variable [28]. For example, there are 16 possible values for the plane allocation scheme, we create a list with the length of 16. When LearnedSSD selects one scheme, it will set the value of the corresponding index of the list to 1, and others to 0. Conî€›guration constraints.LearnedSSD allows users to specify the conî€›guration constraints for its conî€›guration tuning. Typical examples include the SSD storage capacity and the interface (e.g., NVMe or SATA) supported by the SSD for interacting with the host machine. When LearnedSSD sets diî€erent values for its ML parameters, it will simply abandon those conî€›gurations that violate the speciî€›ed constraints. And LearnedSSD will always check the constraints when it learns a new conî€›guration. Note that LearnedSSD mainly works on the tunable parameters in SSD speciî€›cations. For the lower-level circuit-relevant speciî€›cations that are strictly limited by the hardware, LearnedSSD does not cover them in its tuning model. 3.5 Learning-based Parameter Pruning After we transfer the SSD speciî€›cations into ML parameters, we can start to train the model. However, modern SSDs usually have hundreds of hardware speciî€›cations or parameters. Although ML models today can handle a large set of parameters, it is still desirable to develop eî€œcient and lightweight models for reducing both training and learning time as well as saving computation cycles. For example, we develop a model with 64 SSD parameters, it takes 30.7 hours to converge the model on a modern multi-core server (see the experimental setup inÂ§4.1). Moreover, we î€›nd that not all SSD parameters are strongly correlated to the storage performance, and it is not needed to include these insensitive parameters in the learning model. To this end, we propose a parameter pruning approach to identify the impactful parameters that could aî€ect the storage performance of SSDs. However, we have to overcome two major challenges within the parameter pruning. First, we need an accurate measurement method to examine the importance of a parameter. This is challenging as SSD parameters usually have dependencies. As we tune a single parameter while keeping the values of other parameters î€›xed, this may violate the conî€›guration constraints. For example, increasing the number of î€ash channels could violate the constraint of the SSD capacity. On the other hand, as we tune a single parameter while updating the values of other parameters accordingly for meeting the conî€›guration constraints, we cannot accurately determine which parameters aî€ect the storage performance signiî€›cantly. Second, removing some of the SSD parameters may hurt the overall accuracy of the learning model. And it is challenging to quantify how each SSD parameter could aî€ect the learning accuracy. To address these challenges, we conduct the parameter pruning procedure with two stages. Coarse-grained parameter pruning.We î€›rst adopt a coarse-grained pruning method that adjusts the values of continuous and discrete numerical parameters with large stride length. But we ensure the values of these parameters are conî€›gured in a reasonable range, and still satisfy the conî€›guration constraints. At this stage, we eliminate the parameters that do not have much impact on the storage performance, no matter how we change their values. As shown in Figure 4, we increase the values of the 23 numerical parameters of SSDs from their baseline setting to 16Ã—, and measure the storage performance with diî€erent workloads. We observe that some parameters do not aî€ect the storage performance signiî€›cantly (those î€at lines in Figure 4), we call them as insensitive parameters in this paper. We also î€›nd that these insensitive parameters will be diî€erent for diî€erent storage workload types, therefore, LearnedSSD will conduct the coarse-grained parameter pruning for each workload type and identify the corresponding insensitive parameters. In general, we identify 9 insensitive parameters, such as Page_Metadata_Size and SATA_Processing_Delay, according to our study (see Figure 4). Note that these insensitive parameters would be updated for a new workload type. Fine-grained parameter pruning.After eliminating the insensitive parameters with the coarse-grained pruning, we continue the parameter pruning with a î€›ne-grained approach. Figure 5. A study of î€›ne-grained parameter pruning with linear regression for diî€erent storage workloads. It employs the linear regression technique LASSO [6] to identify the linear correlations between the SSD parameters and performance. Following the discussion inÂ§3.4, we set a regression space by maintaining the SSD capacity constraint, as we vary the values of SSD parameters. Since the SSD capacity is mainly determined by the parameters related to the chip layout, such as î€ash page size, the number of î€ash channels, and the î€ash block size, we î€›rst set the values for these parameters. After that, we vary the values of other parameters, and measure the regression coeî€œcient for each SSD parameter. A higher regression coeî€œcient score of a parameter means it has a closer correlation with the SSD performance. Based on the reported coeî€œcient scores, we abandon the parameter whose score is below a threshold (0.01 by default in LearnedSSD). Therefore, we can focus on the parameter tuning for the important ones. As shown in Figure 5, we remove the insensitive parameters identiî€›ed by the coarse-grained parameter tuning, and also include the boolean and categorical parameters in the î€›ne-grained parameter tuning. Given the threshold of 0.01 for the regression coeî€œcient score, we can eliminate more insensitive parameters for diî€erent workloads. Observations.The learning-based parameter pruning of LearnedSSD can not only help us to eliminate the insensitive parameters, but also oî€er interesting insights that would beneî€›t SSD development. Speciî€›cally, we observe that: (1) Diî€erent workloads have diî€erent parameter sensitivity. For example, the performance of latency-critical workloads like Advertisement and WebSearch is not sensitive to the page size and over-provisioning ratio of SSDs, as they are read intensive. In contrast, the I/O-intensive workloads such as key-value stores and LiveMaps are sensitive to the î€ash page size. (2) Not all parameters have linear correlation with SSD performance, which generates diî€œculties for manual tuning, and further motivates us to utilize ML techniques to pinpoint the optimal SSD speciî€›cations. 3.6 Automated Tuning of SSD Conî€›gurations After the SSD parameter pruning, we now develop the ML model to learn the optimal SSD speciî€›cations for various workloads. We present the system workî€ow of LearnedSSD in Figure 6. Given a workload, LearnedSSD will î€›rst use the conî€›gurations stored in the ConfDB as the initial conî€›guration set, and leverage both Gaussian process regression (GPR) and discrete stochastic gradient descent (SGD) algorithms to learn diî€erent conî€›gurations. For each learned conî€›guration, LearnedSSD will use a cycle-accurate SSD simulator to validate its performance until the model converges (i.e., the optimal SSD conî€›guration is identiî€›ed). In the following, we discuss each step of the workî€ow in details. Identify the initial conî€›guration set for a new workload.For a new workload, LearnedSSD will use the learningbased workload clustering as discussed inÂ§3.3 to cluster the workload, and look up the learned conî€›gurations for the corresponding workload cluster in ConfDB (). LearnedSSD will use these conî€›gurations and their delivered performance to initialize the ML model. However, if there are insuî€œcient conî€›gurations in ConfDB (e.g., the ConfDB is empty), LearnedSSD will use a conî€›guration from existing commodity SSDs and its measured performance to initialize the model ( 0 ). Quantify the conî€›gurations with a uniî€›ed grading mechanism.LearnedSSD will start the conî€›guration tuning procedure based on the initial conî€›gurations. In order to check the eî€ectiveness of these conî€›gurations, LearnedSSD develops a grading mechanism () with the goal of unifying diî€erent performance metrics (seeÂ§3.4). To achieve the maximal optimizations for both data access latency and throughput, LearnedSSD uses the Formula 1 as the goal. To ensure the learned conî€›guration for a target workload does not hurt the performance of other workloads, LearnedSSD introduces a new factorğ›½named penalty balance in its grading. Therefore, we deî€›ne the performance grade for a workload as follows: ğºğ‘Ÿğ‘ğ‘‘ğ‘’ (ğ‘ğ‘œğ‘›ğ‘“ ) = (1 âˆ’ ğ›½) Ã— ğºğ‘œğ‘ğ‘™ (ğ‘ğ‘œğ‘›ğ‘“ ) where Goal (conf) is deî€›ned in Formula 1, andğ›½ = 0.9by default based on our study (see Figure 10 in our evaluation). Search optimal conî€›gurations with the stochastic gradient descent technique.With the initial SSD conî€›gurations and their grades, LearnedSSD will use SGD to search an optimal conî€›guration (). Speciî€›cally, LearnedSSD î€›rst identiî€›es the top three best conî€›gurations (i.e., the conî€›gurations whose grades rank at the top) from the learned conî€›gurations, and randomly selects one as the search root. In the gradient descent process, LearnedSSD expands the search space from the root by checking all the adjacent conî€›gurations under conî€›guration constraints (e.g., SSD capacity) in each searching iteration. Given the capacity constraint, LearnedSSD tunes one or two relevant parameters at one time, and then keep those parameters that satisfy the constraint. For other parameters, LearnedSSD will adjust their values back and forth in the search space. Once we î€›nalize the parameters for one conî€›guration, LearnedSSD will use the GPR model to identify the conî€›guration with the best predicted performance grade (). If its performance grade is better than the search root, LearnedSSD will set this conî€›guration as the new search root and continue the next search iteration. The main challenge with the SGD procedure () is to balance the learning accuracy and exploitation overhead. Since there is no guarantee that the initial conî€›guration set will cover the entire search space, LearnedSSD has to gradually expand its search space to ensure it can identify the optimal ones. However, this may cause a search space explosion. To address this issue, we introduce a heuristic exploit factor, which is the minimum Manhattan distance [77] between the conî€›guration being exploited and the existing learned conî€›gurations. We also set a threshold for the number of search iterations (20 iterations by default in LearnedSSD) in the conî€›guration exploration. Predict the grades of explored conî€›gurations.As discussed brieî€y in previous descriptions, LearnedSSD uses GPR [59] to predict the grades for new conî€›gurations (). This is for three major reasons. First, GPR can provide nearly the same performance as the deep neural networks, especially in the modeling of searching optimal conî€›gurations and making recommendations. Second, it oî€ers excellent trade-oî€s between the explorations of new knowledge and learned knowledge [44,67]. Third, GPR provides conî€›dence intervals with low computation overhead by default [9]. In LearnedSSD, we build a new GPR model by specifying its mean function and covariance function. The mean function is conî€›gured as trainable, as the mean of the performance metrics is unknown before the learning in LearnedSSD. We use the covariance function to represent the correlation between two adjacent points in the model, and adopt both radial basis function (RBF) kernel [75] and rational quadratic kernel [76] as the regression covariance. We also add a white kernel [47] for random noise simulation. Validate the explored conî€›gurations.The learning procedure of a new conî€›guration will terminate by checking two conditions: (1) no conî€›guration is better than the current root conî€›guration in the search space; or (2) the search exceeds the threshold for the number of iterations. After that, LearnedSSD will use a cycle-accurate SSD simulator to validate the eî€œciency of the learned conî€›gurations (). LearnedSSD will run all the available workloads in the ConfDB with the SSD simulator, and report the grade for the tested conî€›guration. Before the validation, LearnedSSD will warm up the SSD simulator by running diverse workload traces randomly. In the validation, LearnedSSD maintains a set of optimal conî€›gurations whose grades rank at the top of all the learned conî€›gurations. After a certain number of search iterations, if the overall grade of this conî€›guration set is not signiî€›cantly updated, the learning procedure will be converged. Otherwise, LearnedSSD will update the ConfDB with the new learned conî€›guration and start another search iteration until the learning procedure is converged. 3.7 Implementation Details We implement the LearnedSSD framework with Python programming language. LearnedSSD supports the storage traces collected with blktrace which is available on a majority of computing systems. It uses Principal Component Analysis and k-means algorithms in the learning-based workload clustering. LearnedSSD utilizes the Sklearn library [5] to develop the statistic learning model that supports both SGD and GPR algorithms. LearnedSSD adopts the MQSim [70] as the backend SSD simulator to validate the learned conî€›gurations. Note that LearnedSSD is also compatible with other SSD simulators, therefore, SSD vendors can replace the open-sourced MQSim simulator with their own simulators. LearnedSSD implements the ConfDB with the key-value store LevelDB, in which the key is the workload cluster ID, and the value includes the corresponding SSD conî€›gurations and their performance obtained from the SSD simulator. The value is organized in JSON format. LearnedSSD provides a simple interface set_cons (capacity, interface, î€ash_type) to enable end users to specify their conî€›guration constraints SSD capacity, interface (i.e., NVMe or SATA), and the î€ash type (i.e., SLC, MLC, and TLC). We will open source LearnedSSD to beneî€›t future study. 3.8 Discussion and Future Work In this work, LearnedSSD mainly works under the conî€›gurations constraints that include storage cpacity, interfaces, and î€ash types. However, its learning techniques and workî€ow Table 1. Application workloads used in our evaluation. are also suitable for identifying the optimal SSD conî€›gurations with other constraints, such as the economic cost and energy eî€œciency of the SSD. Unfortunately, a majority of SSD vendors are not willing to open source the speciî€›cations of the cost and energy consumption of each hardware component of the SSD. Therefore, we do not study these conî€›guration constraints in this work. We wish to explore these dimensions as the future work. Our evaluation shows that: (1) LearnedSSD can learn optimal SSD conî€›gurations for a given workload, and the learned conî€›gurations can deliver improved storage performance, compared with commodity SSD conî€›gurations (Â§4.2); (2) LearnedSSD can instantly learn an optimal conî€›guration with low performance overheads (Â§4.3); (3) LearnedSSD works eî€œciently under diî€erent conî€›guration constraints (Â§4.4); and (4) LearnedSSD itself is also tunable for satisfying various performance requirements from end users (Â§4.5). 4.1 Experimental Setup In our evaluation, we use 7 diî€erent workload categories as shown in Table 1. These workloads cover various workload types that include key-value stores, databases, map services, advertisement recommendations, batch data analytics, web search services, and cloud storage [40]. Each workload type includes multiple storage traces. All the storage traces are either collected from university servers or enterprise servers. We run the LearnedSSD framework on a server, which is conî€›gured with 48 Intel Xeon CPU (E5-2687W v4) processors running at 3.0GHz, 96GB DRAM, and 4TB SSD. Since LearnedSSD uses the statistic learning models, it does not require GPUs in its learning procedure. We use the conî€›gurations of Intel 590 SSD, Samsung 850 PRO SSD and Z-SSD as the baselines, and compare the learned conî€›gurations with them to evaluate the eî€œciency of LearnedSSD. 4.2 Eî€œciency of Learned Conî€›gurations We î€›rst evaluate the eî€œciency of the learned conî€›gurations with LearnedSSD. We use the Intel 590 SSD as the reference. We set the conî€›guration constraints as [SSD capacity = 1TB, interface = NVMe, î€ash typ e = MLC]. With its conî€›guration in the SSD simulator, we run all the workloads in Table 1 to measure their performances. After that, we use the reference conî€›guration and the measured performances to initialize ConfDB. And then, we feed the storage traces from diî€erent Table 3.Learned conî€›gurations for diî€erent workloads. AD: workload types into LearnedSSD to learn new conî€›gurations. Once the learning converges, LearnedSSD will report the best SSD conî€›guration for each workload. We show the performance of learned conî€›gurations in Table 2. In comparison with Intel 590 SSD, the learned SSD conî€›gurations can reduces the storage latency by 1.28â€“34.61Ã— for the target workload, while decreasing the storage latency by 8.38Ã— on average for non-target workloads. The learned conî€›gurations can also improve the storage throughput by up to 5.25Ã—for bandwidth-intensive applications such as the database and cloud storage workloads, without hurting the throughput of other workloads. To further understand the learned conî€›gurations, we list the critical parameters of the learned conî€›gurations in Table 3, in comparison with the reference conî€›guration of Intel 590 SSD. As we can see, for diî€erent target workloads, LearnedSSD will learn diî€erent values for these parameters, although some workload types share the same conî€›gurations. To achieve improved SSD performance, LearnedSSD increases the number of channels for most of the workloads, while adjusting the î€ash chip layout (e.g., the number of chips per channel) accordingly to satisfy the SSD capacity constraint. LearnedSSD will also adjusts the page allocation scheme [71] to optimize the data layout for diî€erent workloads. Similarly, LearnedSSD tunes the SSD DRAM capacity and I/O queue depth according to workload characteristics. Our work LearnedSSD proves that it is feasible to utilize ML techniques to automate the tuning of SSD conî€›gurations. 4.3 Learning Time of LearnedSSD We now examine the learning time of LearnedSSD. We report the numbers for diî€erent target workloads in Figure 7. Figure 7.Learning time of LearnedSSD for diî€erent workloads. Table 4. Overhead sources of LearnedSSD. LearnedSSD can learn an optimal conî€›guration in 6.65â€“23.70 hours. And it will incur 121 search iterations on average to pinpoint the optimal conî€›guration. To further understand the overhead source of LearnedSSD, we proî€›le the execution time of its critical components on the multi-core server as described inÂ§4.1, and show the results in Table 4. Our proî€›ling results demonstrate that LearnedSSD can î€›nish each search iteration within only 37.3 seconds. And the major performance overhead of LearnedSSD comes from the simulator validation, as we need to warm up the simulator before each validation. However, LearnedSSD only needs to validate the best conî€›guration (selected based on the predicted grade with GPR) in each search iteration. 4.4 Sensitivity to Conî€›guration Constraints We now evaluate how LearnedSSD performs as we change the conî€›guration constraints that include the î€ash types and device interface. To evaluate the sensitivity to î€ash types, we use Samsung Z-SSD, which is a NVMe SLC SSD, as the reference conî€›guration. To evaluate the sensitivity to device interface, we use Samsung 850 PRO, which is a SATA MLC SSD, as the reference conî€›guration. We present the performance of the learned conî€›gurations for diî€erent workloads in Table 5 and Table 6 respectively. Table 5 shows that the conî€›gurations learned by LearnedSSD can reduce the storage latency by 3.61â€“19.28Ã—, and improve the storage throughput by up to 8.00Ã—for NVMe SLC SSDs for the target workload, compared to the Samsung Z-SSD. Figure 8. The learning procedure of LearnedSSD for NVMe and SATA SSDs, as we target batch data analytics workload. Table 6 demonstrates that our learned conî€›gurations can deliver up to 8.41Ã—latency reduction and 2.63Ã—throughput improvement for SATA SSDs for the target workload, in comparison with Samsung 850 PRO. In order to understand how LearnedSSD tunes the device parameters, we record its learning procedure and conî€›guration grades at runtime. As shown in Figure 8, we present the proî€›ling results of learning the optimal conî€›gurations for the target workload BatachDataAnalytics for NVMe and SATA SSDs, respectively. The top subî€›gures in Figure 8 demonstrate how the conî€›guration grade will be updated after each search iteration. As discussed inÂ§3.6, the learning procedure will converge when the grade of the conî€›gurations becomes stable. We show the learning procedure of the critical SSD parameters in the below subî€›gures in Figure 8. We observe that, with diî€erent conî€›guration constraints (NVMe vs. SATA), (1) the learning procedure will be different; (2) for each parameter, its correlation with the SSD performance is also diî€erent, making it impossible for developers to manually tune them; (3) not all parameters are equal, some parameters are insensitive to storage performance. LearnedSSD framework can help developers identify such parameters for diî€erent workloads under diî€erent conî€›guration constraints, which could improve the productivity of SSD development. 4.5 Performance Impact of the Balance Coeî€œcient As discussed inÂ§3.4 andÂ§3.6, LearnedSSD uses the coeî€œcient factorğ›¼(Formula 1) to balance the storage latency and throughput in the learning procedure, and deî€›nes the coefî€›cient factorğ›½(Formula 2) to balance the penalty (weight) between the target workload and non-target workloads. Both of them are tunable in LearnedSSD, which allows end users Figure 9.Performance impact of the coeî€œcient factor for balancing the latency and throughput for a target workload. to adjust them per their needs. In this part, we evaluate their impact on storage performance. We vary their values from 0.01 to 0.99, and measure the performance of the learned conî€›gurations for the three representative workloads database, key-value store, and LiveMaps. As we examine each value ofğ›¼andğ›½, we reset the ML model and initialize the ConfDB. We show the experimental results in Figure 9 and Figure 10. With the coeî€œcient factorğ›¼, our goal is to achieve the maximum improvement for both latency and throughput. In Figure 9, as we increase the value ofğ›¼from 0.01 to 0.3, the latency of the target workload is dramatically improved, however, its throughput is lower than the reference conî€›guration. As we further increase its value to 0.9, we can achieve both improved latency and throughput for all the three target workloads. Thus, LearnedSSD sets ğ›¼ = 0.9 by default. With the coeî€œcient factorğ›½, our goal is to achieve the maximum performance improvement for both the target workload and non-target workloads. As LearnedSSD learns new conî€›gurations, it is usually easy to achieve the improved performance for the target workload. However, this may decrease the performance for non-target workloads, which could impede the widespread adoption of the learned conî€›gurations. As we vary the value ofğ›½, we observe that there is such a sweet spot (ğ›½ = 0.9) that can delivers maximal performance improvement for the target workload, while having minimal negative impact on the non-target workloads. SSD Performance Optimization.SSDs has been widely used in modern storage systems to meet the I/O performance and storage capacity requirements of data-intensive applications, such as databases, cloud storage, web search, and big-data analytics [35,46,50,63,64,81]. Although these applications have high demands on I/O performance and their workload have unique data access patterns [17,34,82], they normally employ generic SSD devices [13,57,65], which Figure 10.Performance impact of the coeî€œcient factor for balancing the performance between the target workload and non-target workloads. causes suboptimal performance and resource eî€œciency. In this paper, we develop LearnedSSD to facilitate the development of customized SSD devices for applications with improved performance. Recently, researchers proposed the software-deî€›ned î€ash and open-channel SSDs to enable applications to develop their own storage stack for improved storage utilization and performance isolation [36,48,56]. They show that there is an increasing demand on softwaredeî€›ned storage. However, there is a longstanding gap between the application demands and device speciî€›cations. We develop LearnedSSD with the goal of bridging this gap. Machine Learning for Systems.Most recently, researchers have started to leverage machine learning techniques to solve system optimization problems, such as the task scheduling [58,74,83], cluster resource management [12,18,24, 52,79], performance optimizations [33,45,51,84], data management [10,43,49,73], and others [53,78]. However, few studies conduct a systematic investigation of applying the learning techniques to develop SSD devices. To the best of our knowledge, LearnedSSD is the î€›rst work that utilize the learning techniques to enable the automated tuning of SSD speciî€›cations. We believe it will not only beneî€›t SSD vendors and manufacturers but also platform operators such as those for cloud services and data centers. SSD Device Development.Along with the architecture innovation, the industry community has developed mature manufacturing techniques and fabrication process to produce new storage devices, such as Z-SSD [61], Optane SSD [39], ZNS SSDs [60,85]. As the industrial revolution has moved into the fourth/î€›fth generation (Industry 4.0/5.0) powered by the artiî€›cial intelligence [29,38], storage devices should also become highly customizable for applications. Unfortunately, we are lacking an eî€ective framework that can transfer application demands into storage device development. In this work, we focus on building a learning-based framework to address a critical challenge with the SSD development â€“ how to eî€œciently identify the optimal SSD speciî€›cations for meeting the needs from target applications under constraints. We build a learning-based framework named LearnedSSD for enabling the automated tuning of SSD speciî€›cations. Given a storage workload, LearnedSSD can eî€œciently learn an optimal SSD conî€›guration that delivers the maximum performance improvement even under diî€erent conî€›guration constraints. LearnedSSD can signiî€›cantly reduce the manual eî€orts in the SSD device development. Our experiments show that our learned SSD conî€›gurations can signiî€›cantly improve the storage performance for a target workload, without hurting the performance of non-target workloads.