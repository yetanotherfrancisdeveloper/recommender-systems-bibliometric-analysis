How to extract meaningful information in user historical behavior plays a crucial role in sequential recommendation. Userâ€™s behavior sequence often contains multiple conceptually distinct items that belong to different item groups and the number of the item groups is changing over time. It is necessary to learn a dynamic group of representations according the item groups in a user historical behavior. However, current works only learns a predeï¬ned and ï¬xed number representations which includes single representation methods and multi representations methods from the user context that could lead to suboptimal recommendation quality. In this paper we propose an Adasplit model that can automatically and adaptively generates a dynamic group of representations from the user behavior accordingly. To be speciï¬c, AutoRep is composed of an informative representation construct (IRC) module and a dynamic representations construct (DRC) module. The IRC module learns the overall sequential characteristics of user behavior with a bi-directional architecture transformer. The DRC module dynamically allocate the item in the user behavior into different item groups and form a dynamic group of representations in a differentiable method. We formalize the hard allocation problem in the form of Markov Decision Process(MDP), and sample an action from Allocation Agentğœ‹with a Group Controller Mechanism for each item to determine which item group it belongs to. Such design improves the modelâ€™s recommendation performance. We evaluate the proposed model on ï¬ve benchmark datasets. The results show that AutoRep outperforms representative baselines. Further ablation study has been conducted to deepen our understandings of AutoRep, including the proposed module IRC and DRC. ACM Reference Format: Weiqi Shao, Xu Chen, Jiashu Zhao,Long Xia, Dawei Yin. 2022. User Behavior Understanding In Real World Settings. In Proceedings of ACM Conference (Conferenceâ€™17). ACM, New York, NY, USA, 9 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn Recommender system has extensively permeated into peopleâ€™s daily life, ranging from the ï¬elds of e-commerce [14,16,28], education [15,22] to the domains of health-caring [2,8,34], and entertainment [1,19,23]. In the past few years, many promising recommender models have been proposed, among which sequential recommendation is a type widely studied algorithm. In sequential recommendation, the item that a user may interact with is estimated based on their historical behaviors. Comparing with general recommender models like matrix factorization [4,9,31], sequential recommendation is advanced in the capability of capturing item correlations, which provides more informative signals to predict the next behavior. In order to capture item correlations, recent years have witnessed quite a lot of effective sequential recommender models. Essentially, these methods are built by introducing different assumptions on user successive behaviors. For example, FPMC [21] believes that the current user behavior is only inï¬‚uenced by the most recent action, and thus regards user behaviors as a Markov chain. GRU4Rec [10] assumes that all the history behaviors are meaningful for the next item prediction, and therefore leverages recurrent neural network for summarizing all previous behaviors. Despite effectiveness, existing algorithms usually assume user behaviors to be coherent, and leverage uniï¬ed architecture to predict the next item. However, this assumption may not hold in practice due to the diverse and complex user preferences. As exampled in Figure 1, user A is an electronic enthusiast and also likes sports, thus she purchased both digital and sports products. In this scenario, if the next item is a pair of running shoes, then the user preference on digital products can be less important or even bring noises. For more clean and focused estimation, many studies [3,5,6,13,24] propose to disentangle the history information into multi interest subsequences and form multi interest representation for users. However, these models suffer from many signiï¬cant limitations: (1) To begin with, they mostly assume ï¬xed number of interest sub-sequences, which contradicts with the diverse user personalities. In Figure 5, the history information of user A contains two categories of products (i.e., digital and sports). While the preference of user B spans over three domains including books, baby products and clothes. Even for the same user, the number of interest may also vary with different contexts. When a user is aimless, she may explore more diverse products. While once her intent is determined, she may only interact with relevant and consistent items. (2) Then, existing models fail to consider the evolving nature of user preference. Also see the example in Figure 1, in the beginning, user A purchased a phone, which triggered the following interaction with the phone case. Then her interest on digital products ended, and she began to care more about sports items, and purchased sport shirt, sweatpants and smart-watch. Figure 1: Illustration of user diverse preferences, where different users may have various personalities, and thus has different number of interests in their behavior sequence. If we see the smart-watch independently, it can be allocated to the digital products, while by considering the above user evolving preference, we know that the reason of purchasing smart-watch is more likely for sports, such as timing, counting calories, among others. Previous models do not explicitly model such evolving preference, which may lead to inaccurate item allocation, and impact the ï¬nal recommendation performance. In order to solve the above problems, in this paper, we propose a novel sequential recommender model toadaptively disentangle user preference by considering its evolving nature (called AdaSplit for short). The key of our idea is to design a behavior allocator, which is able to automatically determine the number of sub-sequences based on user evolving preference. To achieve this goal, we regard the decomposition of user history behaviors as a Markov decision process (MDP). At each step, the agent selects which sub-sequence the current item should be allocated to. First, we put the history behaviors into a bi-directional architecture transformer [26] to learn the overall sequential characteristics of user behavior and equip the item with the global information which makes the next item allocation task efï¬cient. Whatâ€™s more, in order to adaptively determine the number of sub-sequences, we design a special action called â€œcreating a new sub-sequenceâ€. In the policy rolling out process, the sub-sequence number is gradually increased until reaching a stable value, where all the user diverse preferences have been explored. The reward is associated with the similarity between the target item and candidate sub-sequences, and also designed to encourage orthogonality between the generated sub-sequences. To avoid of generating too much sub-sequences, we introduce a curriculum reward, which adaptively penalizes the action of generating new sub-sequence. In a summary, the main contributions of this paper can be concluded as follows: â€¢We proposed to build sequential recommender models by adaptively disentangling user preferences, which, to the best of our knowledge, is the ï¬rst time in the recommendation domain. â€¢To achieve the above idea, we design a reinforcement learning (RL) model to allocate user behaviors and adaptively create new subsequences, which captures the evolving nature of user preference. â€¢We conduct extensive experiments on four datasets with several benchmarks to demonstrate the effectiveness of our model, and for promoting this research direction, we have released our project at https://no-one-xxx.github.io/Adasplit/. Sequential recommendation predicts the current user behavior by taking the history information into consideration. Formally, we are provided with a user setUand an item setV. The interactions of each userğ‘¢ âˆˆ Uare chronologically organized into a sequence (ğ‘£, ğ‘£, ...ğ‘£), which will be recurrently separated into training samples for model optimization. For the sequence(ğ‘£, ğ‘£, ...ğ‘£), the generated samples are{[(ğ‘¢, ğ‘£, ğ‘£, ...ğ‘£), ğ‘£]|ğ‘— âˆˆ [1, ğ‘™âˆ’ 1]}, where, in each sample,(ğ‘£, ğ‘£, ...ğ‘£)is the history information, andğ‘£ is the target item to be predicted. We denote all the training samples for different users byS = {[(ğ‘¢, ğ‘£, ğ‘£, ...ğ‘£), ğ‘£]}. Given {U, V, S}, we have to learn a modelğ‘“, which can accurately predict the next item that a user may interact with, given her history purchasing records. In the optimization process, sequential recommender models are usually learned based on the cross-entropy loss, that is: ğ¿= âˆ’ğ‘¦log[ğ‘“ (ğ‘¢, ğ‘£, ğ‘£, ...ğ‘£)](1) where the output layer ofğ‘“ (Â·)is a softmax operation, and[ğ‘“ (Â·)] selects itsğ‘˜th element.ğ‘¦= 1ifğ‘˜ = ğ‘£, otherwiseğ‘¦= 0. In the past few years, people have designed a lot of methods to implementğ‘“. However, most of them assume user preference in the history information is coherent, which is less reasonable given the potentially diverse user personalities. To solve the above problem, recent years have witnessed many multi-interests sequential recommendation algorithms. Different from traditional methods, there is a multi interest extractor module ğ‘€, which projects the history information into many multi interest representations. Hopefully, each generated representation can exactly encode one type of user preference. The next item is predicted based on the corresponding multi interest representations. Formally, for the history information(ğ‘¢, ğ‘£, ğ‘£, ...ğ‘£). Multi interest extractorğ‘€ ï¬rst learn a ï¬xed number multi interest representations(MIR) for the sequence behavior. With MIR, current multi interest methods could been divided into two types in the prediction stage, where we call the divided methodğ‘‘: (1) [3,5] use one of the interest representation in MIR for prediction. They choose the target interest representation which gets the maximum inner product between the target itemğ‘£. (2) [6, 13,24,28] use attention mechanisms which combines those interest representations with adaptive weights to represent the sequence behavior. Both methods learn a behavior representation for the item prediction. In the training process, the learning objective is improved as: Figure 2: The behavior allocator interactions in Adasplit. whereğ‘¦= 1ifğ‘˜ = ğ‘£andğ‘£falls into theğ‘¡th user preference, otherwiseğ‘¦= 0. In this objective, the next itemğ‘£is predicted by considering all the multi interest representations or one of the multi interest representations. In our model, there are two major components: the ï¬rst one is a reinforcement learning based allocator agentğœ‹, aiming to disentangle user preferences into different sub-sequences in Figure 2. The other one is a sequential recommender model, which is leveraged to predict the next item, and generate the recommendation list. In the following, we detail these components more in detail. The key of our model lies in how to allocate the items in history into different sub-sequences. We ï¬rst input the sequence behavior into a bi-directional architecture transformer, where with the bidirectional architecture transformer block, conceptually similar item in the user behavior is fusing closer with the adaptive weights and the bi-directional architecture equips item with the global information which helps a lot in the item allocation task. Bi-Directional TransformerFirst, we would omit the subscript i in user behavior. And we project user behavior(ğ‘¢, ğ‘£, ğ‘£, ...ğ‘£) to embedding vectorsğ¸ = {[(ğ‘’, ğ‘’, ğ‘’, ...ğ‘’), ğ‘’]}. Then, we incorporate a learnable position encoding matrixğ‘ƒto enhance the input representations. In this way, the input representationsğ¸can be obtained by adding the two embedding matrices:ğ¸ = ğ¸ + ğ‘ƒ. The attention layer calculates a weighted sum of all values, where the weight between query and key. where the scale factorâˆšğ‘‘is to avoid too large values of the inner product when the dimension is very high. We take E as input, convert it to three matrices through linear projections, and feed them into an attention layer: where the projections matricesğ‘Šğ‘Šğ‘Šâˆˆ R. The projections make the model more ï¬‚exible. And the we use LayerNorm to ensure the stability of the data feature distribution and accelerate the training process. In order to enforce the model with non-linearity and to get more high-order interaction information, we apply a two-layer feed-forward network to all ğ‘†. We add the originalğ‘ in Eq.(6) which could avoid network degradation and learn a more useful information in a deeper stack architecture deep model. Allocator Agent ğœ‹We hope to adaptively determine the number of sub-sequences and also take the evolving nature of user preference into consideration. To this end, we regard the separation of user behavior sequences as a Markov decision process, and design a reinforcement learning model for item allocation. In particular, for a given sample, the agent goes through the history information, and at each step, it chooses a sub-sequence for the current item. At step time =ğ‘‡, let all the generated sub-sequences be:ğº= {ğ‘”, ğ‘”, ...,ğ‘”}, where h is the current generated subsequences number. Whatâ€™s more, we use sub-sequence representationğ‘ƒ= {ğ‘, ğ‘, ..., ğ‘}to represent each generated sub-sequence and each sub-sequence representation is initializing with the corresponding user embeddingğ‘’. The agent at stepğ‘‡aims to assign the ğ‘‡th itemğ‘£in userâ€™s sequence behavior to a sub-sequence. Given ğº= {ğ‘”, ğ‘”, ...,ğ‘”}and the current state theğ‘†= {ğ‘ , ğ‘ , ..., ğ‘ } in Eq.(13) which is made up of theğ‘†andğ‘, the allocator agentğœ‹ is implemented with the following policy network: ğ‘Ÿ= ğ‘†ğ‘–ğ‘”ğ‘šğ‘œğ‘–ğ‘‘ ((ğ‘…ğ‘’ğ¿ğ‘ˆ (ğ‘ ğ‘Š+ğ‘)ğ‘Š+ğ‘)ğ‘Š+ğ‘) ğœ‹ (ğ‘= ğ‘|ğ’“, ğ’“, ...ğ’“) = [SOFT-MAX(ğ‘Ÿ, ğ‘Ÿ, ..., ğ‘Ÿ)] whereğ‘is the action taken by the agent at stepğ‘‡, which means the new itemğ‘£belongs to theğ‘th sub-sequenceğ‘”.ğ‘Š,ğ‘Šand ğ‘Šare adapting parameter for computing the similarity between ğ’†andğ’”.SOFT-MAXis the softmax operator. As a result,ğœ‹ (ğ‘= ğ‘|ğ’”, ğ’”, ...ğ’”)is the probability that itemğ‘£is allocated into theğ‘th sub-sequence. By this equation, the policy network aims to select the sub-sequence inğ‘”, which is semantically more compatible with However, since the number of sub-sequences is ï¬xed, if an item is not coherent with any of existing sub-sequences, it has to be compromisely allocated into the sub-sequence which is not that similar. In such a scenario, a better solution is to adaptively determine the number of sub-sequences, so as to make sure that each subsequence is semantically coherent. To achieve this idea, we introduce a special action called â€œcreating a new sub-sequenceâ€, and the policy network is improved as: ğœ‹ (ğ‘= ğ‘|ğ’“, ğ’“, ...ğ’“) = [SOFT-MAX(ğ‘Ÿ, ğ‘Ÿ, ..., ğ‘Ÿ, ğœ–)](8) whereğœ–is a pre-deï¬ned hyper-parameter and we extend the vector before softmax with an additional dimension.ğ‘ âˆˆ [1, â„ + 1], and whenğ‘ = â„ + 1, we do not assignğ‘£to any existing sub-sequences, but create a new sub-sequence, and regardğ‘£as the ï¬rst item in this new sub-sequence. This equation encodes the following belief: ifğ‘£is not close enough to any existing sub-sequences, that is, ğ‘”< ğœ–, âˆ€ğ‘— âˆˆ [1,â„], then the policy network is more likely to choose actionâ„ +1, which creates a new sub-sequence. However, if there are many sub-sequences, satisfyingğ‘Ÿ> ğœ–, then actionâ„ + 1may not be triggered. As a special case, ifğ‘Ÿ>> ğœ–, âˆ€ğ‘— âˆˆ [1,â„], then equation (8) is equal to equation (??). Sequential recommender modelAgentğœ‹allocates theğ‘£to the sub-sequenceğ‘”, and we use the well-design sequential recommender model Attention-GRU to update the sub-sequence representation ğ‘, which is as follows: And the corresponding Attention-GRU is below: where the output dimension of the ğ‘§and ğ‘ is 1. State TransitionAfter an action is taken by the agent, the state is updated as follows: ï£±ï£´ï£´{(ğ‘”, ...,ğ‘”), ...(ğ‘”, ...,ğ‘”, ğ‘£), ...} , ğ‘– ğ‘“ ğ‘âˆˆ [1,â„] ï£´ï£´ï£´{(ğ‘”, ...,ğ‘”), ...(ğ‘”, ...,ğ‘”), (ğ‘£)} , ğ‘– ğ‘“ ğ‘= â„ + 1 where if the action is in[1, â„], then the corresponding sub-sequence is extended withğ‘£. If the action is â€œcreating a new sub-sequenceâ€, then a new sub-sequence(ğ‘”)is added to the state, where the new sub-sequence representation is initializing with the corresponding user embedding ğ‘’. Whatâ€™s more, in reality, there are complex relationships between the userâ€™s click sequence, like point level, union level with or without skip[25]. For accurately capturing those relationships, we use a welldesigned attention mechanism to deï¬ne the state transition. whereğ‘Šis a2ğ‘‘ğ‘¥ğ‘‘matrix and(Â·)represent the inner product. As for the sub-sequence state ğ‘ in Eq.(7) is as follows: In our model, the reward is associated with the following aspects: â€¢ The allocation task loss.To begin with, we task of allocator agentğœ‹is allocating itemğ‘£to the nearest sub-sequence. Here we use the inner product betweenğ‘£and its target sub-sequence representationğ‘as the accuracy of the "nearest". And the reward of the allocation task is: whereâ„is the current generated sub-sequences number. Whatâ€™s more, theğœ‹optimizes the relevance betweenğ‘£andğ‘, which is same as the the sequential recommender model optimize in Eq.(20). They are basically playing a collaborative game, where the common target is to lower the loss of the objective, that is, better ï¬tting the training data. â€¢ The orthogonality between different sub-sequences.Ideally, each sub-sequence should encode just one type of user preference coherently, which is not leaked into the other ones. Based on this property, each sub-sequence is representative enough, which facilitates more focused and explainable estimation. By the design of our policy networkğœ‹in Eq.(7) (8) , we have tried to allocate each item into the sub-sequence which is most coherent with it. Here, we introduce a reward to encourage that the information overlap between different sub-sequences is as small as possible, that is: where we try to make each pair of sub-sequence representations derived from equation Eq.(9) as orthogonal as possible. â€¢ The penalty on creating new sub-sequence.The special action â€œcreating new sub-sequenceâ€ inï¬‚uences the ï¬nal number of subsequences. If there are too few sub-sequences, the user preference may not be well separated. While if the number of sub-sequences is too large, the preference granularity can be too small, which may fail to capture the high-level connections among user behaviors, and thus may not generalize well on the complex and volatile recommendation environments. In addition, if the number of sub-sequences is too large, then each sub-sequence may only contain very few items, which brings difï¬culties for learning each user preference sufï¬ciently. In order to tune the number of sub-sequences, we introduce a rewardğ‘Ÿto penalize the action of â€œcreating new sub-sequenceâ€, that is: where the reward is a negative value after taking the special action â€œcreating new sub-sequenceâ€, while for the other actions, the reward is 0. By a largerğœ†, we can reduce the number of sub-sequences, while if ğœ† is small, then more sub-sequences can be generated. In our model, we adjustğœ†in a curriculum learning manner. In the beginning, there is only one sub-sequence, and we do not hope to impose much constraint on â€œcreating new sub-sequenceâ€. At this time,ğœ†is a small value. While as the agent takes more actions, more sub-sequences are generated, thus we limit the number of subsequences by settingğœ†as a larger value. To realize the above idea, we exploreğœ†in the following set of functions when the agnet take the action "creating a new sub-sequence". And we use theğ‘‡as the times the action "creating a new sub-sequence" has shown up until now: Where{ğ‘, ğ‘}are hyper-parameters, and we setğ‘> 0,ğ‘> 0 to ensure that the function is monotonic as the agent takes more action "creating a new sub-sequence". We sum theğ‘Ÿ,ğ‘Ÿandğ‘Ÿas the ï¬nal rewardğ‘Ÿ. Whatâ€™s more, the current action value is only related to future rewards without considering previous rewards, we add the future rewards with a decay parameter ğœ†. Where theğœ†is the trade-off parameter to balance the importance of the orthogonal reward. In practice, the discrete action is not differentiable, but we can optimize it with the log trick approximation, which is an unbiased estimation. As a result, our ï¬nal optimization target is: For the target itemğ‘£, the behavior alloatorğœ‹generates its target sub-sequenceğ‘”and the target corresponding sub-sequence representation isğ‘. Then the sequential recommender model is optimized based on the following objective: where the m is the number of the item. Finally, we jointly train the allocator agentğœ‹and sequential recommender with a trade-off parameter ğ›½: When we do the prediction, allocator agentğœ‹ï¬rst allocate the item into the sub-sequences with the maximal probability in Eq.(7) (8) and then the state transition in Eq. (11)(12)(13) , which can be written as follows: We choose the action with the maximal probability in the prediction while we sample action according the action probability in the training process. For each candidate item, allocator agentğœ‹allocates it into a sub-sequence and calculates the inner product between the item representation and its target sub-sequence representation as the score. We then rank all candidate items according to their scores and return the top-ğ‘ scores item as the ï¬nal recommendations. We summarize the complete learning algorithm of our framework in Algorithm 1. The main task of allocator agentğœ‹is allocating training sample ((ğ‘¢, ğ‘£, ğ‘£, ...ğ‘£), ğ‘£) into sub-sequences. To begin with, the training sample is represented with a global information representation through a bi-directional architecture transformer in Eq.(5) (6). Then, the agentğœ‹samples action for each item via its action prbability in Eq.(7) (8) and allocates item into different subsequenceğºand form sub-sequence representationsğ‘ƒin Eq.(9) (10). And then state transition in (11)(12) (13) and reward calculation in Eq.(14) (15) (16) for the item allocation. For the target item prediction, agentğœ‹allocates the item into its target sub-sequence ğ‘”in Eq.(7) (8). And optimization in the joint loss L in Eq.(21). In this section, we will conduct experiments on four datasets to evaluate the effectiveness of Adasplit. We ï¬rst brieï¬‚y introduce the datasets and the state-of-the-art methods, then we conduct experimental analysis on the proposed model and the benchmark models. Speciï¬cally, we try to answer the following questions: â€¢How effective is the proposed method compared to other stateof-the-art baselines? Q1 â€¢What are the effects of the bi-directional architecture transformer and the sequential recommender model Attention-GRU in behavior allocator? Q2 â€¢How sensitive are the hyper-parameter the sequence lengthğ‘¡ and the penalty parameter ğœ† in proposed model Adasplit? Q3 In this section, we introduce the details of the four experiment datasets, evaluation metrics and comparing baselines in our experiments. Datasets.We perform experiments on four publicly available datasets, includingLastFMa music records from Last.fm. And we only use the click behaviors.Garden, Baby, Beautyare composed of user purchasing behaviors from Amazon. And the relative statistics information of the four datasets are shown in Table 1. For each datasets, we ï¬lter out items and users interacted less than ï¬ve times. And all datasets are taken Leave-one-out method in [12] to split the datasets into training, validation and testing sets. Note that during testing, the input sequences contain training actions and the validation actions for training the model. Baeslines.We compare our proposed model Adasplit with the following state-of-the-art sequential recommendation baselines. â€¢ Single representation models: BPR[20] is a famous recommendation algorithm for capturing user implicit feedback. Attention-GRU. Eq.(5) (6). representations ğ‘ƒ. NeuMF[9] is a classic work which leverages the neural networks for capturing the user preference.GRU4Rec[10] is a pioneering work which ï¬rst leverages GRU to model user behavior sequences for prediction.STAMP[16] is a recently proposed neural network-based method capturing sequential pattern by emphasizing the user short preference. NARM[14] is a recently proposed neural attention based method.SASRec[25] is a well-known attention based sequential recommendation. And the number of head in experiments is 1. Figure 3: Case study. The left before t=7 is the user behavior and the task to predict the target item at t=8. And the picture of each movie is downloaded from https://www.amazon.com. â€¢ Multi representations models: MCPRN[28] is a recent representative work for extracting multiple interests.SASRec2[25] has two heads in the representations construction, which is a multi interest recommendation method [24]. Parameter Conï¬guration.For a fair comparison, all baseline methods are implemented in Pytorch and optimized with Adam optimizer. Speciï¬cally, the learning rate and batch size are tuned in the ranges of [0.01,0.001,0.0001] and [32,64,128,256]. For our method, it has three crucial hyper-parameters: the "creating a new sub-sequence" parameterğœ–is tuned in the ranges of [0.2,0.3,0.4,0.5,0.6, 0.7,0.8], and trade-off parameterğœ†andğ›½are tuned in the ranges of [1,0.1, 0.01,0.001]. And the penalty parameterğœ†is tuned in the ranges of [0.9,1,1.1,1.2,1.3]. Whatâ€™s more, we have released our project at https://no-one-xxx.github.io/Adasplit/ Evaluation Metrics.We use two commonly used evaluation criteria: Mean Reciprocal Rank (MRR) and Normalized Discounted Cumulative Gain (NDCG) to evaluate the performance of our model. Table 2 summarizes the performance of Adasplit and baselines. Obviously, Adasplit achieves better performance to other the baselines on all the evaluation metrics. Whatâ€™s more, in the datasets Amazon, compare single representation methods with multi representation methods and Adasplit, it is obvious that recommendation with multiple presentations (MCPRN) and with dynamic representation Adasplit for a user click sequence perform generally better performance than those with single representation (Caser, GRU4Rec, BERT4Rec ...). The userâ€™s always click multiply concept different items in those datasets which gives the evidence that only single representation for next item prediction canâ€™t handle the complex situations. However, in other datasets like LastFM, the multi representations method (MCPRN) doesnâ€™t achieve excellent results, which indicates that users in Lastfm datases show a focus click patterns and single representation would achieve better results. And our model Adasplit could learns a dyanmic group of representations according the user sequence behavior. In other words, Adasplit could learn more representations when the sequence behavior is complex and learn less representations when the sequence behavior is simple. And it shows its adaptability when it faces various datasets. The improvement of Adasplit over the multi representations methods (MCPRN, SASRec-2) and single representation methods (BPR, NARM...) shows that the dynamic representations exploration serves as a better information extractor than ï¬xed and predeï¬ned number representations. This can be attributed to two points: 1) Adasplit Table 2: Overall comparison between the baselines and our models Adasplit. The best results are highlighted with bold fold. All the numbers in the table are percentage numbers with â€™%â€™ omitted. Figure 4: Ablation study in bi-directional architecture transformer. Performance comparison of Adasplit, its variant Adasplit-Single and Adasplit-Zero. adjusts the sub-sequence representations construction according to data characteristics which could take the advantages of both the single representation methods and multi representations methods. 2) Adasplit considers the userâ€™s evolving preference in the user representations construction which gets high adaptability. In our model, a major novelty is that we want to allocate the item into different sub-sequences and learn a dynamic group of representations to represent the user. To obtain a better understanding why Adasplit performs better than other models, we further construct a case study on Baby dataset. Speciï¬cally, we present a snapshot of the interaction sequence from a sampled user, which contains eight items. The results show in Figure 3, and we use different colors to represent different sub-sequences that the allocator agent ğœ‹generates. There are three different sub-sequences: clothes, toy and bottle, which are fully in line with the facts in the user sequence behavior. As for the next item prediction, the target item is a toy guitar and it is accurately allocated to the toy sub-sequence exactly by the allocator agent ğœ‹. In behavior allocator, we use the bi-directional architecture transformer to equip item representations with global information for the next allocation task. In order to explore the effectiveness of the bidirectional architecture transformer, we change the bi-directional architecture transformer to left-to-right directional architecture transformer(AdasplitSingle) and use the original item sequence representation without any operation for the allocation task (Adasplit-Zero). The item in Adasplit-Single could obtain the information form the items on its left while item in Adasplit-Zero knows nothing about the items around itself. From the results reports in Figure 4, we can ï¬nd that Adasplit-Zero achieves the worse performance compare with Adasplit-Single, which indicates that the information between items is important in allocation process. And the Adasplit achieves the better performance than the Adasplit-Single gives the evidence that the effectiveness of the bi-directional architecture transformer for extracting the global information. In other words, two-way information is more useful than only one-way information in item allocation. In behavior allocator, the sequential recommender model AttentionGRU formalize the sub-sequence representation. To validate the effectiveness of the recommender model Attention-GRU, we change the Attention-GRU to two other methods, LSTM and AveragePooling. And We call the two variants Adasplit-LSTM, AdasplitAveragePooling. From the results reports in Table 3, Adasplit achieves a better performance compare with other variants, which validates the effectiveness of the sequential recommender model AttentionGRU in behavior allocator. And we also ï¬nd Adasplit-LSTM achieve better performance to Adasplit-AveragePooling in the Table 3 which gives the evidence that the necessity of the sequential pattern to capture the evolving user preference. Table 3: Ablation study in Attention-GRU. Performance comparison of Adasplit, its variants Adasplit-LSTM and Adasplit-AveragePooling. As we all know, more item in user sequence behavior, more subsequences may occur. Thus, the sequence lengthğ‘™in the Adasplit is important, and we do experiments to investigate the sensitivity of the sequence lengthğ‘™in Adasplit. Figure 5 reports the performance of our model in the metrics of MRR and NDCG in Garden and Baby datasets. In particular, We keep the other parameters in the model consistent with the Q1 settings. From the ï¬gure, we can observe that Adasplit obtains the best performance of MRR and NDCG when Figure 5: Hyperparameter study, where the horizontal coordinates is the sequence length ğ‘™ from 5 to 40. ğ‘¡equals 10. The result increases with the increase of the sequence length ğ‘™. After it comes to a peak, it begins to decrease. In overall performance, Q1, we use the exponential growth in Eq.(17) for penaltyğœ†. Here we use the linear growth for theğœ†in behavior allocator. And we call it Adasplit-Linear. In order to valid the effectiveness of the penalty operation and the increasing penaltyğœ† method on the action "creating a new sub-sequence". We add another two variants, Adasplit-None, where we remove the penalty operation and Adasplit-Keep where we donâ€™t change the penalty parameter ğœ†. In Table 4, obviously, Adasplit-None achieves the worst performance among the four models, which gives the evidence that the effectiveness of the penalty operation. And Adasplit-Linear achieve better performance than Adasplit-Keep shows the effectiveness of the increasing penaltyğœ†method. Last but not the least, AdasplitLinear achieves worse performance compare with Adasplit, which gives us the evidence that the stronger increasing method may bring a better performance. Table 4: Hyperparameter study, the performance of Adasplit and its variant Adasplit-Linear, Adasplit-Keep and Adasplit-None. In this section, we will brieï¬‚y introduce the related works to our study, which includes sequential recommendation and multi interest recommendation The main purpose of sequential recommendation is to discover the underlying patterns of the user sequential behaviors. Rendle et al [21] integrates matrix factorization and the sequential pattern of Markov Chains for prediction, and later Wang et al [27] simultaneously consider the sequence behaviors and user preferences with a hierarchical representation model. Though those methods make progress in recommendation, these methods only model the local sequential patterns between recent clicked item [32]. To model the longer sequential behaviors in user behavior, Hidasi et al [10] ï¬rst adopted recurrent neural network to model the long sequence pattern. Then, Li et al [14] not only consider the sequence pattern in the sequence, but also explore the userâ€™s main purpose through the attention mechanism. And Ma et al [18] takes Hierarchical Gating Networks to capture both the long-term and short-term user interests. Chen et al [7] use Memory Network for exploring the sequential pattern in user behavior. Whatâ€™s more, Tang et al [25] and Yuan et al[33] embed the user historical sequence behavior into an â€œimageâ€ and learn sequential patterns as local features of the image with Convolutional Neural Network. Later, Kang et al [12] considers the importance between different items in the click sequence and fuses the item representation with adaptive weights, which achieves great progress in many real datasets. Wu et al [29] and Huang et al [11] use graph to learn the complex transitions between items and improve the recommendation performance. Multi interest recommendation with multi interest representations have greater expressive power especially when the user shows a wide range of intends. Liu et al [17] proposes a new user representation model to comprehensively extract user sequence behavior into multiple vectors. And Xiao et al [30] explores user diverse interests with a multi-head architecture self-attentive, where the number of the heads is the number of the representations. Wang et al [28] takes an effective mixture-channel purpose routing networks to detect the purposes of each item and assigns items into the corresponding channels to get the multi interest representations. Chen et al [5] thinks that the time interval information is meaningful in extracting the useful information and designs a novel time graph to get the time information in multi representations construction and Tan et al [24] infers a sparse set of concepts for each user from the large concepts to generate user multi interest representations. Chen et al [6] and Cen et al [3] use capsule routing and self-attention as multi interest representations extractor and the proposed models improve the diversity for the next-item prediction. Though those methods have achieved good performance in recommendation, none of them consider the diverse interests between users and the evolving user pattern. In this paper, we proposed a novel model called Adasplit, to improve the recommendation performance by learning a dynamic group representations from the userâ€™s sequence behavior. Adasplit can adaptively and automatically allocates items into different sub-sequences and learns a dynamic group of representations according the user evolving preference. To be speciï¬c, we formalize the allocation task as MDP problem and allocate the item in userâ€™s sequence behavior into different sub-sequences with a novel action "creating a new action", and form the sub-sequence representations through the sequential recommender model Attention-GRU accordingly. And we conducted experiments to veriï¬ed the effectiveness of Adasplit on four real datasets with SOTA methods. However, the proposed model also exists shortcomings in computing speed, where we formalize the allocation task in behavior allocator as a MDP problem and it is computing cost and unstable in training. In the future we will consider how to allocate the user sequence in a more effective way.