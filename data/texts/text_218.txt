<title>Application of Deep Learning Technique to an Analysis of Hard Scattering Processes at Colliders</title> <title>1. Physics task</title> In the scope of high energy physics analysis the measurement of t-channel single top-quark production is used as a benchmark to calibrate the analytical tools and assumptions. Then the developed methods are used to measure deviations from the Standard Model â€“ the Flavor Changing Neutral Currents (FCNC). The physics task is similar to the analysis of CMS collaboration [ ]. Neural networks are used extensively throughout the analysis to separate background and signal events. First neural network model is used to î€›lter out the multi-jet QCD events as these events are hard to model with existing Monte-Carlo methods. This network uses only î€›ve variables as its inputs and has relatively small number of trainable parameters. After the multi-jet QCD suppression a larger Standard Model neural network is used to identify top-quark events. These two diî€˜erent tasks allow us to test the performance of described methods in two separate instances. <title>2. DNN hyperparameter tuning</title> <title>arXiv:2109.08520v1  [physics.data-an]  14 Sep 2021</title> of hidden layers, number of neurons in each hidden layer, the learning rate, regularization constants, etc. In this case trainable parameters are the weights and biases of each neuron. Diî€˜erent hyperparameter combinations give the deep learning model varying degree of complexity and non-linearity. Therefore, hyperparameter tuning can help resolve overî€›tting and underî€›tting to a certain degree. Finding the best combination of hyperparameters can be a challenging task as the modelâ€™s performance can only be evaluated when the training is î€›nished. Luckily, many hyperparameter optimization frameworks exist to automate this tedious process. One of the most established and well-known frameworks is called ]. It provides tools to tune any machine learning model and quickly visualize the results. Another useful feature of is the budget â€“ the amount of time used to tune a single model. This can be useful to compare diî€˜erent models and still give both fair treatment, tuning them for the same time. However, we have opted to use ] for its tight integration with , allowing us to use hyperparameter tuning with minimal code modiî€›cations and dependencies. After each trial this module generates a î€›le containing all required information about a single run. If the user wishes not only get the best performing hyperparameter combination but also to explore the dependencies and tendencies of their deep learning model, the results can be parsed and visualized. The î€›rst major step in setting up any hyperparameter tuning is deî€›ning all possible hyperparameter combinations â€“ the hyperparameter space. Usually the ranges of numeric hyperparameters are deî€›ned either with a distribution or with a set array of values. The latter is done by deî€›ning the minimum and maximum values and setting the step parameter, the distance between two consecutive samples in the range. We have used this approach to visualize the relations between the modelâ€™s performance and the values of its hyperparameters. The non-numerical hyperparameters (the hidden layers activation function, for example) are chosen with the method. The second step is to deî€›ne a score variable â€“ a metric to quantify and compare the modelâ€™s performance. In the default case this variable can be equal to modelâ€™s loss value (binary crossentropy in our binary classiî€›cation task) or to any pre-made or user-deî€›ned metric. With the hyperparameter space expanding with each new variable, a suitable algorithm to navigate it is required. provides three basic Tuner algorithms, each with its own advantages and drawbacks: BayesianOptimization, Hyperband and RandomSearch. algorithm uses tuning with Gaussian process. This is the fastest built-in algorithm, however, it can only î€›nd the local minima in the hyperparameter space. In our tests it converged within approximately 10% of the total combinations in the hyperparameter space. The parameters this algorithm converged on were adequate albeit not the overall best. algorithm uses the performance of the î€›rst epochs to compare diî€˜erent hyperparameter combinations. We decided against this method as models with diî€˜erent learning rates will have diî€˜erent performance which will not reî€œect their overall accuracy. tuning algorithm randomly samples hyperparameter combinations from the hyperparameter space. This method does not use any fancy logic, however, it reliably provides a near-best result when covering 40-50% of the hyperparameter space. The code covering the needed adaptations is available in the Appendix. After the tuning process is compete, all trials results are stored in î€›les. The user can opt to use the best conî€›guration without looking at other models, however, plotting the relations can provide useful insights into how the chosen model is performing. For general overview one can use Facebookâ€™s ] utility (Figure 1). This interface allows the user to quickly analyse the trained models, sort them by their performance and check speciî€›c hyperparameter combinations. To further investigate the hyperparameter space, one can plot the relations between modelsâ€™ performance and the values of used hyperparameters. We give two examples of such visualization in Figures 2 and 3. In the î€›rst set of plots covering the tuning of larger Standard Model neural network we demonstrate the relation between the modelâ€™s performance and a certain hyperparameter value, averaging over the rest of hyperparameters. In the second set we used heatmaps to describe hyperparameter combinations for the smaller QCD suppression neural network. Having investigated the hyperparameter space for two typical High Energy Physics tasks, we can give broader recommendations for neural network design for this î€›eld. First of all, using for hidden layers activation function is advisable. Standard and functions lead to worse performance in deeper, bigger networks. In both cases networks with one or two hidden layers performed better and more stable than their deeper counterparts. The number of nodes in the hidden layers varied depending on the amount of input features: for the bigger network with 50 input features the amount of neurons lied in range between 200 and 400, and for the smaller network with 5 input features it was closer to 120. <title>3. AutoML</title> AutoML approach covers î€›nding the optimal machine learning model, training it, evaluating it and tuning it if its performance is insuî€cient. In theory, given enough time and computational resources, this approach can yield an adequate model without investing researcherâ€™s time into complex architecture tuning and feature engineering. High energy physics data is close in structure to tabular data. In other areas tabular data may contain text and categorical data, but in high energy physics data is primarily numerical and can be organized into columns, so the task is simpler in a certain way. Deep learning has shown comparable performance[ ] in tabular data classiî€›cation to gradient boosting models ( ), which are much cheaper in terms of computational resources. However, our preliminary testing done using the package showed that î€›ne-tuned neural network performs slightly better and overî€›ts less than a tuned model. Googleâ€™s ] package uses genetic algorithms to create a custom neural network structure for each machine learning task. This approach has a promising idea, however, the lack of support for weighted events limits its uses in high energy physics analysis where every event has a very speciî€›c weight value. We have opted for using ] library for automated machine learning. This library is easy to comprehend, has several performance modes (focused on data exploration, speed of inference or maximum accuracy of classiî€›cation). It uses several machine learning algorithms (Linear, Random Forest, Extra Trees, LightGBM, Xgboost, CatBoost, Neural Networks, and Nearest Neighbors) for classiî€›cation and then creates an ensemble of best performing models for î€›nal classiî€›cation. Here we present the results of two AutoML models in (runs in a dozens of minutes and performs Exploratory Data Analysis) and (maximum classiî€›cation accuracy, needs more computational time, we have run it for a day) modes as the speed of inference is not crucial in the current analysis. The performance comparisons are shown in Figure 4. Both classiî€›cation modes provided good accuracy and even outperformed the tuned neural network in terms of metric on the test dataset. However, this was done with a much higher degree of overî€›tting, thus reducing the AutoML modelâ€™s predictive power. 3.4. Resume provided a good baseline in all our use-cases, with its maximum accuracy mode overî€›tting a bit more that we would like it to. As it is much easier to control overî€›tting inside Tensorî€œow package through regularization and early stopping callback, for the time being we will continue to use it in our analysis, but this AutoML package came close to it in terms of classiî€›cation accuracy. We will deî€›nitely monitor the development of this great tool and continue testing it. <title>4. DNN b oosting on errors</title> We have also tried boosting on errors. The concept of this method is simple: This method did not work as the results of classiî€›cation worsened after each iteration. The illustration of this performance degradation can be found on Figure 5. The best explanation we have come up with was that deep neural network is not a weak learner, which were noticed to beneî€›t from such manipulations. <title>5. L-regularisation</title> Through experimentation with input features we have found out that certain features worked extremely well for î€›rst-order classiî€›cation, causing the model to increase its weights associated with these features, in turn lowering the importance of other, still needed, features. The proposed method included using l-regularization [ ] to limit high weights so that they will not overshadow other weights as much. We have conducted the l-regularization study by getting a good baseline model from the , î€›xing its hyperparametes and varying only the regularization constant. This was done for L1, L2 and L12 regularization types. The results showed that when the regularization constant is chosen right, the discriminant distribution curve becomes smoother and general classiî€›cation performance increases. However, when the regularization constant is too small, there are no perceivable improvements. High regularization constant values can be even more detrimental as they will outright decrease the classiî€›cation performance of otherwise decent model. The illustration of these relations can be found in Figure 6. We have not noticed considerable diî€˜erences between the regularization types, L2 performed slightly better, but that disparity was within the margin of error. <title>6. Conclusion</title> We have demonstrated several approaches to improve the accuracy of classiî€›cation based on a model of a Deep Neural Network in High Energy Physics. Here we provide a short summary of the methods described in the paper. DNN hyperparameter tuning is an eî€˜ective method of improving the accuracy of the model, but it requires a lot of computational resources. The required computing time can be reduced by making hyperparameter space smaller and using a suitable optimization algorithm. We also provided recommendations based on our hyperparameter space exploration for typical high energy physics datasets. AutoML for tabular data can be used in High Energy Physics with relative success and little machine learning experience, however, the degree of overî€›tting is hard to control. Boosting on errors is not advised to use with DNNs, and using any type of L-regularization is advisable if the regularization constant is chosen correctly. <title>References</title> [1] V. Khachatryan, et al. (CMS), Search for anomalous Wtb couplings and î€œavour-changing neutral currents in t-channel single top quark production in pp collisions at ğ‘  = 7 and 8 TeV, JHEP 02 (2017) 028. doi:10.1007/JHEP02(2017)028. arXiv:1610.03545. [2] T. Oâ€™Malley, E. Bursztein, J. Long, F. Chollet, H. Jin, L. Invernizzi, et al., Keras tuner, https://github.com/keras-team/keras-tuner, 2019. [3] T. Akiba, S. Sano, T. Yanase, T. Ohta, M. Koyama, Optuna: A next-generation hyperparameter optimization framework, in: Proceedings of the 25rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, 2019. [4] D. Haziza, J. Rapin, G. Synnaeve, Hiplot, interactive high-dimensionality plots, https:// github.com/facebookresearch/hiplot, 2020. [5] Y. Gorishniy, I. Rubachev, V. Khrulkov, A. Babenko, Revisiting deep learning models for tabular data, 2021. arXiv:2106.11959. [6] G. Ke, Q. Meng, T. Finley, T. Wang, W. Chen, W. Ma, Q. Ye, T.-Y. Liu, Lightgbm: A highly eî€cient gradient boosting decision tree, in: Proceedings of the 31st International Conference on Neural Information Processing Systems, NIPSâ€™17, Curran Associates Inc., Red Hook, NY, USA, 2017, p. 3149â€“3157. [7] C. Cortes, X. Gonzalvo, V. Kuznetsov, M. Mohri, S. Yang, Adanet: Adaptive structural learning of artiî€›cial neural networks, 2017. arXiv:1607.01097. [8] A. PÅ‚oÅ„ska, P. PÅ‚oÅ„ski, Mljar: State-of-the-art automated machine learning framework for tabular data. version 0.10.3, 2021. URL: https://github.com/mljar/mljar-supervised. [9] C. Cortes, M. Mohri, A. Rostamizadeh, L2 regularization for learning kernels, 2012. <title>A. Code for Keras Tuner adaptation</title> The function that returns the base model: The function that returns the model adapted to the Keras Tuner environment: