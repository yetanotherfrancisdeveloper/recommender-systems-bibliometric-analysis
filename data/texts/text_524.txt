With the recent success of graph convolutional networks (GCNs), they have been widely applied for recommendation, and achieved impressive performance gains. The core of GCNs lies in its message passing mechanism to aggregate neighborhood information. However, we observed that message passing largely slows down the convergence of GCNs during training, especially for large-scale recommender systems, which hinders their wide adoption. LightGCN makes an early attempt to simplify GCNs for collaborative î€›ltering by omitting feature transformations and nonlinear activations. In this paper, we take one step further to propose an ultra-simpliî€›ed formulation of GCNs (dubbed UltraGCN), which skips inî€›nite layers of message passing for eî€œcient recommendation. Instead of explicit message passing, UltraGCN resorts to directly approximate the limit of inî€›nite-layer graph convolutions via a constraint loss. Meanwhile, UltraGCN allows for more appropriate edge weight assignments and î€exible adjustment of the relative importances among diî€erent types of relationships. This î€›nally yields a simple yet eî€ective UltraGCN model, which is easy to implement and efî€›cient to train. Experimental results on four benchmark datasets show that UltraGCN not only outperforms the state-of-the-art GCN models but also achieves more than 10x speedup over LightGCN. â€¢ Information systems â†’ Recommender systems;Collaborative î€›ltering. Recommender systems; collaborative î€›ltering; graph convolutional networks ACM Reference Format: Kelong Mao, Jieming Zhu, Xi Xiao, Biao Lu, Zhaowei Wang, and Xiuqiang He. 2021. UltraGCN: Ultra Simpliî€›cation of Graph Convolutional Networks for Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/3459637.3482291 Nowadays, personalized recommendation has become a prevalent way to help users î€›nd information of their interests in various applications, such as e-commerce, online news, and social media. The core of recommendation is to precisely match a userâ€™s preference with candidate items. Collaborative î€›ltering (CF) [11], as a fundamental recommendation task, has been widely studied in both academia and industry. A common paradigm of CF is to learn vector representations (i.e., embeddings) of users and items from historical interaction data and then perform top-k recommendation based on the pairwise similarity between user and item embeddings. As the interaction data can be naturally modelled as graphs, such as user-item bipartite graph and item-item co-occurrence graph, recent studies [10,13,24,27] opt for powerful graph convolutional/neural networks (GCNs, or GNNs in general) to learn user and item node representations. These GCN-based models are capable of exploiting higher-order connectivity between users and items, and therefore have achieved impressive performance gains for recommendation. PinSage [31] and M2GRL [26] are two successful use cases in industrial applications. Despite the promising results obtained, we argue that current model designs are heavy and burdensome. In order to capture higher-order collaborative signals and better model the interaction process between users and items, current GNN-based CF models [1,24,27,32] tend to seek for more and more sophisticated network encoders. However, we observed that these GCN-based models are hard to train with large graphs, which hinders their wide adoption in industry. Industrial recommender systems usually involve massive graphs due to the large numbers of users and items. This brings eî€œciency and scalability challenges for model designs. Towards this end, some research eî€orts [4,10,17] have been made to simplify the design of GCN-based CF models, mainly by removing feature transformations and non-linear activations that are not necessary for CF. These simpliî€›ed models not only obtain much better performance than those complex ones, but also brings some beneî€›ts on training eî€œciency. Inspired by these pioneer studies, we performed further empirical analysis on the training process of GCN-based models and found that message passing (i.e., neighborhood aggregation) on a large graph is usually time-consuming for CF. In particular, stacking multiple layers of message passing could lead to the slow convergence of GCN-based models on CF tasks. Although the aforementioned models such as LightGCN [10] have already been simpliî€›ed for training, the message passing operations still dominate their training. For example, in our experiments, three-layer LightGCN takes more than 700 epochs to converge to its best result on the AmazonBooks dataset [9], which would be unacceptable in an industrial setting. How to improve the eî€œciency of GCN models yet retain their eî€ectiveness on recommendation is still an open problem. To tackle this challenge, in this work, we question the necessity of explicit message passing layers in CF, and î€›nally propose an ultra-simpliî€›ed form of GCNs (dubbed UltraGCN) without message passing for eî€œcient recommendation. More speciî€›cally, we analyzed the message passing formula of LightGCN and identiî€›ed three critical limitations: 1) The weights assigned on edges during message passing are counter-intuitive, which may not be appropriate for CF. 2) The propagation process recursively combines diî€erent types of relationship pairs (including user-item pairs, item-item pairs, and user-user pairs) into the model, but fails to capture their varying importance. This may also introduce noisy and uninformative relationships that confuse the model training. 3) The over-smoothing issue limits the use of too many layers of message passing in LightGCN. Therefore, instead of performing explicit message passing, we seek to directly approximate the limit of inî€›nite-layer graph convolutions via a constraint loss, which leads to the ultra-simpliî€›ed GCN model, UltraGCN. The loss-based design of UltraGCN is very î€exible, allowing us to manually adjust the relative importances of diî€erent types of relationships and also avoid the over-smoothing problem by negative sampling. This î€›nally yields a simple yet eî€ective UltraGCN model, which is easy to implement and eî€œcient to train. Furthermore, we show that UltraGCN achieves signiî€›cant improvements over the state-of-the-art CF models. For instance, UltraGCN attains up to 76.6% improvement in NDCG@20 and more than 10x speedup in training over LightGCN on the Amazon-Books dataset. In summary, this work makes the following main contributions: â€¢We empirically analyze the training ineî€œciency of LightGCN and further attribute its cause to the critical limitations of the message passing mechanism. â€¢We propose an ultra simpliî€›ed formulation of GCN, namely UltraGCN, which skips inî€›nite layers of explicit message passing for eî€œcient recommendation. â€¢Extensive experiments have been conducted on four benchmark datasets to show the eî€ectiveness and eî€œciency of UltraGCN. In this section, we revisit the GCN and LightGCN models, and further identify the limitations resulted from the inherent message passing mechanism, which also justify the motivation of our work. GCN [14] is a representative model of graph neural networks that applies message passing to aggregate neighborhood information. The message passing layer with self-loops is deî€›ned as follows: withË†ğ´ = ğ´ + ğ¼andË†ğ· = ğ· +ğ¼.ğ´,ğ·,ğ¼are the adjacency matrix, the diagonal node degree matrix, and the identity matrix, respectively. ğ¼is used to integrate self-loop connections on nodes.ğ¸andğ‘Š denote the representation matrix and the weight matrix for theğ‘™-th layer. ğœ (Â·) is a non-linear activation function (e.g., ReLU). Despite the wide success of GCN in graph learning, several recent studies [4,10,17,29] found that simplifying GCN appropriately can further boost the performance on CF tasks. LightGCN [10] is one such simpliî€›ed GCN model that removes feature transformations (i.e.,ğ‘Š) and non-linear activations (i.e.,ğœ). Its message passing layer can thus be expressed as follows: It is worth noting that although LightGCN also removes self-loop connections on nodes, its layer combination operation has a similar eî€ect to self-loops used in Equation 2, becauase both of them output a weighted sum of the embeddings propagated at each layer as the î€›nal output representation. Given self-loop connections, we can rewrite the message passing operations for userğ‘¢and itemğ‘–as follows: whereğ‘’andğ‘’denote the embeddings of userğ‘¢and item ğ‘–at layerğ‘™.N(ğ‘¢)andN(ğ‘–)represent their neighbor node sets, respectively. ğ‘‘denotes the original degree of the node ğ‘¢. As shown in the left part of Figure 1, LightGCN performs a stack of message passing layers to obtain the embeddings and î€›nally uses their dot product for training. We argue that such message passing layers have potential limitations that hinder the eî€ective and eî€œcient training of GCN-based models in recommendation tasks. To illustrate it, we take theğ‘™-th layer message passing of LightGCN in Equation 3 and 4 for example. Note thatğ‘¢andğ‘£denote users whileğ‘–andğ‘˜denote items. LightGCN takes the dot product of the two embedding as the î€›nal logit to capture the preference of userğ‘¢on itemğ‘–. Thus we obtain: where ğ›¼, ğ›¼, ğ›¼, and ğ›¼can be derived as follows: Therefore, we can observe that multiple diî€erent types of collaborative signals, including user-item relationships (ğ‘¢-ğ‘–andğ‘˜-ğ‘£), item-item relationships (ğ‘˜-ğ‘–), and user-user relationships (ğ‘¢-ğ‘£), are captured when training GCN-based models with message passing layers. This also reveals why GCN-based models are eî€ective for CF. However, we found that the edge weights assigned on various types of relationships are not justiî€›ed to be appropriate for CF tasks. Based on our empirical analysis, we identify three critical limitations of the message passing layers in GCN-based models: â€¢ Limitation I: The weightğ›¼is used to model the item-item relationships. However, given the userğ‘¢, the factors of item ğ‘–and itemğ‘˜are asymmetric (for itemğ‘˜while for itemğ‘–). This is not reasonable since it is counter-intuitive to treat the itemğ‘˜and itemğ‘–unequally. Similarly,ğ›¼that models the user-user relationships also suî€er this issue. Such unreasonable weight assignments may mislead the model training and î€›nally result in sub-optimal performance. â€¢ Limitation II: The message passing recursively combine diî€erent types of relationships into the modeling. While such collaborative signals should be beneî€›cial, the above message passing formula fails to capture their varying importance. Meanwhile, stacking multiple layers of message passing as in Equation 5 likely introduce uninformative, noisy, or ambiguous relationships, which could largely aî€ect the training eî€œciency and eî€ectiveness. It is desirable to î€exibly adjust the relative importances of various relationships. We validate this empirically in Section 4.4. â€¢ Limitation III: Stacking more layers of message passing should capture higher-order collaborative signals, but in fact the performance of LightGCN begins to degrade at layer 2 or 3 [10]. We partially attribute it to the over-smoothing problem of message passing. As graph convolution is a special form of Laplacian smoothing [16], performing too many layers of message passing will make the nodes with the same degrees tend to have exactly the same embeddings. According to Theorem 1 in [5], we can derive the inî€›nite powers of message passing which take the following limit: whereğ‘›andğ‘šare the total numbers of nodes and edges in the graph, respectively. The above limitations of message passing motivate our work. We question the necessity of explicit message passing layers in CF and Figure 1: Illustrations of training of LightGCN (left) and UltraGCN (right). LightGCN nee ds to recurrently perform ğ¿layers message passing to get the î€›nal embeddings for training, while UltraGCN can â€œskipâ€ such message passing to make the embeddings be directly trained, largely improving training eî€œciency and helping real deployment. further propose an ultra-simpliî€›ed formulation of GCN, dubbed UltraGCN. In this section, we present our ultra-simpliî€›ed UltraGCN model and demonstrate how to incorporate diî€erent types of relationships in a î€exible manner. We also elaborate on how it overcomes the above limitations and analyze its connections to other related models. Due to the limitations of message passing, in this work, we take one step forward to question the necessity of explicit message passing in CF. Considering that the limit of inî€›nite powers of message passing exists as shown in Equation 6, we wonder whether it is possible to skip the inî€›nite-layer message passing yet approximate the convergence state reached. After repeating inî€›nite layers of message passing, we express the î€›nal convergence condition as follows: That is, the representations of the last two layers keep unchanged, since the vector generated from neighborhood aggregation equals to the node representation itself. We useğ‘’(orğ‘’) to denote the î€›nal converged representation of userğ‘¢(or itemğ‘–). Then, Equation 3 can be rewritten as: After some simpliî€›cations, we derive the following convergence state:î³ In other words, if Equation 9 is satisî€›ed for each node, it reaches the convergence state of message passing. Instead of performing explicit message passing, we aim to directly approximate such convergence state. To this end, a straightforward way is to minimize the diî€erence of both sides of Equation 9. In this work, we normalize the embeddings to unit vectors and then maximize the dot product of both terms: which is equivalent to maximize the cosine similarity betweenğ‘’ andğ‘’. For ease of optimization, we further incorporate sigmoid activation and negative log likelihood [2], and derive the following whereğœis the sigmoid function. The loss is optimized to fulî€›ll the structure constraint imposed by Equation 9. As such, we denoteL as the constraint loss and denote ğ›½as the constraint coeî€œcient. However, optimizingLcould also suî€er from the over-smoothing problem asLrequires all connected pairs (ğ›½>0) to be similar. In this way, users and items could easily converge to the same embeddings. To alleviate the over-smoothing problem, conventional GCN-based CF models usually î€›x a small number of message passing layers, e.g., 2âˆ¼4 layers in LightGCN. Instead, as UltraGCN approximates the limit of inî€›nite-layer message passing via a constraint loss, we choose to perform negative sampling during training. This is inspired from the negative sampling strategy used in Word2Vec [19], which provides a more simple and eî€ective way to counteract the over-smoothing problem. After performing negative sampling, we î€›nally derive the following constraint loss: whereğ‘andğ‘represent the sets of positive pairs and randomly sampled negative pairs. Note that we omit the summation overğ‘ˆfor ease of presentation. The constraint lossLenables UltraGCN to directly approximate the limit of inî€›nite-layer message passing to capture arbitrary high-order collaborative signals in the user-item bipartite graph, while eî€ectively avoiding the troublesome oversmoothing issue via negative sampling. Furthermore, we note that ğ›½acts as the loss weight inL, which is inversely proportional toğ‘‘andğ‘‘with similar magnitudes. This is interpretable for CF. If a user interacts with many items or an item is interacted by many users, the inî€uence of their interaction would be small, and thus the loss weight of this (ğ‘¢, ğ‘–) pair should be small. 3.1.1 Optimization. Typically, CF models perform item recommendation by applying either pairwise BPR (Bayesian personalized ranking) loss [22] or pointwise BCE (binary cross-entropy) loss [11] for optimization. We formulate CF as a link prediction problem in graph learning. Therefore, we choose the following BCE loss as the main optimization objective. It is also consistent with the loss format of L.îƒ•îƒ• whereğ‘andğ‘represent positive and randomly sampled negative links (i.e.,ğ‘¢-ğ‘—pairs). Note that for simplicity, we use the same sets of sample pairs withL, but they could also be made diî€erent conveniently. AsLandLdepends only on the user-item relationships, we deî€›ne it as the base version of UltraGCN, denoted as UltraGCN, which has the following optimization objective. whereğœ†is the hyper-parameter to control the importance weights of two losse terms. As Equation 5 shows, except for user-item relationships, some other relationships (e.g., item-item and user-user relationships) also greatly contribute to the eî€ectiveness of GCN-based models on CF. However, in conventional GCN-based models, these relationships are implicitly learned through the same message passing layers with user-item relationships. This not only leads to the unreasonable edge weight assignments as discussed in Section 2.2, but also fails to capture the relative importances of diî€erent types of relationships. In contrast, UltraGCN does not rely on explicit message passing so that we can separately learn other relationships in a more î€exible way. This also enables us to manually adjust the relative importances of diî€erent relationships. We emphasize that UltraGCN is î€exible to extend to model many diî€erent relation graphs, such as user-user graphs, itemitem graphs, and even knowlege graphs. In this work, we mainly demonstrate its use on the item-item co-occurrence graph, which has been shown to be useful for recommendation in [26]. We î€›rst build the item-item co-occurrence graph by linking items that have co-occurrences, which produces the following weighted adjacent matrix ğº âˆˆ R. where each entry denotes the co-occurrences of two items. We follow Equation 9 to approximate inî€›nite-layer graph convolution on ğº and derive the new coeî€œcient ğœ”: whereğ‘”andğ‘”denote the degrees (sum by column) of itemğ‘–and item ğ‘— in ğº, respectively. Similar to Equation 12, we can derive the constraint loss on the item-item graph to learn the item-item relationships in an explicit way. However, as the adjacency matrixğºof the item-item graph is usually much denser compared to the sparse adjacency matrixğ´of the user-item graph, directly minimizing the constraint loss onğº would introduce too many unreliable or noisy item-item pairs into optimization, which may make UltraGCN diî€œcult to train. This is also similar to theLimitation IIof conventional GCN-based models described in Section 2.2. But thanks to the î€exible design of UltraGCN, we choose to select only informative pairs for training. Speciî€›cally, to keep sparse item connections and retain training eî€œciency, we î€›rst select top-ğ¾most similar itemsğ‘† (ğ‘–)for itemğ‘– according toğœ”. Intuitively,ğœ”measures the similarity of itemğ‘– and itemğ‘—, since it is proportional to the co-occurrence number of itemğ‘–and itemğ‘—, yet inversely proportional to the total degrees of both items. Instead of directly learning item-item pairs, we propose to augment positive user-item pairs to capture item-item relationships. This keeps the training terms of UltraGCN being uniî€›ed and decrease the possible diî€œculty in multi-task learning. We also empirically show that such a way can achieve better performance in Section 4.4. For each positive (ğ‘¢,ğ‘–) pair, we î€›rst constructğ¾ weighted positive (ğ‘¢,ğ‘—) pairs, forğ‘— âˆˆ ğ‘† (ğ‘–). Then, we penalize the learning of these pairs with the more reasonable similarity score ğœ”and derive the constraint lossLon the item-item graph as follow:îƒ•îƒ• where|ğ‘† (ğ‘–)| = ğ¾. We omit the negative sampling here as the negative sampling inLandLhas already enabled UltraGCN to counteract over-smoothing. With this constraint loss, we extend UltraGCN to better learn item-item relationships, and î€›nally derive the following training objective of UltraGCN, whereğœ†andğ›¾are hyper-parameters to adjust the relative importances of user-item and item-item relationships, respectively. Figure 1 illustrates the simple architecture of UltraGCN in contrast to LightGCN. Similarly, in inference, we use the dot product Ë†ğ‘¦= ğ‘’ğ‘’between userğ‘¢and itemğ‘–as the ranking score for recommendation. 3.3.1 Mo del Analysis. We î€›rst analyze the strengths of our UltraGCN model: 1) The weights assigned on edges in UltraGCN, i.e., ğ›½andğœ”, are more reasonable and interpretable for CF, which are helpful to better learn user-item and item-item relationships, respectively. 2) Without explicit message passing, UltraGCN is î€exible to separately customize its learning with diî€erent types of relationships. It is also able to select valuable training pairs (as in Section 3.2), rather than learn from all neighbor pairs indistinguishably, which may be mislead by noise. 3) Although UltraGCN is trained with diî€erent types of relationships in a multi-task learning way, its training losses (i.e.,L,L, andL) are actually uniî€›ed, following the form of binary cross entropy. Such uniî€›cation facilitates the training of UltraGCN, which converges fast. 4) The design of UltraGCN is î€exible, by settingğ›¾to 0, it reduces to UltraGCN, which only learns on the user-item graph. The performance comparison between UltraGCN and UltraGCNis provided in Table 2. Note that in the current version, we do not incorporate the modeling of user-user relationships in UltraGCN. This is mainly because that usersâ€™ interests are much broader than itemsâ€™ attributes. We found that it is harder to capture the user-user relationships from the user-user co-occurrence graph only. In Section 4.4, we empirically show that learning on the user-user co-occurrence graph does not bring noticeable improvements to UltraGCN. In contrast, conventional GCN-based CF models indistinguishably learn over all relationships from the user-item graph (i.e., Limitation II) likely suî€er from performance degradation. The user-user relationships may be better modeled from a social network graph, and we leave it for future work. 3.3.2 Relations to Other Models. In this part, we discuss the relations between our UltraGCN and some other existing models. Relation to MF. UltraGCN is formally to be a new weighted MF model with BCE loss tailored for CF. In contrast to previous MF models (e.g., NeuMF [11]), UltraGCN can more deeply mine the collaborative information using graphs, yet keep the same concise architecture and model eî€œciency as MF. Relation to Network Embedding Methods. Qiu et al. [21] have proved that many popular network embedding methods with negative sampling (e.g., DeepWalk [20], LINE [25], and Node2Vec [7]) all can be uniî€›ed into the MF framework. However, in contrast to these network embedding methods, the edge weights used in UltraGCN are more meaningful and reasonable for CF, and thus lead to much better performance. In addition, the random walk in many network embedding methods will also uncontrollably introduce uninformative relationships that aî€ect the performance. We empirically show the superiority of UltraGCN over three typical network embedding methods on CF in Section 4.2. Relation to One-Layer LightGCN. We emphasize that UltraGCN is also diî€erent from one-layer LightGCN with BCE loss, because LightGCN applies weight combination to embeddings aggregation while our constraint coeî€œcients are imposed on the constraint loss function, which aims to learn the essence of inî€›nitelayer graph convolution. On the contrary, UltraGCN can overcome the limitations of one-layer LightGCN as described in Section 3.2. 3.3.3 Model Complexity. Given the embedding sizeğ‘‘,ğ¾similar items for eachğ‘† (ğ‘–),ğ‘…as the number of negative samples for each positive pair, and|ğ´|as the number of valid non-zero entries in the user-item interaction matrix, we can derive the training time complexity of UltraGCN:O((ğ¾ +ğ‘… +1) âˆ— |ğ´| âˆ— (ğ‘‘+1)). We note that the time complexities to calculateğ›½andğœ”areO(1), since we can pre-calculate them oî€Ÿine before training. As we usually limit ğ¾to be small (e.g., 10 in our experiments) in practice, the time complexity of UltraGCN lies in the same level with MF, which is O((ğ‘… +1) âˆ— |ğ´| âˆ— ğ‘‘). Besides, the only trainable parameters in UltraGCN are the embeddings of users and items, which is also the same with MF and LightGCN. As a result, our low-complexity UltraGCN brings great eî€œciency for model training and should be more practically applicable to large-scale recommender systems. We î€›rst compare UltraGCN with various state-of-the-art CF methods to demonstrate its eî€ectiveness and high eî€œciency. We also perform detailed ablation studies to justify the rationality and effectiveness of the design choices of UltraGCN. Datasets and Evaluation Protocol.We use four publicly available datasets, including Amazon-Book, Yelp2018, Gowalla, and MovieLens-1M to conduct our experiments, as many recent GCNbased CF models [10,27,28,32] are evaluated on these four datasets. We closely follow these GCN-based CF studies and use the same data split as them. Table 1 shows the statistics of the used datasets. For the evaluation protocol, Recall@20 and NDCG@20 are chosen as the evaluation metrics as they are popular in the evaluation of GCN-based CF models. We treat all items not interacted by a user as the candidates, and report the average results over all users. Baselines.In total, we compare UltraGCN with various types of the state-of-the-art models, covering MF-based (MF-BPR [15], ENMF [3]), metric learing-based (CML [12]), network embedding methods (DeepWalk [20], LINE [25], and Node2Vec [7]), and GCNbased (NGCF [27], NIA-GCN [24], LR-GCCF [4], LightGCN [10], and DGCF [28]). Parameter Settings.Generally, we adopt Gaussian distribution with 0 mean and 10standard deviation to initialize embeddings. In many cases, we adoptğ¿regularization with 10weight and we set the learning rate to 10, the batch size to 1024, the negative sampling ratioğ‘…to 300, and the size of the neighbor setğ¾to 10. In particular, we î€›x the embedding size to 64 which is identical to recent GCN-based work [10,24,27,28] to keep the same level of the number of parameters for fair comparison. We tuneğœ†in [0.2, 0.4, 0.6, 0.8, 1.0, 1.2, 1.4], andğ›¾in [0.1, 0.5, 1.0, 1.5, 2.0, 2.5, 3, 3.5]. For some baselines, we report the results from their papers to keep consistency. They are also comparable since we use the exactly same datasets and experimental settings provided by them. For other baselines, we mainly use their oî€œcial open-source code and carefully tune the parameters to achieve the best performance for fair comparisons. To allow for reproduciblility, we have released the source code and benchmark settings of UltraGCN at Github. Table 2 reports the performance comparison results. We have the following observations: â€¢UltraGCN consistently yields the best performance across all four datasets. In particular, UltraGCN hugely improves over the strongest GCN-based baseline (i.e., DGCF) on AmazonBook by 61.4% and 71.6% w.r.t. Recall@20 and NDCG@20 respectively. The results of signiî€›cance testing indicates that our improvements over the current strongest GCN-based baseline are statistically signiî€›cant (ğ‘-value<0.05). With additional learning on the item-item graph, UltraGCN performs consistently better than its simpler variant UltraGCN. We attribute such good performance of UltraGCN to the following reasons: 1) Compared with network embedding models and the other GCN-based models, UltraGCN can respectively î€›lter uninformative user-item and item-item relationships in a soft way (i.e., optimize withğ›½) and a hard way (i.e., only selectğ¾most similar item pairs). The edge weights for the learning of user-item and item-item relationships in UltraGCN are also more reasonable; 2) Compared with other baselines, UltraGCN can leverage powerful graph convolution to exploit useful and deeper collaborative information in graphs. These advantages together lead to the superiority of UltraGCN than compared state-of-the-art models. â€¢Overall, network embedding models perform worse than GCN-based models, especially on Gowalla. The reason might be that the powerful graph convolution is more eî€ective than traditional random walk or heuristic mining strategies in many network embedding methods, to capture collaborative information for recommendation. â€¢Since UltraGCN is a special MF which only needs the dot product operation for embeddings, its architecture is orthogonal to some state-of-the-art models (e.g., DGCF). Therefore, similar to MF, UltraGCN can be deemed as an eî€ective and eî€œcient CF framework which is possible to be incorporated with other methods, such as enabling disentangled representation for users and items as DGCF, to achieve better performance. We leave such study in future work. As highlighted in Section 3.3, UltraGCN is endowed with high training eî€œciency for CF thanks to its concise and uniî€›ed designs. We have also theoretically demonstrated that the training time complexity of UltraGCN is on the same level as MF in Section 3.3.3. In this section, we further empirically demonstrate the superiority of UltraGCN on training eî€œciency compared with other CF models, especially GCN-based models. To be speciî€›c, we select MF-BPR, ENMF, LightGCN, and LR-GCCF as the competitors, which are relatively eî€œcient models in their respective categories. To be more convincing, we compare their training eî€œciency from two views: â€¢The total training time and epochs for achieving their best performance. â€¢Training them with the same epochs to see what performance they can achieve. Note that the validation time is not included in the training time. Considering the fact that the oî€œcial implementations of the compared models can be optimized to be more eî€œcient, we use a uniform code framework implemented by ourselves for all models for fair comparison. In particular, our implementations refer to their oî€œcial versions and optimize them with uniform acceleration methods (e.g, parallel sampling) to ensure the fairness of comparison. We will release all of our code. Experiments are conducted on AmazonBook with the same Intel(R) Xeon(R) Silver 4210 CPU @ 2.20GHz machine with one GeForce RTX 2080 GPU for all compared models. Results of the two experiments are shown in Table 3 and Table 4 respectively. We have the following conclusions: (1) Table 3 shows that the training speed (i.e., Time/Epoch) of UltraGCN is close to MF-BPR, which empirically justiî€›es our analysis that the time complexities of UltraGCN and MF are on the same level. UltraGCN needs 75 epochs to converge which is much less than LR-GCCF and LightGCN, leading to only 45 minutes for total training. Finally, UltraGCN has around 14x, 4x, 4x speedup compared with LightGCN, LR-GCCF, and ENMF respectively, demonstrating the big eî€œciency superiority of UltraGCN. (2) Table 4 shows that when UltraGCN converges (i.e., train the î€›xed 75 epochs), the performances of all the other compared models are much worse than UltraGCN. That is to say, UltraGCN can achieve much better performance with less time, which further Table 2: Overall performance comparison. Improv. denotes the relative improvements over the best GNN-base d baselines. Table 3: Eî€œciency comparison from the î€›rst view. Table 4: Eî€œciency comparison from the second view. All models are trained with the î€›xed 75 epochs except MF-BPR. Since MF-BPR needs less than 75 epochs to converge, we report its actual training time. demonstrates the higher eî€œciency of UltraGCN than the other GCN-based CF models. We perform ablation studies on Amazon-Book to justify the following opinions: (i) The designs of UltraGCN is eî€ective, which can î€exibly and separately learn the user-item relationships and item-item relationships to improve recommendation performance; (ii) Augmenting positive user-item pairs for training to learn itemitem relationships can achieve better performance than optimizing between item-item pairs; (iii) User-user co-occurrence information is probably not very informative to help recommendation. For opinion (i), we compare UltraGCN with the following variants to show the eî€ectiveness of our designs in UltraGCN: â€¢UltraGCN(ğœ† =0,ğ›¾ =0): when settingğœ†andğ›¾to 0, UltraGCN is simply reduced to MF training with BCE loss function, which does not leverage graph information and cannot capture higher-order collaborative signals. â€¢UltraGCN(ğ›¾ =0): this variant is identical to UltraGCN, which only learns on the user-item graph and lacks more eî€ective learning for item-item relationships. â€¢UltraGCN(ğœ† =0): this variant lacks the graph convolution ability for learning on the user-item graph to more deeply mine the collaborative information. Results are shown in Figure 2. We have the following observations: (1) UltraGCN(ğ›¾ =0) and UltraGCN(ğœ† =0) all perform better than UltraGCN(ğœ† =0,ğ›¾ =0), demonstrating that the designs of UltraGCN can eî€ectively learn on both the user-item graph and item-item graph to improve recommendation; (2) Relatively, UltraGCN(ğœ† =0) is inferior to UltraGCN(ğ›¾ =0), indicating that user-item relationships may be better modeled than item-item relationships in UltraGCN; (3) UltraGCN performs much better than all the other three variants, demonstrating that our idea to disassemble various relationships, eliminate uninformative things which may disturb the model learning, and î€›nally conduct multi-task learning in a clearer way, is eî€ective to overcome the limitations (see Section 2.2) of previous GCN-based CF models. For opinion (ii), we change Lto L: which is instead to optimize between the target positive item and its mostğ¾similar items. We compare the performance of UltraGCN usingLandLrespectively with careful parameters tuning. Results are shown in Figure 3. It is clear that no matter incorporatingLor Table 5: Performance comparison of whether learning on the user-user co-occurrence graph. Figure 2: Performance comparison of variants of UltraGCN. Figure 3: Performance comparison of using Land L. not, usingLcan achieves obvious better performance than using L, which proves that our designed strategy to learn on item-item graph is more eî€ective. Furthermore, the performance gap between usingLand usingLbecomes large when incorporatingL, indicating that our strategy which makes the objective of UltraGCN uniî€›ed can thus facilitate training and improve performance. For opinion (iii), we derive the user-user constraint lossL with the similar method of Section 3.2 and combine it to the î€›nal objective. We carefully re-tune the parameters and show the comparison results of whether usingLin Table 5. As can be seen, incorporatingLto learn user-user relationships does not bring obvious beneî€›ts to UltraGCN. We attribute this phenomenon to the fact that the usersâ€™ interests are broader than itemsâ€™ properties, and thus it is much harder to capture user-user relationships just from the user-user co-occurrence graph. Therefore, we do not introduce the modeling of user-user relationships into UltraGCN in this paper, and we will continue to study it in the future. Figure 4: Performance comparison of setting diî€erent ğ¾. Figure 5: Performance comparison with diî€erent ğœ† and ğ›¾. We investigate the inî€uence of the number of selected neighborsğ¾ and the weights of the two constraint losses (i.e.,ğœ†andğ›¾) on the performance for a better understanding of UltraGCN. 4.5.1 Impact ofğ¾. We test the performance of UltraGCN with diî€erentğ¾in [5, 10, 20, 50] on Amazon-Book and Yelp2018. Figure 4 shows the experimental results. We can î€›nd that whenğ¾ increases from 5 to 50, the performance shows a trend of increasing î€›rst and then decreasing. This is because that whenğ¾is 5, the item-item relationships are not suî€œciently exploited. While when ğ¾becomes large, there may introduce some less similar or less conî€›dent item-item relationships into the learning process that aî€ect model performance. Such phenomenon also conî€›rms that conventional GCN-based CF models inevitably take into account too many low-conî€›dence relationships, thus hurting performance. 4.5.2 Impact ofğœ†andğ›¾. We î€›rst setğœ† =0 and show the performance of diî€erentğœ†from 0.2 to 1.4 (0.2 as the interval). Then we test with diî€erentğ›¾in [0.1, 0.5, 1, 1.5, 2, 2.5, 3, 3.5] based on the bestğœ†. Experiments are conducted on Amazon-Book, and we show results in Figure 5. Forğœ†, we î€›nd that the small value limits the exertion of the user-item constraint loss, and a value of 1 or so would be suitable forğœ†. For the impact ofğ›¾, its trend is similar to ğœ†but is more signiî€›cant, and 2.5 is a suitable choice forğ›¾. In general, our investigations forğœ†andğ›¾show that these two parameters are important to UltraGCN, which can î€exibly adjust the learning weights for diî€erent relationships and should be carefully set. In this section, we brieî€y review some representative GNN-based methods and their eî€orts for model simpliî€›cation toward recommendation tasks. With the development and success of GNN in various machine learning areas, there appears a lot of excellent work in recommendation community since the interaction of users and items could be naturally formed to a user-item bipartite graph. Rianne van den Berg et al. [1] propose graph convolutional matrix completion (GCMC), a graph-based auto-encoder framework for explicit matrix completion. The encoder of GC-MC aggregates the information from neighbors based on the types of ratings, and then combine it to the new embeddings of the next layer. It is the î€›rst work using graph convolutional neural networks for recommendation. Ying et al. [31] î€›rst applys GCN on web-scale recommender systems and propose an eî€œcient GCN-based method named Pinsage, which combines eî€œcient random walks and graph convolutions to generate embeddings of items that incorporate both graph structure as well as item feature information. Then, Wang et al. [27] design NGCF which is a new graph-based framework for collaborative î€›ltering. NGCF has a crafted interaction encoder to capture the collaborative signals among users and items. Although NGCF achieves good performance compared with previous non-GNN based methods, its heavy designs limit its eî€œciency and full exertion of GCN. To model the diversity of user intents on items, Wang et al. [28] devise Disentangled Graph Collaborative Filtering (DGCF), which considers user-item relationships at the î€›ner granularity of user intents and generates disentangled user and item representations to get better recommendation performance. Although GNN-based recommendation models have achieved impressive performance, their eî€œciencies are still unsatisfactory when facing large-scale recommendation scenarios. How to improve the eî€œciency of GNNs and reserve their high performance for recommendation becomes a hot research problem. Recently, Dai et al. [6] and Gu et al. [8] extend î€›xed-point theory on GNN for better representation learning. Liu et al. [18] propose UCMF that simpliî€›es GCN for the node classiî€›cation task. Wu et al. [29] î€›nd the non-necessity of nonlinear activation and feature transformation in GCN, proposing a simpliî€›ed GCN (SGCN) model by removing these two parts. Inspired by SGC, He et al. [10] devise LightGCN for recommendation by removing nonlinear activation and feature transformation too. However, its eî€œciency is still limited by the time-consuming message passing. Qiu et al. [21] demonstrate that many network embedding algorithms with negative sampling can be uniî€›ed into the MF framework which may be eî€œcient, however, their performances still have a gap between that of GCNs. We are inspired by these instructive studies, and propose UltraGCN for both eî€œcient and eî€ective recommendation. In this work, we propose an ultra-simpliî€›ed formulation of GCN, dubbed UltraGCN. UltraGCN skips explicit message passing and directly approximate the limit of inî€›nite message passing layers. Extensive experimental results demonstrate that UltraGCN achieves impressive improvements over the state-of-the-art CF models in terms of both accuracy and eî€œciency. This work was supported in part by the National Natural Science Foundation of China (61972219), the Research and Development Program of Shenzhen (JCYJ20190813174403598, SGDX20190918101201696), the National Key Research and Development Program of China (2018YFB1800601), and the Overseas Research Cooperation Fund of Tsinghua Shenzhen International Graduate School (HW2021013). To further demonstrate the eî€ectiveness of UltraGCN, we additionally provide the results compared to some more recent stateof-the-art CF models, including NBPO [33], BGCF [23], SCF [34], LCFN [32], and SGL-ED [30]. For simplicity and fairness of comparison, we use the same dataset and evaluation protocol provided by each paper. We also duplicate the results reported in their papers to keep consistency. The results in Table 6 again validate the eî€ectiveness of UltraGCN, which outperforms the most recent CF models by a large margin. Table 6: Performance comparison with some more models, including SCF, LCFN, NBPO, BGCF, and SGL-ED.