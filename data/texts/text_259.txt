Traditional approaches to next-item and next basket recommendation typically extract usersâ€™ interests based on their past interactions and associated static contextual information (e.g. a user id or item category). However, extracted interests can be inaccurate and become obsolete. Dynamic attributes, such as user income changes, item price changes (etc.), change over time. Such dynamics can intrinsically reî€ect the evolution of usersâ€™ interests. We argue that modeling such dynamic attributes can boost recommendation performance. However, properly integrating them into user interest models is challenging since attribute dynamics can be diverse such as time-interval aware, periodic patterns (etc.), and they represent usersâ€™ behaviors from diî€erent perspectives, which can happen asynchronously with interactions. Besides dynamic attributes, items in each basket contain complex interdependencies which might be beneî€›cial but nontrivial to eî€ectively capture. To address these challenges, we propose a novel aware encoder to allow the model to capture various temporal patterns from dynamic attributes. To eî€ectively learn useful item relationships, intra-basket attention module is proposed. Experimental results on three real-world datasets demonstrate that our method consistently outperforms the state-of-the-art. CCS Concepts: â€¢ Information systems â†’ Recommender systems; â€¢ Computing methodologies â†’ Neural networks. Additional Key Words and Phrases: Dynamic Attributes, Context Interaction Learning, Next Basket Recommendation ACM Reference Format: Yongjun Chen, Jia Li, Chenghao Liu, Chenxi Li, Markus Anderle, Julian McAuley, and Caiming Xiong. 2021. Modeling Dynamic Attributes for Next Basket Recommendation. In CARS: Workshop on Context-Aware Recommender Systems (RecSys â€™21), October 2nd, 2021, Online, Worldwide. ACM, New York, NY, USA, 10 pages. https://doi.org/nn.nnnn/nnnnnnn.nnnnnnn Sequential recommendation systems have been successfully applied to various applications such as product recommendation, food recommendation, music recommendation, etc. There are two lines of sequential recommendation tasks based on diî€erent assumptions about usersâ€™ interaction behavior. Next item recommendation assumes that users interact with items sequentially, so that recommendations can be made by modeling the sequential semantics of the interaction history [ ttributes (namedAnDa). AnDa separately encodes dynamic attributes and basket item sequences. We design a periodic Next basket recommendation [16,28] assumes that users interact with multiple items during each round (i.e., a basket). The goal is to recommend a basket of items that a user is likely to interact with at the next time step. In the next basket recommendation task, in addition to the sequential patterns underlying historical interactions, items in each basket , and context of users or items often provides useful information. . Existing solutions for next-basket recommendation tasks train a sequential recommender based on the usersâ€™ interaction history [8,11,18,28], or with additional static contextual attributes [2] (product category, brand, etc.) to extract usersâ€™ interests. However, extracted interests can be inaccurate and obsolete. Dynamic attributes, which change over time, appear in many applications, and can provide more accurate descriptions of the shift of a userâ€™s interests or changes in item properties. For example, in a bank product recommendation scenario (see Figure 1), products from the bank are recommended to customers. Given only a sequence of monthly records of a customerâ€™s products, the next basket recommended to the user is deterministic. Instead, dynamic attributes such as the household income and the customer membership type , which are changing overtime, help a recommender to beter capture a userâ€™s changing interests. Case 1 and 2 in Figure 1 illustrate that two customers with the same historical purchase behaviors but diî€erent household income and membership type sequences can have diî€erent interests. Although it is essential to model dynamic attributes, properly integrating dynamic attributes into sequential models is challenging. First, the temporal patterns underlying dynamic attributes can be diverse. There can be time-interval (i.e., two items purchased with diî€erent time-intervals has diî€erent impact on usersâ€™ future purchase behaviors.) and periodic patterns (i.e., seasons, weekday/weekend patterns, etc.). Second, dynamic attributes represent usersâ€™ behaviors from diî€erent perspectives, and can happen asynchronously with interactions. Directly concatenating basket items with dynamic attributes at each time step may not be stable to model diverse sequential patterns. Besides dynamic attributes, in usersâ€™ historical interactions, items in each basket contain complex interrelationships (item correlations, co-purchases, etc.). Existing solutions [3,11] either pre-deî€›ne static item correlations based on co-occurring items, or (vanilla) attention to extract item correlations based on the whole items the last basket. However, multiple item interrelationships can exist based on multiple subsets of items in a basket and together inî€uence next basket items. For example, in a grocery shopping, a customer bought apple, banana, TV and Speaker. The apple and banana are high correlated while TV and Speaker are also high correlated. The user may purchase some fruits again with TV accessories. Using multi-head attention allows the model to capture diî€erent item relationships under diî€erent subset of items in a historical basket. To address the above challenges, we propose a novelAttentivenetwork to model theDynamicattributes as well as usersâ€™ historical interacted items (AnDafor short). AnDa separately encodes and learns representations from dynamic attributes and interactions with basket items. To allow the model to capture time-interval aware and periodic patterns, we propose an input encoder containing a time-aware padding and periodic index embedding to encode dynamic attributes. To capture complex item relationships in each basket, an intra-basket attentive module is introduced. It is applied to each basket item to extract useful item relationships. We conduct experiments on three real-world datasets for next basket recommendation. Our experimental results demonstrate that our proposed method signiî€›cantly outperforms baseline approaches. 2 RELATED WORK 2.1 Next Basket Recommendation To capture sequential patterns at a basket level for next basket recommendation, DREAM [ basket using max and average pooling and learns the sequence representation through an RNN-based network. ANAM [2] improves upon DREAM by considering static item attributes using vanilla attention. Sets2Sets [ as a multiple baskets prediction and proposes a RNN based encoder-decoder method to improve the performance. To capturing item relationships at each basket, Beacon [ on the co-occurring items in the observed training baskets and then incorporates it into an RNN-based model. IIANN [ learns the correlation between the most recent basket items and the target item to summarize usersâ€™ short-term interests. In this work, we use multi-head self-attention within each basket so that complex item interrelationships (e.g., co-occurrences) can be captured. MITGNN [ across diî€erent usersâ€™. In comparison, our work focus on leverage dynamic attributes for providing more accurate user interests. 2.2 Feature Interaction Learning Factorization Machines (FMs) [ for recommendation [ feature interactions using DNN. NFM [ Wide&Deep [ interactions. Diî€erent from Wide&Deep, Deep&Cross [ and xDeepFM [ to capture high-order feature interactions implicitly, lack good explanation ability in general. To this end, AutoInt [ uses a self-attention mechanism to model high-order interactions with a more precise explanation of the interacted features. Inspired by this, we also apply multi-head self-attention to learn higher-order feature interactions and item interrelationships in each basket. 2.3 Aî€ention Mechanisms With the success of Transformer networks in machine translation tasks [ is the î€›rst work that uses a pure self-attention mechanism to model sequential recommendation and demonstrates better performance than RNN-based methods. TiSASRec [ interval between two adjacent interactions. BERT4Rec [ FDSA [29 vanilla attention to capture usersâ€™ interests. However, it does not incorporate dynamic user attributes and only models the sequential patterns at the item level instead of the basket level. Although this approach could be extended for 5] model uses a wide part to model second-order interactions and a deep part to model the higher-order 13] proposes a CIN module to take the outer product at a vector-wise level. These works, which use DNN ] further improves sequential recommendation by incorporating the usage of static item attributes and using ğ´predicted relevance score for item ğ‘– at time ğ‘¡ modeling dynamic features, it uses vanilla attention to average out diî€erent attributes at each time step. As a result, it is not able to learn high-order feature interactions. Also, it loses the temporal aspects of individual features. For example, if the trend of a usage metric is going up or down in the past three months, FDSA will not capture such a trend due to its averaging operation. Instead, our time-level attention module can capture these temporal patterns. 3 PROPOSED APPROACH In this section, we î€›rst deî€›ne the notation and formalize the next basket recommendation task with dynamic user attribute information. And then we present the proposed frameworkAnDain detail. The framework is illustrated in Figure 2 and the notation is summarized in Table 1. 3.1 Problem Statement In next basket recommender systems with dynamic attributes, historical basket interactions and dynamic attribute sequences are given, and the goal is to recommend the next basketâ€™s items. Formally, we denotesğ‘ˆ,ğ‘‰andğ¹a sets ofno users, items and user attributes respectively. For a userğ‘¢ âˆˆ ğ‘ˆ, a sequence of basketsğµ=ğµ, ğµ, Â· Â· Â· , ğµrepresents his or her item interactions sorted by time.ğ‘‡is the maximum time steps of each sequence, andğµâŠ† ğ‘‰is a set of itemsno that userğ‘¢interacted with at time stepğ‘¡. A sequenceğ´=ğ´, ğ´, Â· Â· Â· , ğ´represents the value of dynamic user attributes of userğ‘¢ordered by time. Speciî€›cally,ğ´âˆˆ ğ¹are all the attribute values ofğ‘¢at time stepğ‘¡. The goal is to predict basket items that user ğ‘¢ will interact with at time step ğ‘‡ + 1 given ğ‘‡ historical baskets ğµand attributes ğ´. 3.2 Time-Interval and Periodic Aware Input Encoder Embedding Lookup:For each basket of dynamic attributesğ´, we model categorical and numerical features diî€erently. Categorical attributesğ´âŠ† ğ´are represented by an|ğ¹|-dimensional multi-hot vector denoted byğ‘’âˆˆ R. Numerical attributes are normalized into the range[âˆ’1,1]using min-max normalization, denoted asğ‘ âˆˆ R. Each basket of itemsğµis represented by a|ğ‘‰ |-dimensional multi-hot representation, denoted byğ‘’âˆˆ R. After that, we apply a concatenation-based lookup function [2] to encode ğ‘’and ğ‘’: whereğ‘… âˆˆ Randğ‘„ âˆˆ Rare learnable embedding matrices for categorical attributes and items. The â€œCONCAT-LOOKUPâ€ function retrieves the corresponding embedding vectors and then concatenates them together to form matricesğ‘ âˆˆ R, andğ‘ âˆˆ R.ğ·is the embedding dimension of each item and categorical attribute. |ğ‘‰|is the total number of items inğµ. Since the number of items in each basket varies, we set the maximum number of items in the basket as the largest basket size in the training set|ğ‘‰|, and add padding vectors for baskets smaller than Time-aware Padding Operation: stepsâ€™ information. If the sequence length is shorter than Otherwise, we truncate to the last the sequence length is ğ‘†, and ğ‘† Periodic Index Embedding: periodic patterns. The index repeats over every capturing seasonal patterns when the time interval between each two baskets is one month. Positional embeddings [ that commonly used to identify item positions is also used in this paper. Formally, we concatenate the periodic and positional index embedding with periodic and positional embedding vectors. Then a basket sequence is represented as add positional and periodic index embeddings to ğ‘† 3.3 Time Level Aî€ention Module To capture temporal patterns from (MHSA) [24]. Formally, let ğ¿ where learned parameters (ğ¶ = (|ğ¹ as the padded basket item, categorical attribute, and numerical attribute sequences respectively. â„is the number of sub-spaces,ğ‘Šâˆˆ R,ğ‘Šâˆˆ R,ğ‘Šâˆˆ Randğ‘Šâˆˆ Rare Following [10], we add causality mask to avoid future information leek. To enhance the representation learning of the self-attention block, residual connections [6], dropout [22], layer normalization [1], and two fully connected layers with ReLU activation functions are added to form the entire multi-head self-attention block (MHSAB) as follows: We stack multiple attention blocks to capture more complex feature interactions: whereğ¿= ğ¿andâ„is the number of heads at theğ‘˜attention block.ğ¿is the output after stacking multiple timelevel attention layers. The extracted representation vector at time stepğ‘¡can be denotedğ¿and contains information extracted from time 1 to time ğ‘¡ of the input sequence. 3.4 Intra-Basket and Intra-Aî€ribute Self-Aî€ention Modules Item correlations in each basket can reveal some useful information such as co-purchase relationships. The key problem is how to determine which items should be combined or are correlated. In this paper, we use multi-head self-attention to learn information such as itemsâ€™ correlation relationships. Specially, given representations of all items in a basket ğ¿, a single-head self-attention module will î€›rst compute the similarity matrix which is seen as item correlation scores, and then it updates the item representationğ‘™by combining all relevant items using the similarity coeî€œcients (generated based on the similarity matrix). We use MHSAB to enhance the modelâ€™s capability of capturing complex item correlations, which is formed as follows: where ğ¿is the output of the time level attention module at time ğ‘¡, and â„is the (ğ‘˜ + 1)attention block. We stack Eq. 5ğ‘štimes to capture more complex item relationshipsğ¿. Similarly, we can stack Eq. 5 on dynamic attribute (named Intra-Attribute Attention) to get higher level categorical attribute interactions ğ¿. 3.5 Model Training The encoded user representations are projected as:ğ¿= FFNN ([ğ¿, ğ¿, ğ¿]), whereFFNNis a feed forward network,ğ¿âˆˆ Ris the î€›nal representation given dynamic attributes and basket items from time step 1 to ğ‘¡. We then adopt the binary cross-entropy loss as the objective for training the model deî€›ned as: whereğœis the sigmoid functionğœ (ğ‘¥) =1/(1+ğ‘’).ğ‘„is the item embedding matrix which is shared for encoding basketî€ˆî€‰ items in input encoders. The target basket items for userğ‘¢are a shifted version ofğµ, denoted byğµ, ğµ, Â· Â· Â· , ğµ. 4 EXPERIMENTS 4.1 Experimental Seî€ing 4.1.1 Datasets. Table 2 summarizes the statistics of the datasets. EPR is a private dataset sampled from a leading enterprise cloud platform. The task is to recommend products to businesses. Examples of the dynamic attributes are behavior metrics on the website, sales information, and marketing activities of the business. SPR product recommendations for the Santander bank. Ta-Feng to create train, validation, and test sets by chronological order. The <train, validation, test> sets for EPR, SPR, and Ta-Feng datasets are <1 respectively. The interval between each two time steps is 1 month for EPR and SPR datasets, and 1 day for Ta-Feng dataset. 4.1.2 Baselines & Evaluation Metrics. We include three groups of baseline methods: PopRec considers no sequential patterns; FMC [ with (FDSA+) or without (DREAM [ methods on the whole item set without sampling. All the items are î€›rst ranked and then evaluated by Hit Rate ( Normalized Discounted Cumulative Gain ( and NDCG with K=5. 4.1.3 Parameter Seî€ings. We tune the embedding dimension 0.01}, and dropout from {0.0, 0.1, 0.2, 0.5}. For DREAM, we tune with RNN, GRU, and LSTM modules. AdamOptimizer is used to update the network with moment estimates layers from 16, and 30 in EPR, SPR, and Ta-Feng respectively. We report the hyper-parameter sensitivity study results in Figure 3. 4.2 Overall Performance Comparison Table 3 shows overall results compared with baseline approaches. We observe that, î€›rst, our proposed approach consistently outperforms all baselines signiî€›cantly in terms of Hit Rate, NDCG, and MAP by 3.65% - 21.87%, 9.09% - 43.76%, and 2.32% - 24.53%, which demonstrate the eî€ectiveness of our proposed method. Beside, The next basket recommenders (DREAM, Beacon, CTA, and Sets2Sets) outperform those for next item recommendation (FMC, FPMC). This indicates that learning the sequential patterns with the encoding of the intra-baseket information can better capture usersâ€™ dynamic interests. The FDSA+ method performs the best among baselines in the EPR and SPR datasets, while Sets2Sets performs the best in the Ta-Feng dataset. The main reason is that FDSA+ leverages attribute information where EPR and SPR have more attributes. We also report the modelsâ€™ average inference time (milliseconds per sequence) on 400 sequence inputs in Table 3 (last row). The proposed method takes more time to generate recommendation lists than baseline methods, though is comparable with Set2Sets, DREAM, CTA, and FDSA+. Datasets/Information # Users # Items # Attributes Sparsity Avg. Baskets Avg. Basket Size 18] and FPMC [18] are Markov Chain-based sequential methods; and Neural Network based methods {1,2,4}and head number on each attention block from{1,2,4,6}. Maximum sequence lengths are set as 12, Table 3. Performance Comparison of diî€›erent methods on next basket recommendation. Bold/underlined scores are the best/second best in each row. The last column shows AnDaâ€™s relative improvement over the best baseline. 4.3 Ablation Study To understand the impact of diî€erent components in AnDa, we conduct a detailed ablation study using the SPR dataset in Table 4.AnDa(P)is AnDa without periodic index embedding. The results show that the periodic index can help capture usersâ€™ seasonal purchase patterns, and thus helps to improve performance.AnDa(B)is AnDa with basket information only. Without dynamic attributes,AnDa(B-)removes the intra-basket module from AnDa(B),AnDa(T) is AnDa without using intra-basket and intra-attribute modules on both items and attributes, andAnDa(I)is AnDa without applying the time level attention module. The performance degradation on the sub-models shows the beneî€›ts of each component. 4.4 Aî€ention Visualization We visualize the attention weights of time-level, intra-attribute, and intra-basket attentions on sampled sequences from the SPR dataset in Figure 3 (B) to gain more insights. (a) and (b) are attention weights from two diî€erent layers (layer 1 and 4) of time level basket attention, (c) and (d) are from two diî€erent heads of the î€›rst intra-attribute layer, and (e) and (f) are from two diî€erent head of the î€›rst intra-basket layer. From (a) and (b), we can see the attention varies over diî€erent layers. While the weights in layer 4 focus more on recent items, the weights in layer 1 attend more evenly to all previous histories. From (c) and (d), we observe that the attention weights vary over diî€erent heads, and the module captures meaningful feature interactions. For example, in (c), the position(11,1)(marked by a red square) corresponding to interacted feature value <"Foreigner index": NO, "Customerâ€™s Country residence": ES> (the bank is based in Spain, so a customer who lives in Spain is not a foreigner). We can also observe that intra-basket attention can capture diî€erent item relationships under diî€erent heads comparing with (e) and (f). Fig. 3. (A): hyperparamter sensitivity study results of AnDa. (B): visualization of aî€ention weights on diî€›erent MHSA modules. 5 CONCLUSION In this paper, we propose a novel attentive network AnDa, which models dynamic attributes to better capture usersâ€™ dynamically changing interests and intentions. AnDa separately extracts temporal patterns from dynamic attributes and user historical interactions with a novel input encoder. AnDa also generates feature interactions and uncovers item interrelationships in each basket with proposed intra-attribute and intra-basket modules respectively. We evaluate AnDa on three real-world datasets and demonstrate the usefulness of modeling dynamic attributes for next basket recommendation.