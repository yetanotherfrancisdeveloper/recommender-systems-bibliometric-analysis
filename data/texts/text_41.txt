By providing explanations for users and system designers to facilitate better understanding and decision making, explainable recommendation has been an important research problem. In this paper, we proposeCounterfactualExplainableRecommendation (CountER), which takes the insights of counterfactual reasoning from causal inference for explainable recommendation. CountER is able to formulate the complexity and the strength of explanations, and it adopts a counterfactual learning framework to seek simple (low complexity) and eî€ective (high strength) explanations for the model decision. Technically, for each item recommended to each user, CountER formulates a joint optimization problem to generate minimal changes on the item aspects so as to create a counterfactual item, such that the recommendation decision on the counterfactual item is reversed. These altered aspects constitute the explanation of why the original item is recommended. The counterfactual explanation helps both the users for better understanding and the system designers for better model debugging. Another contribution of the work is the evaluation of explainable recommendation, which has been a challenging task. Fortunately, counterfactual explanations are very suitable for standard quantitative evaluation. To measure the explanation quality, we design two types of evaluation metrics, one from userâ€™s perspective (i.e. why the user likes the item), and the other from modelâ€™s perspective (i.e. why the item is recommended by the model). We apply our counterfactual learning algorithm on a black-box recommender system and evaluate the generated explanations on î€›ve real-world datasets. Results show that our model generates more accurate and eî€ective explanations than state-of-the-art explainable recommendation models. Source code is available at https://github.com/chrisjtan/counter. â€¢ Computing methodologies â†’ Machine learning;â€¢ Information systems â†’ Recommender systems. Explainable Recommendation; Counterfactual Explanation; Counterfactual Reasoning; Machine Learning; Explainable AI ACM Reference Format: Juntao Tan, Shuyuan Xu, Yingqiang Ge, Yunqi Li, Xu Chen, and Yongfeng Zhang. 2021. Counterfactual Explainable Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3482420 Explainability for recommender systems is crucial, because in recommendation scenarios we can rarely say that some recommendation is absolutely right or some other recommendation is absolutely wrong, instead, it all depends on good explanations to help users understand why an item is recommended so as to increase the transparency and trust and enable better decision making; good explanations also help system designers to track the behavior of the complicated recommendation models for better debugging [49,50]. One prominent approach is aspect-aware explainable recommendation [9,11,40,50], which takes the explicit item features/aspects to construct explanations. For example, Zhang et al.[50] proposed Explicit Factor Model (EFM) which aligns latent factors with explicit features such as color and price to generate sentence explanations in the form of â€œYou might be interested in [feature], on which this product performs well.â€ Wang et al.[40] learned the user-aspect preferences in a multi-task joint tensor factorization framework to construct the aspect-aware explanations. Chen et al.[9] explored attribute-aware collaborative î€›ltering for explainable substitute recommendation. Li et al.[29] proposed a Personalized Transformer to generate aspect-inspired natural language explanations. A more comprehensive review of related work is provided in Section 2. However, existing methods on aspect-aware explainable recommendation face several issues: 1) Most of the methods are designed as intrinsic explainable models, although they have the advantage of providing faithful explanations, it may be diî€œcult for them to explain other black-box recommendation models. 2) Existing methods do not consider the explanation complexity, in particular, they do not have the ability to decide how many aspects to use when generating explanations. Most methods generate explanations using exactly one aspect, however, the real reason of the recommendation may be triggered by multiple aspects. 3) Existing methods do not consider the explanation strength, i.e., to what extent the explanation really inî€uences the recommendation result. This is mostly because existing methods are designed based on matching algorithms, which extracts associative signals such as feature importance and attention weights to construct explanations, while they seldom consider what happens if we intervene these signals to alternative values. Recent advances on counterfactual reasoning shed light on the possibility to solve the above problems. We use a toy example in Figure 1 to illustrate the intuition of counterfactual explanation and its diî€erence from matching-based explanation. In this example, similar to Zhang et al.[50], each user has his/her preference score on each aspect, while each item has its performance score on each aspect. The recommendation algorithm uses the total score to rank items. For example, the total score of Phone A is the recommendation list. To explain the recommendation of Phone A, a matching-based model would select screen as the explanation because the multiplication score for screen (4.0Ã—4.5=18) is the highest compared to the other aspects (5.0Ã—3.0=15 for battery and 3.0Ã—3.0=9 for price). However, Phone A actually performs the worst on screen among all products, which makes this explanation unreasonable. This shows that the aspects with high matching scores may not always be the reason of the recommendation. Counterfactual reasoning, on the contrary, aims to understand the underlying mechanism that really triggered the recommendation by applying interventions on the aspects and see what happens. As shown in the lower part of Figure 1, we can look for the minimal change on Phone Aâ€™s features such that Phone A will not be recommended anymore. In this case, the aspect battery will be selected as explanation because we only need to slightly change its score from 3 to 2.1 to reverse the recommendation decision, indicating that battery is a very inî€uential factor on the recommendation result. Following the above intuition, in this paper, we propose Counterfactual Explainable Recommendation (CountER), which adopts counterfactual reasoning to extract aspect-aware explanations for recommendation. Inspired by the Occamâ€™s Razor Principle [3], CountER is built on the fundamental idea of explanation complexity and explanation strength, where complexity measures how much change must be applied on the factual observations, and strength shows to what extent the applied change will inî€uence the recommendation decision. Through our proposed counterfactual constrained learning framework, CountER aims to extract simple (low complexity) and eî€ective (high strength) explanations for the recommendation by looking for minimal changes on the facts that would alter the recommendation decision. Another important challenge in explainable recommendation (and explainable AI) research is how to evaluate the explanations. Due to the lack of standard oî€Ÿine evaluation measures for explanation, previous research heavily relied on human subjects for evaluation, which makes the evaluation process expensive, unscalable, hard to standardize, and unfriendly to academic research settings [49]. Fortunately, counterfactual explanations are very suitable for standard quantitative evaluation. In this paper, we propose two types of evaluation methods, one is user-oriented evaluation, and the other is model-oriented evaluation. For user-oriented evaluation, we adopt similar ideas as [26,28,29] by using each userâ€™s mentioned aspects in their reviews as the ground-truth. By comparing our generated explanation with the ground-truth, we can Figure 1: Matching-based vs. counterfactual reasoning. The numbers beside the three aspects (screen, battery, and price) show how much the user cares about an aspect and how well the item performs on an aspect. In this example, matchingbased explanation methods would use â€œscreenâ€ to construct an explanation, while counterfactual reasoning method will use â€œbatteryâ€ to construct the explanation. evaluate the feature coverage, precision, recall, andğ¹scores. For model-oriented evaluation, we adopt the Probability of Necessity (PN), Probability of Suî€œciency (PS) and their harmonic meanğ¹ to evaluate the suî€œciency and necessity of the explanations. More details are provided in the experiments. In summary, this work has the following contributions: (1)For the î€›rst time, we explore the complexity and the strength of explanations in explainable recommendation and formulate the concepts in mathematical ways. (2)We formulate a counterfactual reasoning framework based on counterfactual constrained learning to extract simple and eî€ective explanations for recommendation. (3)We design both user-oriented and model-oriented metrics for standard evaluation of explainable recommendation. (4)We conduct extensive experiments on î€›ve real-world datasets to validate the eî€ectiveness of our proposed method. In this section, we review some related work on explainable recommendation and counterfactual reasoning. Explainable recommendation is a broad research area with many diî€erent types of models, and it is diî€œcult to cover all of the works on this direction. Since our work is more closely related with aspectaware explainable recommendation, we mainly focus on this subarea in the section. A more complete review of explainable recommendation can be seen in [19, 37, 49]. Explainability of recommender systems is important because it improves the transparency, user satisfaction and trust over the recommendation system [49]. One representative way to generate user-friendly explanations is by modeling aspects in the recommended items. For instance, Zhang et al.[50] introduced an Explicit Factor Model (EFM) for explainable recommendation. It î€›rst extracts the item aspects and the user opinions on these aspects from user reviews. Then, it trains a matrix factorization-based recommendation model to generate aspect-aware explanations by aligning the latent factors with the item aspects. Chen et al.[11] and Wang et al.[40] advanced from matrix factorization to tensor factorization models for explainable recommendation. He et al.[22] proposed a tripartite graph model to improve the interactivity of aspect-aware recommendation models. Gao et al.[18] proposed an explainable deep model to learn multi-level user proî€›le and infer which level of features best captures a userâ€™s interest. Balog et al.[2] presented a set-based recommendation technique to improve the scrutability and transparency of recommender systems. Ren et al.[35] proposed a collaborative viewpoint regression for explainable social recommendation. Wang et al.[42] proposed a tree-enhanced embedding method to combine embedding-based and tree-based models for explainable recommendation. More recently, Chen et al.[9] applied a residual feed-forward neural network to model the user and item explicit features and generates explainable substitute recommendations. Pan et al.[33] presented a feature mapping approach to map the latent features onto the interpretable aspects to achieve both satisfactory accuracy and explainability. Li et al.[29] proposed a Personalized Transformer to generate aspect-inspired natural language explanations. Xian et al.[46] developed an attribute-aware algorithm for explainable item-set recommendation and deployed in Amazon. Some other explainable recommendation methods include knowledge graph-based explanations [1,44,45], neural logic explanations [53], visual explanations [10], natural language explanations [7,28â€“30], dynamic explanations [12], reinforcement learningbased explanations [41], conversational explanations [13,52], fair explanations [17], disentangled explanations [31], review-based explanations [6, 32], etc. Most of the existing approaches generate explanations based on a very similar hypothesis: If there exists an aspect that best matches between the userâ€™s preference and the itemâ€™s performance, then this aspect would be the explanation of the recommendation. However, our method generates explanations from a counterfactual perspective: if an item would not have been recommended had it performed slightly worse on some aspects, then these aspects would be the reason for the model to recommend this item. Counterfactual reasoning, together with logical reasoning [8,36], are two important types of cognitive reasoning approaches. Recently, counterfactual reasoning has drawn attention in explainable AI research. It has some successful applications in several machine learning î€›elds such as computer vision [21], natural language processing [16,23], and social fairness [15,39]. In the recommendation î€›eld, recent works used counterfactual reasoning to improve both recommendation accuracy [43,47] and explainability [20,38,48] based on heterogeneous information networks [20], perturbation model [48] or inî€uence functions [38], e.g., Ghazimatin et al.[20] tried to generate provider-side counterfactual explanations by looking for a minimal set of userâ€™s historical actions (e.g. reviewing, purchasing, rating) such that the recommendation can be changed by removing the selected actions. Tran et al.[38] adopted inî€uence functions for identifying training points most relevant to a recommendation while deducing a counterfactual set for explanations. A common factor between our work with prior work is that both of the proposed methods generate explanations based on extracted causalities rather than associative relationships. Yet, our work is diî€erent from prior works on two key points: 1) In terms of problem deî€›nition, prior works generate counterfactual explanations on the user-side based on user actions while our method generates counterfactual explanations on the item-side based on item aspects, which are two diî€erent types of explanations. 2) In terms of technique, our method adopts a counterfactual learning framework driven by the Occamâ€™s Razor Principle [3] to directly learn an explanation of small complexity and large strength, so that our desire of î€›nding simple and eî€ective explanation is directly encoded into the model objective. In this section, we î€›rst describe the preliminaries and the counterfactual explainable recommendation problem. Then, we introduce the concepts of explanation complexity, explanation strength, and their relations. Finally, we introduce the intuition of counterfactual reasoning. We leave the more formal and mathematical deî€›nition of our counterfactual reasoning framework to the next section. Suppose we have a user set withğ‘šusersU = {ğ‘¢, ğ‘¢, Â· Â· Â· ,ğ‘¢}and an item set withğ‘›itemsV = {ğ‘£, ğ‘£, Â· Â· Â· , ğ‘£}. Let binary matrix ğµ âˆˆ {0,1}be the user-item interaction matrix, whereğµ=1 if userğ‘¢interacted with itemğ‘£; otherwise,ğµ=0. We useR(ğ‘¢, ğ¾) to represent the top-ğ¾recommendation list for a userğ‘¢, and we sayğ‘£ âˆˆ R (ğ‘¢, ğ¾)if itemğ‘£is recommended to userğ‘¢in the userâ€™s top-ğ¾list. Following the same method described in Zhang et al. [50], we apply the sentiment analysis toolkitbuilt in [51] to extract (Aspect, Opinion, Sentiment) triplets from the textual reviews. For example, in the Cell Phone domain, the extracted aspects would include color, price, screen, battery, etc. Besides, suppose we have a total number ofğ‘Ÿitem aspectsA = {ğ‘, ğ‘, Â· Â· Â· , ğ‘}. Same as [51], we further compute the user-aspect preference matrixğ‘‹ âˆˆ R and the item-aspect quality matrixğ‘Œ âˆˆ R.ğ‘‹indicates to what extent the userğ‘¢cares about the item aspectğ‘. Similarly,ğ‘Œ indicates how well the itemğ‘£performs on the aspectğ‘. More speciî€›cally, ğ‘‹ and ğ‘Œ are calculated as: ğ‘Œ=0, if item ğ‘£is not reviewed on aspect ğ‘1 +, else whereğ‘is the rating scale in the system, which is 5-star in most cases.ğ‘¡is the frequency that userğ‘¢mentioned aspectğ‘.ğ‘¡ is the frequency that itemğ‘£is mentioned on aspectğ‘, whileğ‘  is the average sentiment of these mentions. For both theğ‘‹andğ‘Œ matrices, their elements are re-scaled into the range of(1, ğ‘ )using the sigmoid function (see Eq.(1)) to match with the systemâ€™s rating scale. Since the matrix construction process is not the focus of this work, we only brieî€y describe this process and readers may refer to [50,51] for more details. The same user-aspect and item-aspect matrix construction technique is also used in [18, 27, 40]. With the above deî€›nitions, the objective of our counterfactual reasoning problem is to search for aspect-driven counterfactual explanations for a given black-box recommendation model. More speciî€›cally, for a given recommendation model, if itemğ‘£ is recommended to userğ‘¢, i.e.,ğ‘£âˆˆ R(ğ‘¢, ğ¾), then we look for a slight change vectorÎ” = {ğ›¿, ğ›¿, Â· Â· Â· , ğ›¿}for the item-aspect quality vectorğ‘Œ, such that ifÎ”is applied on itemğ‘£â€™s quality vector, i.e., ğ‘Œ+ Î”, then it will change the recommendation result to make item ğ‘£disappear from the recommendation list, i.e.,ğ‘£âˆ‰ R(ğ‘¢, ğ¾). All the values inÎ”are either zero or negative continuous values since an item will only be removed from the recommendation list if it performs worse on some aspects. With the optimized vectorÎ”, we can construct the counterfactual explanation for itemğ‘£, which is composed of the aspects corresponding to the non-zero values in Î”. The counterfactual explanation takes the following form, If the item had been slightly worse on [aspect(s)], then it will not be recommended. where the [aspect(s)] are selected byÎ”as mentioned above. In the following, we will deî€›ne the properties ofÎ”in more formal ways. To better understand the counterfactual explainable recommendation problem, we introduce two concepts to motivate explainable recommendation under the counterfactual reasoning background. The î€›rst is Explanation Complexity (EC), which measures how complicated the explanation is. In our aspect-based explainable recommendation setting, the complexity can be deî€›ned as 1) how many aspects are used to generate the explanation, which corresponds to the number of non-zero values inÎ”, i.e.,âˆ¥Î” âˆ¥, and 2) how many changes need to be applied on these aspects, which can be represented as the sum of square ofÎ”, i.e.,âˆ¥Î” âˆ¥. The î€›nal complexity takes a weighted sum of the two factors: whereğ›¾is a hyper-parameter to control the trade-oî€ between these two terms. The second is Explanation Strength (ES), which measures how eî€ective the explanation is. In our counterfactual explainable recommendation setting, this can be deî€›ned as to what extent applying the slight change vectorÎ”will inî€uence the recommendation result of itemğ‘£. This can be further deî€›ned as the decrease ofğ‘£â€™s ranking score in user ğ‘¢â€™s recommendation list after applying Î”: whereğ‘ is the original ranking score of itemğ‘£, andğ‘ is the ranking score ofğ‘£afterÎ”is applied to its quality vector, i.e.,ğ‘Œ+ Î”. We should note that Eq.(2)and(3)are not the only way to deî€›ne explanation complexity and strength. The deî€›nition depends on what we need in practice. Our counterfactual reasoning framework introduced in Section 4 is î€exible and can easily adapt to diî€erent deî€›nitions of explanation complexity and strength. It is also worth discussing the relationship between explanation complexity and strength. Actually, complexity and strength are two orthogonal concepts, i.e., a complex explanation is not necessarily strong, and a simple explanation is not necessarily weak. There may well exist explanations that are complex but weak, or simple and strong. According to the Occamâ€™s Razor Principle [3], if two explanations are equally eî€ective, we prefer the simpler explanation than the complex one. As a result, counterfactual explainable recommendation aims to seek the simple (low complexity) and eî€ective (high strength) explanations for recommendation. In this section, we î€›rst brieî€y introduce the black-box recommendation model for which we want to generate explanations. Then, we describe the details of our counterfactual constrained learning framework for counterfactual explainable recommendation. Suppose we have a black-box recommendation modelğ‘“that predicts the user-item ranking score ğ‘ for user ğ‘¢and item ğ‘£by: whereğ‘‹andğ‘Œare the user-aspect vector and item-aspect vector, respectively, as deî€›ned in Eq.(1);Î˜is the model parameter, andğ‘ represents all other auxiliary information of the model. Depending on the application,ğ‘could be ratings, clicks, text, images, etc., andğ‘is optional in the recommendation model. Basically, the recommendation modelğ‘“can be any model as long as it takes the userâ€™s and the itemâ€™s aspect vectors as part of the input, which makes our counterfactual reasoning framework applicable to a wide scope of models. In this work, to demonstrate the idea of counterfactual reasoning, we use a very simple deep neural network as the implementation of the recommendation modelğ‘“, which includes one fusion layer followed by three fully connected layers. The network concatenates the userâ€™s and the itemâ€™s aspect vectors as input and outputs a onedimensional ranking score ğ‘ . The î€›nal output layer is a sigmoid activation function so as to mapğ‘ into the range of(0,1). Then, we train the model with a cross-entropy loss: whereğµ=1 if userğ‘¢previously interacted with itemğ‘£, otherwiseğµ=0. In practice, sinceğµis a very sparse matrix, we sample the negative samples with ratio 1:2, i.e., for each positive instance we sample two negative instances. With this pre-trained recommendation model, for a target user, we are able to recommend top-ğ¾ items according to the predicted ranking scores. We build a counterfactual reasoning model to generate explanations for any item in the top-ğ¾recommendation list provided by an existing recommendation model. The essential idea of the proposed explanation model is to discover slight changeÎ”on the itemâ€™s aspects via solving a counterfactual optimization problem which is formulated in the following. Suppose itemğ‘£is in the top-ğ¾recommendation list for userğ‘¢ (ğ‘£âˆˆ R (ğ‘¢, ğ¾)). As mentioned before, our counterfactual reasoning model aims to î€›nd simple and eî€ective explanations forğ‘£, which can be shown as the following constrained optimization framework, Mathematically, according to our deî€›nition of explanation complexity and strength in Section 3, the framework can be realized with the following speciî€›c optimization problem, whereğ‘ = ğ‘“ (ğ‘‹, ğ‘Œ| ğ‘, Î˜),ğ‘ = ğ‘“ (ğ‘‹, ğ‘Œ+ Î” | ğ‘, Î˜). In the above equation,ğ‘ is the original ranking score of itemğ‘£,ğ‘ is the ranking score ofğ‘£when the slight change vectorÎ”is applied onğ‘£â€™s aspect vector. The intuition of Eq.(7)is trying to î€›nd an explanationÎ”that is both simple and eî€ective, where â€œsimpleâ€ is reî€ected by the optimization objective, i.e., explanation complexity ğ¶ (Î”)is minimized, while â€œeî€ectiveâ€ is reî€ected by the optimization constraint, i.e., the explanation strengthğ‘† (Î”)should be big enough to remove item ğ‘£from the top-ğ¾ list. To realize the second goal (i.e., eî€ective/strong enough), we take the thresholdğœ–as the margin between itemğ‘£â€™s score and theğ¾ +1â€™s itemâ€™s score in the original recommendation list, i.e., whereğ‘ = ğ‘“ (ğ‘‹, ğ‘Œ| ğ‘, Î˜)is the ranking score of theğ¾ +1â€™s item, and thus Eq.(7) can be simpliî€›ed as, In this way, itemğ‘£will be ranked lower than theğ¾ +1â€™s item and thus be removed from the top-ğ¾ list. A big challenge to optimize Eq.(9)is that both the objectiveâˆ¥Î” âˆ¥+ ğ›¾ âˆ¥Î” âˆ¥and the constraintğ‘ â‰¤ ğ‘ are not diî€erentiable. In the following, we relax the two parts to make the equation optimizable. For the objective, sinceâˆ¥Î” âˆ¥is not convex, we relax it withâ„“normâˆ¥Î” âˆ¥. This is shown to be eî€œcient and provides good vector sparsity in [4,5], thus helps to minimize the explanation complexity in terms of the number of aspects in the explanation. For the constraint ğ‘ â‰¤ ğ‘ , we relax it as a hinge loss: and add it as a Lagrange term into the total objective. Thus, the î€›nal optimization equation for generating explanation becomes: minimizeâˆ¥Î” âˆ¥+ ğ›¾ âˆ¥Î”âˆ¥+ ğœ†ğ¿(ğ‘ , ğ‘ ) where ğ‘ = ğ‘“ (ğ‘‹, ğ‘Œ+ Î” | ğ‘, Î˜), ğ‘ = ğ‘“ (ğ‘‹, ğ‘Œ| ğ‘, Î˜) In Eq.(11),ğœ†andğ›¼are hyper-parameters to control the explanation strength. A sacriî€›ce of using relaxed optimization is that we lose the guarantee that itemğ‘£is removed from the top-ğ¾ list, though the probability of removing is high due to minimizing theğ¿(ğ‘ , ğ‘ )term. As a result, it requires a post-process to check ifğ‘ is indeed smaller thanğ‘ . We should only generate counterfactual explanations when the removal is successful. In the experiments, we will report the î€›delity of our explanation model to show what percentage of items can be explained by our method. Besides, there is a trade-oî€ in the relaxed model: by increasing the hyper-parameterğœ†, the model will focus more on the explanation strength but less on the explanation complexity. We will also explore the inî€uence ofğœ†and the relationship between explanation complexity and strength in the ablation study of the experiments. 4.4.1Explanation Complexity for Items at Diî€›erent Positions. With the above framework, we can see that the diî€œculty of removing diî€erent items in the top-ğ¾list are diî€erent. Suppose for a certain user, the recommender system generates top-ğ¾recommended items asğ‘£, ğ‘£, Â· Â· Â· , ğ‘£according to the ranking scores. Intuitively, removingğ‘£from the list is more diî€œcult than removing ğ‘£from the list. The reason is that to removeğ‘£, the explanation strength should be at leastğœ– = ğ‘ âˆ’ ğ‘ , which is bigger than the strength needed for removingğ‘£, which isğœ– = ğ‘ âˆ’ ğ‘ . As a result, the generated explanations for the items at a higher position in the list will likely have higher explanation complexity, because the reasoning model has to apply larger changes or more aspects to generate high-strength explanations. This is a reasonable and desirable property of the counterfactual explainable recommendation frameworkâ€”if the system ranks an item at a very high position, then it means that the system strongly recommends this item, which needs to be backed by strong explanation that contains more aspects. On the contrary, for an item ranked at lower positions in the list, it could be easily removed by changing only one or two aspects, which is in line with our intuition. In the experiments, we will show the average explanation complexity for items at diî€erent positions to verify the above discussion. 4.4.2Controlling the Number of Aspects. Through Eq.(11), the model can automatically decide the number of aspects to construct the explanation. We believe this is better than choosing only one aspect as was done in previous aspect-aware explainable recommender systems [9,22,40,51]. However, if needed, our method can also generate explanations with a single aspect. To generate single aspect explanation, we adjust Eq.(11)by adding a trainable one-hot vectoraas a mask to make sure that only one aspect is changed during the training. The optimization problem is: Since we force the model to generate single aspect explanation, the â„“-norm term ofğ¶ (Î”)vanishes becauseâˆ¥Î” âˆ¥=1. We will explore both single- and multi-aspect explanations in experiment. How to quantitatively evaluate explanations is a very important problem. Fortunately, compared to other explanation forms, counterfactual explanation is very suitable for quantitative oî€Ÿine evaluation. In this section, we mathematically deî€›ne two types of evaluation metricsâ€”user-oriented evaluation and model-oriented evaluation, which we believe can help the î€›eld to move forward with standard evaluation of explainable recommendations. In user-oriented evaluation, we adopt the userâ€™s review on the item as the ground-truth reason about why the user purchased the item, which is similar to previous works [6,14,28,30,40]. More speciî€›cally, from the textual review that a userğ‘¢provided on an itemğ‘£, we extract all the aspects thatğ‘¢mentioned with positive sentiment,î€‚î€ƒ which is deî€›ned asğ‘ƒ=ğ‘, ğ‘, Â· Â· Â· , ğ‘.ğ‘ƒis a binary vector, whereğ‘=1 if userğ‘¢has positive sentiment on the aspect ğ‘in his/her review for itemğ‘£. Otherwise,ğ‘=0. On the other hand, our model will produce the vectorÎ” = {ğ›¿, ğ›¿, Â· Â· Â· , ğ›¿}, and those aspect(s) corresponding to the non-zero values inÎ”will constitute the explanation. Then, for each user-item pair, we calculate the precision and recall of the generated explanationÎ”with regard to the groundtruth vector ğ‘ƒ: Precision =Ãğ¼ (ğ›¿), Recall =Ã(13) whereğ¼ (ğ›¿)is an identity function such thatğ¼ (ğ›¿) =1 whenğ›¿ â‰ 0, andğ¼ (ğ›¿) =0 whenğ›¿ =0. In our case, the Precision measures the percentage of aspects in our generated explanation that are really liked by the user, while Recall measures how many percentage of aspects liked by the user are really included in our explanation. We also calculate theğ¹score as the harmonic mean between the two, i.e.,ğ¹=2Â·. Then, we average the scores of all pairs as the î€›nal Precision, Recall, and ğ¹measure. The user-oriented evaluation only answers the question of whether the generated explanations are consistent with userâ€™s preferences. However, it does not tell us whether the explanation properly justiî€›es the modelâ€™s behaviour, i.e., why the recommendation model recommends this item to the user. To test if our explanation model correctly explains the essential mechanism of the recommendation system, we use two scores, Probability of Necessity (PN) and Probability of Suî€œciency (PS) [34, p.112], to validate the explanations with model-oriented evaluation. In logic and mathematics, necessity and suî€œciency are terms used to describe a conditional or implicational relationship between two statements. Suppose we haveğ‘† â‡’ ğ‘, i.e., ifğ‘†happens thenğ‘ will happen, then we sayğ‘†is a suî€œcient condition forğ‘. Meanwhile, we have the logically equivalent contrapositiveÂ¬ğ‘ â‡’ Â¬ğ‘†, i.e., ifğ‘does not happen, thenğ‘†will not happen, as a result, we say ğ‘ is a necessary condition for ğ‘†. Probability of Necessity (PN)[34]: In causal inference theory, probability of necessity evaluates the extent that a condition is necessary. To calculate PN for the generated explanation, suppose a set of aspectsAâŠ‚ Acompose the explanation for the recommendation of itemğ‘£to userğ‘¢. The essential idea of the PN score is: if in a counterfactual world, the aspects inAdidnotexist in the system, then what is the probability that itemğ‘£wouldnotbe recommended for user ğ‘¢. Following this idea, we calculate the frequency of the generated explanations that meet the PN deî€›nition. Letğ‘…be userğ‘¢â€™s original recommendation list. Letğ‘£âˆˆ ğ‘…be a recommended item that our framework generated a nonempty explanationAâ‰  âˆ…. Then for all the items in the universal item setV, we alter the aspect values in the item-aspect quality matrixğ‘Œto 0 if they are inA. In this way, we create a counterfactual item setVwhich results in a counterfactual recommendation listğ‘…for userğ‘¢by the recommendation algorithm. Then, the PN score is: PN =ÃÃğ¼ (Aâ‰  âˆ…), where PN=1, if ğ‘£âˆ‰ ğ‘…0, else whereğ¼ (Aâ‰  âˆ…)is an identity function such thatğ¼ (Aâ‰  âˆ…) =1 if the conditionAâ‰  âˆ…holds and 0 otherwise. Basically, the denominator is the total number of items that the algorithm successfully generated an explanation for, and the numerator is the number of explanations that if we remove the related aspects then it will cause the item to be removed from the recommendation list. Probability of Suî€œciency (PS)[34]: The PS score evaluates the extent that a condition is suî€œcient. The essential idea of the PS score is: if in a counterfactual world, the aspects inAwere theonlyaspects existed in the system, then what is the probability that item ğ‘£would still be recommended for user ğ‘¢. Similarly, we calculate the frequency of the generated explanations that meet the PS deî€›nition. For all the items inV, we alter the aspect values in the item-aspect quality matrixğ‘Œto 0 if they are not inA. In this way, we create a counterfactual item setV which results in a counterfactual recommendation listğ‘…for user ğ‘¢by the recommendation algorithm. Then, the PS score is: PS =ÃÃğ¼ (Aâ‰  âˆ…), where PS=1, if ğ‘£âˆˆ ğ‘…0, else whereğ¼ (Aâ‰  âˆ…)is still the identity function as above. Basically, the denominator is still the total number of items that the algorithm successfully generated an explanation for, and the numerator is the number of explanations that alone can still recommend the item to the recommendation list. Similar to the user-oriented evaluation, we also calculate the harmonic mean of PS and PN to measure the overall performance, which is ğ¹=. In this section, we î€›rst introduce the datasets, the comparison baselines and the implementation details. Then we present studies on the two main expected properties in this paper: complexity and strength of the explanations. We also present ablation studies to explore how our model performs under diî€erent conditions. We test our method on Yelpand Amazondatasets. The Yelp dataset contains usersâ€™ reviews on various kinds of businesses such as restaurants, dentists, salons, etc. The Amazon dataset [32] contains user reviews on products in Amazon e-commerce system. The Amazon dataset contains 29 sub-datasets corresponding to 29 product categories. We adopt four datasets of diî€erent scales to evaluate our method, which are Electronic, Cell Phones and Accessories, Kindle Store and CDs and Vinyl. Since the Yelp and Amazon datasets are very sparse, similar as previous work [40,44,45,50], we remove the users with less than 20 reviews for Yelp dataset, and 10 reviews for Amazon dataset. The statistics of the datasets are shown in Table 1. We compare our model with three aspect-aware explainable recommendation models. We also include a random explanation baseline to show the overall diî€œculty of the evaluation tasks. EFM[50]: The Explicit Factor Model. This work combines matrix factorization with sentiment analysis technique to align latent factors with explicit aspects. In this way, it predicts the user-aspect preference scores and item-aspect quality scores. The top-1 aligned aspect is used to construct aspect-based explanation. MTER[40]: The Multi-Task Explainable Recommendation model. This work predicts a tensorğ‘‹ âˆˆ R, which represents the aî€œnity score among the users, items, aspects, and an extra dimension for the overall rating. This tensorğ‘‹is acquired via Tucker decomposition [24,25]. We should note that since the overall rating for a user on an item is predicted in the extra dimension via decomposition, which is not directly predicted by the explicit aspects, this method is not suitable for the model-oriented evaluation. As a result, we only report this modelâ€™s explanation performance on user-oriented evaluation. A2CF[9]: The Attribute-Aware Collaborative Filtering model. This work leverages a residual feed-forward network to predict the missing values in the user-aspect matrixğ‘‹and the item-aspect matrixğ‘Œ. The method originally considers both the user-item preference and the item-item similarity to generate explainable substitute recommendations. We remove the item-item factor to make it compatible with our problem setting to generate explanations for any item. Similar to [50], the top-1 aligned aspect will be used for explanation. Random: For each item recommended to a user, we randomly choose one or multiple aspects from the aspect space and generate the explanation based on them. The evaluation scores of the random baseline can indicate the diî€œculty of the task. 6.3.1Preprocessing. The preprocessing includes two parts: 1) Generating the user-aspect vector ğ‘‹ and the item-aspect vector ğ‘Œ from the user reviews. 2) Training the base recommender system. In the preprocessing phase, we hold-out the last 5 interacted items for each user, which serve as the test data to evaluate both the recommendation and the explanation. The deep neural network in the base recommendation model consists of 1 fusion layer and 3 fully connected layers with {512, 256, 1} output dimensions, respectively. We apply ReLU activation function after all the layers except the last one, which is followed by a Sigmoid function to re-scale the predicted scores to the range of(0,1). The model parameters are optimized by stochastic gradient descent (SGD) optimizer with a learning rate of 0.01. After the recommendation model is trained, all the parameters will be î€›xed in the counterfactual reasoning phase and explanation evaluation phase. 6.3.2Generating Explanations. The base recommendation model generates the top-ğ¾recommendation list for each user.ğ¾is set to 5 in the experiment. We then apply the counterfactual reasoning method to generate explanations for the items in the list. The hyper-parameterğœ†is set to 100 for all the datasets. The ablation study on the inî€uence ofğœ†can be seen in Section 6.5. We notice that theâ„“-normâˆ¥Î” âˆ¥and theâ„“-normâˆ¥Î” âˆ¥is almost in the same scale, so that we always setğ›¾to 1 in our model. For the margin valueğ›¼in the hinge loss, we tested diî€erent values for ğ›¼in{0.1,0.2,0.3, Â· Â· Â· ,1.0}and î€›nd that the performance does not change too much forğ›¼ =0.1,0.2, Â· Â· Â·0.5 and then signiî€›cantly drops forğ›¼ >0.5. As a result, we setğ›¼ =0.2 throughout the experiments. To compare with the baselines, for each recommended item, we generate both multi-aspect and single-aspect explanation through Eq.(11) and Eq.(12), respectively. 6.3.3Aspect Masking. When generating explanations, our model directly chooses aspects from the entire aspect space, which is reî€ected by the change vectorÎ”. However, in the user-oriented evaluation, a strong bias exists in the userâ€™s ground-truth review, which is that a user is more possible to mention the aspects that they have mentioned before. This may result from the personal linguistic preferences. As a result, all the baseline models (EFM, MTER, A2CF) only choose aspects from the userâ€™s previously mentioned aspects to construct the explanation. To fairly compare with them in the user-oriented evaluation, we provide an adjusted version of our model. Letğ‘€âˆˆ {0,1}be the userğ‘¢â€™s mask vector.ğ‘€is a binary vector, whereğ‘€=1 if aspectğ‘is an aspect thatğ‘¢cares about, i.e.,ğ‘‹â‰ 0. Otherwise,ğ‘€=0. We apply this mask onÎ” to generate explanation by choosing aspects only from the userâ€™s preference space. which is: minimizeâˆ¥Î”âˆ¥+ ğ›¾ âˆ¥Î”âˆ¥+ ğœ†ğ¿(ğ‘ , ğ‘ ) where Î”= ğ‘€âŠ™ Î”; ğ‘ = ğ‘“ (ğ‘‹, ğ‘Œ+ Î”|ğ‘, Î˜)(16) In this case, theÎ”is used to generate explanation. This aspect mask can also be applied on the single-aspect formula, i.e., Î”= ğ‘€âŠ™ a âŠ™ Î”. Notice that applying the mask does not introduce the data leakage problem because the mask is calculated based on the training set. In the evaluation, we evaluate both the original CountER model and the masked CountER model in both useroriented and model-oriented evaluations. Table 3: User-oriented evaluation of the explanations. All numbers in the table are percentage numbers with â€˜%â€™ omitted. CountER (w/ mask) 33.94 25.67 28.31 29.21 20.26 22.85 40.94 36.19 37.73 39.06 25.93 29.33 12.96 12.96 12.96 CountER (w/ mask) 25.68 45.78 29.73 21.72 42.82 26.97 32.24 84.20 44.57 20.95 68.98 30.00 9.09 28.57 13.57 6.3.4Compatible with the Baselines. One issue in comparison with baselines is that our model is able to generate both multi-aspect and single-aspect explanations. When generating multi-aspect explanations, the model automatically decides the best number of aspects in model learning. However, the baseline methods can only generate single-aspect explanations using the top-1 aligned aspect since they do not have the ability to decide the number of aspects. Thus, to make the baseline models also comparable in multi-aspect explanation, we use our model as a guideline to tell the baseline models how many aspects should they use to generate multi-aspect explanations. For this reason, we only compare the explanations generated for the intersection of the items which are recommended by both our model and the baseline models in multi-aspect setting. 6.3.5Evaluable Explanations. Even if the explanation model can generate explanations for all the recommended items, not all of the explanations can be evaluated from the userâ€™s perspective. This is because the user-oriented evaluation requires the user reviews as the ground-truth data. Thus, we can only evaluate the explanations generated for the â€œcorrectly recommendedâ€ items which appear in the test data. For model-oriented evaluation, all the explanations can be evaluated. We î€›rst report the î€›delity of our explanation method in Table 2, which shows for what percentage of recommended items that our method can successfully generate an explanation. We notice that the î€›delity of the single-aspect version is lower than that of the multi-aspect version. This indicates that for our model, using only one aspect as the explanation may not lead to enough explanation strength to eî€ectively explain the recommendations. However, if we allow the model to use multiple aspects, we can î€›nd strong enough explanations in most cases (80%âˆ¼100%). The overall average number of aspects in multi-aspect explanation is 2.79. We then evaluate the explanations generated by CountER (original version and masked version) and the baselines. The useroriented evaluation is reported in Table 3 and the model-oriented evaluation is reported in Table 4. We note that the random baseline performs very bad on both evaluations, which shows the diî€œculty of the task and that randomly choosing aspects as explanations can barely reveal the reasons of the recommendations. For the user-oriented evaluation, the results show that when applying the mask for fair comparison, CountER outperforms all the baselines on all the datasets in terms ofğ¹scores. Moreover, CountER performs better than the baselines on precision in 90% cases and on recall in 80% cases. We note that our model has a very huge improvement than the baselines on Yelp dataset even without mask, and the reason may be that the Yelp dataset is much denser and has more reviews than other datasets so that the userâ€™s review bias has smaller impact. This also indicates that CountER has more advantages when the size of dataset increases. For model-oriented evaluation, the mask limits CountERâ€™s ability to explain the modelâ€™s behavior. However, no matter with or without the mask, our model has better performance than all the baselines according toğ¹score. With mask, CountER has 15.63% average improvement than the best performance of the baselines onğ¹. Without the mask, the average improvement is 38.49%. Another observation is that the baselines commonly have higher PS scores, despite much lower on the PN scores. One possible reason is due to the mechanism of the matching based explanation methods. For an item on the recommendation list, they try to î€›nd well-aligned aspects where the user and the item both perform well. When computing the PS score, the baselines only reserve these well-aligned aspects in the aspect space for all the items, thus the recommended item will highly possibly still stay in the recommendation list, which results in a high PS score. However, when computing the PN score, though the baselines remove all these aspects, the recommended item may still perform better on other aspects compared with other items and thus still remain in the recommendation list, which results in a lower PN score. This also sheds light on why the matching based methods may not discover the true reasons behind the recommendations. Sinceğœ†is applied on the hinge lossğ¿(ğ‘ , ğ‘ )(Eq.(11)and(12)), a largerğœ†emphasizes more on the explanation strength and reduces Table 4: Model-oriented evaluation of the explanations. All numbers in the table are percentage numbers with â€˜%â€™ omitted. MTER is not suitable for the model-oriented evaluation and the reason can be found in Section 6.2. CountER (w/ mask) 56.73 62.03 59.26 70.11 54.71 61.46 35.39 46.91 40.34 75.17 49.18 59.46 58.52 52.56 55.38 CountER (w/ mask) 77.96 89.26 83.23 86.62 91.78 89.13 60.70 80.10 69.06 72.47 67.72 70.01 96.73 94.39 95.55 Figure 2: Ablation Studies. (a) Change of ğ¹score, explanation complexity and î€›delity w.r.t. ğœ†. ( b) Change of explanation strength and ğ¹score w.r.t. ğœ†. (c) Average explanation complexity/strength for items at diî€erent positions. (d) Relationship between ğ¹score with explanation complexity/strength. (e) Distribution of ğ‘ƒğ‘ the importance of complexity. As shown in Figure 2(a), whenğœ† increases, the model successfully generates explanations for more items, but the explanation complexity also increases because more aspects are needed to explain those â€œhardâ€ items. However, the higherğœ†is, the worse our model performs on the user-oriented evaluation. Besides, Figure 2(b) shows the explanation strength does not change withğœ†. This is because theğ›¼in the hinge loss controls the required margin on the ranking score to î€ip a recommendation. Additionally, we notice that the model-oriented performance is also independent to ğœ†, which is the same as the explanation strength. Based on the above results, we hypothesize that the user-oriented performance may be related to the explanation complexity, while the model-oriented performance may be related to the explanation strength. This hypothesis is justiî€›ed in Section 6.6. In this section, we study the Explanation Complexity and Explanation Strength. In Section 4.4.1, we discussed that the diî€œculty of removing the items in the top-ğ¾list are diî€erent based on their positions. Figure 2(c) shows the mean complexity and strength of the explanations for items at diî€erent positions (i.e., from the î€›rst to the î€›fth). Generally speaking, it requires 1.59 more aspects to remove the items at the î€›rst position from the recommendation list than the items at the î€›fth position. Because they require larger change to be removed. This is in line with the fact that the strongly recommended items have more reasons to be recommended. In Figure 2(d), we illustrate the relationship betweenğ¹score and explanation complexity/strength. The scale of the marker points represent how large theğ¹score is. It shows that the user-oriented evaluation scoreğ¹decreases as the complexity increases, meanwhile, ğ¹is relatively independent from the explanation strength. On the contrary, in Figure 2(e), we plot the distribution of the explanations that are both necessary and suî€œcient (i.e.,PNâˆ§ PS=1), as well as the distribution of explanations that are either unnecessary or insuî€œcient (i.e.,PNâˆ§ PS=0). We can see that as the explanation strength increases, more and more portion of the explanations are both necessary and suî€œcient. However, this tends to be irrelevant with the complexity of the explanations. These observations are important because: 1) They indicate that the Explanation Complexity and Explanation Strength are two orthogonal concepts. Both of them are very important since the complexity is related to the coverage on the userâ€™s preference and the strength is related to the modelâ€™s mechanism. 2) It legitimatizes the Occamâ€™s Razor Principle and further justiî€›es the motivation of our CountER model, which is to extract both simple (low complexity) and eî€ective (high strength) explanations for recommendations. In this paper, we proposed CountER, a counterfactual explainable recommendation framework, which generates explanations based on counterfactual changes on item aspects. Counterfactual reasoning is still in early stages for explainable recommendation, which has a lot of room for further explorations. For instance, CountER only explored changes on the item aspects. However, we can also explore counterfactual changes on other various forms of information such as images and textual descriptions. The essential idea of CountER is also suitable for explainable decision making over knowledge graphs or graph neural networks, which are very promising directions to explore in the future. We appreciate the valuable feedback and suggestions of the reviewers. This work was supported in part by NSF IIS-1910154 and IIS-2007907. Any opinions, î€›ndings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily reî€ect those of the sponsors.