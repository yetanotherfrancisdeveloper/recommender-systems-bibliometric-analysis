Graph convolutional networks (GCNs) have recently enabled a popular class of algorithms for collaborative ï¬ltering (CF). Nevertheless, the theoretical underpinnings o f their empirical successes remain elusive. In this paper, we endeavor to obtain a better understanding of GCN-based CF methods via the lens of graph signal processing. By identifying the critical role of smoothness, a key concept in graph signal processing, we develop a uniï¬ed graph convolution-based framework for CF. We prove that many existing CF methods are special cases of this framework, including the neighborhood-based methods, low-rank matrix factorization, linear auto-encoders, and LightGCN, corresponding to diï¬€erent lowpass ï¬lters. Based on our framework, we then present a simple and computationally eï¬ƒcient CF baseline, w hich we shall refer to as Graph Filter based Collaborative Filtering (GF-CF). Given an implicit feedback matrix, GF-CF can be obtained in a closed form instead of expensive training with back-propagation. Experiments will show that GF-CF achieves competitive or better performance against deep learning-based methods on three well-known datasets, notably with a 70 % performance gain over LightGCN on the Amazon-book d ataset. â€¢ Information systems â†’ Recommender systems. collaborative ï¬ltering, graph convolution, graph signal processing ACM Reference Format: Yifei Shen, Yongji Wu, Yao Zhang, Caihua Shan, Jun Zhang, Khaled B. Letaief, Dongsheng Li . 2021. How Powerful is Graph Convolution f or Recommendation?. In CIKM â€™21: ACM International Conference on Information and Knowledge Management, Nov 01â€“05, 2021, Queensland, Australia. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3459637.3482264 Recommender systems have achieved great successes in many businesses, e.g., for produc t recommendation on Amazon [25] and playlist generation on Youtube [7], etc. As the algorithmic eï¬€ectiveness will have a direct impact on the commercial success, building a good recommendation engine, especially via collaborative ï¬ltering (CF), remains an active research area, with consistent innovations in both conventional methods [6, 21, 31] and recently emerged deep learning approaches [13, 15, 24]. Over the past decade, we have witnessed great progress in CF algorithms. Mod el-based methods largely resort to low-dimensional structures in high-dimensional data [42], e.g., low-rank matrix factorization [6, 17, 22, 28] and autoencoders [24, 36, 43]. On the other hand, neighborhood-based methods [1, 39] achieve competitive performance based on simple similarity measures, e.g., the cosine similarity between items. Furthermore, these two types of methods can be incorporated together to improve the performance, e.g., SVD++ [21]. From t he graph perspec tive, the neighborhood-based methods and SVD++ eï¬€ectively exploit the one-hop information in the user-item interaction graph. To take advantage of the rich multi-hop neighborhood information, graph convolutional networks (GCNs), e.g., GC-MC [4], NGCF [41], LightGCN [13], have been recently propo sed and become state-of-the-art methods for CF. NGCF [41] was inspired by the GCNs developed for attribute graphs [19], and it inherits the key ingredients from GCNs, including initial embeddings, feature transformation, neighborhood aggregation, and nonlinear activation. As the graphs in CF tasks are non-attributed, these operations may not be necessary [13]. Therefore, in LightGCN [13], only the most import ant components, i.e., trained initial embeddings and graph convolution, are preserved. Removing the unnecessary components leads t o easier training and better generalization [46, 47 ], and thus LightGCN signiï¬cantly outperforms NGCF in both accuracy and eï¬ƒciency. While these empirical studies have produced promising results, the underlying reasons for the eï¬€ectiveness of these methods remain elusive. From the theoretical perspective, an intriguing q uestion is what plays an essential role in the success of GCN-based methods for CF. From the practical perspective, it is interesting to investigate to what extent we can reduce the training cost while eï¬€ectively exploiting the rich information of the user-item interaction graph. This paper endeavo rs to obtain a better understanding of GCNbased methods and develop a uniï¬ed framework based on graph convolution that incorporates classic method s. In particul ar, we identify the importance of a key concept in graph signal processing in developing CF algorithms, namely, smoothness. Conceptually, if a user interacted with an item, then their embe ddings should be similar. In graph signal processing, the similarity between the embeddings of the interacted user-item p air deï¬nes the smoothness of the embedding. Meanwhile, low-pass ï¬lters on graphs, e.g., the light convolution in LightGCN [13], are used to promote the smoothness of graph signals. We will therefore argue that it is the smoothness of the embeddings and the low-pass ï¬ltering that play a pivotal role in GCN-based methods. By theo retical analysis and experiments, we will show that the performance of untrained LightGCN is competitive to a trained one when the embedding dimension is suï¬ƒciently large, due to the smoothing eï¬€ect of the light convolution. Inspired by this ï¬nding, we derive a closed-form solution for the untrained LightGCN with Inï¬nitely Dimensional Embe dding (LGCN-IDE). It is shown that LGCN-IDE outperforms LightGCN by more than 40% on the Amazon-book dataset. Motivated by its simplicity and eï¬€ectiveness, we extend LGCNIDE t o incorporate general low-pass ï¬lters, which form a uniï¬ed framework for CF. Surprisingly, it is proved that the neighborhoodbased methods [1], low-rank matrix factorization [6] , and linear auto-encoders [36] are all special cases of this framework with various classic low-pass ï¬lters. This ï¬nding veriï¬es the eï¬€ectiveness of graph convolution with low-pass ï¬lters for CF. We further present a simple and computationally eï¬ƒcient CF method, which is an integration of linear ï¬lters and an ideal low-pass ï¬lter. Given an implicit fe edback matrix, our proposed method has a closed-form solution and as such it would not require expensive training. More importantly and despite of its simplicity, the proposed method achieves competitive or better performance compared with deep learning methods. To summarize, this work has made the following contributions. (1) By identifying the critical role of the smoothness and lowpass ï¬ltering, we provide a novel perspective to understand the algorithms fo r CF. (2) Using both theoretical justiï¬cation and experiments, we show that the untrained LightGCN can achieve competitive performance as a trained one when the embedding dimension is suï¬ƒciently large. We further derive a closed-form solution for untrained LightGCN with inï¬nitely dimensional embedd ing. (3) Built up on the closed-form solution, we develop a general graph ï¬lter-based framework for CF. We prove that the neighborho odbased methods, linear au to-encoders, and low-rank matrix factorization are special cases of this framework, corresponding to various classic low-pass ï¬lters. (4) We present a simple and computationally eï¬ƒcient method, named GF-CF. With a small fractio n of training time, GFCF achieves competitive or higher performance compared with the state-of-the-art deep learning methods on three well-known datasets. The rest of this paper is organized as follows. Section 2 introduces some preliminaries for the rest of this paper. Sec tion 3 demonstrates the importance of smoothness. Section 4 provides the details of our method. Section 5 presents the experimental results. Section 6 discusses the related works in CF and GCNs. Finally, we conclude this work in Section 7. The code to reproduce the experiments is available at https://github.com/yshenaw/GF_CF. This subsect io n presents some useful notations and deï¬nitions. We ï¬rst deï¬ne user set U and item set I. As in [13], this paper considers the recommendation problem with implicit feedback. The implicit feedback matrix ğ‘¹ âˆˆ {0, 1}is deï¬ned as follows: ğ‘¹=1, if (ğ‘¢, ğ‘–) interaction is observed, and ğ’“denotes the ğ‘¢-th row of ğ‘¹. The adjacency matrix of the user-item interaction graph is given In this bipartite graph, we denote the neighbors of node ğ‘˜ as N, and its cardinality as ğ‘= |N|. We denote the all one column vector of any dimension as 1, and degree matr ices as ğ‘«= Diag(ğ‘¹ Â· 1) and ğ‘«= Diag(1ğ‘¹). The normalized rating matrix is denoted as withËœğ’“as the ğ‘¢-th row ofËœğ‘¹. Similarly, the normalized user-item adjacency matrix is given by We t hen deï¬ne an impo rtant concept, namely, Stiefel manifold, which can help to connect low-rank matrix factorization and GCNbased methods in Section 4.2. Deï¬nition 2.1. (Stiefel manifold) The Stiefel manifold St(ğ‘›,ğ‘š) is deï¬ned as the subspace of orthonormal N-frames in R, namely, where ğ‘° is the identity matr ix . In this subsection, we introduce basic concepts of graph signal processing [8, 29]. We consider a weighted undirected graph G = (V, E) with ğ‘› nodes where V and E denote the vertex set and edge set, respectively. The graph can be represented as an adjacency matrix ğ‘¨ âˆˆ R. A graph signal is deï¬ned as a function ğ‘¥ : V â†’ R and it can be represented as a ğ‘›-dimensional vector ğ’™ = [ ğ‘¥ (ğ‘–)]. For a graph signal, the derivative is deï¬ned asp The smoothness of a graph signal can be measured by the graph quadratic form, which is t he squared norm of the graph derivative as deï¬ned below: Here, ğ‘³ = ğ‘« âˆ’ ğ‘¨ is the graph Laplacian matrix. A smaller indicates smoother signals. In many applications, a graph signal is often described in a vector form ğ‘¥ : V â†’ Rand its smoothness can be written as follows: As ğ‘³ is real and symmetric, its eigendecomposition is given by ğ‘³ = ğ‘¼ ğš²ğ‘¼where ğš² = Diag(ğœ†, Â·Â·Â· , ğœ†), ğœ†â‰¤ ğœ†â‰¤ Â·Â·Â· â‰¤ ğœ†, and ğ‘¼ = [ğ’–, Â·Â·Â· , ğ’–] with ğ’–âˆˆ Rbeing the eigenvector for eigenvalue ğœ†. Next we discuss the frequency of the graph signal and deï¬ne Fourier Transform on graphs. Intuitively, the graph signal has a higher frequency if it is more oscillatory and not smooth. As ğœ† is the smallest eigenvalue, for any graph signal ğ’™ âˆˆ R, we have â‰¥. Thus, the eigenve c tor with a smaller eigenvalue corresponds to a lower frequency signal component. We can deï¬ne the Graph Fourier Transform (GFT) basis as the eigenvector matrix ğ‘¼ and we callË†ğ’™ = ğ‘¼ğ’™ as the GFT of the graph signal ğ’™. Similar to the Fourier transform, GFT is a linear orthogonal transform and its inverse transform is given by ğ’™ = ğ‘¼Ë†ğ’™. GFT enables us to deï¬ne graph ï¬lters and graph convolution. Deï¬nition 2.2. (Graph Filter) Given a graph Laplacian matrix, as well as its eigenvectors and eigenvalues, then the graph ï¬lter H(ğ‘³) is deï¬ned as follows: where â„(Â·) is the ï¬lter deï¬ned on the eigenvalues. Deï¬nition 2.3. (Graph Convolution) The graph convolution of a input signal ğ’™ and the ï¬lter H(ğ‘³) is deï¬ned as follows: Similar to the deï¬nition o f convolution in classic signal processing, the graph signal is transformed b y GFT ğ‘¼, multiplied by a ï¬lter â„(Â·), and t ransformed back by inverse GFT ğ‘¼ . In the context of CF, the graph signal is often the observed ratings for a given user [6], or the initial embeddings of users/items [1 3, 41]. In the signal processing literature, the signal is often smooth and with low frequency, and the noise is often non-smooth and with a high frequency. One important class of ï¬lters is the low-pass ï¬lters, which promotes smoothness of graph signals for denoising. The graph low-pass ï¬lters are deï¬ned as follows. Deï¬nition 2.4. (Low-pass Filter) For ğ‘˜ = 1, Â·Â·Â· , ğ‘› âˆ’ 1, we deï¬ne the ratio The graph ï¬lter H(ğ‘³) is ğ‘˜-low-pass if and only if the low-pass ratio satisï¬es ğœ‚âˆˆ [0, 1). The low-pass ratio deï¬nes how much of t he high-frequency component of a signal is allowed to pass compared to the lowfrequency components. If ğœ‚< 1, then the ï¬lter passes low-frequency signals and is called a low-pass ï¬lter. We here list so me impo rtant low-pass ï¬lters and will connect these ï¬lters with cl assic methods for recommendation in Section 4.2. Linear Fil te r. T he linear ï¬lter is given by where ğ›¼is the ï¬lterâ€™s coeï¬ƒcient. It is called linear due to its similarity with linear time invariant ï¬lters in classic signal processing. We w ill show that this ï¬lter corresponds to LightGCN and neighborhood-based methods. Ideal Low-pass Filter. The ideal l ow-pass ï¬lter has a cut-oï¬€ frequencyÂ¯ğœ†. The ï¬lter is d eï¬ned as It is called id e al as the high-frequency signals are ideally cut oï¬€ with no leakage. We will show that this ï¬lt er corresponds to the low-rank matrix factor iz ation method. Opinion Dynamics. The opinion dynamics are a graph diï¬€usion process, which is a GF-AR(1) model [11]: ğ’š= (1âˆ’ğ›½)(ğ‘° âˆ’ğ›¼ğ‘³)ğ’š+ ğ›½ğ’™. The steady state opinions are given by ğ’š = limğ’š= (ğ‘° + Ëœğ›¼ğ‘³)ğ’™ = H(ğ‘³)ğ’™ where Ëœğ›¼ = ğ›½ (1 âˆ’ ğ›¼)ğ›¼. Thus, the corresponding graph ï¬lter is In opinion dynamics, the matrix inverse or eigenvalue decomposition is required. Thus, applying this ï¬lter introduces a high memory cost. We will show that it is closely related to the linear autoencoder method. LightGCN [13] is a state-of-the-art GCN-based method in CF. In this paper, LightGCN will be used as the vehicle to elaborate our theory and adopted as t he main baseline for performance comparison. LightGCN leverages the user-item interaction graph to propagate the embedding as follows: where ğ‘¬âˆˆ Ris the learnable initial embedding matrix of users and items. For a ğ¾-layer LightGCN, the ï¬nal embeddings can be computed as follows: The model prediction is deï¬ned as the inner product of the userâ€™s and itemâ€™s ï¬nal representation ğ‘¦= ğ’†ğ’†, where ğ’†and ğ’†are the corresponding rows of ğ‘¬ . To optimize LightGCN, the Bayesian personalized ranking (BPR) loss [31] is adopted: Figure 1: Performance of untrained LightGCN versus SOTA, where the SOTA line is LightGCNâ€™s performance reported in [13] and LightGCN-R denotes the untrained LightGCN with diï¬€erent embedding dime ns ions. In this section, we identify the importance of smoothness and lowpass ï¬lters in CF, by using the light convolution in LightGCN as a speciï¬c example. The embeddings play an essential role in CF while smoothness is a key concept in graph signal processing. We observe that there are strong connections between the good embeddings and their smoothness on the graph. We consider the dot product based embedd ing model. Speciï¬cally, let ğ’†denote the embedding for the ğ‘¢-th user and ğ’†denote the embedding for the ğ‘–-th item. The predicted score for the ğ‘¢-th user and ğ‘–-th item is deï¬ned by the dot product ğ’†ğ’†. If ğ‘¹= 1, we should promote the similarities between ğ’†and ğ’†. By the deï¬nition of smoothness of graph signals (3), if ğ’†and ğ’†are similar for connected user-item pairs, the embedd ings are smooth signals on the graph. Consequently, optimizing loss functions, e.g., BPR loss (9), and enhancing the smoo thness of the embeddings share the same goal: promoting the similarity between ğ’†and ğ’†for ğ‘¹= 1. The above discussion provides a qualitative intuition for the role of smoothness in embeddings. We will now analyze the linear ï¬lter in LightGCN to obtain quantitative results. The LightGCN consists of two components: the initial embedding and a linear ï¬lter. If the untraine d LightGCN achieves good performance, it must be the linear ï¬ lter playing the essential role as the initial embedding is random. The next proposition shows that untrained LightGCN will have a low BPR loss under certain conditions. Theorem 3.1. Denote ğ‘= maxğ‘and ğ‘= minğ‘ where ğ‘– âˆˆ U âˆª I. If ğ‘¬âˆˆ Rfollows an i.i.d. uniform distribution over the unit sphere with then for a one-layer untrained LightGCN, we have Pğ’†ğ’†> ğ’†ğ’†|(ğ‘¢,ğ‘–) âˆˆ S, (ğ‘¢, ğ‘—) âˆˆ Sâ‰¥ 3/4, (11) where ğ¶ is an absolute constant, S= {(ğ‘¢, ğ‘–)|ğ‘¹= 1}, S= {(ğ‘¢, ğ‘—)|ğ‘¹= 0}. Remark. (Interpretations of Theorem 3.1) Equation (11) implies a low BPR loss as the predicted score of any positive pair is larger than that of any negative pair. Due to the smoot hing eï¬€ect of light convolution (linear ï¬lters), the ï¬nal embe ddings between interacted pairs are similar even if the initial embeddings are random. In Equation (10), ğ‘and ğ‘are adopted for a worst-case analysis, and in pr actice, we can replace them with the average degree. Equation (10) shows that the required embedding dimension of untrained LightGCN grows with the dataset density, which implies untrained LightGCN is more eï¬€ective on sparse datasets. For the Gaussian initialization adop ted in [13], the results are similar as high dimensional Gaussian random vectors concentrate around a sphere (refer to Sectio n 3.1 in [38]). The probability 3/4 can be improved to any probability approaches arbitrarily close to 1. The high-level reason for untrained LightGCN performing well is that the information contained in the rating matrix and the graph are identical. The BPR loss is adopted for exploiting t he information in rating matrix while the low-pass ï¬lters are to exploit information in the graph. T hus, a proper use of low-pass ï¬lters can accelerate the training or even avoid the training. Interestingly, some recent works also reveal that inï¬nitely wide random CNNs achieve better performance than trained ones [2]. Based on Theorem 3.1, we argue that the performance of untrained LightGCN improves with the embedding dimension and it should be co mpetitive to a trained o ne when the embedding dimension is suï¬ƒciently large. To verify t his argument, we follow the experiment settings in [13] and condu ct the experiments for a 3 -layer untrained LightGCN. The initial embeddings ğ‘¬is initialized following an i.i.d. Gaussian distribution N(0, 0.1) as in the original paper. Once the model is initialized, we do not train it but simply compute the user/item embeddings using Equation (8) and then directly test it on the test dataset. We use two sp ar se datasets, i.e., Gowalla and Amazon-book. The test performance versus the embedding dimension is shown in Fig. 1. As the training/test splitting of two datasets is identical to [13], we regard the LightGCNâ€™s performance reported in [13] as the state-of-the-art. We will also co mpare to LightGCN with large embedding dimensions in Table 4. The experiments agree with our theory well. As the linear ï¬lter is t o promote the smoothness, it demonstrat es t he crucial role of smoothness in CF. However, the untrained LightGCN is not a practical algorithm for recommendation as the large embedding dimension leads to an expensive memory cost and inference time. Fortunately, the untrained LightGCN with inï¬nitely dimensional embedding has a closed-form solution for predicted scores, as shown in the next theorem. Theorem 3.2. Consider an untrained LightGCN with ğ‘¬âˆˆ Rfollowing an i.i.d. distribution with zero mean and non-zero variance. As ğ‘‘ â†’ âˆ, the predicted score of the untrained LightG CN follows where ğ›½are constants depending on [ ğ›¼]in (8). Remark. (Interpretations o fËœğ‘¹Ëœğ‘¹) As shown in Theorem 3.2, the gram matrixËœğ‘¹Ëœğ‘¹ plays a pivotal role. For sparse binary data, (ğ‘«ğ‘¹ğ‘¹ğ‘« deï¬nes the cosine similarity between item ğ‘– and item ğ‘— [1]. Likewise, (Ëœğ‘¹Ëœğ‘¹)provides a similarity measure between item ğ‘– and item ğ‘—. Directly using the gram matrix as the item-item similarity results in the neighborhood-based method [ 1], which was the winner of Millions of Song Competitio n. The similarity between the neighborhood-based method and LightGCN is not surprising as LightGCN is based on the neighborhood p ropagation. As LightGCN consists of multi-hop propagation, the termËœğ‘¹Ëœğ‘¹ appears as polynomials. From the graph signal processing persp ective, it is a linear ï¬lter, which is low-pass. We call (12) as LightGCN with Inï¬nitely Dimensional Embedding (LGCN-IDE). The performance of LGCN-IDE is shown in Table 3. Remarkably, we see that on Amazon-book dataset, it outperforms the performance of LightGCN reported in [13] by more than 40% under exactly the same t raining/test data splits. In this section, we ï¬rst extend LGCN-IDE to incorporate general low-pass ï¬lters, which form a uniï¬ed framework. Then we p rove that this framework uniï¬es the neighborhood-based approaches, low-rank matrix factorization, linear auto-encoders, and linear graph convolutional networks, where diï¬€erent methods correspond to diï¬€erent low pass-ï¬lters. Finally, we present a simple yet eï¬€ective algorithm for CF. In this subsection, we extend (12) to incorporate general graph ï¬lters. To simplify the notations, we denoteËœğ‘· =Ëœğ‘¹Ëœğ‘¹ in the remaining of the article. Note thatËœğ‘· can also be seen as a normalized adjacency matrix for an item-to-item graph, whose eigenvalues are between 0 and 1. The graph Laplacian of the item-to-item graph is deï¬ned as Ëœğ‘³ = ğ‘° âˆ’Ëœğ‘·. In t his way, we can apply graph signal processing to the item-to-item graph. Next we elaborate our uniï¬ed framework, which is an extension of (12) with general graph ï¬ lters. We consider an input graph signalÂ¯ğ’“, which is so me transformation of the usersâ€™ observed ratings ğ’“. Then a low-pass ï¬lter is applied to the graph signal to obtain a ï¬ ltered signal. Finally, we may scale the obtained graph signal to get the ï¬nal prediction scores. Denoting the eigendecomposition byËœğ‘³ = ğ‘¼ ğš²ğ‘¼, the framework is given whereÂ¯ğ’”is the ï¬ltered predicted score, and â„ (Â·) is a low-pass ï¬l ter. From the graph signal processing perspective, it is a graph convolution, i.e., a graph signal ğ’“âˆˆ Rconvolving with a low-pass ï¬lter â„(Â·). Interestingly, some classic works for recommendation can be interpreted as graph signal processing approaches, where the low-pass ï¬lter plays an essential role. The classic methods typically involves auto-encoder-based [24, 27, 36], matrix factorization-based [6, 31], and GCN-based ones [13, 41, 51]. In this subsection, we will p rovide a uniï¬ed view of the linear methods from the graph signal processing perspective. As the spectral convolution can be transformed into a sp atial convolution in GCNs by ï¬rst-order approximation [19], it is interesting to investigate what kind of GCNs will these classic methods induce. These GCNs induced by classic algorithms can also be se en as white-box neural networks [5]. A test of performance for these GCNs is left for future works. 4.2.1 Low-rank Matrix Factorization. Low-rank matrix factorization is one of the most classic algorithms for CF. Note that GFT is also a matrix factorization where the low-frequency signal comp onents correspond to t he principle comp onents of the rating matrix. This observation allows us to connect MF and graph-based methods. We take t he obje ctive function in a recent work [6] as an example. Denote ğ‘‘ as the embedding dimension, the model is given As shown in [ 6], ğ‘½contains the smallest ğ¾ eigenvectors ofËœğ‘³ and ğ‘¼= ğ‘¹ğ‘«ğ‘½ . Viewing the eigendecomposition as GFT, it can be interpreted as an ideal low-pass ï¬lter (6) We then turn low-rank matrix factorizat ion into a spatial convolution fashion. This is more diï¬ƒcult t han the conversion in GCN [19] due to the orthogonal constraint and non-convexity of problem (14). Observing that t he optimal solution ğ‘½to (14) is also the optimal solution to the fol lowing problem We can rewrite (15) as spatial convolution by ï¬rst-order expansion like GCNs [19]. We begin with a random ğ‘¬âˆˆ R, and the update rule is given by where (a) follows Proposition 7 in [ 18]. The ï¬nal embeddings are given b y Note that (16) is a spatial graph convolution, and (17) is coincidentally equivalent to convolutional normalization for CNNs [26] (refer to (6)-(8) in [26]). The convolutional normalization was proposed to accelerate the training of convolutional networks and improve robustness. From this view, the low-rank matrix factorization is equivalent to an inï¬nite layer GCN with convolutional normalization. As the number of layers is large, it suï¬€ers from the over-smoothing issue [23], w hich hurts the performance. 4.2.2 Linear Auto-encoders. In the linear auto-encoders, e.g., EASE [34] and SLIM [ 27], the predicted score vector of a u ser (Â¯ğ’”) is obtained by the dot product where ğ‘© âˆˆ Ris a learnable weight matrix. The training ob-Ã jective is some regularized or constrained version of minkÂ¯ğ’“âˆ’ Â¯ğ’“ğ‘©k. From the graph signal pro cessing view, it can be interpreted as a graph signalÂ¯ğ’“convolving with a ï¬lter ğ‘©, andÂ¯ğ’“=Â¯ğ’“ğ‘© is a steady state. This deï¬nes a graph diï¬€usion on the corresponding graph like op inion dynamics (7) . Next, we show the e quivalence between a speciï¬c version of linear auto-encoders and the graph diï¬€usion ï¬lter. As shown in [36], the foll owing linear auto -encoder is able to achieve competitive performance compared with the deep ones [24, 43]. Speciï¬cally, we consider the following formulation in [36] for simplicity: As (18) is a ridge regression, we can write down t he closed-form solution as Viewing the eigenvalue d ecomposition as GFT, the graph ï¬lter in (19) is given by To understand (20) in the content of low-pass ï¬lters, the low-pass ratio ğœ‚in (4) is given by ğœ‚==(1 âˆ’ ğœ†)(1 + ğœ‡ âˆ’ ğœ†)(1 âˆ’ ğœ†)(1 + ğœ‡ âˆ’ ğœ†) The convolutional ï¬lter in (20) is similar to opinion dynamics and is a kind of graph diï¬€usion ï¬lter. Like other diï¬€usion-based methods, t he memory cost of linear auto-enco der is high as we need to store matrix ğ‘© in (18). In the literature, the Neumann series are often adopted to convert the graph diï¬€usion into a spatial convolution [20, 44 ]. Similarly, we can use it to interpret (19) as spatial GCNs. For ğœ‡ > 1, (19) can be written as ! From this view, the initial embedding is an identity matrix ğ‘¬= ğ‘° âˆˆ R, the update of corresponding spatial convolution is given by and the ï¬nal embeddings can be obtained as The layer combination appears naturally and the coeï¬ƒcients decrease quickly. As discussed in [13], the layer co mbination is the key to alleviate the over-smoothing issue and improve performance. 4.2.3 Neighborhood-based Approaches. The neighborhood-based approaches are often considered as exploiting ï¬rst-order graph information in the literature discussions [13]. We consider the following formulation, which utilizes the gram matrix as the similarity matrix [1], i.e.,Â¯ğ’”= ğ’“Ëœğ‘·. Obviously, the corresponding ï¬lter is a ï¬rst-order linear ï¬lter and the corresponding spatial GCN is a one-layer GCN. This ap proach is simple and scalable. However, it lacks higher-order information on the graph. 4.2.4 LGCN-IDE. For completeness, we analyze LGCN-IDE (12). By eigendecompo sition, the corresponding ï¬lter takes the form of Since it is still a LightGCN, LGCN-IDE naturally corresponds to multi-layer spatial GCN w it h a layer combination. In this subsection, we develop a simple yet eï¬€ective baseline algorithm, whose training is as eï¬ƒcient as the inference of LightGCN with a big-O notation. We ï¬rst analyze the inference computational complexity of LightGCN. We denote the number of nonzero elements in ğ‘¹ as ğœ‚. For a LightGCN with ğ‘‘-dimensional embedd ing, the inference time is O(ğœ‚ğ‘‘). The general graph ï¬lters require eigendecomposition and thus are not eï¬ƒcient for large-scale recommendation [13]. Fortunately, there are some graph ï¬lters that enjoy a high computational eï¬ƒciency, i.e., linear ï¬lt ers and ideal low-pass ï¬lters. In order to obtain linear ï¬lters, only the normalization is required during training, and thus the t raining complexity is O(ğœ‚). A major drawback of the linear ï¬lters is that they can hardly obtain a high-order information of the graph. For the ideal low-pass ï¬lter, only t he top-K eigenvectors ofËœğ‘· are required. Nevertheless, a direct computation for the top-K eigenvector ofËœğ‘· is far from computation and memory eï¬ƒcient because Ëœğ‘· is not as sparse as ğ‘¹. By using the equivalent formulation in (15), the largest eigenvectors can be computed by (16), (17), and this it erative algorithm is called the generalized power method (GPM) in the optimization literature [18]. In GPM, we only need to store ğ‘¹ instead ofËœğ‘·, and the computational complexity is O((ğ‘‘ğœ‚ + ğ‘‘) log(1/ğœ–)) where ğœ– is the desired accuracy for the eigenvectors. This algorithm is eï¬ƒcient as long as ğ‘‘< ğœ‚. As discussed before, the ideal low-pass ï¬lter is equivalent to an inï¬nite layer GCN without layer combination and it suï¬€ers from over-smoothing, which means that it lacks a low-order information in the graph. As a result, we argue that combining the linear ï¬lter and ideal low-pass ï¬lter will result in a strong baseline. Speciï¬cally, our proposed algorithm, named as Graph Filter based Collaborative Filtering (GF-CF), has the following form where ğ’”and ğ’“denote predict ed and o bserved scores, respectively. Likewise,Â¯ğ‘¼ is the top-K singular vectors ofËœğ‘¹, and ğ›¼ is the tuned parameter. We acknowledge that learning ğ›¼ or transforming (22) into GCNs may lead to better performance. Nevertheless, we will demonstrate that (22) already achieves the state-of-the-art performance. In this section, we ï¬rst describe the experimental settings, which exactly follow [1 3]. Next, we compare our method with the stateof-the-art deep learning methods. To keep the comparison fair, we use the same datasets, the same train/test splitting, and the identical evaluation metric as in [13]. The statistics of the datasets are listed in Table 2. The evaluation metrics are recall@20 and ndcg@20. Table 2: Statistics of the experimented data. Table 3: The comparison of overall p erformance among GFCF and competing methods. The performance of benchmarks is reproduced from [13]. Mult-VAE0.1641 0.1335 0.058 4 0 .04 50 0.0407 0.0315 GRMF0.1477 0.1205 0.057 1 0 .04 62 0.0354 0.0270 GRMF-norm0.1557 0.1261 0.056 1 0 .04 54 0.0352 0.0269 LightGCN0.1830 0.1554 0.0649 0.0530 0.0411 0.0315 GF-CF0.1849 0.1518 0.0697 0.0571 0.0710 0.0584 5.1.1 Benchmarks. We follow [13] to set up the benchmarks. (1) LightGCN [13]: LightGCN is the state-of-the-art method for CF. Please refer to Section 2.3 for a detailed description. (2) NGCF [41]: NGCF is a nonlinear deep GCN-based method. Besides the components in LightGCN, it contains of feature transformation, and nonlinear activation. (3) GRMF and GRMF-norm[13, 30]: GRM F ad ds a graph Laplacian regularizer to the training objective of BPR loss in matrix factorization. In GRMF-norm, the normaliz ed Laplacian is adopted instead of the graph Laplacian. (4) Mult-VAE [24]: This is a variational autoencoder based method. The d ata is assumed to be generated by a multinomial distribution and variational inference is adopted to estimate the parameters. In [41], it has been shown that NGCF outperforms GC-MC [4], Pinsage [49], NeuMF [15], CMN [10], MF [31], H OP-Rec [48] on the same train/test splitting. Thus, we will not include these methods as benchmarks. We also do not compare with full rank models [27, 35] due to the out of memory on Amazon-book dataset. The hyperparameter settings are identical to [13]. For the proposed graph ï¬lter based methods, we focus on the following two variants: (1) GF-CF: The proposed simple baseline method fo r CF in (22). (2) LGCN-IDE: The untrained LightGCN with inï¬nitely dimensional embedding. The closed-form is given in (12). For the implementation of graph ï¬l ters, we adopt Scipy [40] for sparse operation. The performance of the proposed methods and other benchmarks are shown in Table 3. Despite the simplicity, GF-CF achieves competitive or better performance than deep learning-based methods. 5.2.1 LGCN-IDE versus LightGCN. LGCN-IDE is an untrained LightGCN with an inï¬nitely dimensional embedding. On Gowalla and LightGCN-128 LightGCN-256 LightGCN-512 Yelp2018, which are of small sizes, LightGCN outperforms LGCNIDE. H owever, LGCN-IDE o utperforms LightGCN by a large margin on the large-scale dataset, i.e., the Amazon-book dataset. In LightGCN, the known scores are compressed into limited dimensional vectors, which restricts the expressiveness. In contrast, in LGCN-IDE, the ratings are directly used as the graph signal without compression. Additionally, LightGCN is trained with a stochastic gradient descent (SGD) while LGCN-IDE has a closed-form solution. As the size of the dataset increases, the optimization by SGD becomes more diï¬ƒcult. We suspect that these two reasons contribute to the large performance gain of LGCN-IDE over LightGCN in the Amazon-book dataset. 5.2.2 Graph filters versus deep learning-based methods. In Table 3, the simple graph ï¬lter achieves competitive o r better performance compared with deep learning-based method s. LightGCN also outperforms NGCF by removing the non-linear transformations. From the universal approximation theory [16], deep neural networks can approximate linear functions easily. Nevertheless, linear functions are non-trivial to learn for a neural network trained with SGD. A recent theoretical study demonstrates that it is impossible for neural networks with tanh, cosine, or quadratic activation to extrapolate the linear functions well [ 46]. With ReLU activation, A neural network can extrapolate linear functions well if the training dat a cover all directions (e.g., a hypercube covering the origin) [46], which is not trivial to satisfy in practice. This theoretical result suggests that learning linear functions is a non-trivial task. In addition, deep neural networks do well in extracting complicated features, but CF with implicit feedback is in lack of rich features. Owing to these two facto rs, the linear models are able to outperform deep models in CF with implicit fee dback. In this subsection, we compare GF-CF with LightGCNs of diï¬€erent embedding dimensions. For the untrained LightGCN, the performance improves signiï¬cantly with the dimension as shown in Fig. 1. The natural questions are 1) does the performance of trained LightGCN increase signiï¬cantly as the dimension grows; 2) how does GF-CF perform compared with LightGCN with large embed ding dimensions. We validate these questions empirically in Table 4. The experiments in this subsection are conducted on a server with an Intel Xeon(R) CPU E5-2698 v4 @ 2.20GHz and a Tesla V100 GPU. For the implementation of LightGCN, we download the source code from https://github.com/gusye1234/LightGCN-PyTorch and train 1000 epochs as the original paper. Due to the excessive training cost, we do not train LightGCN with an embedding dimension of more than 512. As shown in Table 4, GF-CF still achieves competitive or higher performance than LightGCN with large embedd ing dimensions. As the embedding dimension grows, the performance improvement of LightGCN becomes marginal, which is similar to matrix factorization and neural collaborative ï¬ltering [32]. The overall training time of GF-CF is even smaller than 1 training epoch consumed by LightGCN. It demonstrates that GF-CF is a simple but hard-to-beat baseline method for CF. Collaborative ï¬ltering (CF) plays a fundamental role in modern recommender systems [7]. One popular paradigm is the model-based CF methods. In such methods, the users and items are parameterized by (low-dimensional) vectors and the interactions are reconstructed based on the embeddings and mod el weights. The classic matrix factorization (MF) maps the ID of users and items as embedd ing vectors and uses the dot product between embedding vectors as predicted scores. The dot product model can be further improved by using neural networks [15, 37]. Another classic modelbased CF is to reconstruct the score for an item by a transformation of the scores for other items, from linear aut o-encoders (e.g., SLIM [27]) to de ep auto-encoders (e.g., Multi-VAE [24]). Another paradigm is graph-based CF methods. The early works (e.g., Item-rank [12] and Bi-rank [14]) exploit the label propagation on graph and belong to the neighborhood-based methods. These methods are often co nsidered as heuristics and inferior to model-base d methods due to the lack of training. Recent works address this issue by developing GCN-based methods and train GCNs in an end-to-end manner, e.g., GC-MC [4], NGCF [41], and LightGCN [13]. Notice that the information contained in the sparse rating matrix or graph formulation are identical and GFT is a matrix factorization. In this paper, we unify the two paradigms from the graph signal processing view and identify that the low-pass ï¬lters are the underlying key comp onent in the two paradigms. In addition, we show that diï¬€erent paradigms correspond to diï¬€erent low-pass ï¬lters and these ï¬lters can be incorporate d together to improve the performance. The spectral GCNs are developed from graph signal processing with learned graph ï¬lters, which enjoy theoretical guarantees from graph signal processing theory [33]. Nevertheless, GFT requires full eigendecomposition, which induces prohibitive computation for large-scale graphs. The spe ctral CF [52] and LCF [ 50] belong to this category and thus they cannot be applied on large-scale datasets. To speed up the computation, the spatial GCNs based on 1-hop neighbor propagation were propo sed [45]. In each layer of spatial GCNs, only neighborhoo d aggregations are requ ired, and thus the computational cost is extensively reduced. In the context of recommendation, spatial GCNs contain GCMC [4], NGCF [41], LightGCN [13], and PinSage [49]. A unique advantage o f these methods is the scalability, meaning that they can be applie d to large-scale sparse datasets. A recent theoretical study uniï¬ed the spect ral and spatial GCNs and demonstrates that they are all lowpass ï¬lters [3]. In the paper, we also unify the classic CF methods via low-pass ï¬ltering, which explains the success of GCNs in CF. In this paper, we identiï¬ed the importance o f smoo thness in the embeddings in a successful recommendation both theoretically and empirically, which bridges CF and graph signal processing theory. Via the lens o f graph signal processing, we showed that the neighborhood-based methods, low-rank matrix comp letion, and linear au to-encoders are all graph convolution with low-pass ï¬lters. This further validat ed the power of graph convolution for recommendation. In addition to our the oretical analysis, we also developed a simple but hard-to-beat baseline algorithm, GF-CF. It was demonstrated that GF-CF achieves competitive or better performance than deep learning-based methods. We believe that t he insights of this investigation are inspirational to the principled GCN architecture design for recommender systems. In the fu ture, we will implement the GCNs induced by classic algor it hms in Table 1 and exploit additional information, e.g., so c ial networks and knowledge graphs. Proof. We ï¬rst prove that (11) holds when the mutual coherence [9]) of the embeddings satisï¬es and then show that as ğ‘‘ >, (23) holds with probability at least 3/4. where (a) follows the assumption that ğœ– <. With Lemma 8.1, we see that ğœ– <as ğ‘‘ > Lemma 8.1. (Theorem 3.5 in [42]) Let ğ‘¨ âˆˆ Rwith rows i.i.d. chosen from the uniform distribution on the sphere. Then with probability at least 3/4, where ğ¶ is an absolute constant. Proof. We ï¬rst separate t he embeddings ğ‘¬into user embeddings ğ‘¼and item embeddings ğ‘½, and the individual update is given by The ï¬nal embeddings are and ğ‘¼ can be compu ted similarly. The ï¬nal prediction of untrained LightGCN with inï¬nitely dimensional embedding is given by For a pair of matrices ğ‘¿, ğ’€ , if the rows of ğ‘¿ âˆˆ R, ğ’€âˆˆ R follow independently identical distribution, due to the linearity of dot product, we have limğ‘¿ğ’€= E[ğ’™ğ’š], where ğ’™(resp. ğ’š) denotes the ï¬rst column of ğ‘¿ (resp. ğ’€ ). Thus, as ğ‘‘ â†’ âˆ, we have where ğ›½depends on [ğ›¼]. For a given user ğ‘¢, the estimated scores is shown asÃ diagonal, eigenvalues ofËœğ‘¨are a concatenation of eigenvalues of Ëœğ‘¹Ëœğ‘¹andËœğ‘¹Ëœğ‘¹. For the largest eigenvalue, we have where (a) follows Lemma 8.2. AsËœğ‘¹Ëœğ‘¹ is positive semi-deï¬nite, Lemma 8.2. Let ğœ†â‰¤ ğœ†â‰¤ Â·Â· Â·ğœ†be eigenvalues ofËœğ‘¨. Then âˆ’1 â‰¤ ğœ†â‰¤ ğœ†â‰¤ Â·Â· Â·ğœ†= 1. Thus, âˆ’1 â‰¤ ğ’™(ğ‘° âˆ’Ëœğ‘¨)ğ’™ â‰¤ 1. Furthermore, using the vector ğ’™ = ğ‘«1, we get Ëœğ‘¨ğ’™ = ğ‘«ğ‘¨ğ‘«ğ‘«1 = ğ‘«ğ‘¨1 = ğ‘«diag(ğ‘«) = ğ‘«1. This implies that the largest eigenvalue ofËœ