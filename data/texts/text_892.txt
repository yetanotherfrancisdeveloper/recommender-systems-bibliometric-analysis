Session-based recommendation (SBR) is a challenging task, which aims at recommending next items based on anonymous interaction sequences. Despite the superior performance of existing methods for SBR, there are still several limitations: (i) Almost all existing works concentrate on single interest extraction and fail to disentangle multiple interests of user, which easily results in suboptimal representations for SBR. (ii) Furthermore, previous methods also ignore the multi-form temporal information, which is signiî€›cant signal to obtain current intention for SBR. To address the limitations mentioned above, we propose a novel method, called Temporal aware Multi-Interest Graph Neural Network (TMI-GNN) to disentangle multi-interest and yield reî€›ned intention representations with the injection of two level temporal information. Speciî€›cally, by appending multiple interest nodes, we construct a multi-interest graph for current session, and adopt the GNNs to model the itemitem relation to capture adjacent item transitions, item-interest relation to disentangle the multi-interests, and interest-item relation to reî€›ne the item representation. Meanwhile, we incorporate item-level time interval signals to guide the item information propagation, and interest-level time distribution information to assist the scattering of interest information. Experiments on three benchmark datasets demonstrate that TMI-GNN outperforms other state-ofthe-art methods consistently. â€¢ Information systems â†’ Recommender systems. ACM Reference Format: Qi Shen, Shixuan Zhu, Yitong Pang, Yiming Zhang, and Zhihua Wei. 2018. Temporal aware Multi-Interest Graph Neural Network For Session-based Recommendation . In WXXXX, June 03â€“05, 2021, XXX, XX. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/1122445.1122456 Recommender system has become the basis to relieve the information overload problem. Most recommendation methods capture usersâ€™ interest by modeling usersâ€™ long-term preference for predicting their future interactions, e.g., collaborative î€›ltering [6,7] and neural network based models [21,30]. Diî€er from traditional recommendation tasks, in many practical scenarios, there is only a session available without access to user identiî€›cation and historical interactions. This kind of task called session-based recommendation (SBR), aims to extract useful information as much as possible from limited data in current session. Existing SBR methods mostly concentrate on modeling sequential information among items of current session by using Recurrent Neural Networks (RNNs) [8,10] or Graph Neural Networks (GNNs) [16,27]. However, these works simply regard a session as a short sequence with single intention, consider that the basic intention of user in a session usually remains the same, and try to capture userâ€™s current interest directly from the entire session. They overlook the fact that even in a relatively short-term session, the userâ€™s î€›ner granular interests can be multiple in various views, drift over time, or interweave with item overlaps. Some studies [1,4,9,14,24,31] in other recommendation areas have veriî€›ed the eî€ectiveness of modeling userâ€™s multi-interest, but in SBR, the multi-interest methods have not been fully explored. As there is no interest disentangle mechanism in existing SBR methods, the mixture of major interests and minor interests may mislead the session representation learning so that itâ€™s hard to conî€›rm usersâ€™ true intentions and lacks of interpretability. Moreover, the auxiliary temporal information is also ignored in session modeling. Intuitively, two typical temporal information of interaction sequences can be recognized for multi-interest modeling: (i) item-level time inter val signals are the intervals between item transitions, which reî€ect the relatedness between adjacent items. Generally, the tight time interval indicates the higher relevance of items, while loose time interval may mean the drifting of interest. (ii) interest-level distribution information represents the interest distribution through virtual location and coverage scope in timeline, Figure 1: A toy example for two sessions with the same item sequence but diî€erent time intervals, where the circles connected to items denote the userâ€™s interests. Positions of the circles reî€ected the distribution of interest in the timeline, and the size indicates the importance of these interests. which is helpful to model the relation between item and interest from the time perspective. For the chunked interest distribution in the session, the closer distance between items and interest factors in the timeline usually reveals the higher relatedness of them. We illustrate this with an example in Figure 1, including two sessions with the same item sequence under diî€erent interaction time distributions. For the î€›rst session with short time intervals, as userâ€™s intention tends to be continuous over a short period of time, we can conclude that even item relatively far from current position may still represent userâ€˜s strong interest. So user may mostly be interested inPhoneproduct andApplebrand. Under this circumstance, previous methods without introducing auxiliary time information will regard it as a session with evenly and moderate length time intervals. Therefore, a deviation from the major interests may be caused by the latest items, which are laptops. While for the second session with a relatively long time interval between iPhone and Macbook, we argue that the interest drift may occurred during the interval. Thus the interests distributed in the front portion of timeline such asPhone, ought to be a minor factor for recommendation, while the interestLaptopin the latter portion should be taken into special consideration. But for SBR methods ignoring time information, the multiple interests in two chunks of the session cannot be eî€ectively disentangled and rearranged. The minor interestPhoneis likely to be over weighted for userâ€™s current intention, while the crucial interestLaptopmay be diluted. As discussed above, even sessions with exactly the same items and orders, but for diverse time distributions, may have diverse interests intensity, leading to completely diî€erent next items. In this case, the ignorance of time information makes existing methods insuî€œcient to distill eî€ective intention signals from the active session. To address these problems, we propose Temporal aware MultiInterest Graph Neural Network (TMI-GNN), a novel method to distill the disentangled interest and inject the temporal information for better inferring the user intentions of the current session. To be more speciî€›c, we î€›rstly construct a multi-interest graph for current session by appending multiple interest nodes into original itemitem graphs, which builds the potential relations between items and diî€erent interest factors. Then, to generate the item and interest representations, we synchronously apply item information propagation with item-level time interval signals, interest extraction in a soft clustering way, and interest attaching with the interest-level distribution information. Finally, we integrate item embeddings and interest embedding under the guide of temporal distance, to represent userâ€™s preference for next item prediction. Our main contributions of this work are summarized below: â€¢We propose to construct a multi-interest graph with item and interest nodes for representing multi-interest session eî€ectively, involving the explicit item transitions and potential connections between diî€erent items and interests. â€¢For the constructed graph, we develop a novelTMI-GNNmodel for SBR to capture adjacent item transitions, distill usersâ€™ current multi-interests from noisy interaction sequences and feedback related interest information to enhance session representations. Moreover, it explicitly injects item-level and interest-level temporal information into the above process to reî€›ne current intention representation. â€¢Extensive experiments on three datasets demonstrate that our model is superior compared with state-of-the-art models. Session-based Recommendation.Following the development of deep learning, many neural network based approaches have been proposed for SBR. Due to well sequence modeling capability of RNNs, RNN-based methods have been widely used for SBR [8,10,13,19,22]. For instance, GRU4Rec [8] was î€›rstly proposed to utilize GRU layer to capture information in interaction sequences. Based on GRU4Rec, NARM [10] added an attention mechanism after RNN, which refers to last interaction item, in order to capture the global and local preference representation of user in current session. But in an ongoing session, interaction patterns are always more complex than simple sequential signals, which cannot be eî€ectively captured by RNN-based models. More recently, motivated by the superior performance of GNNs in extracting complex relationships between objects, quite a few recommendation methods relying on GNNs were proposed to extract the item transition patterns for SBR [2,3,16â€“18,25,27,28]. For example, SRGNN [27] converted the interaction sequence into a directed graph and employed the gated GNN (GGNN) on the graph to learn item embedding. LESSR [3] formed better graph structure from the session by proposing a lossless encoding scheme, and proposed a SGAT layer to model the long-range dependency, which propagates information along shortcut graph. StarGNN [16] put forward a star graph neural network to model the complex transitions between items with additional node to connect the long-range item relations. DATMDI [2] learned the cross-session and local session representations via the GNN and GRU and combined them by a soft-attention mechanism. Temporal information in Recommendation.Meanwhile, temporal information is also a key feature in many recommendation scenarios, such as e-commerce and video recommendation. There are a few works to utilize the temporal information as contextual feature for recommendation [5,11,23,26]. However, most existing methods simply reduce the temporal information to order relationship, subsequently use RNN-based model to capture the sequential signal. For instance, [30] used RNN to learn a dynamic representation of a user which reveal userâ€™s dynamic interests at diî€erent time in next basket recommendation. [29] discovered absolute time patterns and relative time patterns based on insightful data analysis to model usersâ€™ temporal behaviors for recommender systems. [32] proposed a framework called RIB, which takes dwell time into account.By using the time a user spends on one item as a part of so called micro behavior, it can model more practical user intent. As outlined above, previous works on SBR have some limitations. First, temporal information is rarely or crudely exploited in these works. Whatâ€™s more, none of these methods explicitly disentangle the multiple interests of user in a session. These limitations may lead to suboptimal performance. In this work, we aim to explore the eî€ectiveness of abundant temporal information and multiple disentangled interest for SBR. Given the entire item setV, we î€›rst deî€›ne a timestamp-augmented item session sequence asğ‘  = {(ğ‘£, ğ‘¡), (ğ‘£, ğ‘¡), ..., (ğ‘£, ğ‘¡)}, where(ğ‘£, ğ‘¡) represents the user interacted itemğ‘£âˆˆ Vat timeğ‘¡andğ‘¡< ğ‘¡, andğ¿is the session length. Given sessionğ‘ , the goal of SBR is to predict a probability score for any itemğ‘£ âˆˆ Vsuch that an item with higher score is more likely to be interacted next. In this section, we detail the design of our model. As shown in Figure 2, it contains four main components: Multi-interest Session Graph Construction, Disentangle Graph Modeling Layer, Session Representation Learning Layer and Prediction Layer. By constructing item-transition sequences as multi-interest graphs with additional interest nodes, we explicitly build the potential connection between latent user preference and explicit items sequence. Furthermore, at the Disentangle Graph Modeling Layer, we extract item transition information, distill multi-interest representations and feedback disentangled interests to related item embeddings, respectively. With the guidance of item-level and interest-level temporal information, the reî€›ned attention is better estimated for disentangling userâ€™s multi-interest. Then, Session Representation Learning Layer adaptively generates the î€›nal intention representations with the injection of last-item based time interval information. Finally, the predictor estimates the probability of candidate items based on the each disentangled session representations. As mentioned above, the extraction of multi-interest is meaningful for obtaining user intention representation from the interaction sequence. Therefore, we propose multi-interest session graph for eî€ectively organizing the interactions in the session and modeling userâ€™s multi-interest. For given sessionğ‘ , we construct the multi-interest session graph G=(V, E),whereV=({ğ‘£, ğ‘£, ğ‘£, . . . , ğ‘£}, {ğ‘¢, ğ‘¢, . . . , ğ‘¢}) indicates the node set of the constructed graph which contains ğ‘item nodes andğ»interest nodes. Besides the basic transition between items in the session, we additionally introduce theinterest nodesto represent each independent interest, which can be explained as diî€erent distributions of itemsâ€™ contribution to userâ€™s intention. Forğ‘—-th interest node, it fully connects to all items in the session with edges{(ğ‘¢, ğ‘£)|1â‰¤ ğ‘– â‰¤ ğ‘ }. For each session, each item nodeğ‘£has corresponding item-interest edges{(ğ‘£, ğ‘¢)|1â‰¤ ğ‘— â‰¤ ğ» } to each interest node, and item-item transition edge to contextual item nodes. Through the full connection between the explicit item and interest node, the soft assignment of each item to corresponding interest can be estimated by subsequent GNN as edge attribute. 4.2.1 Temporal Information. Furthermore, we attach auxiliary multiform temporal information to the multi-interest graph for distilling more precise interest representations. For the original timestamp sequence(ğ‘¡, ğ‘¡, . . . , ğ‘¡)of sessionğ‘ , item-level transition interval ğ‘¡is attached to the edge of item ğ‘– and ğ‘—:î€Œî€Œ whereğ‘¡denotes the pre-deî€›ned length of time bucket. Moreover, the relative time-stepğ‘¡is attached to the interaction position ğ‘–, compared to the start time at the î€›rst position 1. For the constructed multi-interest session graph, there are three types of relation, i.e., item-item, item-interest and interest-item relation, and we represent them by superscriptğ‘£ âˆ’â†’ ğ‘£,ğ‘£ âˆ’â†’ ğ‘¢and ğ‘¢ âˆ’â†’ ğ‘£ respectively. Next, we present how to obtain item representations and disentangle interest representations on the constructed heterogeneous multi-interest graph. Based on the message propagation of GNNs, the item and interest node embeddings are updated based on the previous results with neighbor information. The disentanglement of multi-interest can be consider as the process of iteratively reî€›ning interest node embedding, and the explainable assignments for each interest factor can be estimated as the weight of item-interest edges based on the full connection of each independent interest node and the item nodes, i.e., each interest factor is generated by item information pooling with diî€erent assignment scores. For the adjacent item transition, previous GNN-based model, like SRGNN [27], has achieved superior performance for SBR. Therefore, for each GNN layer, we î€›rstly aggregate the neighbor messages in each relation respectively. Then we gather the semantic information to update the item and interest representations. Letu, vdenote the embedding of interestğ‘¢and itemğ‘£ afterğ‘˜layers GNN propagation. The item IDs are embedded into ğ‘‘-dimensional space and are used as initial node features in our model,vâˆˆ R. For the multiple temporal information, we embed them by a learnable temporal matrixT = [T, T, T, . . . , T]for the rounded time value 0â‰¤ ğ‘¡ â‰¤ ğ‘š, whereğ‘šis the max time-step intervals. Moreover, for each interest factor as related interest node, we adopt average operation on the item nodes to initialize the inter-Ã est representation, i.e.,u=v. For the interest-side temporal information, we utilize the the center timestampğ‘¡ to indicate the location ofğ‘–-th interest factor on the timeline, and the temporal compactness valueğ‘¡to indicate the coverage of interest factors in the item sequence. Here we initialize these two characteristics of each interest factor with average pooing, i.e.,ÃÃ Figure 2: The overview of TMI-GNN. The multi-interest graph contains three typ es of relations: item to item, item to interest and interest to item.The Disentangled Graph Modeling layer learns the interest and item embeddings under the guide of rich temporal information. Finally, we aggregate the interest and item representations to generate the î€›nal session embedding. Moreover, an additional interest independent loss is considered to encourage the diversity of interests. Then, we will detail the modeling for item-item, item-interest and interest-item relations, respectively. 4.3.1 Item-level Information Propagation Layer. For the item-item relation, we adopt SRGNN [27] with detail time interval information to propagate adjacent node information. It assembles neighbor node information with the normalized coeî€œcientğ‘’under the guide of time interval signal ğ‘¡:î€î€î€‘î€‘ whereMLPrepresents a simple multilayer perceptron for time interval embeddingT, and shares with diî€erent GNN layers. Similar to GGNN, we feed the neighbor informationmand the previous layer itemvinto GRU to update the item-item relation node representations:î€î€‘ where the GRU unit is parameters-shared for all item nodes updating at current layer. With the combination of contextual interaction items representations and previous cross-semantic item presentations, we integrate the chronological item transition information into node embedding in the item-item relation. 4.3.2 Interest Extraction Layer. For the item-interest relation at layerğ‘˜, a graph attention neural network is utilized for updating interest node embedding, as the process of distilling interest representation. In particular, we compute the corresponding correlation weightğ›¼between target interest nodeğ‘¢and neighbor item ğ‘£, as the explainable assignment scores for disentangled interest. And interest node representation is updated via the sum pooling as following:î€î€î€‘î€‘ ğ›¼= softmaxLeakyReLUWu+ Wv, whereWâˆˆ Rrepresents the information transformation matrix of neighbor nodes,W, Wâˆˆ Rare the parameters of linear transformation for the target interest and the source item, respectively. Meanwhile, we also update the center timestamp and temporal compactness of each interest node based on the normalized correlation coeî€œcient:îƒ•îƒ• Through the assignment scoreğ›¼, we not only distill the latent representations of multi-view interests from the sessions with overlapped and interwove interests, but also oî€er an explanations parameter for each interest factor. 4.3.3 Interest Aî€aching Layer. To reî€›ne the item representation via the disentangled interest, we adopt a graph attention network to update the item representation under the interest-item semantic. At î€›rst, we estimate the attention coeî€œcient between target item and source interest node. Here, we consider both the similarity and temporal continuity of each item and interest node pair: whereW, Wâˆˆ Rare the transforming matrices for the item and interest.ğ‘¡=denotes the distance between item and interest in timeline, andwâˆˆ R, ğ‘are the linear transformation of it. Here, we utilize the residual between the center timestep of interest and timestep of items regularized by the interest compactness value, to measure the similarity of interest and item pair in the temporal view. Then, gathered interest factors are scattered to item nodes for generating the intention-augmented item representations via parameters Wâˆˆ R:îƒ• The edges between interests and items indirectly connect each item to all other items through the intermediate interest node. Compare to the original sparse item-item graph, the additional interest nodes and edges help GNNs to eî€ectively capture long-range dependencies in sessions with any length because it propagates information along intermediate interest paths within two-hop. 4.3.4 Layer Combination. By updating the node representation based on diverse semantic relations synchronously, we obtain the item representations from adjacent items and related interest, and the interest embeddings based on item-level soft clustering. Then, we gather cross-semantic information through average operation like RGCN [20], i.e.,v= ğœ (v+v)/2,u= ğœ (u), and considervanduas the output of item node ğ‘£and interest node ğ‘¢after ğ‘˜-th GNN layers. Moreover, in order to mine deeper items transition relations, multi-layers of GNN are stacked to propagate high-order information. Moreover, we utilize the gated mechanism to balance the item node representations between initial embedding andğ¾layers output, as follows:hi whereâˆ¥is the concatenate operation,Wâˆˆ Rand sigmoid functionğœ (Â·)generate the balance factorğ‘”to alleviate over-smooth problems of deep GNN. As for the interest node, we simply adopt the multi-layersâ€™ output as the î€›nal latent interest representation, i.e., u= u. After the Disentangle Graph Modeling, we obtain the î€›nal item and interest node embeddings. Then we aggregate the representation of items in the session based on each abstract interest factor to generate the session representations. Moreover, the items clicked later in the session usually reveal the userâ€™s current intentions better, which draws greater attention for SBR. Therefore, we incorporate additional last-item based time interval information into interestbased attention to capture user activate intention dynamically. In detail, inspired by the reverse position embedding in GCEGNN[25], we integrated the last-item based time interval information with the obtained item representations, which extends the position information to a more î€›ne-grained time domain to make a better prediction:î€€î€‚î€ƒî€ whereWâˆˆ Randbâˆˆ Rare trainable parameters,Tis the embedding for last-time based interval ğ‘¡. Then, the intentions of user are learnt by a shared attention mechanism, which dynamically weights item representation based on each interest u: whereğœis an activate function,W, Wâˆˆ Randq , b âˆˆ R are trainable parameters. Then we combine theâ„-th coarse-level intention and reî€›ned item-augment representations to generate the session representation for each interest:î€€î€‚î€ƒî€ where Wâˆˆ Ris the trainable parameter. With the incorporation of auxiliary last-item based interval information, we capture session representations involved in both the disentangled interest factors and interaction session items in relative chronological temporal-aware patterns. Based on each disentangled interest representationSlearnt above and the normalized initial embeddingsvof candidate items, we then estimate the interaction probabilityË†yof candidate items for current session:î€î€‘ whereË†ğ‘¦âˆˆË†ydenotes the probability that the user will click on item ğ‘£in the current session, andğ»is the pre-deî€›ned parameter of the interest node number. As mentioned above, î€exible number of interest nodes encourages the chunked interest representations conditioned on diî€erent behavior patterns. However, the diî€erence constraint drove by multiple interest extractions is insuî€œcient: there might be redundancy among latent interests representation, which conî€icts with the target of disentangling multi-view user interest. We hence introduce interest independents loss, which hires distance correlation measures as a regularizer, with the target of encouraging the multiinterest representations to be diverse. We formulate this as follows: whereğ‘ğ‘œğ‘ (Â·)indicates the similarity distance between two innersession interest representation pair. The î€›nal optimization process is to minimize the cross entropy loss function together and the interest-independence loss jointly: L = âˆ’ylog(Ë†y) + (1 âˆ’ y)log(1 âˆ’Ë†y) + ğœ†L,(18) whereyâˆˆ Ris a one-hot vector of ground truth, andğœ†is the coeî€œcient controlling interest-independence term. In this section, we conduct experiments on SBR to evaluate the performance of our method compared with other state-of-the-art models. Our purpose is to answer the following research questions: RQ1:How does our model perform compared with state-of-the-art SBR methods?RQ2:How does the temporal information of the sequence aî€ect the recommendation results?RQ3:Is the design of our model reasonable and eî€ective? How do the key modules of TMI-GNN inî€uence the model performance?RQ4:How do the hyper-parameters aî€ect the eî€ectiveness of our model? Table 1: Statistics of datasets use d in experiments. 5.1.1 Dataset. We conduct extensive experiments on three public datasets: RetailRocket, Yoochoose and Jdata, which are widely used in the SBR research [3,15,25] and can support our work with seconds-level timestamp information. â€¢RetailRocketcontains behavior data and item properties that collected from a real-world e-commerce website. â€¢Yoochooseis a RecSys Challenge dataset, which consists of clickstreams from an E-commerce website. Diî€erent from [10,13], we use the most recent fractions 1/16 sequences of Yoochoose as the total dataset Yoochoose 1/16. â€¢Jdatarecords the historical interactions from the JD.com. It contains a stream of user actions within two month. We extract the session data with the setting of the duration time threshold 1 hour. To î€›lter poorly informative sessions and items, following [10,27], we î€›rst î€›ltered out all sessions of lengthâ‰¤2 and items appearing less than 5 times in all datasets. Then we applied a data augmentation technique described in [10]. The statistics of all datasets after prepossessing are summarized in Table 1. 5.1.2 Baseline Models. To demonstrate TMI-GNNâ€™s superiority performance, we compare it with several representative competitors, including the state-of-the-art SBR models and several temporalconcerned methods. â€¢ GRU4Rec[8] employs the GRU to capture the representation of the item sequence simply. â€¢ NARM[10] is a RNN-based model which combines with attention mechanism to generate the session embedding. â€¢ RIB[32] is a framework using RNN and attention layer to model user micro behavior including behavior and dwell time. Here we consider the time intervals as the dwell time and ignore the behavior types. â€¢ SRGNN[27] converts session sequences into directed unweighted graphs and utilizes a GGNN layer [12] to learn the patterns of item transitions. â€¢ LESSR[3] adds shortcut connections between items in the session and considers the sequence information in graph convolution by using GRU. â€¢ DATMDI[2] combines the GNN and GRU to learn the crosssession enhanced session representation. Besides, we combine the time interval embdding with ID embedding as input following [32], and inject additional time information into the graph by adding learnable time interval weights like subsubsection 4.3.1 for SRGNN, LESSR and DATMDI method separately, named SRGNN, LESSR, DATMDI. 5.1.3 Evaluation Metrics. To evaluate the recommendation performance, we employ two widely used metrics: Hit ratio (H@ğ‘˜) and Normalized discounted cumulative gain (N@ğ‘˜) following [25,27], whereğ‘˜is 10 or 20. The average results over all test session are reported. 5.1.4 Implementation Details. We implement the proposed model based on Pytorch and DGL. The embedding dimension is set to 128. All parameters are initialized through a Gaussian distribution with a mean of 0 and a standard deviation of 0.1. We employ the Adam optimizer to train the models with the mini-batch size of 512. We conduct the grid search over hyper-parameters as follows: learning rate in{0.001,0.01,0.1}, learning rate decay in factorğœ†in{1,3,10,30}. The maximal time-stepğ‘šis set to 300, which is large enough for all sessions. To make the comparison fairer, we range the hyper-parameters of baseline methods with the same tuning scopes of our experiments. To demonstrate the overall performance of the proposed model, we compared it with the state-of-the-art methods for SBR. We can obtain the following signiî€›cant observations from the comparison results shown in Table 2. Comparison of Diî€erent Baselines.The NARM performs signiî€›cantly better than GRU4Rec, which indicates the eî€ectiveness of attention mechanism to capture the userâ€™s main motivation. In the comparison between RNN-based models and GNN-based models for Yoochoose and Jdata, the GNN-based models generally outperform the RNN-based models, which veriî€›ed the certain advantage of GNN for SBR. Moreover, we notice that LESSR can outperform SRGNN in most cases. This may be due to the special design in LESSR for capturing long-range dependency between items in a session, which contributes to better performance on some long sessions. Meanwhile, for dataset RetailRocket, the simple model NARM outperforms other complex models because of the insuî€œcient training caused by the small amount of data. Signiî€›cance of Temporal Information.Then we turn to the temporal information attached methods. The RNN-based model RIB achieves superior performance to GRU4Rec but performs slightly inferior than NARM, because RIB employs a relatively simple attention instead of last-item based attention. Compared to the original models, the temporal-enhanced methods all achieve better performance in some degrees, which indicates the signiî€›cance of temporal information in SBR. Moreover, the adapted methods perform better on small dataset RetailRocket, which indicates that the auxiliary temporal information is more helpful for sparse interaction data. Table 2: Experimental results (%) of diî€erent models in H@{10, 20}, and N@{10, 20} on three datasets. ğ¼ğ‘šğ‘ğ‘Ÿğ‘œğ‘£ . means improvement over the state-of-art methods.The b old number indicates the improvements over the best baseline (underlined) are statistically signiî€›cant (ğ‘ < 0.01) with paired t-tests. Model Eî€ectiveness.We î€›nd that our model comprehensively outperforms all other baselines substantially on almost all metrics, which justiî€›es the eî€ectiveness of our model. The performance improvement can be explained in two aspects. One is that TMI-GNN can disentangle user intention via extracting multi-interests from the multi-interests session graph, so it can portray more profound representation of user interests. This strategy break the limitation of expression ability of only one interest. Another one is that we introduce multi-form temporal information to the process of disentangle graph modeling and session representation learning. Compare with other time-aware models, TMI-GNN models diversiî€›ed temporal information adequately eî€ectively. Table 3: The performance comparison w.r.t diî€erent temporal information and module design. YoochooseH@20 66.32 66.25 66.28 66.37 65.86 66.35 66.46N@20 37.54 37.63 37.53 37.79 37.23 37.71 38.05 JdataH@20 44.89 44.75 44.82 45.14 44.25 45.21 45.38N@20 21.98 21.94 21.97 22.02 21.80 22.03 22.18 Impact of temporal information.In this part, we compare our model with partially temporal information masked versions in Table 3 to test whether considering the multi-form temporal information can boost model performance. The method with "-V2V" means skipping the time intervals in item-level message propagation, "U2V" indicates ignoring the temporal factors in interest-item relation, "-Last" represents removing the last-item based time-interval signals, and "First" means utilizing the î€›rst-item based time-interval embedding in session representation learning module. By comparing methods mentioned above, we î€›nd that the loss of any type of temporal information will cause the decline of model performance. Besides, compare to "First" time embedding, the last-item based time-interval information is more helpful for SBR. Moreover, Figure 3: Performance comparison w.r.t. diî€erent depths of GNN. the loss of interest-item temporal continuity factors is more signiî€›cant in our model compared with other temporal information types. Based on the above illustrations, we demonstrate that injecting multi-form temporal information in our framework is indeed meaningful. Impact of diî€erent Designs.In this part, we compare our method with diî€erent variants to verify the eî€ectiveness of the critical components of TMI-GNN. Speciî€›cally, we remove the additional interest node in session graph (denote as â€-Interestâ€), and mask the interest independent loss (denote as â€-Lossâ€), respectively. The experimental results are presented in Table 3. It can be observed that the abstract interest nodes is pivotal for the model capability of intention capturing. Meanwhile, for the diverse intention modeling, the removal of interest independent loss leads to great impact on model results, which demonstrates that the forced cross-interest separation is helpful for disentangling multiple interest of user. In summary, we can infer that the key components of TMI-GNN are eî€ective through the comparison and analysis above. Figure 4: Performance comparison w.r.t. diî€erent time bucket width and max interest no de number. Impact of GNN depth.To study the impact of depth of GNN, we conducted experiments under diî€erent GNN depth settings (from 1 to 5). Figure 3 shows the corresponding results. We can observe that as the depth goes up from 1 to 3, the performance of our model increases on both datasets, which clariî€›es the signiî€›cance of using multi-layer GNN to distill userâ€™s multi-interests. Moreover, the performance declines as the depth grows from 3 to 5 on Yoochoose, showing that excessive depth of GNN may lead to over-smoothing problem and less distinguishable item representation. As a comparison, with the number of GNN layers increasing, the performance of SRGNN drops on both two datasets, while our modelâ€™s performance degrades slowly on Yoochoose, even stay a slightly increase on larger dataset Jdata. This indicates that our model is more efî€›cient in handling the over-smoothing problem of GNN through the recombination of item representation, thus achieves more reî€›ned modeling of user interest through stacking more GNN layers without performance degradation. Impact of time bucket widths.As discussed in section 4, we divide the time interval into buckets to utilize time signals. So we conduct tests by ranging the time bucket width within{2,4,8,16,32} to explore how does the bucketing setting aî€ect the modelâ€™s performance. As shown in î€›gure 4, we can see that the performance does not î€uctuate dramatically as the time bucket width changes, which indicates that our model is not sensitive to the time bucket width. Impact of interest no de num.To investigate the impact of the interest node number, we range this parameter in{1,2,3,4}. According to the results in Figure 4, we can see that for both Yoochoose and Jdata, the model with 2 interest nodes achieves the best performance in most metrics. Compare to single interest node with mixed interest representation, our model with two interest nodes disentangles the latent interest representation for better next item prediction. When the number becomes larger, performance will drop due to the redundancy of interest representation. In this paper, we pay special attention to the disentangled multiinterest representations of user and multiple temporal information for session based recommendation. We construct a multi-interest graph and devise the TMI-GNN model, which utilizes the multiinterest graph to capture adjacent item transitions, distill multiinterest representations with the injection of multi-form temporal information. In the experiments, our model outperforms other stateof-the-art session-based models, showing the eî€ectiveness of our model.