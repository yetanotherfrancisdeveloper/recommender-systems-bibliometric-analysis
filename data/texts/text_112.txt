University College LondonNoahâ€™s Ark Lab, HuaweiBeijing Key Laboratory of Big Data Management and Analysis Top-N recommendation, which aims to learn user ranking-based preference, has long been a fundamental problem in a wide range of applications. Traditional models usually motivate themselves by designing complex or tailored architectures based on diî€erent assumptions. However, the training data of recommender system can be extremely sparse and imbalanced, which poses great challenges for boosting the recommendation performance. To alleviate this problem, in this paper, we propose to reformulate the recommendation task within the causal inference framework, which enables us to counterfactually simulate user ranking-based preferences to handle the data scarce problem. The core of our model lies in the counterfactual question: â€œwhat would be the userâ€™s decision if the recommended items had been diî€erent?â€. To answer this question, we î€›rstly formulate the recommendation process with a series of structural equation models (SEMs), whose parameters are optimized based on the observed data. Then, we actively indicate many recommendation lists (called intervention in the causal inference terminology) which are not recorded in the dataset, and simulate user feedback according to the learned SEMs for generating new training samples. Instead of randomly intervening on the recommendation list, we design a learning-based method to discover more informative training samples. Considering that the learned SEMs can be not perfect, we, at last, theoretically analyze the relation between the number of generated samples and the model prediction error, based on which a heuristic method is designed to control the negative eî€ect brought by the prediction error. Extensive experiments are conducted based on both synthetic and real-world datasets to demonstrate the eî€ectiveness of our framework. â€¢ Information systems â†’ Recommender systems. Recommender Systems, Bayesian Personalized Ranking, Structure Causal Model, Counterfactuals ACM Reference Format: Mengyue Yang, Quanyu Dai, Zhenhua Dong, Xu Chen, Xiuqiang He, Jun Wang. 2021. Top-N Recommendation with Counterfactual User Preference Simulation. In Proceedings of the 30th ACM Intâ€™l Conf. on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, Australia. ACM, New York, NY, USA, 11 pages. https://doi.org/10. 1145/3459637.3482305 Recommendation system basically aims to match a user with her most favorite items. In a typical recommendation process, the system î€›rstly recommends an item list to a user, and then the user provides feedback on the recommendations. In real-world scenarios, people only access a small amount of items, which makes the observed dataset extremely sparse. Recommendation task is usually formulated as a ranking problem. Early models mostly base themselves on the simple matrix factorization method [26]. In order to achieve better performance and adapt diî€erent scenarios, recent years have witnessed much eî€ort on neuralizing the recommendation models [18,39]. However, the eî€ectiveness of neural models usually depend on a large amount of training samples, which contradicts with the aforementioned sparse user behaviors. In another research line, causal inference (CI) has been recently introduced into the machine learning community to augment the training data for more comprehensive model optimization [1]. The basic idea is î€›rstly assuming an underlying structure causal model (SCM), and then learning the model parameters based on the observed data. At last, the new training samples are generated by actively changing the input variables (called intervention) and collecting the cared outputs. Such sample enrichment method has been successfully applied to the î€›elds of neural language processing (NLP) [41] and computer vision (CV) [2,6,12]. In this paper, we adapt this method to the recommendation domain, which is expected to alleviate the contradiction between the more and more heavier neural recommender architectures and the sparse user behaviors. In a nutshell, the main building block of our idea lies in the counterfactual question: â€œwhat would be the userâ€™s feedback if the recommendation list had been diî€erent?â€. More speciî€›cally, we formulate the recommendation task by a causal graph including three nodes (see Figure 1(b)):Urepresents the user proî€›les,Ris the recommendation list, andSindicates the selection of the user fromR. The new training samples are generated by collecting the user feedback S with diî€erent interventions on R. While the counterfactual idea seems to be promising, there are many challenges when applying it to the recommendation domain: to begin with, how to formally deî€›ne the recommendation task by a structure causal model is still unclear. Then, the space of the candidate recommendation lists (R) can be very large, and the samples induced from diî€erent recommendation lists (i.e., intervention) Figure 1: (a) is the general framework of our idea. (b) is the structure causal model of the recommender simulator. (c) and (d) are two diî€erent intervention metho ds. may vary on the eî€ects of optimizing the target model [14,31]. How to design an eî€ective method to selectRremains to be an open problem. At last, the predeî€›ned structural equation models can be not perfect. How the prediction error inî€uences the quality of the generated samples and how to lower the negative impact need to be carefully considered. For solving these challenges, in this paper, we propose a novelcounterfactualpersonalizedranking framework (calledCPR). In general, our framework is composed of two parts (see Figure 1(a)), i.e., the target ranking model and the recommender simulator. The ranking model is leveraged to provide the î€›nal recommendation list, and the recommender simulator aims to assist the optimization of the ranking model by generating additional training samples. When building the recommender simulator, we follow Pearlâ€™s [23] counterfactual framework to deî€›ne the recommendation process, where the structural equation models (SEMs) betweenU,RandSare deî€›ned in a stochastic manner and learned by variational inference to capture the randomness in the recommender system. In order to handle the extremely large space of the recommendation list (i.e.,R), we design a learning-based method to selectRwhich can induce more informative training samples. Considering that the learned SEMs can be not perfect, we theoretically analyze the relation between its prediction error and the number of generated samples. Inspired by this theory, we propose a simple but eî€ective strategy to control the quality of the generated samples. The main contributions of this paper are summarized as follows: (1) We formulate the recommendation problem within Pearlâ€™s causal inference framework, which allows us to generate more training samples for more suî€œcient model optimization. (2) We design a learning-based intervention method, which can lead to more informative training samples for optimizing the target ranking model. (3) We theoretically analyze the relation between the potential prediction error of the structural equation models and the number of generated samples. (4) Inspired by the above theory, we propose a heuristic method to control the quality of the generated samples. (5) We conduct extensive experiments based on both synthetic and real-world datasets to verify the eî€ectiveness of our model. In this section, we î€›rstly recapitulate the key concepts and methodologies in Pearlâ€™s causal inference framework. Then, we brieî€y introduce the ranking-based recommender models. Pearlâ€™s causal inference framework holds the promise of providing a complete and self-contained tool for studying causalities under both experimental and observational settings [25]. It is famous for the proposed three layer causal hierarchy, i.e., association, intervention and counterfactual. In Pearlâ€™s causal inference framework, the î€›rst key concept is the structure causal model (SCM), which helps to formulate real-world problems with causal languages. Deî€›nition 1(Structure Causal Model [23]).A structural causal modelğ‘´is composed of a causal graphğ‘®and a set of structure equation models (SEMs)ğ‘­.ğ‘®is usually a directed graph, where the edges indicate the causal relations between diî€erent variables. The nodes inğ‘®are classiî€›ed into two groups: (i)exogenous nodes ğ‘¼ = {ğ‘¢, ...,ğ‘¢}, which are independent with each other, and summarize the environment when the data was generated, and (ii)endogenous nodes ğ‘¿ = {ğ‘¥, ..., ğ‘¥}, which corresponds the variables we need to model in the problem.ğ‘­ = {ğ‘“}basically parameterizes the node relations, that is,ğ‘¥= ğ‘“(ğ‘ƒğ´, ğ‘¢), where ğ‘ƒğ´is the parent of ğ‘¥in ğ‘®. The structure causal model builds the basis of studying the three layer causal hierarchy. In this paper, we mainly focus on counterfactual estimation. In general, this tool aims to predict the outcome if several input variables in the causal graph had been diî€erent. We brieî€y introduce the estimation process in the following deî€›nition: Deî€›nition 2(Counterfactual Estimation [23]).Suppose we have two variable setsğ’€, ğ’ âŠ‚ ğ‘¿, and we use small lettersğ’šandğ’›to denote their instantiations. The notationğ‘ (ğ’š(ğ’›))deî€›nes the distribution ofğ‘Œifğ’had been set asğ’›in structural causal model ğ‘€given thatğ’is currently observed asğ’›. This distribution corresponds the counterfactual question â€œwhat would beğ‘Œifğ’had been set asğ’›given its current observationğ’›?â€, which is typically derived according to the following three steps: (i)Abduction:deriving the posterior of the exogenous variablesğ‘ (ğ‘¼ |ğ’ = ğ‘§)based on the priorğ‘ (ğ‘¼ )and the observationğ’ = ğ‘§. (ii)Action:modifyingğºby removing the edges going intoğ’and setğ’ = ğ‘§(called intervention) to deriveğ‘ (ğ‘¦|ğ’ = ğ‘§, ğ‘¼ ). (iii)Prediction:computingâˆ« the distribution ğ‘ƒ (ğ‘¦(ğ‘§)) byğ‘ (ğ‘¦|ğ’ = ğ‘§, ğ‘¼ )ğ‘ (ğ‘¼ |ğ’ = ğ‘§)dğ‘¼ . The key motivation of the above â€œabduction-action-predictionâ€ procedure is to make sure that the new (counterfactual) and observed data enjoy the same generation environment, so that they can be compatible and just like to be produced at the same time [25]. Since Pearlâ€™s causal inference framework is not the focus of this paper, we refer the readers to [23] for more details. Based on the tool of counterfactual estimation, one can generate additional training samples to assist the downstream tasks, where a remaining problem is how to indicate ğ‘§ to obtain eî€ective samples. Ranking-based recommender models are powerful tools for solving the Top-N recommendation task, which can be optimized in either pair-wise or point-wise manners. In the pair-wise method, there are usually three inputs, i.e., a userğ‘¢and a positive-negative item pair (ğ‘–, ğ‘—). The goal is to maximize the user preference margin between the positive and negative items. Suppose the user preference is estimated from a function ğ‘“ (Â·), then the optimization target is: whereğœ (Â·)is the sigmoid function to avoid trivial solutions.ğ‘‚ = {(ğ‘¢, ğ‘–, ğ‘—)|ğ‘– âˆˆ I, ğ‘— âˆˆ I \ I}is the set of training samples.I is the whole item set, andIindicates the set of items which have receivedğ‘¢â€™s positive feedback. For the point-wise method, the input is a user-item pair(ğ‘¢, ğ‘–), and the user preference estimation is treated as a classiî€›cation problem, whereğ‘“is optimized by the following cross entropy objective: ğ¿(ğ‘‚) = âˆ’log ğ‘“ (ğ‘¢, ğ‘–) âˆ’(1 âˆ’ log(ğ‘“ (ğ‘¢, ğ‘–))),(2) whereğ‘‚= {(ğ‘¢, ğ‘–)|ğ‘– âˆˆ I}andğ‘‚= {(ğ‘¢, ğ‘–)|ğ‘– âˆˆ I \ I}are the sets of positive and negative samples.ğ‘‚ = ğ‘‚âˆªğ‘‚denotes the complete training set. Ranking-based recommender models diî€erentiate themselves by the various implementation ofğ‘“. Simpleğ‘“can be realized by matrix factorization [26], and more advancedğ‘“includes multi-layer perception [18,21], attentive neural network [17] and graph convolutional network [16], among others. Our framework is composed of two parts (see Figure 1(a)): one is the recommender simulator, which is responsible for generating new training samples. The other is the target ranking model, which is learned based on both of the observed and generated data and leveraged to provide the î€›nal recommendation list. Our framework can be applied to any ranking-based recommender models, and we detail the recommender simulator in the following sections. To begin with, we reformulate the recommendation problem by a structure causal modelğ‘´ = {ğ‘®, ğ‘­ }. In speciî€›c, the causal graphğ‘® is deî€›ned as follows (see Figure 1(b)): (1)U,RandSare the nodes representing the user, the recommendation list and the positive itemsselected by the user. (2)U â†’ Rencodes the fact that the recommendation list is generated according to the user preference. (3)U â†’ SandR â†’ Sindicate that the positive items are jointly determined by the recommendation list and user preference. The structure equation modelsğ‘­are deî€›ned in a stochastic manner as follows:ï£± whereğ‘¼,ğ‘¹andğ‘ºare endogenous variables.ğœ¶ âˆˆ Randğœ· âˆˆ R are exogenous variables.ğ‘is the probability of recommending item list ğ‘¹. ğ‘is the probability of selecting ğ‘º as the positive feedback. N (0, ğ‘° ) is the standard Gaussian distribution. Remark. (1) In order to consider the potential noisy information and randomness in the recommender system, the structure equation models (i.e.,ğ‘­) are deî€›ned in a stochastic manner, which helps to learn user preference in a more accurate and robust manner. (2) The exogenous variables in the recommendation problem can be explained as the conditions (e.g., system status, user habit, etc.) which induces the currently observed data. Take movie recommendation as an example, the users may be more likely to watch movies in their leisure time. So if the data is collected at night, then the movie click probability should be higher. Such temporal condition is latent but can inî€uence the data generation process, which is expected to be captured by the exogenous variables, and recovered by inferring the corresponding posteriors. 3.1.1 Specification ofğ‘­. We implementğ‘­by considering the unique characters of the recommender system. Recall thatğ‘is the probability of recommending item listğ‘¹. Suppose we have|I|items in the system, then the numberof candidate item lists isğ¶, which is extremely large in the recommendation task. To alleviate this problem, we ignore the combinatorial eî€ect between diî€erent items, and the probability of recommending an item list can be factorized into the multiplication of the single item recommendation probabilities, that is: ğ‘(ğ‘¹ = ğ’“ |ğ‘¼ = ğ‘¢, ğœ¶ ) =ğ‘ (ğ‘…= ğ‘Ÿ|ğ‘¼ = ğ‘¢, ğœ¶ ) whereğ’“ = {ğ‘Ÿ, ğ‘Ÿ, ..., ğ‘Ÿ}is the recommendation list. For the single item recommendation probability, we predict it by a softmax operator. We useğ‘· âˆˆ Randğ‘¸ âˆˆ Rto represent the embeddings of the users and items. They can be initialized by the ID or proî€›le information of the users and items.ğ‘‘is the embedding size, and ğ’˜âˆˆ Ris a weighting parameter. ğ‘represents the probability of selecting item setğ‘ºfromğ‘¹, which is speciî€›ed as: ğ‘(ğ‘º = ğ’”|ğ‘ˆ = ğ‘¢, ğ‘¹ = ğ’“, ğœ·) =exp (ğ‘¿ğ’€+ ğ‘¤ğ›½)Ã(5) whereğ’” = {ğ‘ , ğ‘ , ..., ğ‘ }is the set of selected items.ğ’˜âˆˆ Ris a weighting parameter. The user and item properties are encoded by the metrics ofğ‘¿ âˆˆ Randğ’€ âˆˆ R, respectively, andğ‘‘ is the embedding size. Inğ‘andğ‘, we represent the users and items with diî€erent embedding metrics. This can more î€exibly characterize the system impression model (i.e.,U â†’ Rin Figure 1(b)) and the user response model (i.e.,U â†’ Sin Figure 1(b)), which usually hold diî€erent promises in real recommender systems. 3.1.2 Learningğ¹. Suppose the training set isO = {(ğ‘¢, ğ’“, ğ’”)}, whereğ‘¢is the target user,ğ’“is the recommendation list,ğ’”is the item set selected byğ‘¢fromğ’“, N is the sample number. Forğ‘, since the number of items (i.e.,|I|) can be very large, it is hard to directly optimize the softmax term in equation (4). Thus we resort to the negative sampling technique [18,26], which induces the following optimization target: ğ¿=log (ğœ (ğ‘·ğ‘¸+ğ‘¤ğ›¼)) +log (1âˆ’ğœ (ğ‘·ğ‘¸+ğ‘¤ğ›¼)) where{ğ‘·, ğ‘¸, ğ’˜}are the parameters to be learned.ğ’“is the set of negative samples, which are randomly sampled from the nonrecommended items.ğœ¶ = [ğ›¼]is sampledfromN (0, ğ‘° ). With this objective, the parameters are optimized to maximize the recommended item probability and simultaneously minimize the ones which are not presented to the users. Forğ‘, since the length of the recommendation list (i.e., K) is usually not large, we directly maximize the following livelihood without approximation: where{ğ‘¿, ğ’€,ğ’˜}are the parameters to be optimized.ğ‘ andğ‘Ÿ are theğ‘¡th andğ‘—th items inğ’”andğ’“, respectively.ğœ· = [ğ›½]is sampled fromN (0, ğ‘° ). It seems that this objective only maximizes the probability of the items which are selected by the user, but with the softmax operator, the probabilities of the non-selected items are automatically lowered. 3.1.3 Counterfactual estimation based onğ¹. Once we have learned ğ‘andğ‘, we follow Pearlâ€™s â€œabduction-action-predictionâ€ procedure [24] to conduct counterfactual estimation. In the step of abduction, we estimate the posterior ofğœ¶andğœ·given the observed datasetO. Forğœ·, the posterior can be derived based on the following Bayesian rules: whereğ‘ (O|ğœ·)can be easily derived based on equation (5). Recall that our goal is to sample from the posteriors. However, the result of equation (8) is too complex for sampling, which motivates us to use variational inference [3] for approximation. In speciî€›c, we î€›rstly deî€›ne a Gaussian distributionğ‘(ğœ·) âˆ¼ N (ğ, ğˆ), whereğ“ = {ğ, ğˆ } is set of learnable parameters. Then we optimize ğ“ by minimizing the KL-divergence betweenğ‘(ğœ·)andğ‘ (ğœ· |O), where the evidence lower bound (ELBO) [3] we need to maximize is: ELBO = E[log ğ‘ (ğœ·, O)] âˆ’ E[log ğ‘(ğœ·)](9) Similarly, we can learn a variational distribution forğ‘ (ğœ¶ |O). Due to the space limitation, readers are referred to [3] for more technical details of the variational inference. In the action step, we select a userË†ğ‘¢, and setğ‘¹ =Ë†ğ’“. When making prediction, we î€›rstly sampleË†ğœ·fromğ‘(ğœ·), and then compute the probability of itemË†ğ‘Ÿby: At last,Ë†ğ’”is predicted by collecting M items with the highest probabilities. Based on the above designed recommender simulator, one can attempt diî€erentË†ğ’“â€™s and derive the correspondingË†ğ’”â€™s to form new (counterfactual) training samples. A nature question is how to set Ë†ğ’“. Straightforwardly, one can exploreË†ğ’“in a random manner (e.g., show the users with random recommendation lists). However, as mentioned before, the space ofË†ğ’“can be extremely large, and it is well known that diî€erent training samples are not equally important for model optimization [31], thus the random method can be less eî€ective in hitting the optimal samples. In order to achieve better optimization results, we design a learning-based method to selectË†ğ’“. Our key idea is to make the generated samples more informative for the target ranking model. It has been studied in the previous work [12,13] that the samples with larger loss can usually provide more knowledge for the model to learn (i.e., more informative). They can well challenge the model and bring more inspirations to improve the performance. Following these studies, we use the loss of the target ranking model as the reward, and build a learning-based method to generateË†ğ’“. Formally, suppose the target ranking model isğ‘“, and we denote its loss function asğ¿, which can be speciî€›ed as equation (1) or (2). The goal of the agent is to conduct actions (generating recommendation lists) which can lead to largerğ¿. Considering that the action space can be very large, we follow the previous work [40] to learn a Gaussian policy to generate the continuous item center ofË†ğ’“, after which the discrete item IDs are recovered based on equation (4). The î€›nal learning objective is: whereğœ‹is the Gaussian policy implemented as a two-layer fully connected neural network with ReLU as the activation function.Ë†ğ‘¢ indicates the target user.ğ¿(ğ¶(Ë†ğ‰))denotes the loss of the generated training samplesğ¶ (Ë†ğ‰). We generate T sets of training samples, and each one is derived fromË†ğ‰based on the following steps: â€¢DerivingË†ğ’“by selecting K items near the item centerË†ğ‰according to the scores {(Ë†ğ‰ğ‘¸+ ğ’˜ğ›¼)}. â€¢DerivingË†ğ’”by selecting M items with the largest probabilities of ğ‘(Ë†ğ‘Ÿ|ğ‘ˆ =Ë†ğ‘¢, ğ‘¹ =Ë†ğ’“,Ë†ğœ·) (ğ‘˜ âˆˆ [1, ğ¾]) . â€¢For objective (1),ğ¶ (Ë†ğ‰) = {(Ë†ğ‘¢, ğ‘–, ğ‘—)|ğ‘– âˆˆË†ğ’”, ğ‘— âˆˆË†ğ’“\Ë†ğ’”}. For objective (2), ğ¶ (Ë†ğ‰) = {(Ë†ğ‘¢, ğ‘–) |ğ‘– âˆˆË†ğ’”} âˆª {(Ë†ğ‘¢, ğ‘–) |ğ‘– âˆˆË†ğ’“ \Ë†ğ’”}. We summarize the complete learning process in Algorithm 1 and 2. In speciî€›c, the target ranking model is î€›rstly trained based on the original dataset. And then, we generate many counterfactual training samples based on the Gaussian policy. At last, the target ranking model is retrained based on the generated datasets. In this section, we theoretically analyze the proposed framework within the probably approximately correct (PAC) learning framework. We focus on the pair-wise learning objective, and the conclusions can be easily extended to the point-wise case. 3.3.1 Theoretical insights on the learning-based intervention method. The key motivation of the learning-based method is to generate harder samples, so that the target model can be more informed and Algorithm 2: Learning algorithm of ğœ‹ achieve better performance. The eî€ectiveness of this idea has been empirically demonstrated in the previous work [13,31]. Here, we provide a theoretical justiî€›cation based on the following theory: Theorem 1.Suppose the usersâ€™ feedback on an item pair(ğ‘–, ğ‘—)is probabilistic, and we can observeğ‘– > ğ‘—andğ‘– < ğ‘—with the probabilities ofğœ‚and 1âˆ’ğœ‚, respectively. Then,ğœ‚can actually measure the hardness of the sample, whenğœ‚is closer to, then the sample is harder, since the propensity between the items is more ambiguous. Supposeğœ–, ğ›¿ âˆˆ (0,1), and we use a simple voting mechanism to determine the relation betweenğ‘–andğ‘—, then we need to havesamples on(ğ‘–, ğ‘—)to ensure that the prediction error is smaller that ğ›¿. This theory suggests that we need to generate more samples (i.e., larger) for the harder (i.e., smallerğœ‚) item pairs. It agrees with our goal in learning-based intervention method, where the produced samples are expected to be hard and can well challenge the target model. We present the proof of this theory in the Appendix. 3.3.2 Imperfection of SEMs. Since the new samples are generated based on the learned SEMs, careful readers may have concerns on how would the approximation error ofğ‘­inî€uence the sample generation. To shed lights on this problem, we theoretically analyze the relation between the prediction error ofğ‘­and the number of generated samples. To begin with, we assume that the recommender simulator can recover the true ranking of the item pairs with a noisy parameterğœ âˆˆ (0,0.5), i.e., suppose the true triplet is (ğ‘¢, ğ‘–, ğ‘—), then the recommender simulator generates the true (i.e., (ğ‘¢, ğ‘–, ğ‘—)) and wrong (i.e.,(ğ‘¢, ğ‘—, ğ‘–)) samples with the probabilities of 1âˆ’ ğœandğœ, respectively. As the special cases,ğœ =0 means the recommender simulator is perfect, and there is no noisy information in the produced samples.ğœ =0.5 means the recommender simulator is a totally random predictor, and the generated data is nothing but noise. Then, we have the following theory: Theorem 2.Supposeğœ–, ğ›¿ âˆˆ (0,1), andğ‘“ âˆˆ Fis the target ranking model. If the number of training samples is larger than, then the estimation error ofğ‘“in terms of ranking prediction is smaller than ğœ– with the probability larger than 1 âˆ’ ğ›¿. The proof of this theory is similar to that of theory 1 in [34]. From this theory, we can see, as the noisy parameterğœbecomes larger, more samples (i.e.,) are needed to achieve suî€œciently well performance (the prediction error is smaller thanğœ–). Extremely, whenğœ â†’0.5, we need to produce inî€›nite training samples. This implies that if the recommender simulator is completely random, then we can not expect well performance by training on the generated data, which is aligned with the intuition. 3.3.3 Controlling the noisy information. Inspired by this theory, we propose a simple but eî€ective method to control the noisy information. The general idea is to remain the samples with higher conî€›dence. In speciî€›c, for a given recommendation listË†ğ’“, we use the selection probabilityğ‘(Ë†ğ‘Ÿ|ğ‘ˆ =Ë†ğ‘¢, ğ‘¹ =Ë†ğ’“,Ë†ğœ·)to measure the conî€›dence of the recommender simulator. We denote byË†ğ’”and Ë†ğ’”the sets of k items with the largest and smallest selection probabilities. Then the training set for the pair-wise objective is built asğ¶ (Ë†ğ‰) = {(Ë†ğ‘¢, ğ‘–, ğ‘—)|ğ‘– âˆˆË†ğ’”, ğ‘— âˆˆË†ğ’”}. For the point-wise objective, ğ¶ (Ë†ğ‰)is set as{(Ë†ğ‘¢, ğ‘–) |ğ‘– âˆˆË†ğ’”} âˆª {(Ë†ğ‘¢, ğ‘–) |ğ‘– âˆˆË†ğ’”}. In this method, if ğ‘˜is small, then the model has more conî€›dence on the generated samples, which may reduce the noisy information, but at the same time, less samples can be generated, which may lead to insuî€œcient model optimization. Ifğ‘˜is large, more samples will be generated, but the noisy level can also be high. In this sense,ğ‘˜is actually a parameter to trade-oî€ the noisy information and the number of generated samples. In this section, we conduct extensive experiments to verify the eî€ectiveness of our framework. In the following, we begin with the experiment setup, and then report and analyze the results. 4.1.1 Datasets. Our experiments are based on both synthetic and real-world datasets. With the synthetic dataset, we aim to evaluate our framework in a controlled manner under clean environment. By the real-world dataset, we try to verify our frameworkâ€™s eî€ectiveness in real-world settings. We follow the previous work [42] to build the synthetic dataset, where we simulateğ‘(=600)users andğ‘(=300)items. For each userğ‘–(or itemğ‘—), the preferencesğ’‘âˆˆ R(or properties ğ’’âˆˆ R) are generated from a multi-variable Gaussian distribution N (0, ğ‘° ), whereğ‘‘andğ‘°represent the vector size and unit matrix, respectively. In order to generate the recommendation list for user ğ‘–, we î€›rstly compute the score of recommending itemğ‘—based on a neural network:ğ‘Ÿ=1âˆ’ğœ (ğ’‚ğœ…(ğœ…([ğ’‘, ğ’’])) +ğ‘), whereğ’‚ âˆˆ R is speciî€›ed as an all-one vector, andğ‘is set as zero. We follow [42] to setğœ…(Â·)andğœ…(Â·)as piecewise functions, andğœ…(ğ‘¥) = ğ‘¥ âˆ’0.5 ifğ‘¥ >0, otherwiseğœ…(ğ‘¥) =0,ğœ…(ğ‘¥) = ğ‘¥ifğ‘¥ >0, otherwise ğœ…(ğ‘¥) =0. Givenğ‘Ÿ, the probability of recommending itemğ‘—is . For each user, we generate 25 impression lists, each of which is composed of 5 items. When generating the feedback of a userğ‘–on an itemğ‘—, we follow the previous work [42] to explore both linear and non-linear user response models, that is: whereğœ…(ğ‘¥) = ğ‘¥ +0.5 ifğ‘¥ <0, otherwiseğœ…(ğ‘¥) =0.ğ‘is used to set the noisy level of the datasets, and its default value is 0.I(ğ‘¥)is an indicator function, which is 1 if ğ‘¥ > 0, and 0 otherwise. The real-world experiments are based on the recently released MINDdataset [35]. This data is collected from the user behavior log of Microsoft News, and we uses the MIND-small dataset for experiments, where we are provided with 156,965 recommendation lists and 234,468 interactions between 50,000 users and 20,288 items. 4.1.2 Baselines. In order to evaluate the eî€ectiveness of our proposed framework, we compare our model with the following representative methods:ItemPopis a non-personalized model, and the ranking score of an item is based on its popularity in the training set.ItemKNN[30] is an item-based k-nearest-neighborhood (KNN) model.BPR[26] is a well-known recommender model based user implicit feedback.GMF,MLPandNeuMF[18] are neural recommender models, among which NeuMF is a combination between GMF and MLP.CDAE[36] is a method based on denoising auto-encoder, which is proved to be the generalization of several well-known collaborative î€›ltering models.LightGCN[16] is a graph-based recommender model, where the user-item structure information is considered in the modeling process. In order to verify the generality of our framework, we apply it to MF, GMF, MLP, NeuMF and LightGCN, respectively, which leads to the models ofCPR-MF,CPR-GMF,CPR-MLP,CPR-NeuMF andCPR-LightGCN. To demonstrate the necessity of learningbased intervention method, we also evaluate the performance our framework with random intervention, where the recommendation listğ‘¹is assigned in a random manner. We denote the corresponding baselines asCPR-MF-r,CPR-GMF-r,CPR-MLP-r, CPR-NeuMF-r and CPR-LightGCN-r. 4.1.3 Implementation details. We follow the previous work [17, 18,26] to employ standard leave-one-out protocol for evaluation. 10 items are recommended from each model to compare with the usersâ€™ actually interacted ones. We leverage the widely used metrics including Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) to compare diî€erent models. Among these metrics, HR aims to measure the overlapping between the predicted and ground truth items, while NDCG further takes the ranking of the prediction results into consideration, and higher ranked accurate items contribute more to the î€›nal score. The implementations of our model and all the baselines are based on PyTorch [22] with mini-batch Adam [19] optimizer. For ItemKNN and CDAE, we follow the settings of the original paper. For the other baselines, we determine their hyper-parameters based on grid search, and the search ranges for the embedding size, batch size, regularization coeî€œcient and learning rate are set as{32,64,128,256,512}, negative samples are selected from the whole item set or the impression list. When optimizingğ‘andğ‘, we empirically set the learning rate as 0.001, and the user/item embedding sizes are both tunned in{16,32,64,128,256,512}. The length of the impression list (i.e.,|ğ‘¹|) is set as 5, and the size ofğ‘º(i.e.,ğ‘˜) is determined in {1,2,3}. The Gaussian policy is implemented as a two-layers fullyconnected neural network, where the hidden dimension is searched in {16, 32, 64}. In this subsection, we present and analyze the experimental results on the synthetic dataset. 4.2.1 Overall performance. In the experiments, we explore diî€erent user/item representation dimensions as well as both linear and non-linear user response models. The overall comparison results are shown in Table 1. It is interesting to see that the relative simple baselines (e.g., ItemPop and ItemKNN) are competitive in many cases. This observation is consistent with the previous work [9], and suggests that simple methods may still be useful in some recommendation scenarios. It is encouraging to see that our framework can consistently improve the performance of diî€erent target models. The improvement is consistent across diî€erent settings. In speciî€›c, our framework can on average improve the performance of the target model by about 10.08% and 11.17% on HR and NDCG, respectively. This result demonstrates the eî€ectiveness of our framework. The reason can be that the recommender simulator enables us to explore the potential user preferences, which provides useful signals to widen the model visions and improve the performance. The importance of the learning-based intervention method is evidenced in the lowered performance when we use random strategy as a replacement, which veriî€›es our claims in the introduction. It is interesting to see that the performance gains on neural models (e.g, MLP, NeuMF and LightGCN) are usually larger than that of the matrix factorization method. In many cases, the performance is even lowered for GMF and BPR after applying our framework, e.g., when we use nonlinear user response model and setğ‘‘ =32. This observation agrees with our expectation, that is, neural models need more training samples to display its strong expressiveness, and our framework should be more eî€ective for them. Since most of the promising recommender models in practice are based on neural architectures, this observation demonstrates the potential of our framework in real-world settings. Table 1: Performance comparison based on the synthetic dataset. We present the relative improvements of our framework over the target model in the parentheses. Table 2: Eî€ects of the noisy control parameter ğ‘˜, the best performance for each method and setting are labeled by bold fonts. 4.2.2 Eî€›ects of the noisy control parameterğ‘˜. In this section, we investigate the eî€ectiveness of the noisy control method proposed in section 3.3. By this method, we hope our framework can adaptively tune itself to accommodate diî€erent noisy recommendation scenarios. In order to evaluate such capability, we setğ‘=0 or sample it fromN (0, 0.2)to simulate the settings where the data is observed with diî€erent noisy levels. We base the experiment on the nonlinear user response model, and we tuneğ‘˜in the range of{1,2,3}. The results are presented in table 2. It is not surprising to see that the performance of the same model is lowered when the noisy level is increased from 0 to sampling fromN (0, 0.2). Whenğ‘is sampled fromN (0, 0.2), the best performance is mostly achieved when ğ‘˜ =1, while if we setğ‘=0,ğ‘˜ =2 orğ‘˜ =3 can usually lead to more favored results. We speculate that: when the dataset is noisy, the recommender simulator can be not trained well, thus the quality of the generated samples are not high. In such a scenario, removing the noisy samples seems to be more important. On the contrary, if the dataset is noise-free, then the recommender simulator can be more accurate. In this case, the sample quality is not the main issue, and more samples are favored to achieve suî€œcient model optimization and better performance. For diî€erent noisy-level datasets, our Table 3: Overall results on the real-world dataset. framework can always adapt itself to achieve better performance, which demonstrates the eî€ectiveness of the noisy control method. The above synthetic experiments suggest that our idea is eî€ective under ideal environment. In this section, we experiment with the real-world dataset, which is more challenging but more practical. 4.3.1 Overall comparison. To begin with, we compare our model with the baselines introduced in section 4.1.2. The results are presented in Table 3, from which we can see: similar to the synthetic dataset, our CPR framework can consistently lead to improved performances comparing with the target models, and the learningbased intervention method outperforms the random strategy in most cases. The performance gains on neural models are usually larger than that of the shallow ones, which again demonstrates the potential of our model for deep recommender models. Remarkably, the improvement of our framework is not that large comparing with the results on the synthetic dataset. We speculate that the real-world dataset can be noisy. Training based on it can lead to imperfect recommender simulator, which lowers the quality of the generated samples and limits the î€›nal recommendation performance. 4.3.2 Performance on the items with diî€›erent coldness. Cold start has long been a fundamental problem in the recommendation domain. In this section, we study the eî€ectiveness of our framework on the items with diî€erent coldnesses. More speciî€›cally, we î€›rst run the models based on the original training set, and then evaluate their performances on three diî€erent testing sets varying on the item coldness. For the î€›rst testing set (denoted as â€œLowâ€), each item has less than 5 interactions with the users in the training set, which means the model is trained in a quite insuî€œcient manner on these items. In the second testing set (denoted as â€œMiddleâ€), each item has 5 to 15 interactions, thus the model is provided with more knowledge during the training process. In the last testing set (denoted as â€œHighâ€), each item has more than 15 interactions, which is the â€œwarmestâ€ setting in the experiment. The model parameters are set as their optimal values tunned in the above experiments. From Table 4 we can see: our framework can enhance the performance of the target models in most cases, which demonstrates its eî€ectiveness under diî€erent cold start settings. An interesting observation is that the performance improvement on the â€œLowâ€ and â€œMiddleâ€ testing sets are much larger than that of the â€œHighâ€ dataset. For example, CPR-LightGCN can produce a remarkable 151.74% and 162.75% improvements over LightGCN on HR@10 and NDCG@10 for the â€œLowâ€ dataset, and the improvements on the â€œMiddleâ€ dataset is about 40%-50%. However, on the â€œHighâ€ dataset, the performance is even lowered on the metric of HR@10, and the improvement on NDCG@10 is also limited. We speculate that, in the â€œLowâ€ dataset, the testing items are trained in a quite insufî€›cient manner. A large amount of knowledge has been ignored by the target model, which provides more opportunities for the generated samples to introduce useful signals and improve the performance. In real-world recommendation scenarios, cold-start is a notorious problem obsessing people for a long time, this experiment demonstrates the potential of our framework in alleviating this problem. In this section, we compare our model with the previous work to highlight the contributions of this paper. Relation with causal recommendation.Recent years have witnessed many contributions on incorporating causal inference into the recommendation domain [5,11,27,28,37]. For example, [28] explains the recommendation problem by a treatment-eî€ect model, and designs a re-weighting method to remove the bias in the observed data. However, this method is based on user explicit feedback, and the loss function is restricted to root mean square error (RMSE). In order to handle user implicit feedback, [27,37] extend this method by incorporating cross-entropy loss and designing tailored debiasing models. In addition, [20] proposes a general knowledge distillation framework to debias the training data. [5] provides a thorough discussion on the recent progress on debiased recommendation. [4] leverages uniform data to learn causal user/item embeddings for more fair and unbiased recommendation. These methods have achieved many successes in the recommendation domain. However, they mostly leverage causal inference to debias the training data, which is diî€erent from our data augmentation purpose. In addition, previous work mostly follow Rubinâ€™s potential outcome framework. However, our idea is inspired from Pearlâ€™s structure causal models, where we can explicitly model the data generated environment to ensure compatibleness between the generated and observed data. Relation with counterfactual data augmentation.Counterfactual data augmentation stems from the human introspection behavior. It has been recently leveraged to alleviate the training data insuî€œciency problem in the machine learning community. In the past few years, this idea has achieved great successes in the î€›elds of neural language processing (NLP) [41] and computer vision (CV) [2,6,12]. In this paper, we apply it to the top-N recommendation task, which, to the best of our knowledge, is the î€›rst time in this î€›eld. The major diî€erences between our framework and the previous work is: (i) we design a leaning-based intervention method to encourage the informativeness of the generated samples. (ii) We theoretically analyze the noisy information in the generated samples, and design a tailored noise control method. Relation with ranking based recommendation model.Ranking based recommender models have been widely studied for the Top-N recommendation task. The most famous model in early years is Bayesian personalized ranking (BPR), which is optimized by maximizing the user preference margin between the positive and negative items. This method has inspired many promising models. For example, CKE [38] proposes a hybrid model to integrate collaborative î€›ltering and knowledge base for recommendation. [10] improves BPR with a better negative sampler which leverages additional data in E-commerce. AMF [17] applies adversarial training method to enhance the performance of BPR. With the ever prospering of deep neural network, recent years have witnessed the surge of neural recommender models [7,8,15,18,32]. For example, NeuMF [18] is designed to capture the non-linear user-item correlations. NGCF [33] and LightGCN [16] regard the user-item interactions as a graph, and explicitly incorporate the structure information to enhance the utilization of the collaborative î€›ltering signals. By looking back the history of ranking based recommendation models, it is evident that the model architectures are becoming deeper and heavier. While the comprehensive parameters can indeed lead to improved recommendation performance, an unprecedented problem is also emerging, that is, more training data is needed to satisfy the heavy architectures, which contradicts with the sparse user behaviors in realities. In this paper, we propose a solution to this problem based on causal inference, which is parallel with the previous model-drive research. In this paper, we propose to enhance Top-N recommendation with Pearlâ€™s causal inference framework, where we can simulate user preference and generate counterfactual samples for alleviating the training data insuî€œciency problem. We design a learning-based intervention method for discovering the informative samples, and conduct theoretical analysis to reveal the relation between the number of generated samples and the potential model prediction error. Extensive experiments based on both synthetic and real-world datasets are conducted to verify our modelâ€™s eî€ectiveness. This paper actually opens the door of incorporating Pearlâ€™s causal inference framework into the recommendation domain. There is still much room for improvement. For example, one can design more advanced exogenous node structures to introduce more reasonable prior knowledge for diî€erent recommendation scenarios. In addition, it should be also interesting to incorporate side information into the structure equation models, which can help to capture more comprehensive user preference and obtain more accurate recommender simulators. 7.1.1 Proof of theory 1. Proof.Without loss of generality, we supposeğœ‚ >. We deî€›ne the random variableğ‘‹as theğ‘–th observation of the relation betweenğ‘–andğ‘—. We useğ‘‹=1 to representğ‘– > ğ‘—, andğ‘‹=0 to indicateğ‘– < ğ‘—. Suppose, we have N observations, and we deî€›neÃ ğ‘‹ =ğ‘‹. Following the voting mechanism, ifğ‘‹ <, then the prediction is wrong, since it contradicts withğœ‚ >. Obviously, According to the Hoeî€dingâ€™s inequality [29], we have: ğ‘ (ğ‘‹ <12) = ğ‘ (ğ‘‹ âˆ’ ğ¸ [ğ‘‹ ] <12âˆ’ ğœ‚) < exp (âˆ’2ğ‘ (12âˆ’ ğœ‚)). (14) 7.1.2 Proof of theory 2. Proof.For a hypothesisğ‘“ âˆˆ F, suppose its prediction error isğ‘ Ã (i.e.,I(ğ‘”â‰  ğ‘”) = ğ‘ ), then the mis-matching probability between the observed and predicted results comes from two parts: â€¢The observed result is true, but the prediction is wrong, that is, ğ‘  (1 âˆ’ ğœ ). â€¢The observed result is wrong, but the prediction is right, that is (1 âˆ’ ğ‘ )ğœ . Thus, the total mis-matching probability isğœ +ğ‘  (1âˆ’2ğœ ). Suppose the prediction error ofğ‘“(i.e.,ğ‘ ) is larger thanğœ–, Then, at least one of the following statements hold: â€¢The empirical mis-matching rate ofğ‘“is smaller thanğœ + â€¢The empirical mis-matching rate of the optimalğ‘“is larger than ğœ +. These statements are easy to understand, since if both of them do not hold, we can conclude that the empirical loss ofğ‘“is larger than that ofğ‘“, which does not agree with the ERM deî€›nition. However, according to the Hoî€ding inequality [29], both of the above statements hold with the probability smaller thanğ›¿, which implies that the prediction error ofğ‘“is smaller thanğœ–with the