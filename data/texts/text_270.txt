For a multilingual podcast streaming service, it is critical to be able to deliver relevant content to all users independent of language. Podcast content relevance is conventionally determined using various metadata sources. However, with the increasing quality of speech recognition in many languages, utilizing automatic transcriptions to provide better content recommendations becomes possible. In this work, we explore the robustness of a Latent Dirichlet Allocation topic model when applied to transcripts created by an automatic speech recognition engine. Speciî€›cally, we explore how increasing transcription noise inî€uences topics obtained from transcriptions in Danish; a low resource language. First, we observe a baseline of cosine similarity scores between topic vectors from automatic transcriptions and the descriptions of the podcasts written by the podcast creators. We then observe how the cosine similarities between topic vectors decrease as transcription noise increases and conclude that even when automatic speech recognition transcripts are erroneous, it is still possible to obtain high-quality topic vectors. CCS Concepts: ing. Additional Key Words and Phrases: Podcasts, Automatic Speech Recognition, Topic modeling, Recommendation Systems ACM Reference Format: Raluca Alexandra Fetic, Mikkel Jordahn, Lucas Chaves Lima, Rasmus Arpe Fogh EgebÃ¦k, Martin Carsten Nielsen, Benjamin Biering, and Lars Kai Hansen. 2021. Topic Model Robustness to Automatic Speech Recognition Errors in Podcast Transcripts. In RecSys â€™21: ACM Conference Series on Recommender Systems, September 27â€“October 05, 2021, Amsterdam, NL. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/1122445.1122456 Podcasts have become an increasingly popular audio format in recent years. Podcasts encompass a variety of on-demand audio, such as radio, news, and entertainment in the form of informal discussions, interviews, or even narrated content similar to audiobooks, found in several diî€erent categories. Despite the growth in popularity of podcasts, an open challenge is how to î€›nd a new podcast to listen to. Research in Podcast recommendation has not yet been able to follow â€¢ Information systems â†’ Information retrieval;â€¢ Computing methodologies â†’ Natural language processup and provide users with eî€œcient and high-quality recommendations which are key to ensuring a high quality of streaming services [18]. Podcast recommendation is a challenging task considering the very large amount of podcast episodes that lack metadata on both podcast and episode levels. Previous work has shown that transcription-based topic modeling plays a crucial role in podcast recommendation, as users tend to focus on the topics of the podcasts instead of podcast audio style [22, 35]. Topic modeling techniques such as Latent Dirichlet Allocation (LDA) [6,7] and Probabilistic Latent Semantic Indexing (PLSI) [17] are widely used to discover topics over high-quality texts (e.g. news, blog posts, etc.). As podcasts are usually not scripted, a transcript generated from an Automatic Speech Recognition (ASR) system is necessary to perform topic modeling. Podcasts represent a particularly challenging audio format for speech recognition systems as a large number of artifacts are commonly present. Some of the challenges are that multiple speakers and speech overlap, background music and jingles, audio eî€ects, and "real-life" recording conditions (e.g. background noise), which makes the ASR-generated transcripts from podcasts prone to errors. While ASR systems have been widely explored and developed for high-resource languages such as English, there is a signiî€›cant lack in many low-resource languages. As such the systems available in such setups can be expected to produce more errors than their high-resource counterparts. This motivates research for downstream tasks in the low-resource setup to ensure multilingual application. In this paper, we study the robustness of an LDA topic model used on podcast transcriptions generated by an ASR engine in Danish; a low resource language. We utilize podcast episodes with author descriptions and assume that these descriptions contain a good representation of topics. Hence, a high similarity between topic vectors from author descriptions and automated transcripts also indicates a good representation of topics in the automated transcript. To evaluate the robustness of the topic model, we î€›rst construct a baseline by computing a cosine similarity between the topic vector representations of the author descriptions and automated transcripts. We then introduce noise to the automated transcripts, controlled by a variable noise injection rate, determining how often words should be replaced, and observe how the cosine similarity changes. We experiment with two types of noise; simulated ASR noise sampled from a conditional distribution derived from a transcription error dataset (see Section 3.2), and words sampled uniformly from the topic model vocabulary for reference. Empirically, over a dataset of 587 episodes from 24 Danish podcasts, we î€›nd that the topic model is much more robust to simulated ASR noise than it is to noise from a uniform distribution. We present evidence that the LDA topic model is robust and captures an informative representation of topics, even in the face of imperfect transcriptions. The remainder of this paper is structured as follows. In Section 2, we present the necessary background on topic modeling and robustness of downstream NLP systems against ASR errors. Section 3 deî€›nes the problem and details the methods used by each component to test the podcast topic modeling robustness. Section 4 presents the experimental setup. In Section 5, we present and discuss the results. Lastly, in Section 6, we conclude the paper and propose future research directions. 2 BACKGROUND 2.1 Automatic Speech Recognition Converting speech in audio to raw text is done using an Automatic Speech Recognition (ASR) system. Speech recognition systems have drastically improved during recent years with advancements such as various data augmentation techniques [28], pre-training procedures on unlabelled speech data [2] and noisy student training [29]. State of the art performance on the common English benchmark dataset LibriSpeech [ on the test-clean and the test-other partitions, respectively [ accessible in English [ CommonVoice from Mozilla [ quite high. However, transcripts from ASR systems for low resource languages are likely to be error-prone, especially for complex audio data such as podcasts. 2.2 Topic Mo deling and Evaluation Topic models are used to explore and structure a large set of documents according to latent semantic content. To improve downstream tasks such as search and recommendation of podcasts, a promising method is to utilize a topic model to extract the relevant topics of the podcast and hence enhance the podcast representations [ modeling has been extensively studied, and various approaches exist. For instance, Latent Semantic Indexing (LSI) [ uses Singular Value Decomposition (SVD) or Non-negative Matrix Factorization (NMF) [ matrix to construct a latent space representation, which can be queried for comparison of documents. An extension of LSI, Probabilistic Latent Semantic Indexing (PLSI), models topics as distributions over words, and documents as a probabilistic mixture of those topics [ (LDA). LDA diî€ers from PLSI by utilizing prior Dirichlet distributions to model the topic-word distributions making it more robust to unseen data [ the Correlated Topic Model (CTM) [ extensions include Collective LDA [ that explore the inî€uence of the age of the documents in the topics [23, 34]. The quality of a topic model can be evaluated in diî€erent ways. A common practice is to evaluate a trained model in terms of perplexity [ humans judge the quality of topics [ coherence [21], and coherence based on word embeddings [12]. 2.3 Robustness to Noise Downstream NLP tasks on ASR transcripts need to be robust to noise due to the commonality of transcription errors. Robustness to noise is often evaluated by constructing a baseline result with clean text and then injecting varying degrees of noise to the text and examining how the result changes [ necessary to select diî€erent ways of injecting plausible ASR noise into transcripts. A recent study explored the feasibility of improving the robustness of speech-enabled systems with three methods of noise [ randomly substitutes a candidate word with a phonetically similar one, statistic-based confusion substitution which samples replacement words from a pre-constructed ASR confusion matrix and î€›nally, model-based substitution utilizing a generative GPT model to directly produce ASR-like text. Another study investigated the stability of topics over noisy sources, by testing for topic model agreements [ deletion and rule-based phonetic substitution errors [32]. Pre-training speech recognition models with cross-lingual data has helped bridge the gap signiî€›cantly [8]. The robustness of a downstream task varies a lot depending on the speciî€›c task and the type of noise. For instance, topic modeling has previously been shown to be robust to deletion of random words, whereas the insertion of new words and phonetic substitution errors has a larger negative impact on topic stability [32]. Another relevant downstream task, neural machine translation with character-based models, has shown to struggle even with small perturbations to the input data [4]. 3 METHODS 3.1 Transcript Generation We produce podcast transcripts by parsing podcast audio through a danish transcription system developed at the Technical University of Denmark (DTU) as part of the Danspeech project.The system is based on the wav2vec 2.0 framework [3]. The model was pre-trained using approximately 945 hours of podcast episodes and 400 hours of audiobooks, and î€›ne-tuned using the Connectionist Temporal Classiî€›cation [13] (CTC) loss function on 200 hours of labeled data from the Nordisk SprÃ¥kteknologi (NST) danish training datasetand 267 hours of aligned audiobook data. The Fairseq library [26] was used for both the pre-training procedure as well as î€›ne-tuning. During inference, the ASR engine performs preî€›x beam search [16] with an open-source danish 3-gram language modelwhen decoding the probabilities emitted from the wav2vec 2.0 model. 3.2 Noise Injection We inject noise into the ASR transcripts by means of a noise injection rate parameter,ğ›½, that determines the frequency at which we substitute words in the transcript. More speciî€›cally, for each word in a given transcript we independently decide if a substitution should take place with probabilityğ›½. Examples of substitutions at various levels ofğ›½are presented in Table 1. Table 1. Examples of how a transcription changes with automatic speech recognition statistics-based substitutions at varying levels of the noise injection rate parameter ğ›½. 0historien er rig pÃ¥ spÃ¦ndende fortÃ¦llinger om drama krigHistorien er rig pÃ¥ spÃ¦ndende fortÃ¦llinger om drama, krig, voldelig politiske omvÃ¦ltninger og fyldt med mystik hem-voldelige politiske omvÃ¦ltninger og er fyldt med mystik, meligheder og fascinerende menneskeskÃ¦bnerhemmeligheder og fascinerende menneskeskÃ¦bner. {...} When a word is selected for substitution we apply one of two methods for the substitution. The î€›rst method samples a word uniformly at random from the topic model vocabulary, and the second method uses a statistics-based confusion matrix approach (see Section 4.2). When performing statistics-based confusion substitutions, we sample replacement wordsÂ¯ğ‘¤ from a conditional error distribution with the candidate probability given as Here ğ‘Š (Â¯ğ‘¤) denotes the weighting of a candidate word and ğ‘‰ (ğ‘¤) denotes the candidate set for word ğ‘¤. 3.3 Topic Mo deling and Document Vector Representation Similarity To produce topic representations we use a general-purpose danish topic model trained on an external, multi-domain text corpus using LDA. Our motivation for doing so is two-fold; most prominently we choose to rely on an external text corpus because the topic modeling process requires large amounts of textual data, which is something that is not readily available to us in a pure podcast domain setup. Secondly, we choose LDA as the modeling framework due to its inherent probabilistic approach of representing topics, which have been shown to be robust to unseen data. The LDA model can be seen as a probabilistic function vectorğ’… similarity of topics present in a pair of documents, compute the Similarity between the resulting document-level topic vectors as follows, where cos is the cosine similarity. When comparing two ordered sets of document pairs, average similarity between the sets as the CorpusSimilarity (CS) computed as, where ğ‘†(ğ‘–) is document ğ‘‘ 4 EXPERIMENTS 4.1 Podcast Dataset The podcast dataset we use to investigate the robustness of topic modeling consists of 587 episodes from 24 podcasts shows in Danish. The 24 podcasts shows belong to 8 categories, assigned by the content creators, such as â€œCulture & leisureâ€, â€œHealth & personal developmentâ€, â€œHistory & religionâ€ and â€œTrue crime & mysteriesâ€. The podcast shows are all single speaker with a limited amount of audio artifacts such as background music and jingles. We extend the descriptions with the episode title, podcast title, podcast description, and podcast category. We only include episodes with a high-quality author description, deî€›ning high-quality descriptions as any description-transcription pairings that have an initial cosine similarity above 0 presented in Figure 2 in the Appendix, in which it can be seen that the distribution of episodes across the shows is imbalanced, with some podcasts containing the majority of the episodes. We produce ASR transcripts for all the episodes by leveraging the ASR engine described in Section 3.1. 4.2 Statistics Based Automatic Speech Recognition Noise To create the word-level conditional ASR error distributions used for statistics-based confusion substitution (see Section 3.2), we î€›rst construct a dataset containing clean text and ASR transcription pairs. We use the same ASR engine as = [ğ‘, .., ğ‘], whereğ‘represents the probability of each topicğ‘¡for the documentğ‘‘. Thus to compute the described in Section 3.1 with the exception that the wav2vec 2.0 model was only î€›ne-tuned on the training dataset from NST. The data consists of approximately 77 hours of data from the NST test dataset and 267 hours of audiobook data resulting in 229,499 pairs of reference-transcript pairs. The probability of a candidate word for a given word is then obtained by counting the frequency at which the word is wrongfully transcribed as the candidate word, normalized by the number of candidates as shown in Equation(1). Examples of how candidate distributions may look for speciî€›c words are presented in Table 2 and Table 3, where the random candidate is a grouping of many potential errors with very low probability. We can see from the tables that typical ASR errors tend to retain semantic meaning to some degree which gives rise to the hypothesis that topic models are likely to be robust to ASR noise. If an unknown word is encountered then it is simply deleted. Across the transcripts of the podcasts, unknown words occur 20% of the time. Table 2. Candidate distribution for the word "nogensinde".Table 3. Candidate distribution for the word "lavet". 4.3 Topic mo del We train an LDA topic model on a Danish Wikipedia dataset consisting of 264,505 documents from The Danish Gigaword Corpus [11] using the Gensim framework [30]. All documents are preprocessed by î€›rst performing word tokenization and removing punctuation and other special characters, including numbers. Next, we apply Part of speech (POS) î€›ltering, keeping adjectives, nouns and verbs. Finally, we perform lemmatization, lowercase all letters, before î€›nally vectorizing the documents into bag of word (BOW) representations. The BOW vocabulary may contain n-grams and is limited by removing uncommon n-grams that appear in less than ten documents and very common n-grams that occur in more than 90% of the documents. We î€›x the LDA parametersğ›¼andğœ‚asğ›¼ =andğœ‚ =0.1, and choose our remaining hyper-parameters by performing grid search optimizing for topic coherence, using theğ‘ˆ-coherence metric to choose the best model. We tune the following hyper-parameters: Topics[10,20, 30,40,50,60,70,80,90,100], Vocabulary of BOW[Unigrams, Unigrams + Bigrams]and Variational Bayes iterations[5,10,15, 20]. The underlined values are the values that yielded the best model in terms of ğ‘ˆcoherence score. 4.4 Evaluation of Topic Robustness over Noisy Sources For the experiments, we construct a baseline by computing the CS between topic vectors of two document sets(ğ‘†, ğ‘†) as described in Section 3.3. We then measure the robustness to noise as the average change in magnitude of cosine similarity scores after injecting noise to documents inğ‘†at varying values ofğ›½as described in Section 3.2. We alter the noise substitution method between experiments to allow for a comparison between uniform and statistics based noise. We conduct two complementary experiments: (1)Testing for similarity between podcast descriptions (ğ‘†) and noisy ASR transcripts (ğ‘†) at varying levels of noise. This allows us to identify if the topic model is robust to transcription errors, and whether ASR transcripts contain enough information for the topic model to produce meaningful topic vectors. (2)Testing for similarity between raw ASR transcripts ( This further investigates how robust the topic model is to transcription errors, under the assumption that the transcript provides meaningful topic vectors. For each experiment, we vary report the average and the standard error. 5 RESULTS AND DISCUSSION The results of the two series of experiments are presented in Figure 1a and Figure 1b, respectively. (a) CS score between descriptions and noisy ASR transcripts at various levels of ğ›½. Fig. 1. Analysis of topic model robustness using cosine similarity, shaded area showing standard error of the mean. As can be seen from Figure 1, injecting noise in ASR transcripts has an eî€ect on the CS for both noise distributions. We observe 45 impact of 8 are signiî€›cantly more robust to statistics-based ASR noise than uniformly distributed noise. We also observe that the mean CS score between the descriptions and transcripts at observe as the lower bound, which can be roughly estimated to be 0 which is evidence that ASR transcripts can be used to produce meaningful topic vectors. We suspect that the reason for the small change in topic distributions under the statistics based noise is mainly due to the type of errors produced by the ASR engine which, as seen in Table 2 and Table 3, have a tendency to retain semantic meaning. Note that seen as an estimation of the WER in a podcast transcript (see Figure 3 in Appendix A), which suggests that even when an ASR engine produces a transcript with a high word error rate, the transcript will still be viable for topic modeling because the errors will have little impact on the topic distribution after LDA preprocessing. Table 4. The number of diî€›erent podcast shows that are present in the cosine similarity deciles at ğ›½ = 0. .9% (0.85-0.46) and 49% (1.00-0.51) relative decreases in CS when injecting uniform noise, while limited .8% (0.85-0.775) and 9% (1.00-0.91) is found for statistics based ASR noise. This is evidence that topic vectors When comparing transcripts and author descriptions for the baseline withğ›½ =0, the standard deviation has a value of 0.128. Furthermore, the diî€erence between the minimum value of 0.500 and the max value of 0.997 is large. We investigate why this occurs by splitting the podcast episodes into deciles based on their cosine similarity and counting the number of unique podcast in each decile as seen in Table 4. The lowest amount of unique podcast shows are in the top and bottom deciles. Furthermore, in the 1st decile, 42 out of the 59 episodes comes from the same podcast "Sagen om Amagermanden" (note, it is also the podcast show that is the most represented) and in the 10th decile, 52 of the 58 episodes are from podcast show "Hvor er mit ansigt?". This result suggests that either the quality of the podcast descriptions or the performance of the ASR engine is very dependent on the speciî€›c podcast show. However, since all podcast shows in the dataset are single speaker with limited amount background music, we suspect that quality of the author descriptions is the primary reason why episodes originating from the same podcast show consistently perform poorly in some cases. 6 CONCLUSION In this work, we conducted experiments to evaluate the quality and robustness of topic representations produced by a general LDA topic model when exposed to noisy ASR transcripts in a low-resource language (Danish). More speciî€›cally, we investigated how injecting two diî€erent noise proî€›les to raw ASR transcripts inî€uence the topic distributions across a podcast dataset at varying levels of noise. We created the dataset by leveraging an ASR system to obtain podcast transcripts. We choose to only include podcast episodes that had a high-quality author-written description as part of the meta-data in the dataset, to allow for comparison between the description and transcript topic vectors, relying on the assumption that a well-written description is very similar in terms of topic distribution to that of the episode content. To obtain topic vectors we trained a general-purpose LDA model on Danish Wikipedia data. We experimented with injecting two types of noise into the raw ASR transcripts, namely uniformly random noise and simulated ASR noise. We obtained similarity baselines by computing vector similarities of the raw transcripts with their respective descriptions and the raw transcripts with themselves at various levels of noise injection. We found that injecting random noise to the transcripts signiî€›cantly inî€uenced the CS score (45.9% and 49% relative to baseline) whereas injecting simulated ASR noise only slightly inî€uenced the CS score (8.8% and 9% relative to baseline). Given our î€›ndings, we conclude that even when an ASR engine produces podcast transcripts at higher WERs, we can still obtain meaningful topic representations. We hypothesize that this is because even when an ASR engine produces erroneous text, the majority of the word-level errors will carry similar semantic meaning to the underlying truth. This encourages the use of ASR engines and transcriptions for podcast recommendations. Even in cases where the ASR engine is challenged by complex audio scenery, the transcripts could be suî€œcient to obtain topic representations that are relevant when representing the semantic content of a podcast episode. Furthermore, we also found implications that using ASR transcripts would be generally more robust than relying on human tinkered descriptions which are largely reliant on the individual content creators when it comes to performing topic modelling on podcast content. We conclude this based on the fact that episodes originating from the same podcast shows either consistently showed very high or very low topic overlap between their transcripts and their descriptions. The ASR noise investigated in this paper was based on ASR errors from clean recordings without common podcast artifacts such as the overlap of speech and ambient sounds. In the future, it would be worth investigating whether topic models also are robust to ASR noise stemming from these types of errors. Furthermore, there are indications that other downstream tasks might not be as robust to noise as topic modeling, hence analyzing the robustness of other relevant downstream tasks to ASR noise remains an open research question. 7 ACKNOWLEDGMENTS We gratefully acknowledge support from Innovation Fund Denmark in the forms of their Innobooster and Innoexplorer grants (Grant numbers 0173-00670B and 0160-00023 respectively).