Practical recommender systems experience a cold-start problem when observed user-item interactions in the history are insuî€œcient. Meta learning, especially gradient based one, can be adopted to tackle this problem by learning initial parameters of the model and thus allowing fast adaptation to a speciî€›c task from limited data examples. Though with signiî€›cant performance improvement, it commonly suî€ers from two critical issues: the non-compatibility with mainstream industrial deployment and the heavy computational burdens, both due to the inner-loop gradient operation. These two issues make them hard to be applied in practical recommender systems. To enjoy the beneî€›ts of meta learning framework and mitigate these problems, we propose a recommendation framework calledContextualModulationMetaLearning (CMML). CMML is composed of fully feed-forward operations so it is computationally eî€œcient and completely compatible with the mainstream industrial deployment. It consists of, a context encoder that can generate context embedding to represent a speciî€›c task, a hybrid context generator that aggregates speciî€›c user-item features with task-level context, and a contextual modulation network, which can modulate the recommendation model to adapt eî€ectively. We validate our approach on both scenario-speciî€›c and user-speciî€›c cold-start setting on various real-world datasets, showing CMML can achieve comparable or even better performance with gradient based methods yet with higher computational eî€œciency and better interpretability. â€¢ Information systems â†’ Information retrieval. Recommender system; Cold-start problem; Meta learning ACM Reference Format: Xidong Feng, Chen Chen, Dong Li, Mengchen Zhao, Jianye Hao, Jun Wang. 2021. CMML: Contextual Modulation Meta Learning for Cold-Start Recommendation. In Proceedings of the 30th ACM Intâ€™l Conf. on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, Australia. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/ 3459637.3482241 Personalized recommendation has been rapidly developed and thoroughly inî€uenced various î€›elds including web services, Ecommerce, and social media. Based on userâ€™s feature and past interaction history with items, the personalized recommender system can generate reasonable recommendation results for userâ€™s preferences. Among the personalized recommendation approaches, the data-driven deep learning based recommendation algorithms have gained increasing attention because of their superior performance compared with traditional recommendation algorithms. However, both traditional and data-driven machine learning based recommendation algorithms struggle to tackle the cold-start problem, since the recommender system can only get access to very limited userâ€™s interaction history. To address cold-start recommendation problems, some works directly utilize content information such as userâ€™s proî€›les [2,30] or incorporate it into traditional collaborative î€›ltering [5,28,34], but they still often fail to generalize to users with only a few interactions. Some other works [14,22] try to solve the cold-start problem by transferring domain knowledge from cross-domain datasets, but they still cannot get rid of the need for shared examples from diî€erent domains. Meta learning, also known as leaning to learn, is a prominent machine learning paradigm aiming at learning meta knowledge among tasks to achieve fast adaptation with limited data examples when facing a new task. Inspired by this, some works [7,8,19,35] leverage the latest progress on meta learning to model and solve the cold-start recommendation problem. They treat diî€erent entities (like users, scenarios) in recommender system as tasks, so the coldstart recommendation problem can be transformed into a newtask adaptation problem in meta learning. They commonly adopt MAML [9], which is a representative gradient based meta learning framework to solve the problem. Speciî€›cally, they try to optimize the recommendation modelâ€™s initial parameters, to make it capable of fast adaptation from limited data examples through taking only a few gradient steps. Practically, they have achieved signiî€›cant improvements over previous deep learning based recommendation algorithms, which validates the eî€ectiveness of introducing meta learning framework into the cold-start problem. However, they have also brought some new issues which can be basically summarized into two parts. (1) The additional innerloop gradient adaptation will lead to much lower computational eî€œciency with respect to both the training phase and inference phase. (2) The inner-loop gradient operator is incompatible with current industrial recommender systemâ€™s framework, where most recommendation algorithms only consist of feed-forward network like Multilayer perception (MLP). These two problems prevent the broad use of these gradient based approach on cold-start problems. In order to both enjoy the beneî€›ts from meta learning and mitigate the computational or deployment problems, we are trying to answer the following question:from limited data examples, how can we conduct meta learning model adaptation based on feed-forward operations, rather than backward propagation as done in the gradient base d approach?An intuitive idea is to directly map limited samples to continuous representation for guiding the model adaptation. Speciî€›cally, we need a powerful embedding network for information extraction from samples and a î€exible modulation network for modelâ€™s adaptation. These modules can be trained via the meta learning framework and thus equip the model with fast adaptation ability even given limited data. Based on these motivations, we propose aContextualModulation MetaLearning (CMML) recommendation framework for cold-start problems. Note that the term contextual here does not represent any speciî€›c context information, such as time, userâ€™s features. Here we treat the interaction history as contextual information to reveal entitiesâ€™ (users/scenarios) implicit features. In our framework, instead of utilizing inner-loop gradient adaptation, we make use of modulation techniques for model adaptation when a new task comes. Speciî€›cally, our model consists of three parts: (1) A context encoder that maps the limited data examples to eî€ective context aggregation. (2) A hybrid context generator, which combines the above encoded task-level context (scenario/user-speciî€›c information) and instance-level feature (user-item features) together. By modelling the interaction between task-level context and local useritem pairâ€™s features, î€exible hybrid context can largely enhance the modelâ€™s capacity. (3) A context based modulation network, which can modulate the neural network for fast adaptation based on context generated by two parts above. Compared with previous gradient based meta recommendation algorithms, one of the most appealing characteristics for our method is that it is fully composed of feed-forward neural network and successfully gets rid of the gradient adaptation. It can signiî€›cantly improves the computational eî€œciency and is compatible with industrial recommender systems. The remainder of this paper is organized as follows: in Section 2, we introduce the problem formulation for meta learning based cold-start recommendation and the setting for two representative types of important cold-start problems. Section 3 depicts the whole framework of the proposed CMML approach and also provides computational eî€œciency analysis. Experimental results about detailed experimental settings, comparison, visualization, ablation study are shown in Section 4. We review related work in Section 5. Finally, we conclude our paper in Section 6. In this section, we formally deî€›ne the problem formulation for meta learning based recommendation and detail the settings for scenario-speciî€›c and user-speciî€›c meta recommendation problems. Traditional recommendation is composed of two sets, user setU, item setI, which includes userâ€™s features and itemâ€™s features, respectively. It conducts the mapping function:U Ã— I â†’ R, which means they will generate prediction results based on usersâ€™ and itemsâ€™ features. For context-aware recommendation problem, the recommender system is required to conduct the mapping function: U Ã—I Ã—C â†’ R, whereğ¶represents contextual information set. In this paper, we are trying to solve cold-start problem by meta learning based recommendation, where model can conduct task adaptation based on limited data from speciî€›c entities (users/scenarios). For meta learning based recommendation, the framework needs to transform interaction history into implicit contextual information and conduct rapid adaptation to predict results. This mapping function corresponds toU Ã— I Ã— {U, I}â†’ U Ã— I Ã— C â†’ R, where{U, I}represents interaction history. The mapping function{U, I}â†’ Ccan be gradient descent operator like [19], [8], or the contextual modulation framework introduced in this paper. For meta learning problems, there are some important terms that need to clarify for clear comprehension. The terms meta-training and meta-testing: these two terms correspond to the standard deî€›nition of training set and testing set in traditional machine learning. The major diî€erence is that data examples in meta learning are tasks. Support-set and query-set: in meta learning, it commonly assumes that there exist limited labeled data examples from which the model can take rapid adaptation. Such labeled samples are denoted as support-set. The samples which need to be predicted are denoted as query-set. Note that in the meta-training phase, we will get labels for both support-set and query-set. The labeled support-set is used for inner-loop adaptation, while the labeled query-set is used for outer-loop optimization. In the meta-testing phase, we will only get labeled data examples from support-set, and leaving the adapted model evaluated on the unlabeled query-set. With above deî€›nitions, the meta recommendation problems can be formulated as the following machine learning problem. It assumes that the recommendation tasksğ‘‡subjects to a î€›xed task distributionğ‘ƒ (ğœ). The î€›nal objective of modelğ‘“is to minimize the expectation loss on the prediction results for query-set samples ğ·(ğ·) after conducting adaptation from support-set samples ğ·(ğ·). Thus, the objective function can be written as follows: where we denote all parameters and loss function withÎ˜andL respectively. For meta recommendation problem, theğ·here represents interaction history for recommended entities andğ· denotes the user-item pairs the adapted model needs to predict. For our proposed CMML framework, we split the parametersÎ˜ into two parts: backbone networkâ€™s parametersÎ¦and meta model parametersÎ˜. Practically, we split the tasks of the dataset into meta-training tasksğ·and meta-testing tasksğ·and try to minimize the empirical risks on ğ·. In all, the loss function can be represented as Equation (2) and (3). whereğ‘“represents the whole model,(ğ‘¢, ğ‘–), ğ‘¦denotes the ğ‘˜-th user-item pair and corresponding label respectively.{ğ‘¢, ğ‘–} is the user-item interaction history in support-set. Note that we neglect the symbol of real labels for{ğ‘¢, ğ‘–}in support-set in the whole paper for notation simplicity. In this paper, we choose two cold-start recommendation settings, scenario-speciî€›c [8] and user-speciî€›c settings [19], to show the eî€ectiveness and generalization ability of our proposed framework. Note that our framework is not limited to these two settings and can also be applied into broader industrial recommendation problems. The scenario-speciî€›c recommendation utilizes scenario characteristics to enhance the recommendation performance. For instance, for sports related scenarios, items related with outdoor activities should gain more popularity. In meta learning, our framework treats diî€erent scenarios as diî€erent tasks. Without explicit information, the algorithm needs to extract implicit scenario characteristics from limited support-set, and conducts scenario-speciî€›c adpatation. Following [8], we model the learning objective as a click-throughrate(CTR) prediction problem and utilize hinge loss on query-set as our meta objective function. We denoteğ‘–andğ‘–as positive items and negative items for CTR. Following Equation (3), the loss function can be written as follows: â„“ = maxî€€0, 1 âˆ’ ğ‘“î€€ğ‘¢, ğ‘–; {ğ‘¢, ğ‘–}î€+ ğ‘“(ğ‘¢, ğ‘–; {ğ‘¢, ğ‘–})î€. (4) For user-speciî€›c meta recommendation setting, the tasks in meta learning framework corresponding to usersğ‘ˆin recommendation. The interaction history of{ğ‘¢, ğ‘–}will be treated as the supportset, which can implicitly reveal the userâ€™s proî€›les and preferences. We consider a score regression problem [19] in this setting. The objective of meta model is to minimize the mean square error on new user-item pairsâ€™ prediction scores. Following Equation (3), the loss function can be written as follows: where ğ‘¦is the real score for user-item pair (ğ‘¢, ğ‘–). In this part, we illustrate the backbone network and detailed structure for our proposed CMML framework. In all, the framework consists of three components: a context encoder that can aggregate the interaction history in the support-set for implicit contextual information extraction and eî€ective task representation, a hybrid context generator that aggregates task-level information and instance-level user-item features, and modulation network, which modulates the backbone network for fast adaptation on new scenarios/users. The detailed procedure is summarized as follows. First, the context encoder will map interaction history for task-level contextual informationğ¶. Second, the embedded context will be combined with speciî€›c user-item feature embedding, generating hybrid context. Conditioned on hybrid context, the backbone network will be modulated for new scenario/user adaptation. The detailed algorithm chart is shown in Algorithm 1. We also provide Figure 1 for better illustration of the overall framework and speciî€›c modules. The recommender backbone network maps user-item features to prediction results, and will be modulated by the modulation modules introduced in Section 3.4. We adopt a common feed-forward neural network as the backbone network structure for simplicity. Algorithm 1 Contextual modulation meta learning Require: the meta-training dataset ğ· From support-setğ·, global contextğ¶is obtained by context encoder using equ 6, 7 For each user-item pair in query-setğ·, conduct hybrid context aggregation by equ 8 Evaluate the loss functionğ‘™on query-setâ€™s labels, But note that our framework can also be plugged into many sophisticated backbone networks. The whole backbone network consists of three components: embedding layers, hidden layers, and output layer. All parameters mentioned here belong to Î¦. The embedding layer consists of user embedding matricesğ‘€ and item embedding matricesğ‘€. The high-dimensional user features/idsğ‘¥and item features/idsğ‘¥can be transformed into low dimensional representationsğ‘’, ğ‘’byğ‘’= ğ‘€ğ‘¥andğ‘’= ğ‘€ğ‘¥. Hereğ‘€, ğ‘€can be learned through the end to end training process [19] or be pre-generated [8] by collaborative î€›ltering [17]. The hidden layers are ReLU activated MLP which map the concatenated user-item feature embedding(ğ‘’, ğ‘’)to the continuous representationâ„. The î€›nal output is a linear layerğ‘“, which maps the hidden layerâ€™s output to î€›nal scoreğ‘œfor speciî€›c user-item pair. The transformation corresponds toğ‘œ= ğ‘¤â„+ğ‘, whereğ‘¤, ğ‘are weights and bias for the last layer respectively. Context encoder can be regarded as the embedding network that maps interaction history of user-item pairs to implicit contextual information:{U, I}â†’ C. Here we oî€er two alternatives: pooling aggregated encoder (PE) and sequential aggregated encoder (SE). 3.2.1 Pooling aggregated encoder. For pooling aggregated encoder, it mapsğ‘›user-item pairs{ğ‘¢, ğ‘–}âˆˆ ğ·to user/scenario context. Each user-item pair(ğ‘¢, ğ‘–)will be î€›rstly mapped to the corresponding embeddingğ‘’, ğ‘’by embedding layers of the backbone network. Then the embedding is fed into ReLU activated MLPğ‘“, which is shared among all user-item pairs. At last, mean/max pooling operation will be conducted to aggregate user-item pairs to single user/scenario contextğ¶. The above process can be formulated as Equation (6): whereğ‘“denotes the context encoderğ‘“parameterized by meta parametersğœƒâˆˆ Î˜,ğ¶is the context embedding forğ‘˜-th useritem pair and ğ¶ is the î€›nal task-speciî€›c context. Pooling aggregated encoder is a fairly simple model for implementation and it has the property of permutational invariance, which means the output of neural network will not be aî€ected for diî€erent permutations of inputs. Since we usually get no access to explicit sequential data in our setting, the permutation for user-item pairs contains no useful information for identifying task-speciî€›c features. The permutational invariance property enables the model to neglect the sequential order for items in support-set. 3.2.2 Sequential aggregated encoder. To further enhance the information aggregation ability of the context encoder, we also provide sequential aggregated encoder (SE). We treat all user-item pairs {ğ‘¢, ğ‘–}âˆˆ ğ·as a sequence of information and leverage sequential model like Gated Recurrent Unit(GRU) [6] to handle the representation of the support-set. The user-item pairs are î€›rstly fed into GRU network sequentially, and the embedded sequence representation from it will be mapped into the global context by ReLU activated MLP. The process can be formulated as Equation (7): whereGRUis the GRU based context encoder parameterized by meta parametersğœƒâˆˆ Î˜,{â„}, {ğ‘œ}denote the output sequence of the GRU model andğ‘“represents the MLP parameterized by meta parametersğœƒâˆˆ Î˜that maps sequence embedding ğ‘œinto context ğ¶. It deserves to be pointed that since data examples in support-set usually contain no sequential information, we choose sequential model because of its better context aggregation ability compared with mean-pooling operation in PE rather than its sequential property. Note that though in principle, the sequential aggregated encoder does not hold the property of permutational invariance, we can still randomly permutate the support-set at each iteration in practice so the sequential aggregated encoder can learn to be agnostic to the order information. This is also the technique utilized in [12]. Note that the context encoder in previous section only extracts user/scenario information for task-level context from support-set ğ·. However, the task-level context will be the same for different user-item pairs in the query-set. The interaction between task-level context and speciî€›c instance-level user-item feature is neglected. Thus, in this section, we provide the module of hybrid context generator to combine both information eî€ectively. There are many classical works in recommender systemâ€™s literature like [10,33] about how to conduct low-order and high-order feature interactions. Usually, low-order interaction is obtained by taking low-order computation like dot-product among features while high-order interaction is obtained by deep neural network. We borrow the idea of this to formulate the hybrid context generator. By taking dot-product or MLP to model the relationship of these features, we provide two ways to combine information in Equation (8). whereğ¶denotes the speciî€›c hybrid context for speciî€›c useritem pair{ğ‘¢, ğ‘–},ğœƒâˆˆ Î˜represents the meta parameters of hybrid context generator. After obtaining hybrid context, the next step for our framework is to eî€ectively adapt for task-speciî€›c model. Speciî€›cally, we propose context based modulation network which modulates the backbone networkÎ¦for fast adaptation. In this section, we provide three ways to conduct network modulation: weight modulation, layer modulation and soft modularization. And we will useğ¶to denote the speciî€›c hybrid contextğ¶in Equation (8) for simplicity in the following. 3.4.1Weight Modulation. The directest way of modulating network is to generate weights by hyper-network [11] for the backbone network based on hybrid context. However, the backbone network usually has thousands of parameters, making it diî€œcult to generate such high-dimension output. Even we can reduce the parameters dimension, it is still unstable for training if all the parameters are generated via hyper-network. Thus, we choose to only generate weights and bias by hyper-networkğ‘“for the î€›nal linear layer. In fact, Vartak et al.[31] also try to generate weights for a linear model. But they only conduct modulation on a single linear model rather than the last layer of MLP, which limits the representation ability of the modulated network. The detailed equation is shown as follows.ğ‘¤, ğ‘= ğ‘“(ğ¶), where we denote the hybrid context asğ¶, hyper-networkâ€™s parameter asğœƒâˆˆ Î˜, the generated modulation parameters asğ‘¤, ğ‘, the output of î€›nal hidden layer as â„and the î€›nal output as ğ‘œ. The strength for weight modulation is its simplicity, which makes it quite easy to implement and suitable for some simple tasks. When tasks are quite diî€erent, this modulation may not have enough representation ability and capacity for rapid adaptation. We oî€er the second way, layer modularion, to solve the problem. 3.4.2Layer Modulation. As mentioned above, it is hard to directly generate weights for all hidden layerâ€™s parameters because of its high dimensionality. Therefore, we adopt layer modulation rather than layer weightsâ€™ modulation, where we modulate layers ouput by hyper-network. It can eî€ectively lower the dimensionality of parameter space fromğ‘›toğ‘›, whereğ‘›refers to the amount of nodes in one layer. Here we show two ways of layer modulation. The î€›rst one is that hyper-network generates weights activated by Sigmoid function and the modulated output of each layer is the dot-product of the original output and the generated weights. The second way follows [23](FiLM) by utilizing feature-wise Linear Modulation on backbone network. The hyper-network generates weights and bias for linear modulation on layersâ€™ output. For layer ğ‘–â€™s outputğ‘™, the sigmoid-dot modulation and FiLM modulation can be written by Equations (10) and (11) respectively. where we denote the hyper-networkâ€™s parameter asğœƒâˆˆ Î˜, the generated modulation parameters asğ‘¤, ğ‘and the modulated output of layer ğ‘– as ğ‘œ. Layer modulation increases its representation ability for task adaptation compared with the weight modulation. The weakness is that the modulation weights are still high dimensional and even though we can interpret it by visualizing the modulation weightsâ€™ clusterings, it is still hard to interpret how the model works for speciî€›c tasks. That is why we provide the third modulation way called soft modularization. 3.4.3Soî€œ Modularization. In this section we introduce soft modularization method. The model modulation is conducted by controlling mixture of experts network. Some works like [21] discuss how mixture of experts with shared bottom layer can be used for handling multi-task recommendation problems. The multi-task setting has some similarity with meta learning setting since both need to adapt model when facing one speciî€›c task. For modulating the network in our setting, we adopt the similar way of soft modularization from [37] to generate route weights for sub-networks. A main diî€erence is that our modulation is conditioned on continuous hybrid context which can easily generalize to completely new task while the task indicator used in [37] can only handle multitask problem. The soft modularization consists of two parts: base network and route network. In base network, there existsğ‘˜layers and each layer has several sub-networks (modules). For instance, there exists four 32Ã—32 fully connected sub-networks at each layer, which has equivalent amount of parameters with a 64Ã—64 fully connected network. Conditioned on the hybrid context, the route network will generate dynamic routing for base network, which consists of ğ‘˜ probability distributions. Each distribution is used to aggregate the output of sub-networ k. The detailed procedures are as follows: when a hybrid context is fed into the route network, it will be used to generate a probability distribution of moduleâ€™s route weights by Softmax activation function for each base network layer. For ağ‘˜-layer base network withğ‘šmodules each layer, the route network with the same depth ğ‘˜will generateğ‘˜ Rmatrices for all layers. In each matrix, the ğ‘–-th (ğ‘– âˆˆ 1, 2...ğ‘š) column vector sums up to 1 and corresponds to the weights betweenğ‘–-th module at layerğ‘™to allğ‘šmodules at layer ğ‘™ +1. We denote the route vector logits of layerğ‘™asğœâˆˆ R, the probability scalar fromğ‘–-th module in layerğ‘™to module j in layerğ‘™ +1 asğ‘âˆˆ R, the output ofğ‘™-th hidden layer asâ„, the meta model parameters asğœƒâˆˆ Î˜, the modulated output ofğ‘–-th modules at layerğ‘™asğ‘œğ‘. Equations (12) and (13) depict how the route function and aggregated base network work in layer ğ‘™. Soft modularization can somehow be treated as shared layer modulation, since the route weight is shared among all nodes in one module. The representation ability of soft modularization will be discounted compared with layer modulation. However, the route weights generated by route network are now low-dimensional and we can easily get access to the weight distribution to know how Table 1: Time and Space complexity for gradient based and context-based Meta Learning algorithms the sub-modules are activated for a speciî€›c task, which strongly increases the interpretability. Though the gradient based meta learning approach brings a notably performance improvement over previous traditional recommendation algorithms, it imposes considerable computational and memory burdens due to the the inner-loop gradient operator. In order to show the strengths of CMML framework with respect to the computational eî€œciency, we brieî€y analyze the time and space complexity for MAML-like algorithms and CMML in training and inference phase. Note that the analysis here only considers the computational complexity for basic MAML based algorithms like [19]. [8] and [7] will have larger computational complexity. In the following analysis, we denoteğ‘“, ğ‘“, ğ‘, ğ‘as the time and space complexity for forward process and backward process respectively. Assume we haveğ‘šdata points to be fed into the neural network and we take k gradient steps in the inner-loop. In the inference phase, the MAML-like algorithms need to perform feed-forward operation and backward operation for m data points each step, so the î€›nal time and space complexity for MAMLlike algorithms isğ‘‚ (ğ‘šğ‘˜(ğ‘“+ğ‘))andğ‘‚ (ğ‘šğ‘˜(ğ‘“+ğ‘)). For CMML, it only needs to take feed-forward operation, resulting in the î€›nal time and space complexity:ğ‘‚ (ğ‘šğ‘“),ğ‘‚ (ğ‘šğ‘“). Note that in most cases,ğ‘> ğ‘“andğ‘> ğ‘“holds. So the time and space complexity of MAML-like algorithms is 2k times larger than that of CMML in the inference phase. In the training phase, the eî€œciency strength of CMML can be more clear since the gradient based method needs to diî€erentiate through the whole gradient paths in the inner-loop, which requires the computation of the hessian-vector product. In this part, we neglect the complexity analysis for forward process. [24] and [27] show that in the reverse mode of gradient calculation, the time and space complexity of hessian-vector product are typically no more than a constant over that of î€›rst-order gradient calculation. Usually, the constant is 5 and 2 for time and space complexity respectively. Thus, for MAML-like algorithms, the time and space complexity isğ‘‚ (5ğ‘˜ğ‘šğ‘)andğ‘‚ (2ğ‘˜ğ‘šğ‘), while for CMML, the time and space complexity is ğ‘‚ (ğ‘šğ‘) and ğ‘‚ (ğ‘šğ‘). The analysis is conducted given î€›xed gradient steps k. Combining our analysis and the Prop 3.1 of [27], we can also show that when we are trying to get ağ›¿ âˆ’ accurateoptimal solution in the inner-loop (which means the distance between optimized parameters and optimal parameters of the inner-loop problem is bounded byğ›¿), the time and space complexity isğ‘‚ (5ğ‘šğ‘ğœ… log)î€î€‘ andğ‘‚ (2ğ‘šğ‘ğœ… log), whereğœ…is the condition number for innerloop optimization andğ·is the diameter of parametersâ€™ search space. We summarize the analysis results in Table 1. In this section, we detail the experimental settings and results to validate the eî€ectiveness of CMML. This section includes the experimental settings like dataset conî€›guration, evaluation metrics and baseline algorithms. The performance comparison section shows that CMML can achieve both outstanding performance and high computational eî€œciency. Visualization of the learned embedding and route is shown for demonstrating the interpretability of CMML. And î€›nally, the ablation study is conducted to analyze how speciî€›c structure inî€uences the performance of CMML. 4.1.1 Dataset. For scenario-speciî€›c recommendation setting, we evaluate our algorithm on two public datasets from MovieLens-20M [13] and Taobao [8]. MovieLens-20M is a large movie-rating dataset from movie recommendation service MovieLens, and it consists of 138,493 users, 27,279 movies, and 20,000,263 rating records. Following similar CTR problem formulation of [8], we transform the MovieLens-20M dataset from explicit rating into implicit feedback. All movies rated by the user will be regarded as positive samples with the rest as negative samples. 306 diî€erent genres of movie are treated as scenarios for scenario-speciî€›c setting. The second Taobao datasetis from the cloud theme click log on Taobao recommender system. This dataset consists of usersâ€™ purchase history in 355 diî€erent themes such as "things to prepare when a baby is coming", which are treated as diî€erent scenarios. There are 700k users, 1400k items, and 5717k purchase history in Taobao. We also build a synthetic dataset called hybrid Movie-Taobao which concatenates scenarios in both datasets to test our algorithms on multi-domain cold-start problems. The meta-training and meta-testing dataset of all three datasets are split based on diî€erent scenarios. For user-speciî€›c recommendation setting, we use the MovieLens1M[13] dataset following [19]. MovieLens-1M is an older and smaller version of MovieLens dataset compared with MovieLens-20M. It consists of 6,040 users, 3,706 movies, and 1,000k rating records. Diî€erent from the CTR problem in scenario-speciî€›c setting, we need to solve the cold-start regression problem for user-item score prediction. The meta-training and meta-testing dataset of it are split based on diî€erent users, denoted asğ‘Š âˆ’ğ‘Šandğ¶ âˆ’ğ‘Š(which is short for two settings of training on warm users - testing on warm users and training on cold users - testing on warm users respectively). Note that the split method is diî€erent from that in [19] because cold users are deî€›ned as earlier users than warm users and meta learning problem is motivated to the goal of generalizing towards unseen or new tasks (users). Here we also detail the data preprocessing procedures. In scenariospeciî€›c setting, we only select scenarios with less than 1000 but more than 100 items in each dataset to make sure it can guarantee the cold-start property. There exists 232 scenarios for meta-training set with the rest as meta-testing set. In the training process, we will randomly sample 64 user-item positive pairs as support-set and 128 user-item pairs as query-set from each scenario. Note that since the amount of negative samples is much larger than that of positive samples, we will also sample the same amount of negative pairs in support and query-set. For user-speciî€›c setting, we trim the dataset and only choose 50 rating records for each user to match the cold-start setting. There exists 4832 users in meta-trainng set with the rest as meta-testing set. For each user, we randomly sample 10 items for query-set with the rest as support-set. 4.1.2 Evaluation Metrics. For scenario-speciî€›c setting, since it is modeled as a CTR prediction problem in [8], we adopt the same metric and utilize top-N recall for performance evaluation. And for user-speciî€›c setting, which is modeled as a score regression problem in [19], we utilize both mean absolute error (MAE) and normalized discounted cumulative gain (NDCG[15]) for evaluation as used in [19], and the higher the better in NDCG, the lower the better in MAE. The experimental results also reveal that our method generally works for diî€erent loss functions. 4.1.3 Baseline algorithms. We select many state-of-the-art baselines of deep learning or meta learning based recommendation algorithms for comparison. For scenario-speciî€›c setting, â€¢ Deep Cross Network(DCN): Deep cross network [33] is one of the most classical item recommendation algorithms and it has been widely applied in industrial recommendation system. It proposes cross layer which successfully combines beneî€›ts of FM and DNN for item recommendation. We train our DCN model on all scenarios in the meta-training set. â€¢ Deep cross network with î€›ne-tuning(DCN-F). We also provide a baseline which î€›netunes the pretrained DCN model on speciî€›c scenario based on data in the support-set. â€¢ ItemPop: ItemPop is a heuristic algorithms used in [26] which ranks items according to its popularity, measured by the amount of corresponding interactions in the support-set. â€¢ CoNet: CoNet is a cross-domain recommendation algorithms for tackling cold-start problem. It utilizes cross connection to enable dual knowledge from source domain to target domain. We reimplement the algorithm on MovieLens-20M by regarding movies without labeled genre as the source domain. For Taobao datast and hybrid dataset, the authors did not oî€er the source domain data used in [8]. Thus here we can only show the reported result in [8] in Taobao dataset. â€¢ ğ‘ Meta[8]: The state-of-the-art meta learning approach for scenario-speciî€›c cold-start problem. It proposes update and stop controllers for better inner-loop optimization. For user-speciî€›c setting, â€¢ DCNandDCN-F: Same with previous baselines and we modify the model into a regression model. â€¢ MetaCS-DNN[4]: A meta-learning baseline that tries to tackle the cold-start user recommendation based on MAMLlike algorithms. For a speciî€›c user, the whole model will take a few gradient steps for adaptation. â€¢ MELU[19]: MELU basically shares similar idea with MetaCS- DNN by utilizing MAML-like algorithm. The only diî€erence is the inner-loop optimization will only happen in fully connected layers rather than the whole network. 4.1.4 Features and Hyper-parameters. For MovieLens-1M dataset, we select the same features as used in [19], and for MovieLens20M dataset and Taobao dataset, we seclect the same features as adopted in [8]. The network architectures involved are designed to be comparable with the baselines for fairness. The base network is constructed as a ReLU activated MLP with hidden units(64,64,64), the context encoder includes a GRU and a ReLU activated MLP with hidden units(128,128). For CMML-FiLM, we also utilize a ReLU activated MLP with hidden units(64,64,64)for layer modularization. For CMML-Soft-M, it containsğ‘˜ =3 module layers,ğ‘š =4 modules per layer and each module outputs ağ‘‘ =32 representation. The user embeddings and item embeddings are pre-generated [8] by collaborative î€›ltering [17]. We optimize our model with Adam [16] optimizer with learning rate 1e-4. These hyper-parameters are conî€›gured by grid search. Clariî€›cation: During our preparation for code, we î€›nd out in the scenario-based experiments, the random seeds have some inî€uences on our î€›nal experimental results. So we reconduct experiments in the scenario-based setting for 5 seeds and report the results in the appendix. In this part we summarize the performance comparison for CMML (based on FiLM layer modulation and soft modularization) and other baseline algorithms in scenario/user-speciî€›c setting, respectively. For scenario-speciî€›c setting, Table 2 shows that CMML generally achieves comparable or better results among all three datasets in terms of top-N recall. For Taobao dataset, we notice that DCNF and other scenario-speciî€›c algorithms exceptğ‘ Meta perform worse than DCN. The counterintuitive phenomenon reveals that scenario-speciî€›c support-set in Taobao may contain noisy and irrelevant information, which might be detrimental since adaptation may easily cause over-î€›tting on limited noisy support-set samples. Under this condition, CMML still achieves improvement comparable to the best baselineğ‘ Meta, which validataes the eî€ectiveness of CMML. The results also reveal that CMML can work for diî€erent scales of datasets. Hybrid MovieLens-Taobao evaluates the algorithmsâ€™ performance on datasets composed of multi-modal data. CMML achieves much improvement over the best baselineğ‘ Meta. The reason might be that gradient based algorithms can only lean one group of initial parameters, which can work pretty well dealing with single-domain dataset, yet not suitable for multi-domain dataset. CMML works better here due to its î€exible representation and modulation ability for multi-domain datasets. For user-speciî€›c setting, we basically observe similar results. The î€›ne-tuning of DCN-F helps for identifying users preferences and increasing the performance. Three meta learning based algorithms achieve comparable results and are better than DCN baselines. We remark that CMML does not achieve remarkably better results on single modal datasets compared with gradient based method. That is because the gradient descent is a strong baseline for adaptation and can guarantee performance improvement even without any meta-training. We leave further investigation of combining CMML with gradient based methods for future work. We also provide the comparison results of time and memory eî€œciency ofğ‘ Meta, MELU and CMML in Figure 2. We use sequential encoder, dot hybrid context generator and FiLM based modulation network for CMML, and the inner gradient steps of ğ‘†and MELU are both set with 1,5,10,20. It can be shown that the time and memory consumption increases with more gradient steps. For scenario speciî€›c setting, our algorithm achieves remarkable computational eî€œciency compared withğ‘†Meta. It consumes less memory and time compared with even one-stepğ‘†Metawhile it usually conducts 20-step gradient descent in the implementation. For user-speciî€›c setting, our algorithm achieves comparable time and memory demands with one-step MELU, while MELU needs Figure 4: Activated routes for two diî€erent tasks to take 5-step gradient descent in implementation. Note that here CMML has more modelâ€™s parameters compared withğ‘†Metaand MELU because of all three additional modules, but it is still quite computationally eî€œcient. In this section, we provide the visualization of learned context embedding and the activated route of soft modularization to show the interpretability of our proposed CMML in Figure 3 and 4 respectively. In Figure 3, we show the visualization for task-level context on MovieLens-20M dataset and hybrid dataset. For the clusterings on MovieLens, we sample the support-set for 8 diî€erent genres(tasks) and the corresponding context embedding can be successfully clustered. It demonstrates that the context encoder is capable of distinguishing movies from diî€erent genres. In addition, we investigate the clustering of semantic groups by classifying these 8 genres into 4 broader types: horror, suspense, romance and war. We can î€›nd that the tasks with semantically similar genres (shown with the same color) are closer, revealing that context encoder can learn some implicit semantic information from the dataset. The second î€›gure shows the context embedding clustering within the hybrid dataset. All task context embeddings are divided into two clusters which exactly correspond to two datasets - MovieLens20M and Taobao. In Figure 4, we display the activated route visualization of two diî€erent tasks for soft modularization, where darker color represents higher probability. Due to the low-dimension property of soft modulairzation, we can easily know what modules are activated for a speciî€›c task. The task-level context and route visualization can successfully validate the appealing property of better interpretability of CMML. Method Recall@10 Recall@20 Recall@20 Recall@50 In this section, we conduct ablation study to illustrate the eî€ectiveness of the context encoder, hybrid context generator, and context modulation network. In Table 9, we show the ablation results for diî€erent context encoder and hybrid context generator on Moviesen20M dataset. All results here are based on î€›nal weight modulation. Firstly, in order to validate the importance of useful interaction history, we conduct one setting called â€™Non-negative-PEâ€™, which means we remove the negative user-item pairs in the support-set and utilize pooling aggregated encoder. The comparison between â€™Non-negative-PEâ€™ and â€™PEâ€™ shows without useful interaction history, it will be hard to generate scenario-speciî€›c context and further modulate the network. The comparison between pooling aggregated encoder(PE) and sequential aggregated encoder(SE) shows context extraction requires high representation ability for context encoder. The comparison between SE and SE plus hybrid MLP generator shows the importance of building hybrid context for speciî€›c instance example. And between hybrid MLP generator and hybrid dot generator, the latter one is not only free of additional parameters, but also achieves better performance. Low-order interaction is enough to build the relationship between task-level context and instanceâ€™s features. Thus, we set sequential aggregated encoder plus hybrid dot generator as the default setting for CMML. Table 5 shows the ablation study on diî€erent approaches for modulating the model on MovieLens-20M dataset and Taobao dataset. The method linear correpsonds linear weight modulation. S-LM and F-LM denote Sigmoid based and FiLM based layer modulation respectively. And Soft-M represents the soft modulation approach. In MovieLens-20M dataset, all modulation ways except S-LM achieves comparable results. In Taobao dataset, F-LM and Soft-M achieve better results compared with rest two approaches. The complexity of dataset will inî€uence the need for modulation networkâ€™s capacity. For relatively simple Movielen dataset, the ability of linear weight modulation is enough to handle the model adaptation, while in Taobao dataset, stronger network capacity for the model adaptation is required, and thatâ€™ why F-LM and Soft-M achieve better results. There are many works [1â€“3,29,30,36] showing that by leveraging extra explicit contextual information like time, location, or usersâ€™ proî€›le, the recommendation algorithms can improve performance. Compared with traditional algorithms that map user-item pairs into some scores, context aware recommendation framework will take additional context information as input. The diî€erence between our framework and context-aware recommendation is that we do not have access to any explicit contextual information. We believe the historical interaction information itself is already a great source of contextual information which can reveal characteristic of the recommendation entities. Our solution is also somewhat conceptually related with the domain-adaptation problem, where they focus on domain transfer from source domain to target domain, and invariant contextual invariant information is extracted to achieve domain transfer [18], [20]. CMML focuses more on the few shot cold-start recommendation problem. Meta Learning, also known as learning to learn, is a new rising machine learning paradigm for learning meta knowledge. Speciî€›cally, the meta learning framework aims at learning useful inductive bias, which is helpful when the available data is limited. There are many applications of meta learning, like [32] for one-shot classiî€›cation in computer vision, and [25] for few-shot meta reinforcement learning problem. One of the most classical algorithms is MAML [9], which aims at learning modelâ€™s initial parameters which are capable of adapting to new tasks with only a few gradient steps. There exists work [31] that incorporates the meta learning into recommendation problem. Many recent works introduce the framework of MAML into the cold-start recommendation problems. MELU [19] introduces the MAML framework into user-speciî€›c cold-start recommendation problems, in which it transforms the cold start recommendation problem for new coming users/items as new coming tasks in the setting of MAML.ğ‘†Meta [8] tries to tackle the coldstart problem of scenario setting by learning proper initial MLP parameters, better gradient controller, and an RL stop-controller for better inner-loop adaptation. MAMO [7] introduces task-speciî€›c memories and feature-speciî€›c memories to get rid of problems brought by global parameter sharing in MELU. [35] equips MAML with dynamic subgraph sampling, which can solve the problem for dynamic arrival of new users. [38] models the training process of recommender system as a meta learning problem and propose a framework for meta learning model retraining mechanism. [39] generates sequential recommendation results by similarity metric between query-set and support-set. In this paper, we propose a contextual modulation meta learning framework for solving cold-start recommendation problems with several algorithmic alternatives. We formulate the cold-start recommendation problem as a meta learning problem. By context encoder, hybrid context generator and modulation network, our framework can easily adapt to new tasks even with limited interaction examples. Extensive experiments on real-world datasets successfully validate that eî€ectiveness of CMML with much higher computational eî€œciency and interpretability. Also, the whole framework is completely compatible with the current practical industrial framework for broader application.