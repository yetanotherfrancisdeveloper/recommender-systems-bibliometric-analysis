Most of the existing recommender systems are based only on the rating data, and they ignore other sources of information that might increase the quality of recommendations, such as textual reviews, or user and item characteristics. Moreover, the majority of those systems are applicable only on small datasets (with thousands of observations) and are unable to handle large datasets (with millions of observations). We propose a recommender algorithm that combines a rating modelling technique (i.e., Latent Factor Model) with a topic modelling method based on textual reviews (i.e., Latent Dirichlet Allocation), and we extend the algorithm such that it allows adding extra user- and item-speciî€›c information to the system. We evaluate the performance of the algorithm using Amazon.com datasets with diî€erent sizes, corresponding to 23 product categories. After comparing the built model to four other models we found that combining textual reviews with ratings leads to better recommendations. Moreover, we found that adding extra user and item features to the model increases its prediction accuracy, which is especially true for medium and large datasets. â€¢ Information systems â†’ Re commender systems; Personalization; Retrieval tasks and goals; e-commerce, LDA, Latent Factor Model, recommender systems, textual reviews ACM Reference Format: Tatev Karen Aslanyan and Flavius Frasincar. 2021. Utilizing Textual Reviews in Latent Factor Models for Recommender Systems. In The 36th ACM/SIGAPP Symposium on Applied Computing (SAC â€™21), March 22â€“26, 2021, Virtual Event, Republic of Korea. ACM, New York, NY, USA, 10 pages. https://doi.org/10. 1145/3412841.3442065 Throughout the last decade, the importance of the Web as a medium for business and electronic transactions has increased drastically, forcing the IT to rapidly develop as well, making humans daily life much easier and more eî€œcient. On its turn, this large development in IT has increased the popularity of online shopping and services. Making purchases online instead of buying products from physical shops, which can be very time-consuming, is one of the major consequences of IT development. However, this large increase in online sales has not only led to an increase in the number of customers but also an increase in the number of products and variety of these products. Therefore, when making purchase decisions, users are forced to process large amounts of information. According to [18] this information overload has a big impact on the human decision process and quality. Hence, it aî€ects peopleâ€™s online purchase experience signiî€›cantly. Therefore, to overcome this problem, one usually rely on suggestions from others, who have more experience on the topic [2]. This idea is used in the recommender systems aiming to employ various sources of information to recommend products to the users by inferring their interests. Besides solving the problem of information overload, the use of recommender systems also results in increased sales, customer satisfaction and loyalty [27], which explains increasing popularity of these systems. On the one hand, the information overload motivates the use of recommender systems in order to make the usersâ€™ online purchases more convenient. On the other hand, the increasing variety of ways that users can discover, evaluate, and review online products motivates companies and researchers to create even more revealing recommender algorithms, which will enable to sell more products. The Web enables users to provide their personal feedback about the product that they have purchased in the form of ratings and textual reviews. Assuming that the past interests and preferences are often good proxies of future choices, the previous interactions between items and users can be used for predicting which items might be interesting for a user in the future. Therefore, in order to correctly recommend the users their desired products, one should predict how the user will respond to a new product [1]. Recommender systems are usually categorized as: Collaborative Filtering systems based on ratings data [26], Content-Based systems based on content (often textual) data [21], and Hybrid systems that combine these two types of systems [7]. Most of the existing recommender systems are of the î€›rst type (based only on ratings), and they ignore the enormous information incorporated in the usersâ€™ review texts [31]. Ignoring such an important source of information, that can potentially increase the accuracy of recommendations, seems not optimal. Moreover, adding extra user- and item-speciî€›c information, not included in the ratings or textual reviews, to the recommender system might also increase the quality of its recommendations [6, 12, 34]. Figure 1 presents the percentage of items having less than 10 ratings and more than 30 words in their review text per product category in the datasets of the largest e-commerce Amazaon.com [23]. We observe that for almost all product categories it holds that at least 80% of items have very few ratings (less than 10) while over 40% of items have long textual reviews (with more than 30 words per review). Therefore, textual reviews can be considered as a potential source of information that can used to complement the absent ratings to increase the prediction accuracy of the recommender system. In this paper, we propose a recommender algorithm that is based on product ratings as well as textual reviews of customers, and it allows adding extra user- and item-speciî€›c information in order to make product recommendations. We focus on Latent Factor Models (also called Matrix Factorization) for modeling the item features and user preferences in a shared topic space and Topic Models (more precisely Latent Dirichlet Allocation) for modelling review features. Although, there exist a large amount of literature regarding recommender systems that are based on a single type of data, such as ratings or textual reviews, there have been only few attempts of combining user-item ratings and textual reviews to uncover the latent rating and latent review dimensions [4,11,20,22,30]. [11] combined the predictions of the Latent Factor Model (LFM) with the predictions of the neighborhood model to generate more accurate recommendations. A similar approach was taken in case of the recommender system of â€˜Bellkorâ€™s Pragmatic Chaosâ€™, the Netî€ix Prize contest winner [17]. This system compares the watching and searching habits of similar users, and then recommends movies that share the characteristics with movies that are highly rated by the current user. Since then, LFM became the most popular Collaborative Filtering (CF) technique used for both rating and item recommendations [25]. [30] have developed an algorithm called Collaborative Topic Modeling, combining CF and probabilistic topic modeling, which recommends scientiî€›c papers to an online community of users. Authors found that the proposed recommender system, based on both contents of articles and usersâ€™ ratings, performs better than the recommender system based on standard Matrix Factorization (MF) methods. Among all the Hybrid recommender systems, one of the most known systems combining ratings with textual reviews for making recommendations is the Hidden Factors and Topics (HFT) algorithm proposed in [22]. HFT combines latent rating dimensions learned by LFM, with latent review topics learned by the topic modeling technique Latent Dirichlet Allocation (LDA), in order to make rating predictions. [22] stated that, the HFT algorithm results in highly interpretative textual labels for the hidden rating dimensions helping to â€˜justifyâ€™ ratings with review text, and in increased prediction accuracy of the recommender system. Another example of a recommender algorithm that combines ratings with textual reviews has been introduced in [20], called Ratings Meet Reviews (RMR). The proposed method is a probabilistic generative model combining the topic modeling technique LDA with the MF method for ratings. The main diî€erence between HFT and RMR is the way the authors combine the two models. More speciî€›cally, HFT uses the MF method to model the ratings, whereas RMR uses a mixture of Gaussian distributions. [20] found that RMR outperforms the standard MF based approach and results in similar prediction accuracy compared to HFT. The TopicMF algorithm introduced by [4] is also an example of a recommender algorithm combining ratings and reviews in order to make recommendations for the users. TopicMF uses biased MF for modeling the ratings and uses Non-negative Matrix Factorization (NMF) for modeling the latent topics in the textual reviews. The main diî€erence between this algorithm and the earlier mentioned recommender algorithms is that it uses NMF instead of LDA as the topic modeling approach. The î€›nal example related to the model introduced in this study is the Rating-Boosted Latent Topics (RBLT) algorithm introduced by [29]. RBLT used LDA for extracting topics from the reviews like HFT and RMR and it also uses MF for modeling the ratings like HFT. The main diî€erence between RBLT and HFT is that HFT uses item-features in rating prediction and topic-distributions as a regularization for these item-features, whereas the RBLT includes the topic-distributions in the rating prediction procedure but not in the regularization term. [29] found that adding textual reviews to the CF system increases its prediction accuracy signiî€›cantly. One similarity that is shared by the previously surveyed papers is that they propose to use textual reviews as well as ratings to model item features and user preferences in a shared topic space and consequently bring them into LFM to generate recommendations. Our research will also be focused on utilizing recommender systems with the MF approach by using product ratings as well as textual reviews of customers. There have been also few attempts of building a recommender system that allows adding user- or item-speciî€›c characteristics, not present in the rating or review data [6], [12], [34]. [6] and [12] introduced CF recommender systems that also allow adding user- and item-speciî€›c features on the top of the ratings. As extra user and item information [12] used the browsing data. The CF system extension in [12] has been done by adding extra rows and columns to the user-item rating matrix. However, all these extended recommenders that allow adding user or item features to the system, are based only on ratings. To our knowledge, there are no studies of recommender systems combining ratings and textual reviews that also allow adding extra user or item information to the system. Another limitation of the existing literature is that most of the proposed recommender systems are modeled and implemented on a dataset consisting of very few product categories or a small number of observations. To address the previously identiî€›ed limitations we propose a recommender algorithm called LDA-LFM, which combines the topicmodeling technique LDA with the rating-modeling method LFM and allows adding (latent) extra user- and item-speciî€›c features to make recommendations. LDA-LFM is a generalization of the HFT model proposed by [22], but it will use an alternative approach for model regularization and will allow adding extra user- and itemspeciî€›c features to the recommender system. These extra features will behave as additional factors in MF driving the ratings following the approach proposed in [12], while these extra features do not appear in the topic modelling method LDA. This system is applied on both small and large datasets, with or without a large number of product categories. In this section, we introduce all models and techniques used to build and evaluate the proposed LDA-LFM model. We describe the technical details and optimization approach of LFM and the topic modeling technique LDA used in this study. In CF recommender systems, the Latent Factor Model (LFM), also called Matrix Factorization, has become very popular especially after the earlier mentioned Netî€ix Prize Contest [14,15]. Usually, the rating matrix contains lots of missing elements, thus suî€ers from the sparsity problem. In order to overcome this problem, LFM uses the idea of dimensionality reduction to estimate and î€›ll in all missing entries of the sparse user-item rating matrix. The goal of dimensionality reduction is to rotate the axis system such that the pairwise correlations between dimensions can be removed and a large sparse matrix can be decomposed into smaller and dense matrices. Accordingly, the reduced, rotated, and complete data matrix representation can be eî€œciently estimated from a sparse data matrix. The key idea of the Matrix Factorization method is that any m x n sparse matrix R with rank k<min{m,n} can be approximated by rank-k matrices in the following way [28]: where P and Q are m x k and n x k matrices, respectively. So, the user-item sparse matrix R is approximately equal to the product of P and Q matrices, such that the vectors of R can be represented by the rows of matrix P and columns of matrix Q. Stated diî€erently, in LFM, sparse ratings matrix R is decomposed into the product of two low-rank rectangular matrices P, the user matrix, and Q, the item matrix, where both P and Q have the same rank k. Each row of matrix P and each column of matrix Q are referred aslatent factors. Let us deî€›ne by ptheuth row of user matrix P, the user factor representing the aî€œnity of user u towards the rating matrix R, and by qtheith row of item matrix Q, the item factor representing the aî€œnity ofith item towards the rating matrix R. Since, some users have a tendency to give higher ratings while other users are more prone to provide lower ratings, and that some products have a tendency to be highly rated compared to other products, baseline predictions (biases) should also be taken into account. [16] referred to biases as the observed variation in rating values due to the eî€ects associated with either items or users independent of any interaction. Correspondingly, the estimate of each rating of theuth user about ith item, denoted by r, can be expressed as follows: whereğ›¼represents the global average of all ratings (an oî€set parameter), band brepresent the user and item biases, respectively. Accordingly, the error which arises in this estimation is deî€›ned as e= r-Ë†ğ‘Ÿand in order to learn the latent factors pand q the following optimization problem should be solved, where we minimize the regularized squared error [17]: arg min1| T |(ğ‘’)+ ğœ†Î©(Î˜) arg min1| T |(ğ‘Ÿâˆ’ (ğ›¼ + ğ‘+ ğ‘+ ğ‘ğ‘))+ ğœ†Î©(Î˜) Trepresents the corpus of all ratings (in the training set),Î˜= {ğ›¼, b, b, p, q} is the parameter space of the model. The objective function in Equation 3 can be seen as quadratic loss function which quantiî€›es the loss of accuracy when an element of rating matrix R is approximated by low-rank factorization.âˆ¥.âˆ¥represents the squared Frobenius norm, also called the Lnorm. We use regularization to prevent model overî€›tting, which is required especially when the dataset used for î€›tting the model contains a large number of features, like it is in our case. The constantğœ†from Equation 3, which is often referred as the regularization constant, determines the level of regularization and controls how hard unnecessary features in the model are penalized. Determining the value ofğœ†is a trade-oî€ between prediction variance and bias. One popular way of determining the optimal value is Grid Search. One of the most popular ways of solving the optimization problem deî€›ned in Equation 3 is Stochastic Gradient Descent (SGD) [16,22,32,33]. Other typical methods which can be considered as possible alternatives to the SGD method, like the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) [35] or Orthant-Wise Limited-memory Quasi-Newton (OWL-QN) [3], work very slowly when the model is î€›tted on a large training dataset and performing it by one machine is sometimes intractable. SGD addresses these issues because it scales well with both big data and with the size of the model, therefore it is preferred in this analysis. However, even though the method itself is simple and fast, it is known as a â€œbad optimizer" because it is prone to î€›nding local optimum instead of a global optimum. A popular technique designed to improve the performance of SGD method is the Adaptive Moment Estimation (Adam) introduced by [13]. Adam is the extended version of the SGD (with momentum). The main diî€erence compared to the SGD (with momentum), which uses a single learning rate for all parameter updates, is that Adam algorithm deî€›nes diî€erent learning rates for diî€erent parameters. The algorithm calculates the individual adaptive learning rates for each parameter based on the estimates of the î€›rst two moments of the gradients. Each text review provided by a user, represented as a bag of words, contains valuable information, which can potentially increase the prediction accuracy of the recommender system. For this reason, the textual reviews should be modeled and analyzed. Latent Dirichlet Allocation (LDA) introduced by [5] is one of the most popular text mining methods in the context of recommender systems. Therefore, we will use LDA as a topic modeling technique in this analysis in order to uncover the hidden dimensions in the user review texts. There are three main entities deî€›ned in this method: words, documents, and corpora. The entitywordis deî€›ned as a basic unit of a discrete data from a vocabulary,ğ‘¤where jâˆˆ{1,2,...,N}, which indicates the index of the word in document d. These words are represented in the form of a vector where thejth element of this vector takes value 1 and remaining all elements take value 0. The entitydocumentis a sequence of N words denoted byğ‘‘ âˆˆ Tsuch that Nrepresents the number of words in document d. Finally, the entitycorpusis deî€›ned as a collection of documents denoted by T= (d,d,...,d), where M is the number of all documents in the corpus. For simplicity we will use indices to identify documents. LDA makes a few important assumptions regarding the model. Firstly, it assumes that words carry strong semantic information and that documents discussing similar topics will use similar words. Therefore, latent topics are discovered by identifying a bag of words in a corpus that frequently occur together in a document. Secondly, LDA assumes that documents are probability distributions of latent topics and topics are probability distributions of words. So, every document consists of a certain amount of topics and each of these topics is a distribution of words. Therefore, the model assumes that there are in total K latent topics. Then, LDA assigns to each document d a K-dimensional topic distributionğœƒdrawn from a Dirichlet distribution represented in the form of a stochastic vector, such that thekth entry of it,ğœƒ, represents the fraction of words in document d which discusskth topic. Stated diî€erently, the likelihood that words in document d will be about topic k is equal toğœƒ. Furthermore, each topic k is a distribution of words represented byğœ™such that each word has a particular likelihood of being used in the topic k. Let us denote by zthe topic assigned to thejth word in document d. Then the LDA model is deî€›ned as follows: â€¢ ğœƒâˆ¼ DIR(ğ›¾) with d âˆˆ {1,...,M} â€¢ ğœ™âˆ¼ DIR(ğœˆ) with k âˆˆ {1,...,K} â€¢ zâˆ¼ Multinomial(ğœƒ) â€¢ wâˆ¼ Multinomial(ğœ™) whereğ›¾represents the parameter of the Dirichlet distribution for document-topic distributionğœƒandğœˆis the parameter of the Dirichlet distribution for word-topic distributionğœ™. zrepresents the topic assigned to the jth word in document d. We assume that the total number of words in vocabulary is V. Moreover, we denote the likelihood function of zconditional on topic mixture of document d, ğœƒ, p(z| ğœƒ) as follows: Consequently, the probability ofjth word in document d, w, conditional on the chosen topic zdenoted byğ‘ (ğ‘¤| ğ‘§, ğœˆ)is deî€›ned as follows: Furthermore, using the deî€›nition of the Dirichlet probability distribution, the conditional topic distribution is deî€›ned as follows: whereğœƒ>0 andÎ“ (Â·)represents the Gamma function. Consequently, the joint distribution of a topicğœƒ, K topics z, and N words w is deî€›ned as follows: ğ‘ (ğœƒ, ğ‘§, ğ‘¤ | ğ›¾, ğœˆ) = ğ‘(ğœƒ | ğ›¾ )ğ‘ (ğ‘§| ğœƒ)ğ‘ (ğ‘¤| ğ‘§, ğœˆ)(7) where N =ÃN. Using the properties of discrete and continuous random variablesâ€™ distributions, the marginal distribution of document d is deî€›ned as follows: ğ‘ (ğ‘¤ | ğ›¾, ğœˆ) =ğ‘ (ğœƒ | ğ›¾)ğ‘ (ğ‘§| ğœƒ)ğ‘ (ğ‘¤| ğ‘§, ğœˆ)ğ‘‘ğœƒ Consequently, using Equations 4, 5 and 8 the likelihood of a text corpusTconditional on the word distributionğœ™, topic distribution ğœƒand topic assignments z is deî€›ned as follows: ğ‘ (T | ğ›¾, ğœˆ, ğ‘§) =ğ‘ (ğœƒ| ğ›¾)ğœƒğœ™ğ‘‘ğœƒ This expression can also be rewritten in terms of the topic distribution ğœƒand word distribution ğœ™, in the following way: where parametersğœƒandğœ™should be estimated, which we denote byÎ¦, such thatÎ¦= {ğœƒ, ğœ™}. Then, the log-transformation of the conditional corpus probability p(T | ğœƒ, ğœ™, z) is deî€›ned as follows: Typically, in order to estimate the LDA model parameters, Variational Bayesian (VB) methods or sampling approaches based on Markov Chain Monte Carlo (MCMC) sampling are being used [5,8]. Figure 2 visualizes the dependencies among the LDA model parameters. Highğ›¾indicates that it is likely that each document contains a mixture of most of the topics. Conversely, lowğ›¾indicates that each document contains only few of the topics. Furthermore, highğœˆindicates that each topic contains most of the words of that topic, whereas smallğœˆmeans that each topic contains only small amount of words. The parametersğ›¾andğœˆare at thecorpus level which are both assumed to be sampled once in the process of corpus generation. The random variableğœƒis the only variable at the document level, sampled once per document. Finally, the variables zand ware at the word level sampled once for each word per document. The model that we design, called Latent Dirichlet Allocation-Latent Factor Model (LDA-LFM), aims to combine two main core ideas of two methods discussed in Sections 3.1 and 3.2 to to uncover both hidden dimensions in ratings and textual reviews, respectively. As it was mentioned earlier, one of the three entities on which topic modeling is based on, is the document entity. Therefore, the concept of â€˜documentâ€™ in the LDA-LFM model should be deî€›ned properly. There are diî€erent ways of deî€›ning this concept which should be based on the textual reviews. One can simply consider each text review of user u and item i as a document, denoted by d. On the other hand, one can deî€›ne a document as a set of all reviews corresponding to item i, denoted by d. Finally, the set of all reviews provided by a user u as a document, denoted by d. [22] found that the second deî€›nition, where the concept of a document is deî€›ned as the set of all reviews of item i (d), leads to the best model performance. The motivation behind this choice is that when users provide feedback about the products in terms of textual reviews, they discuss more often the characteristics of the product rather than discussing their personal preferences. Therefore, we will deî€›ne the concept of documents in LDA-LFM in the similar way as in [22]. The idea behind the LDA-LFM model is to î€›nd the K-dimensional topic distributionğœƒof each item using textual reviews of item i which shows the extent to which each topic k is discussed across all the reviews for item i. Consequently, these topic distributions are used as item-factors in combination with user-factors in the Latent Factor Model to fully predict all user-item ratings. In Section 3.1 we stated that parameter qis the rating factor possessing the properties of item i that can be reviewed by users, whereas in Section 3.2 we stated that parameterğœƒis the topic distribution of words that appear in those reviews. Assuming that, if an item i has a certain property, then it will correspond to a particular topic discussed in that itemâ€™s textual review, such that qandğœƒare positively correlated, we need to deî€›ne the exact relation between these two parameters. However, qandğœƒcannot be considered as being equal since the topic distributionğœƒis a stochastic vector describing topic probabilities while latent item factor qcan take an arbitrary value inR. Stating that qis a stochastic vector likeğœƒ would result in a loss of power in the proposed model and changing the structure of the topic distributionğœƒto make it more similar to qwill lead to the loss of probabilistic power in the model. In order to not encounter these problems, the transformation of qtoğœƒÃ should satisfy monotonicity, qâˆˆ R, andğœƒ= 1 assumptions. The following transformation satisî€›es all these criteria: where the parameterğœ…controls for the reaching of the highest possible value of the transformation, often called â€˜pickinessâ€™ parameter. Large value ofğœ…indicates that users discuss only the most important topic, whereas smallğœ…indicates that users discuss all topics equally. We deî€›ne the transformation, in such a way that, whenğœ… â†’ âˆ,ğœƒâ†’ ğœ„(unit vector with 1 for the largest value ofğ‘), and, whenğœ… â†’0,ğœƒconverges to a uniform distribution. To make sure that the word distribution for topic k (ğœ™) is a stochastic vector, the following transformation ofğœ™is deî€›ned with an introduction of a new variable ğœ“ : whereğœ“âˆˆRis used as a natural parameter for the topic distributionğœ™âˆˆR, where V is the size of the vocabulary. Correspondingly,Ã it holds thatğœ™= 1. Then the objective function of the LDALFM model is deî€›ned as follows: âˆ’ ğœ‡ğ‘™ (T | ğœƒ, ğœ™, ğ‘§) whereÎ˜= {ğ›¼, b, b, p, q} andÎ¦= {ğœƒ, ğœ™} represent the set of parameters of the LFM and LDA model, respectively. The î€›rst term of Equation 14 represents the prediction error corresponding to LFM, the second term represents the regularization of model parameters b, b, pand the third term represents the log-likelihood of the corpus of ratings and users from Equation 11. The parameterğœ‡ âˆˆ ğ‘… trades-oî€ the importance of these two eî€ects. We observe that in the LDA-LFM model, the regularization of qis diî€erent compared to the standard Matrix Factorization case, the regularization term does not contain the norm of q. More speciî€›cally, the third term of Equation 14 behaves as a regularization for q[22]. As it was mentioned earlier, the proposed recommender system should allow adding extra user- and item-speciî€›c features. A key aspect in adding extra features to the system is to better describe users and items, in order to better predict the preferences of those users for diî€erent items. Examples of user features are user demographics such as age, living area, gender, occupation, etc. [9]. If our goal is to build a movie recommender, then the genre, year of its release, name of the director, can all be interpreted as item characteristics, which can be added to system for making better recommendations. [6], [12] and [34] introduced CF recommender systems that allow adding user- and item-speciî€›c features on the top of ratings. We will follow the approach of [6] and [12], who proposed adding extra rows and columns to the user-item rating matrix representing the extra features added to LFM. Figure 3 visualizes an example of the Matrix Factorization model extended with three extra features. The main idea is to add the same amount of both extra user- and item-speciî€›c features. This assumption is necessary because LFM, which is used as rating modelling technique in the LDA-LFM model, requires matrix multiplication of two matrices with dimensions NUsers x K and K x NItems. This matrix multiplication is only possible when the number of columns in user-factor matrix P is equal to the number of rows of item-factor matrix Q. The extra features denoted by Kfrom Figure 3 do not appear in the LDA model and represent non-review factors that aî€ect the review ratings. Our goal is to î€›nd the solution to the optimization problem of Equation 14, which is: where the corpusTis given. The LDA-LFM model deî€›nes the following uterative stochastic optimization procedure of two steps: for i in Niter Sample ğ‘§with ğ‘ (ğ‘§= ğ‘˜) = ğœƒğœ™ end for where Niter is the number of iterations, dâ‰¡drepresents the review or set of reviews (document) of item i by user u. In the î€›rst step of this optimization procedure from Equation 16 we î€›x the topic assignments for each word, i.e., the value of latent variable z and we solve the objective function with respect toÎ˜,Î¦ andğœ…. We use the Adam Optimizer for learning the rating related model parametersÎ˜= {ğ›¼, b, b, p, q}, but also the review related parametersÎ¦= {ğœƒ,ğœ™}, andğœ…. As it was mentioned earlier,ğœƒ âˆˆ Î¦ and qâˆˆ Î˜are linked through Equation 12. So, we do not use the textual reviews in order to î€›t the document-topic distributionğœƒ using the LDA approach. Instead, we determineğœƒusing q. Since, we introduced a transformation ofğœ™, to ensure that it is a stochastic vector, instead of learningğœ™we learn the parameterğœ“. Once we learn theğœ“, by using the transformation deî€›ned in Equation 13, the topic-word distributionğœ™can be determined. Moreover, using the same optimization approach, we also learn the parameter ğœ…. In the second step of this iterative procedure, using the updated parameter valuesÎ¦= {ğœƒ,ğœ™} determined in the î€›rst step by the Adam Optimization, we randomly assign a topic k to each word, with a probability that is proportional to the likelihood of the occurrence of that topic with that particular word [30]. That is, the topic assignment probability of assigning kth topic to a word w for user u, item i and in jth position p(z= k) is proportional to the product of topic probability for user u, item i (ğœƒ), and word probability used for that topic (ğœ™). We assume that the terms zand zare equivalent (zâ‰¡z). We iterate through all documents and word positions, d and j, respectively, in order to update the corresponding topics assigned to those terms. Finally, we repeat these two steps for Niter times and report the prediction accuracy of the model corresponding to the last iteration. As a prediction accuracy measure we use the Mean Squared Error (MSE) determined as follows: whereTrepresents the corpus of all ratings in the test set,ğ‘Ÿ represents the real rating from the test data for user u and item i, andË†ğ‘Ÿis the corresponding predicted rating. MSE can take only non-negative values. Moreover, a lower value of MSE is an indication of better performing model. It is worth to mention that the analysis is performed on a commodity machine with a Core i7 processor, 2.2 GHz frequency, and 252gb memory space using the programming language Python 3.7. In this research, we use a collection of datasets provided corresponding to the 23 product categories supplied by one of the largest e-commerce company in the world, Amazon.com. This data without duplicates was prepared by Julian McAuley. It consists of 142.8 million product reviews and a metadata for 9.4 million products, spanning a period of 18 years, from May 1996 to July 2014 [10,23]. The chosen dataset is of 5-core type, that is, the data set excludes all customers and products having less than 5 reviews. The review dataset includes feedbacks of Amazon customers in the form of ratings, textual reviews, and helpfulness score. Meanwhile, the metadata includes various characteristics of the product: price, brand, descriptions, category information, image features and links of â€˜also viewedâ€™ and â€˜also boughtâ€™ products. The raw review data, after removing duplicates and excluding users or items with less than 5 reviews, consists of 42.13 million reviews. Table 1 presents the general overview of the datasets of all product categories. We observe that all datasets are highly sparse and contain a very large amount of missing ratings. For almost all datasets it holds that the average star rating is approximately equal to 4. Moreover, the average number of words per review is at least 18 and at most 67. Finally, the smallest dataset, Musical Instruments, consists of 0.5 million reviews and the largest dataset, Electronics, consists of approximately 8 million reviews. In order to correctly evaluate the chosen model, we split the data into three datasets: training, validation, and test sets. We î€›t the model on the training data and î€›nd a set of optimal model parameters (hyperparmeter tuning) using the validation set. Finally, we use the test set for predicting the ratings and calculating model accuracy measures using the optimal set of parameters from the hyperparmeter tuning. For data separation we use the common 80/20 splitting rule. In order to have enough observations to correctly î€›t the model, we put 80% of all observations in the training set, while the remaining 20% we equally divided into the test and validation sets. However, splitting the data into training, test and validation sets, when some of the users and items appear only in the test set and not in the training set, will result in a loss of information about those users and items during the training of the model. Therefore, after randomly splitting the data into train and test set, we make sure that there is no user or item that is present in the test set but not present in the training set by removing these. For implementing the topic modeling technique LDA, the review data should be cleaned. Therefore, we perform a few Natural Language Processing (NLP) tasks on the textual reviews in review tuples by using the Natural Language Tool Kit (NLTK) library of the programming language Python. Firstly, we apply tokenization to all review texts, which are provided as a group of sentences, and transform them into a group of words. Secondly, we transform them to lower case words and remove from these tokenized reviews the common English stop words and one-letter words. Subsequently, all special characters, digits, punctuation and single or multiple spaces are removed. Next, we apply lemmatization to the processed review text, for removing inî€ectional endings and holding the dictionary (base) form of a word only, known as the lemma of the word. Finally, we combine all those cleaned reviews corresponding to the same item and create a corpus of documents, where each document contains all reviews (represented in the form of a group of words) corresponding to one item. Diî€erent methods introduced earlier contain various parameters which should be initialized. We initialize the oî€setğ›¼by averaging over all ratings in the training set. Vectors b, band matrices P, Q are initialized using the random normal distribution. The î€›tting procedure of all models have been performed by the Adam Optimization with the learning rate 0.01. As initial value forğœ…we take the value 1, which will be updated by the Adam Optimization while î€›tting the model. For each model we run 35 iterations based on [4] (with 20 iterations), [15] (with 20-35 iterations), [24] (with 30 iterations) while updating model parameters inÎ˜,Î¦, andğœ…in each iteration. The prediction accuracy of the model is reported based on the last model corresponding to 35th iteration, assuming that the last model, after all the updates, is the best performing model. As a common practice, for the LDA model we set both parameters ğ›¾andğœˆequal to 0.1. Following the approach of [22] we perform the analysis with the number of latent factors in the LFM model (K) and number of topics in the LDA model (L) equal to 5. LDA-LFM contains two regularization parameters,ğœ†andğœ‡. We tune this set of two parameters using the Grid-Search method, which î€›ts the model for every speciî€›ed combination of these two parameters and evaluates each of these models using validation set. As a result, the most accurate model speciî€›cation, per product category, is then used in the main model prediction applied to the test dataset. Following the approach of [22], forğœ†we use values {0, 0.001, 0.01, 1, 10} as a possible values in the Grid-Search, while for regularization constantğœ‡we use the values {1, 10, 100, 1000, 10,000}. As it was mentioned in Section 3.3, we set the number of documents in the LDA model equal to the number of items in the data, where each document represents all reviews of an item in the training set. We set the size of the vocabulary equal to 5000, by keeping most 5000 frequent words from the corpus of all documents built from the item reviews present in training set. In order to test for the performance of the proposed LDA-LFM model, we use four other models based on diî€erent algorithms. Then we compare the prediction accuracy of the LDA-LFM model with the performance of the following models: Oî€set Model:the predicted rating for all users is the same and is equal to the global average ğ›¼. Baseline Rating Model:the predicted ratingË†ğ‘Ÿ=ğ›¼+Â¯ğ‘Ÿ+Â¯ğ‘Ÿ withğ›¼representing the global average rating,Â¯ğ‘Ÿthe average diî€erence between user ratings and the global averageğ›¼,Â¯ğ‘Ÿrepresents the average diî€erence between item ratings and the global average LFM:standard Latent Factor Model model corresponding to Equation 2. LDAFirst:in this model the user feedback in the form of ratings will be used as an input for the standard LFM, while the textual reviews will be used as an input for the LDA model described in Section 3.2. The key diî€erence between this method and the proposed method LDA-LFM is that, in LDAFirst the topic-distributions ğœƒare sampled from a Dirichlet distribution, where each document is treated as the set of all reviews corresponding to item i, and they are used to set the qvalues, which stays constant while modeling the ratings. Thus we do not learn the qparameter of LFM during the iterative optimization procedure and we only update the parameters b, band pusing the Adam Optimization. In the LDA-LFM model, we do not use the LDA method for determining the topic-distributionsğœƒ. After that we sample the word topics, we learnÎ¦= {ğœƒ,ğœ™} using Adam Optimization as in Equation 16 (where qis dependent onğœƒby means of Equation 12). Since we start this method by the LDA model and use its output (ğœƒ) as an input for the LFM method (ğ‘), we refer to this method as LDAFirst. Firstly, we perform the analysis for the case when no extra features are added to the LDA-LFM model, assuming that the number of topics in the LDA model is equal to the number of latent factors in the LFM model with Kâˆˆ{5,10} [22]. Correspondingly, in order to analyse the impact of adding extra latent features (user- and item characteristics) on the performance of recommender system based on the proposed LDA-LFM model, such that the number of topics in LDA has value 5 and the number of latent factors with 4 diî€erent values of extra features Kâˆˆ {1, 2, 3, 4}. Table 2 presents the prediction per product category with the number of topics equal to 5. We observe that for the majority of supplied datasets it holds that the Oî€set and Baseline models perform the worst, with large MSE values, compared to the LFM, LDAFirst, and LDA-LFM models. Only for Video Games and Tools and Home Improvements datasets the Oî€set method performs better than the LFM and LDAFirst models. Moreover, we observe that compared to the Oî€set and Baseline models, standard LFM improves the recommender systems prediction accuracy for almost all datasets, except datasets Patio, Lawn and Garden, Video Games, and Tools and Home Improvement. This can be seen by the large diî€erence between the MSE values corresponding to LFM, and MSE values corresponding to the Oî€set and Baseline models. We also observe that the MSEâ€™s corresponding to the LFM and LDAFirst models are very close to each other for the majority of datasets. This means that the LDAFirst model does not improve the prediction accuracy a lot compared to the standard LFM model. LDAFirst slightly outperforms LFM in case of the datasets Baby,Oî€œce Products Grocery and Gourmet Food, Apps for Android, CDs and Vinyl. From Table 2 we observe that the LDA-LFM model outperforms all other models in almost all datasets, with its lowest MSE values. Last two columns of Table 2 present the percentage decrease in the MSE of the LDA-LFM model compared to the LFM and the LDAFirst models, respectively. Both improvement columns consist mostly of positive entries. We observe that the proposed LDA-LFM results in at least 0.24% (Health and Personal Care) and at most 14.12% (Kindle Store) improvement in prediction accuracy, compared to the standard LFM. From Imp.we observe that the proposed LDA-LFM results in at least 0.28% (Electronics) and at most 14.12% (Kindle Store) improvement, compared to the LDAFirst model. The improvement columns in Table 2 contain also few negative values which correspond solely to small datasets (Musical Instruments, Amazon Instant Video, Patio, Lawn and Garden). Table 2 also reports the average MSE over all datasets per model in case of K = 10. We observe that average MSEâ€™s per model with K = 5 and K = 10 are similar. Table 3 presents the prediction results in terms of MSE, per product category with the number of topics equal to 5 and extra added features. We observe that for almost all datasets there is at least one LFM-LDA model with extra feature(s) with higher prediction accuracy (lower MSE value compared to the corresponding MSE value in the K= 0 model, i.e., without extra features). Moreover, for the datasets Musical Instruments, Patio, Lawn and Garden, Automotive, Toys and Games, Health and Personal Care, Sports and Outdoors, CDs and Vinyls, Home and Kitchen and Movies and TV all four models with diî€erent number of extra features are performing better compared to the model without extra added features. However, there are few datasets (Amazon Instant Video, Oî€œce Products, and Beauty) for which it holds that adding extra features to the LDA-LFM model either does not change or worsens the performance of the model. We observe that all those datasets, for which adding extra features is not eî€œcient, are either very small or medium size datasets in the set of all 23 Amazon datasets used in this study. The last row of Table 3 presents the number of cases in which adding a particular amount of extra features leads to an increase in the prediction accuracy of the model. We observe that in all four cases (adding 1, 2, 3, and 4 extra features), the number of datasets with better performance are close to each other. More speciî€›cally about 15 out of 23 datasets (from which around 9 cases corresponds to a medium or a large dataset), adding extra features results in more accurate recommendations. Taking into account the current limitations in the existing literature, in this paper, we utilize the LFM model by using both ratings and textual reviews of customers, such that it is applicable on both small and large datasets and also allows adding user- and item-speciî€›c data to it. We have shown how one can combine review-based LDA for topic modeling, with rating-based LFM for rating predictions. From the prediction results where we compared the prediction accuracy of the proposed LDA-LFM model to the prediction accuracyâ€™s of various baseline models applied on various datasets. We found that adding textual reviews to the recommender system leads to an increased prediction accuracy, which is especially true for medium and large datasets. Then we introduced an approach of adding extra latent features to the user-item rating matrix of the proposed LDA-LFM model, representing the user- and item-speciî€›c features not present in the review data. We found that for the majority of datasets (15 out of 23 datasets) it holds that, adding extra features to the proposed recommender system increases the quality of its recommendations, resulting in lower MSE, thus higher prediction accuracy. This indicates that again the improvements are better visible in medium and large datasets. As future work we would like to investigate whether using sentiment analysis improves the quality of recommendations. Correspondingly, one can extend our model so that it combines the rating modeling, topic extraction, and sentiment analysis techniques for making recommendations. For instance, sentiment analysis can be used to classify whether a review is negative or positive. [36] proposed a recommender system combining rating based CF system with sentiment analysis for making recommendations. [19] introduced the Joint Sentiment/Topic (JST) model which combines the topic modeling method LDA with sentiment analysis in order to detect a topic and a sentiment from text simultaneously. For future work, we aim to combine the JST model with our model for potentially better recommendations. In this way, we can exploit topics that carry sentiment and are possibly better proxies for ratings.