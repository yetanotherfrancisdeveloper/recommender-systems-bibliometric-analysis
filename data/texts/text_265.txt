Social recommendation based on social network has achieved great success in improving the performance of recommendation system. Since social network (user-user relations) and user-item interactions are both naturally represented as graph-structured data, Graph Neural Networks (GNNs) have thus been widely applied for social recommendation. Despite the superior performance of existing GNNs-based methods, there are still several severe limitations: (i) Few existing GNNs-based methods have considered a single heterogeneous global graph which takes into account user-user relations, user-item interactions and item-item similarities simultaneously. That may lead to a lack of complex semantic information and rich topological information when encoding users and items based on GNN. (ii) Furthermore, previous methods tend to overlook the reliability of the original user-user relations which may be noisy and incomplete. (iii) More importantly, the item-item connections established by a few existing methods merely using initial rating attributes or extra attributes (such as category) of items, may be inaccurate or sub-optimal with respect to social recommendation. In order to address these issues, we propose an end-to-end heterogeneous global graph learning framework, namely Graph Learning Augmented Heterogeneous Graph Neural Network (GL-HGNN) for social recommendation. GL-HGNN aims to learn a heterogeneous global graph that makes full use of user-user relations, user-item interactions and item-item similarities in a uniî€›ed perspective. To this end, we design a Graph Learner (GL) method to learn and optimize user-user and item-item connections separately. Moreover, we employ a Heterogeneous Graph Neural Network (HGNN) to capture the high-order complex semantic relations from our learned heterogeneous global graph. To scale up the computation of graph learning, we further present the Anchor-based Graph Learner (AGL) to reduce computational complexity. Extensive experiments on four real-world datasets demonstrate the eî€ectiveness of our model. â€¢ Information systems â†’ Recommender system. social recommendation, graph learning, graph neural network ACM Reference Format: Yiming Zhang, Lingfei Wu, Qi Shen, Yitong Pang, Zhihua Wei, Fangli Xu, Ethan Chang, and Bo Long. 2018. Graph Learning Augmented Heterogeneous Graph Neural Network for Social Recommendation. In WXXXX, June 03â€“05, 2021, XXX, XX. ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/1122445.1122456 Recent years have witnessed the rapid development of social recommendation, which leverages social network as side information to eî€ectively alleviate the problem of data sparsity [17,27]. Conceptually, usersâ€™ preferences are often largely inî€uenced by people around them [3,7], including parents, friends, classmates, and so on. Therefore, a social recommendation system based on usersâ€™ social relationships usually signiî€›cantly improves the quality of recommendations. Recently, there are a surge of interests in graph neural networks (GNNs) [10,14,19,24,26,31], which have been proven to eî€ectively learn node representations from graph-structured data. Since social network (user-user relations) and user-item interactions are both naturally represented as graph-structured data [5,28], GNNs have thus been widely employed to learn the representations of users and items, which has been shown to improve the performance of the recommendation system [11,15,29,33]. Furthermore, in order to enrich the types of the potential graphs and extract richer side information, a few existing works have considered the construction of extra item-item graph structure [6, 12, 30]. Despite the promising results current methods have achieved, there are still several severe limitations in their approaches. First, few existing GNNs-based methods have considered a single heterogeneous global graph which takes into account user-user relations, user-item interactions and item-item similarities simultaneously. As a result, these methods may fail to capture high-order crosssemantic information and limit the delivery of messages. Second, previous methods tend to overlook the reliability of the original user-user social graph that may be noisy and incomplete, partially because that the original connections often only record the social relationships between users but rather reî€ect the similarity of users preferences. For example: Bob is Kettyâ€™s husband, Jim is Kettyâ€™s colleague, Bob and Jim both like sports, while Ketty likes to read. However, in the user-user graph, Bob and Jim are not directly connected, while they are connected to Ketty respectively. We can learn from this example that there may be conî€icts in interests between nearby neighbors, while distinct neighbors could have similar preferences. Such a topology will make the users preferences extracted from user-user graph deviate from the real situation, which may lead to the sub-optimal performance of downstream task. Third, previous methods do not fully exploit the relationships between items. Though a few existing methods attempt to construct the item-item graph, they only utilize the itemsâ€™ initial rating attributes or extra attributes (such as category) in an ad-hoc fashion, which barely reî€ects the optimized item-item graph structure with respect to downstream social recommendation. In order to address these issues, we propose an end-to-end heterogeneous global graph learning framework, namely Graph Learning Augmented Heterogeneous Graph Neural Network (GL-HGNN) for social recommendation. Our GL-HGNN aims to learn a heterogeneous global graph that makes full use of user-user relations, user-item interactions and item-item similarities in a uniî€›ed perspective. In order to obtain and optimize heterogeneous global graph structure, we present a Global Graph Learning module. To this end, we î€›rst establish item-item subgraph by calculating the similarity of the rating vectors. Then, our proposed Graph Learner (GL) method is employed to extract richer implicit relationships and î€›lter out the explicit noisy edges in user-user relation subgraph and itemitem similarity subgraph. Speciî€›cally, our GL method can learn the implicit connections between nodes by measuring the embeddings similarity between target nodes in each mini-batch and all nodes. We then combine the learned implicit graph with the initial graph to obtain the reî€›ned heterogeneous global with respect to downstream task. To capture the high-order complex semantic relations from different types of edges in the heterogeneous global graph, we also present a Heterogeneous Graph Neural Network (HGNN), to model the reî€›ned heterogeneous global graph. Technically, it is crucial to scale up the computation of user-user subgraph and item-item subgraph learning, especially when the number of users or items is very large. To this end, we further utilize anchor-based approximation technique [4] to design a scalable Graph Learner, namely Anchor-based Graph Learner (AGL). By selecting the anchor node set instead of all nodes to calculate the similarity to the target nodes, we can signiî€›cantly reduce computational complexity. In addition, we design a joint learning method and a hybrid loss which considers both graph learner loss and rating loss. Through multiple epochs of optimization, we can get more reî€›ned heterogeneous global graph structures with respect to social recommendation, and more reliable vector representations of users and items. To summarize, we highlight our main contributions as follows: â€¢We construct a heterogeneous global graph with diî€erent semantic meta-paths for social recommendation. We propose a novel framework named GL-HGNN to learn the heterogeneous global graph of diî€erent relationships in a uniî€›ed perspective, which can capture the complex semantic relations and rich topological information. â€¢We propose the Global Graph Learning module to construct item-item connections and optimize both user-user and itemitem subgraph structures, so as to obtain the reî€›ned global graph with respect to the downstream social recommendation. In addition, we design an Anchor-based Graph Learner (AGL) method to scale up the proposed method, which can signiî€›cantly reduce computational complexity. â€¢We conduct experiments on four real-world datasets. The experimental results demonstrate the eî€ectiveness of our proposed model over state-of-the-art methods, and also verify that our scalable AGL module can reduce the computational costs. With the popularity of social platforms, social recommendation has become one of the hottest areas in recommendation research. Early research mainly employed matrix factorization-based methods for recommendation, such as SoRec [16], TrustFM [32] and TrustSVD [9]. Recently, deep learning-based methods have become the most successful methods in recommendation research. Plenty of recent works [7,18,21] have applied deep learning to social recommendation tasks and achieved promising performance. In recent years, a lot of works [2,5,29] transform user-user relations and user-item interactions to graph-structured data, and employ the graph neural network (GNN) to learn better user and item representations. In addition, to capture connections among items and enhance the performance of social recommendation, several eî€orts adopted the item attributes to construct item-item graph. For example, GraphRec+ [6] and DANSER [30] leveraged the itemâ€™s collaborative information to build item-item graph, while KCGN [12] utilized inter-dependent knowledge of items to construct graph. However, few of these methods extract high-order cross-semantic information by modeling a joint heterogeneous global graph, which includes three kinds of meta-paths: user-user, user-item and itemitem. As GNNs rely on the good quality of the original graph, graph structure learning method was proposed to alleviate this limitation. Figure 1: The overview of our model GL-HGNN. We î€›rst establish the item-item subgraph structure. We employ the Graph Learner to update and optimize the graph structure. We utilize HGNN to model the global graph to extract complex crosssemantic information. The output embeddings are sent to the predictor for prediction. We design a hybrid loss including Graph Learner loss and rating loss for training. LDS [8] proposed to model each edge inside the adjacency matrix. IDGL [4] jointly and iteratively learned graph structure and graph embedding based on node features. HGSL [34] generated three kinds of graph structures to fuse an optimal heterogeneous graph. However, most of these models are applied to node classiî€›cation or graph-level prediction tasks. To our best knowledge, we are the î€›rst to adopt the graph structure learning to improve the quality of the heterogeneous global graph in social recommendation. In this paper, we deî€›neğ‘ˆ = {ğ‘¢, ğ‘¢, ..., ğ‘¢}andğ‘‰ = {ğ‘£, ğ‘£, ..., ğ‘£} as the sets of users and items, separately. The user-user relations can be deî€›ned asğº= {ğ‘ˆ, E}, in whichEis the set of edges, and(ğ‘¢, ğ‘¢, ğ‘Ÿ)inErepresentsğ‘¢is related toğ‘¢. And the useritem interactions can be represented as the user-item graph with ğ¾kinds of edgesğº= {ğ‘ˆ,ğ‘‰ , E}. The edge inEis deî€›ned as (ğ‘¢, ğ‘£, ğ‘Ÿ), which indicates that the userğ‘¢rates the itemğ‘£as ğ‘˜. Letğ‘ˆ (ğ‘¢)denote the set of users related to userğ‘¢. In addition, ğ‘‰(ğ‘¢)is deî€›ned as the set of items that the userğ‘¢ratesğ‘˜to, while ğ‘ˆ(ğ‘£) as the set of users who give a rating ğ‘˜ to ğ‘£. Problem Formulation. Letp,qâˆˆ Rdenote initial embeddings of the target userğ‘¢and itemğ‘£. Given user-user relations and user-item interactions, the task is to predict the explicit score Ë†ğ‘Ÿthat user ğ‘¢will rate item ğ‘£. Figure 1 provides the overall architecture of our model. We aim to construct a heterogeneous global graph, and extract cross-semantic relations and rich topological information from it. We î€›rst build item-item subgraphğº= {ğ‘‰, E}by similarity between items. Figure 2: The structure of proposed Global Graph Learning. The edge(ğ‘£, ğ‘£, ğ‘Ÿ)inEmeans itemsğ‘£andğ‘£are similar. We also deî€›neğ‘‰ (ğ‘£)to denote the set of items similar to itemğ‘£. We combine these three graphs{ğº, ğº, ğº}into a heterogeneous global graphğº, which contains two kinds of nodes, three kinds of meta-paths andğ¾ +2 kinds of edges, i.e. user-user relation edge, ğ¾kinds of rating edges, and item-item similarity edge. In order to get a better graph structure with respect to the downstream task, capture implicit connections and î€›lter out possible noise, we design the Graph Learner to optimize user-user (u2u) and item-item (i2i) connections. Moreover, the reî€›ned global graph is passed as input to a heterogeneous graph neural network to distill high-order complex semantic information. We employ a rating predictor to predict the score that target user will rate the candidate item. We design a hybrid loss to train our model. Figure 2 shows the architecture of Global Graph Learning module, which constructs the heterogeneous global graph and optimizes the graph structures. It should be noted that there is usually no connection information between items in the raw data, but itemitem connections can enrich the graph structure and improve the receptive î€›eld, which allows us to extract more information of both users and items. For this reason, we need to construct the item-item edges î€›rst. Then, we employ Graph Learner (GL) to optimize the user-user and item-item subgraph by adding or removing the edges with the method of calculating the similarity of node embeddings [4]. We will introduce the details below. Item-item Connections Construction. We utilize the rating matrixR âˆˆ Rto calculate the cosine similarity between items following previous work [6]. For the rating matrixR, we take theğ‘—th column vectoreas the itemğ‘£vector. The similarity calculation formula is denoted as follows: For each item, we choose the most similarğ¾items to create the edges. In this way, we construct the item-item connections. Graph Learner. Due to the noise or lack of possible information in the original graph structure, we propose to adapt Graph Learner to optimize the input u2u and i2i subgraphsâ€™ topologies. For u2u Graph Learner, the input is the initial subgraphğºwith node set{ğ‘¢, ğ‘¢, . . . , ğ‘¢}and embedding set{p, p, . . . , p}. For the target nodeğ‘¢, we apply the multi perspective learning method to calculate the similarity between ğ‘¢and all nodes as follows: ğ‘ ğ‘–ğ‘š(ğ‘¢, ğ‘¢) =1ğ¹ğ‘ ğ‘–ğ‘š(ğ‘¢, ğ‘¢), ğ‘› = 1, 2, . . . , ğ‘(2) Whereğ¹is the number of perspectives. For each perspective, we can choose one from three methods, which are called weighted cosine, attention, and add attention: Equation 3 is the principle of weighted cosine, andWis the weight of neural network. Equation 4 shows the calculation method of attention, whereWis a weight matrix. Equation 5 presents the principle of the add attention, wherewmaps the embedding of the node to 1 dimension andğœis the ReLU function. During the experiment, we mainly use the weighted cosine method, and the other two methods will be compared in the ablation study. For all the target user nodes in one batch, the initial adjacency matrix with all nodes isAâˆˆ R, whereğµis the number of target user nodes in the current batch. And we can obtain a new learned implicit adjacency matrixAwith similarity calculation. Though the initial graph may be noisy or missing information, it still contains rich valuable topological information. Therefore, we employ a weight valueğœ†to combine the implicit matrix with the initial matrix: Each element in the reî€›ned matrixËœArepresents the similarity of two nodes. In order to prevent information redundancy caused by too many edges, we set a truncation lengthğ¿. For each target node, we truncate the î€›rstğ¿nodes with the highest similarity to establish new connections, and the remaining nodes are not connected to the target node. In this way, we can get the reî€›ned subgraph ğº. For the input i2i subgraphğº, we can apply the same method to get the reî€›ned subgraph ğº. Anchor-based Graph Learner. In the real world, the number of nodes is often very huge. For target nodes, if we calculate the similarity of all the nodes to them, the costs of computation are high. Inspired by [4], we proposed a scalable Anchor-based Graph Learner (AGL). Next, we take the item-item subgraph as an example. For target item nodes, we randomly selectğ»(ğ»â‰ª ğ‘€) nodes as the anchor nodes set{ğ‘£, ğ‘£, . . . , ğ‘£}. We can get the initial adjacency matrix between target nodes and anchor nodes Aâˆˆ Rfrom the initial connections. We calculate the target-anchor similarity matrix A, as Equation 2. Then we use the weight valueğœ†and the truncation lengthğ¿to calculate the reî€›ned item-item subgraphğº. Similarly, we can randomly select ğ»(ğ»â‰ª ğ‘ )user nodes as the anchor nodes set, and employ AGL to get the reî€›ned user-user subgraph ğº. In this subsection, we discuss how to extract user and item latent features in a uniî€›ed perspective, based on the reî€›ned global graph ğº= {ğº, ğº, ğº}learned by Global Graph Learning. The global graph includes three kinds of semantic meta-paths: user-user relations, user-item interactions and item-item similarities. Inspired by [20], we employ a Heterogeneous Graph Neural Network (HGNN) to extract high-order information and fuse diî€erent semantic information. We employğ‘‡-layer HGNN to model our reî€›ned heterogeneous global graph to distill cross-semantic information. For the target userğ‘¢and target itemğ‘£, the initial input embeddings of the î€›rst layer arep= pandq= q. Letpandqdenote the representations of userğ‘¢and the itemğ‘£after the propagation of ğ‘¡-th layer. We next introduce the user node aggregation and item node aggregation in each layer of HGNN. User node aggregation. Generally, for each user node in the reî€›ned heterogeneous global graph, there exits one type of edges ğ‘Ÿconnecting the user neighbors andğ¾types of edgesğ‘Ÿ, (ğ‘˜ âˆˆ {1,2, . . . , ğ¾})connecting the item neighbors. For the user-user social semantic connections, we aggregate the features of user neighbors as follows: WhereWis a trainable transformation matrix,bis the bias vector, and ğ‘is the normalization coeî€œcient. Similarly, we perform userğ‘¢node aggregation based onğ¾types of user-item rating connections. Speciî€›cally, for each type of edges ğ‘Ÿ, we also aggregate neighbor items under the same rating level as follows where ğ‘˜ âˆˆ {1, 2, . . . , ğ¾ }. For userğ‘¢, we accumulate all messages propagated by diî€erent ğ¾ +1 types of edges[p, p, . . . , p]. Then, we aggregate the information of these ğ¾ + 1 embeddings: ğœis the ReLU function,pis the output embedding of userğ‘¢ inğ‘¡ +1-th HGNN layer. It is worth noting that, for the current layer, we integrate two kinds of meta-paths (user-user, user-item) information into the userâ€™s features, while the item features already contain the item-item semantic information afterğ‘¡layers aggregation. Therefore, the userâ€™s features can also fuse item-item semantic information by the multi-layer HGNN. Item node aggregation. The target itemğ‘£also involves in two meta-paths: item-item similarity and user-item interactions includingğ¾types of edges. Similarly, for theğ‘¡ +1-th layer, we propagate diî€erent mesages fromğ¾ +1 types of edges and obtainğ¾ + 1 embeddings[q, q, . . . , q]ofğ‘£. Then we aggregate these embeddings into the output embedding q: Afterğ‘‡layers of HGNN, we can extract high-order and crosssemantic information from the reî€›ned heterogeneous global graph, which enables us to distill more latent features of users and items. The initial embeddings and output of each HGNN layer constitute the userğ‘¢embedding lists[p, p, . . . , p]and itemğ‘£embedding lists[q, q, . . . , q]. In this module, we design the shared attention mechanism to get the î€›nal user and item latent embeddings. For the userğ‘¢, the î€›nal embedding is deî€›ned as follows: WhereW,sandbare the shared trainable parameters,ğœis the ReLU function. And the î€›nal embeddingqof itemğ‘£can be calculated in the same way. In this paper, we focus on the rating prediction task in social recommendation, so we design the predictor based on multi layer perceptron (MLP) : Where [, ] is the concatenation operation. To better train our model, we design a special loss function, which contains two aspects of loss: (i) Graph Learner loss, (ii) rating loss. Graph Learner Loss. In our work, the updated graph structure plays an important role in rating prediction. In order to obtain the better graph topology with respect to the social recommendation task, we design the Graph Learner (GL) loss through graph regularization [1,4]. For the u2u GL, we can get the reî€›ned adjacency matrixËœAâˆˆ R. Generally, graph regularization is often applicable for symmetric adjacency matrix. SinceËœAis not symmetric, we î€›rst transform it to be symmetric as follows: Whereâˆ† âˆˆ R(Î”=Ãğ´) is a diagonal matrix. As we all know, that values change smoothly among adjacent nodes is a widely applied assumption. Therefore, we utilizeË†Aand initial user feature matrix P to design the smoothness loss as follows: Wheretr(Â·)indicates the trace of a matrix,L = Dâˆ’Ë†Ais the graphÃ Laplacian, andD=Ë†ğ´denotes the degree matrix. However, only minimizing the smoothness loss will cause over smoothing, so we impose constraints[4] to control smoothness as follows: Where1indicates the vector in which elements are 1, andî€î€Ë†Aî€î€ indicates the Euclidean norm ofË†A. We then deî€›ne the overall Graph Learner loss of u2u GL as the sum of the previously deî€›ned losses: ğ›½ is a non-negative hyper-parameters. While for u2u AGL, we can convert the reî€›ned adjacency matrix ËœAâˆˆ Rto the symmetric matrixË†Aas Equation 17. And we can rewrite Equation 20 to deî€›ne the Anchor-based Graph Learner loss: We can also calculate i2i GL or AGL lossğ¿by the same method. Rating Loss. For the task of rating prediction, we adopt mean square error (MSE) loss function as: Whereğ‘Ÿis the ground-truth value. For our model, we apply a hybrid loss to jointly learn the parameters: ğ›¾,ğ›¾andğœ†are non-negative hyper-parameters.Î˜is the trainable parameters,Î©(Â·)denotes the L2 regularization. Through multiple epochs of optimization, we can iteratively learn an optimized global graph structure with respect to the social recommendation as well as reliable user and item features. GL-HGNN.As for GL-HGNN, the computational cost of the Graph Learner isO(ğ¸ (ğ‘ + ğ‘€)ğ·)forğ‘user nodes,ğ‘€item nodes andğ¸ missing user-item rates to be predicted. The computational cost of HGNN isO(ğ‘‡ğ‘‹ (ğ‘€ + ğ‘ )ğ·), whereğ‘‡denotes the number of layers andğ‘‹indicates the average neighbors of each node. The rating task costsO(ğ¸ğ‘‘ğ·)whereğ‘‘is the hidden size, while the computational complexity of the hybrid loss isO(ğ¸ (ğ‘ + ğ‘€)ğ·). The overall cost is aboutO((ğ‘‡ ğ‘‹ + ğ¸)(ğ‘ + ğ‘€)ğ· + ğ¸ğ‘‘ğ·). If we assume thatğ¸ â‰ˆ ğ‘ + ğ‘€ andğ‘‡ğ‘‹, ğ‘‘ â‰ª ğ‘ + ğ‘€, the overall time complexity isO((ğ‘ + ğ‘€)ğ·). AGL-HGNN.As for AGL-HGNN, the computational cost of the Anchor-based Graph Learner isO(ğ¸ (ğ»+ ğ»)ğ·), while computing node embeddings by HGNN costsO(ğ‘‡ğ‘‹(ğ‘€ +ğ‘ )ğ·), whereğ‘‹indicates the average neighbors of each node. The rating task also costs O(ğ¸ğ‘‘ğ·), and computing the hybrid loss costsO(ğ¸ (ğ»+ ğ»)ğ·). As ğ», ğ», ğ‘‘ â‰ª (ğ‘ + ğ‘€), the overall time complexity of AGL-HGNN is O(ğ‘‡ğ‘‹(ğ‘ + ğ‘€)ğ·), which is linear to the number of user and item. Therefore, AGL-HGNN can signiî€›cantly reduce the computational complexity. In this section, we will detail the settings of our experiment and present the experimental results. To fully demonstrate the superiority of our model, we conduct experiments to verify the following four research questions (RQ): â€¢ (RQ1): Compared with the state-of-the-art models, does our model achieve better performance? â€¢ (RQ2): What are the impacts of key components on model performance? â€¢ (RQ3): How does the setting of hyper-parameters (such as the truncation length in Graph Learner) aî€ect our model? â€¢ (RQ4): How can Global Graph Leaning module improve the performance of our model? 5.1.1 Datasets. We conduct experiments on several public social recommendation benchmark datasets Ciao[23], Epinions[22] and Flixster[13], which all contain rating information and social networks. The detailed statistics of dataset are given in Table 1. â€¢ Ciao: Ciao is drieved from a popular social networking ecommerce platform. We process two available versions of the Ciao datasets, separately called Ciao-5 and Ciao-28. Ciao-5 collects 5 categories of items and their corresponding users, while Ciao-28 contains all 28 categories of items (such as DVDS) and users. The rating range is[1,5]with the step size 1.. â€¢ Epinions: Epinions comes from a social based product review platform. The rating values contain î€›ve discrete numbers, which are {1, 2, 3, 4, 5}. â€¢ Flixster: Flixster comes from a popular movie review website, where people can add others as friends to create the social network. The range of rating value is[0.5,5]with the step size 0.5. For each dataset, we select 20% as the test set, 10% as valid set and remaining 70% as training set. 5.1.2 Evaluation Metrics. In order to better evaluate the performance of models, we employ two widely used metrics, namely RMSE (root mean square error) and MAE (mean absolute error) [25]. The two metrics both indicate the error between the predicted value and the ground-truth, while RMSE is more sensitive to outliers. 5.1.3 Baselines. To evaluate the performance of our model, we select representative seven models, including classic and state-ofthe-art (SOTA) social recommendation models as follows: â€¢ SoRec[16]: It learns usersâ€™ feature vectors by decomposing the scoring matrix and the social relation matrix simultaneously. â€¢ TrustMF[32]: According to the direction of trust, this model maps users to the trusted space and the trustee space, by matrix factorization. â€¢ TrustSVD[9]: This is one matrix factorization-based model, aggregating friends embeddings into target users embeddings to learn explicit and implicit information. â€¢ DSCF[7]: This method proposes a deep learning-based framework, which captures the inî€uence of distant social relationships on target users. â€¢ GC-MC[2]: This model generates the implicit information between users and items in the form of information transfer in the bipartite interaction graph. However, it only models the links between users and item. In the experiment, we also join social network to make predictions. â€¢ GraphRec[5]: This method jointly captures the user-item interaction and opinion between users and items from useritem graph, and learns the heterogeneous social relationship between users from user-user graph. â€¢ DANSER[30]: This method constructs a large graph that contains user-user, item-item, and user-item sub-graphs. By modeling this large graph, it learns the dynamic and static attributes of users and items, and then fuses the dual attributes to predict usersâ€™ ratings on target items through one fusion strategy. â€¢ GraphRec+[6]: On the basis of Graphrec, Graphrec+ not only models user-item and user-user graphs, but adds itemitem graph to aggregate information between similar items. Table 2: Performance comparison of diî€erent models on the four datasets. The smaller the RMSE and MAE, the better the performance. 5.1.4 Parameters Seî€ing. We implement our model based on Pytorch and DGL. We set the embedding dimensionğ· =64, and the batch size as 128. For all trainable parameters, we initialize them with a Gaussian distribution with an average of 0 and a standard deviation of 0.01. We use mini-batch Adam optimizer to train the model parameters with initial learning rate of 0.001. In order to prevent over-î€›tting, we add dropout layers with a probability value of 0.4 during training. In construction of item-item edges, we select top 20 items for each item to build connections, according to the similarity cosine values. For the Graph Learner, we search the weightğœ†of learned implicit graph structure in ity calculation in the Graph Learner, is tuned in the set of[1,2,3,4]. For the truncation lengthğ¿in Graph Learner, we obtain the optimal value in the range[20,40,60,80,100]through the grid search. In addition, we set the number of graph neural network layers in range of [1, 2, 3, 4]. In addition, we also apply Anchor-based Graph Learner module in our experiments. We deî€›ne the anchor rate ğœ = ğ»/ğ‘ = ğ»/ğ‘€. We test the value of ğœ in the set [0.01, 0.02, 0.05, 0.1, 0.15, 0.2]. For all the baselines, in order to achieve the best performance of these models, we set the parameters strictly according to the papers. The experimental results of the baseline models and our models on four datasets are shown in Table 2. Based on the comparison in the table, we can summarize our î€›ndings as follows: â€¢Our model GL-HGNN comprehensively outperforms all the baseline models on the four datasets. The results indicate that our model is eî€ective to the rating prediction task of the social recommendation. Diî€erent from the SOTA methods: GraphRec+ and DANSER, our approach models the heterogeneous global graph to capture high-order features and diî€erent semantic information. In addition, to obtain a better graph structure for social recommendation, GL-HGNN GL-HGNN-w/o GLs&i2i edges 0.9171 0.7077 1.0860 0.8369 employs Graph Learners to optimize initial u2u and i2i connections. Besides, compared with GL-HGNN, AGL-HGNN can achieve comparable results, even better ones sometimes. â€¢Among all the baselines, the performance of deep learningbased methods is better than that of traditional methods, which shows that deep learning-based methods have a stronger learning ability for user relations and user-item interaction signals. Moreover, the GNN-based models achieve better results than other models without graph structure. It proves the eî€ectiveness of GNN for social recommendation. Furthermore, GraphRec+ and DANSER achieve a better performance than other model without i2i subgraph construction. That suggests that adding extra i2i connections into the user-item graph can be helpful for social recommendation. In order to verify the eî€ectiveness of some key modules, we conduct a series of ablation experiments on the Ciao-5 and Epinions datasets. The results are shown in Table 3. Firstly, we compare diî€erent calculation methods of nodes similarity in the Graph learner by replacing weighted cosine with attention and add attention. As can be seen in Table 3, it is clear that the weighted cosine method is the best one of three methods to capture similar attributes between nodes. Figure 3: Comparisons of diî€erent hyper-parameters w.r.t. the weight value ğœ† perspectives ğ‘ƒ and the truncation length ğ¿. Table 4: Performance with diî€erent number ğ‘‡ of HGNN layers. GL-HGNN-1 1.0501 0.8072 1.1276 0.8617 GL-HGNN-2 1.0343 0.7753 1.0912 0.8345 GL-HGNN-3 1.0320 0.7763 1.0709 0.8017 GL-HGNN-4 1.0398 0.7847 1.0846 0.8204 Besides, we explore to evaluate the eî€ectiveness of the critical modules of GL-HGNN. We delete each module of GL-HGNN to observe the change of model performance, e.g., removing the u2u GL module and removing the i2i GL module. We can observe that the Graph Learner module is pivotal for the model performance by seeing "GL-HGNN-w/o GLs". These results demonstrate that a more suitable graph structure with respect to the downstream task plays an important role. In addition, without i2i connection information, the model performance declines to a certain extent, which shows that capturing implicit item relations from the user rating matrix is valuable for the rating prediction task. Global Graph LearningThe performance of the Global Graph Learning is mainly aî€ected by four important parameters, i.e., the weight valueğœ†of learnt implicit graph structure, the number of perspectives ğ‘ƒ, the truncation length ğ¿ and the anchor rate ğœ. â€¢ Graph Leaner: For the î€›rst three parameters, we adjust these parameters respectively for u2u and i2i Graph Learners on Epinions datasets. The results are shown in Figure Figure 4: Performance comparison and running time (seconds) with diî€erent anchor rates. 3 and we can see that: (i) For u2u and i2i Graph Learners, appropriate implicit graph weight values are required. If the weight is too large, a lot of noise may be introduced leading to sub-optimal performance. Too small weight value also hurt model performance since the learnt implicit information would become less. (ii) The increase of numbers of perspectives in GL does not necessarily lead to an increase in performance. On the contrary, too many perspectives may result in the over-î€›tting. (iii) As shown in Figure 3c, the model performance reaches the best values whenğ¿is 40. The performance change in the î€›gure can indicate that too long or too short truncation will bring loss to the model eî€ect. The most suitable truncation length should achieve the balance between eî€ective information and irrelevant information in the graph learning. â€¢ Anchor-based Graph Leaner: For the anchor rateğœ, we perform experiments on a single NVIDIA Tesla V100 GPU on Ciao-28 and Epinions datasets. We record the training time (seconds) of each epoch and RMSE evaluation results. As we can see from Figure 4, with the increase of the anchor rate, the performance of the model improves î€›rst and then Figure 5: Visualization of an example for the case study from Ciao-5 data. Given the social connections among î€›ve users and the corresponding user-item ratings, the prediction target is the rating of user ğ‘¢(white circular) on item ğ‘£(green diamond). The coverage area represents the neighboring area of the target user or item. We obtain the updated graph through the Graph Learner, based on the initial graph. tends to be stable, while the training time is on the rise. It can be concluded that by controlling the anchor rate within a reasonable range, the model running time can be reduced without almost loss of model performance. Heterogeneous Graph Neural Network. Generally, the numberğ‘‡of layers plays an important role for the GNN. We conduct the experiments on two datasets, and Table 4 presents the results of our model with diî€erent number of HGNN layers. Fromğ‘‡ =1 to ğ‘‡ =2, the model performance is greatly improved for both datasets, which shows the necessity of the high-order interconnection. For the Epinions dataset, fromğ‘‡ =2 toğ‘‡ =3, the performance still increases quickly. Generally, appropriate increase in the number of layers will make information fusion deeper. However, whenğ‘‡is too large, the performance will drop, probably because the model introduces too much noise or becomes over-smoothing. To show the eî€ectiveness and rationality of Global Graph Learning module, we conduct a simple case study on several users from Ciao5 dataset. Speciî€›cally, we make a comparison between GL-HGNN and the basic GNN-based methods (HGNN) without Global Graph Learning. Generally, we can build a graph using user-item ratings and user social relationships as the initial graph shown in the left part of Figure 5. However, we can î€›nd that there is noise in this graph structure. Although there exist social connection between userğ‘¢ andğ‘¢, there are huge rating diî€erences between userğ‘¢andğ‘¢ on the same item set. Besides, despiteğ‘¢andğ‘¢do not have the direct social connection, their rating histories are highly overlapped. It illustrates that they may be potential friends with the similar preferences. On the contrary, GL-HGNN propose to adopt the Global Graph Learning module to construct item-item connections and iteratively optimize the graph structure based on the initial graph. As shown in the the right part of Figure 5, the updated graph increases the potential relationship edge and reduces noise compared with the initial graph. We utilize the initial graph and updated graph to make scoring predictions through HGNN, respectively. Given the ground-truth rating 2, HGNN with the updated graph (GL-HGNN) predicts the result as 2.97, which is closer to the ground truth label compared to the value 3.44 generated by HGNN with the initial graph. The result demonstrates the validity and rationality of our proposed Global Graph Learning module. In this paper, we proposed a novel method GL-HGNN to learn the heterogeneous global graph with diî€erent relationships in a uniî€›ed perspective for social recommendation. Our comparative experiments and ablation studies on four datasets illustrate that GL-HGNN can learn better graph structure with respect to social recommendation, and signiî€›cantly improve the performance of recommendation. In addition, to reduce the computational complexity, we propose the Anchor-based Graph Learner. In the future, we plan to introduce more nodes information (such as review information) for mapping multi-relation to multi-type edges in reî€›ned graph automatically.