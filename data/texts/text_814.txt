Nanjing University of Posts andNanjing University of Posts and Sequential recommendation based on multi-interest framework models the userâ€™s recent interaction sequence into multiple diî€erent interest vectors, since a single low-dimensional vector cannot fully represent the diversity of user interests. However, most existing models only intercept usersâ€™ recent interaction behaviors as training data, discarding a large amount of historical interaction sequences. This may raise two issues. On the one hand, data reî€ecting multiple interests of users is missing; on the other hand, the co-occurrence between items in historical user-item interactions is not fully explored. To tackle the two issues, this paper proposes a novel sequential recommendation model called â€œGlobalInteraction AwareMulti-Interest Framework for SequentialRecommendation (GIMIRec)â€. Speciî€›cally, a global context extraction module is î€›rstly proposed without introducing any external information, which calculates a weighted co-occurrence matrix based on the constrained co-occurrence number of each item pair and their time interval from the historical interaction sequences of all users and then obtains the global context embedding of each item by using a simpliî€›ed graph convolution. Secondly, the time interval of each item pair in the recent interaction sequence of each user is captured and combined with the global context item embedding to get the personalized item embedding. Finally, a self-attention based multi-interest framework is applied to learn the diverse interests of users for sequential recommendation. Extensive experiments on the three real-world datasets of Amazon-Books, Taobao-Buy and Amazon-Hybrid show that the performance of GIMIRec on the Recall, NDCG and Hit Rate indicators is signiî€›cantly superior to that of the state-of-the-art methods. Moreover, the proposed global context extraction module can be easily transplanted to most sequential recommendation models. â€¢ Information systems â†’ Recommender systems; Collaborative î€›ltering. sequential recommendation, multi-interest framework, global context extraction ACM Reference Format: Jie Zhang, Ke-Jia Chen, and Jingqiang Chen. 2018. GIMIRec: Global Interaction Information Aware Multi-Interest Framework for Sequential Recommendation. In Woodstock â€™18: ACM Symposium on Neural Gaze Detection, June 03â€“05, 2018, Woodstock, NY . ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/1122445.1122456 The recommendation system predicts usersâ€™ preferences for items based on their historical behaviors. For example in e-commerce application, the users are often recommended some commodities based on their past purchase behaviors. Deep neural networks, especially graph neural networks (GNNs) [4,18], have now been applied on the recommendation systems, such as GRU4Rec [6], GGNN [15] and SR-GNN [25], achieving better performance than traditional collaborative î€›ltering methods [7,16]. However, the graph-based methods are often with high complexity as the result of the large scale of users and items in real world [5]. The sequential recommendation has recently drawn more research attention as it starts with each userâ€™s own interaction data, models the usersâ€™ preferences with only recent interaction behaviors and then gives personalized recommendation for each user. The main eî€ort of sequential recommendation is to learn the representations of users and items. Since the interests of users are usually diverse and diî€œcult to be represented by a single low-dimensional vector, the sequential recommendation methods based on multi-interest framework have emerged [1,2,13]. In these methods, the number of representation vectors for each user is increased from 1 toK, where each vector represents one interest of the user. Multiple interests can be captured by an improved capsule network [13] or a self-attention mechanism [1] and ultimately a more personalized recommendation based on K interests is provided. Most of the existing multi-interest frameworks for sequential recommendation use only usersâ€™ recent interaction sequence as it is believed that the userâ€™s next interaction item is more closely related to his/her recent behaviors. However, we believe that the complete historical interactions of all users (referred to as the global context in this paper) can be better leveraged to reî€ect the diversity of usersâ€™ interests and potential relations between items. An example shown in Figure 1 is to illustrate how the global context aî€ects the prediction of the 10th interactive item for each user although this item did not appear in each userâ€™s recent interactions. Since the popcorn and wine interaction sequence can be found in user Aâ€™s early historical sequence, the 10th interactive item for user B will be probably the wine. The cake could be recommended for user A instead of battery due to the co-occurrence of game console and cake in user Câ€™s historical sequence. The example indicates that the userâ€™s behaviors could be predicted with the reference of other usersâ€™ historical behaviors though diî€erent users may have diî€erent preferences. Figure 1: An example of usersâ€™ interaction sequences. To our knowledge, these have been some sequential recommendation models exploring usersâ€™ complete sequence to learn their preferences [17,20,27â€“29]. But they focus more on each userâ€™s personalized behaviors but do not take the global context into consideration. Therefore, this paper proposes a global interaction information aware multi-interest framework for sequential recommendation (named GIMIRec) with the main contributions as below: â€¢A new multi-interest sequential recommendation model is proposed, which is equipped with a global context extraction module to fully utilize the historical sequences of all users. This module can also provide a performance boost for the recommendation models based on representation learning. â€¢A lightweight approach is designed to calculate the global context embedding of each item, based on the weighted co-occurrence matrix fusing the constrained co-occurrence number of item pairs and their time intervals in the global context. â€¢Our model achieves the state-of-the-art performance on three real-world datasets without introducing any other supportive information (such as item attributes or userâ€™s social information) other than interactions and timestamps. Sequential recommendation.Traditional recommendation methods, such as matrix decomposition and collaborative î€›ltering [7,16] , represent interaction behaviors as a user-item matrix and explore their potential relation. With the development of deep neural networks, diî€erent recommendation methods combined with deep learning have emerged, such as the neural collaborative î€›ltering method NGCF [22]. However, it is hard for the traditional recommendation system to manage huge but sparse interaction data. To solve this problem, sequential recommendation methods are proposed. The objective of sequential recommendation is to learn each userâ€™s preferences from the historical interaction behaviors, and then to achieve recommendation. Typical methods include GRU4Rec [6] and SASRec [10] where the recurrent neural network and the attention mechanism are used respectively. Later, the time of the interaction occurrence is also taken into account. For example, a parallel time-aware mask network is proposed in MTIN [8], which extracts a variety of time information through multiple networks to enhance the short video recommendation eî€ect. Ti-SASRec [14] is another method incorporating time interval of every two items in sequences to further improve the performance of SASRec [10]. Multi-interest framework for sequential recommendation. User preferences may contain a variety of interests. The existing sequential recommendation systems eventually model usersâ€™ preferences as a single low-dimensional vector while the users may have more than one single interest. MIND [13] was the î€›rst to model each user withKvectors, where diî€erent vectors represent diî€erent interests. The extraction of multi-interests is similar to the clustering process and the behavior-to-interest (B2I) approach is proposed to obtainKinterests of users through a capsule network. Subsequently, Cen et al. [1] proposed a multi-interest extraction method based on self-attention mechanism (ComiRec-SA) to instead of capsule network. The latest method PIMI [2] introduced the time and periodic information in usersâ€™ recent interaction sequences into the model. The self-attention mechanism was used to capture usersâ€™ multi-interests and achieved the state-of-art performance. However, all the above multi-interest based sequential recommendation models, including PIMI, do not fully utilize the historical interactions of all users, thus lacking the global context of items. Graph neural networks.In recommendation systems, usersâ€™ historical interaction behaviors can be converted into a graph structure, where users and items are represented as nodes and their interactions are represented as edges. Graph neural networks (GNNs) [4,18] have been proved to better learn the representations of users and items. In non-sequential recommendation models, such as NGCF [22], LightGCN [5] and DGCF [23], simpliî€›ed GNNs are used on the bipartite graph composed of users and items to generate their embeddings. In sequential recommendation models such as SR-GNN [25], GNNs are also widely used to represent userâ€™s sequence and to capture userâ€™s preferences. Before describing the proposed model in detail, the formal deî€›nitions of sequential recommendation and multi-interest framework are given as follows: Deî€›nition 1: Sequential Recommendation.Assuming that Urepresents a collection of all users andIrepresents a collection of all items. For a given userğ‘¢ âˆˆ U, his/her interaction sequenceSno is deî€›ned with an item sequenceI=ğ‘–, ğ‘–, Â· Â· Â· , ğ‘–and a times-no tamp sequence of each interaction occurrenceT=ğ‘¡, ğ‘¡, Â· Â· Â· , ğ‘¡. The task of sequential recommendation is to predict the next interactive item ğ‘–for the user ğ‘¢ based on S. Deî€›nition 2: Multi-Interest Framework.It denotes a framework designed for sequential recommendation. According to Deî€›nition 1, each userğ‘¢will be represented as ağ‘‘-dimensional embedding by learning from his/her interaction sequence. However, one single vector representation may not be suî€œcient to describe the userâ€™s multi-interest preferences. In the multi-interest framework,K ğ‘‘dimensional embeddings of each user will be learned for sequential recommendation. The main purpose of our model GIMIRec is to better capture the multiple interests of each user from the global context information, which is calculated from the historical sequences of all users. The overall model consists of the following modules. Global Context Extraction Module.The module is basically to collect the co-occurrence number and time interval for any item pair that appear one after another in each userâ€™s historical interaction sequence for the calculation of the weighted co-occurrence matrix. Subsequently, a simpliî€›ed graph convolution is used on the above matrix to obtain the global context embedding of each item. Recent Time Interval Representation Module.Though we believe the global context will better reî€ect the diversity of user preferences, the userâ€™s recent interactions directly reî€ect userâ€™s recent preferences. So this module î€›rst identiî€›es the recent interaction sequence based on the predetermined lengthğ¿, and then encodes the time interval between any item pair within the sequence to obtain the time interval embedding for recent items. Aggregation Module.For each item in the userâ€™s recent interaction sequence, the global context embedding and the time interval embedding are merged in this module to get the hybrid embedding. For each user, a graph neural network is used to learn the î€›nal personalized embedding by aggregating the above hybrid embeddings in his/her recent interaction sequence. Multi-Interest Extraction Module.The module extractsK interest vectors from the personalized embedding of userâ€™s recent sequence by the self-attention mechanism [21]. For model training, one interest vector that can best represent userâ€™s current preference is selected from K interest vectors. Finally, the trained model is used to predict the userâ€™s nextğ‘ interactive items (i.e., Top-ğ‘recommendation) in the test set according to userâ€™s K interests. The architecture of GIMIRec is demonstrated in Figure 2, and each module will be detailed in the following subsections. The existing multi-interest frameworks for sequential recommendation extract the userâ€™s interests only fromğ¿items with which user recently interacted. Historical interactions earlier thanğ¿ items are discarded. The Global Context Extraction (GCE) Module tries to learn the global context embedding of items from all historical user interactions. Figure 3 illustrates how the GCE module processes historical interaction sequences with timestamps to get the weighted cooccurrence matrixAby taking three users A, B and C as example. Figure 2: The architecture of the proposed model GIMIRec. On the right-hand side of the Figure 3, a simpliî€›ed graph convolution is used onAto obtain the global context embedding of each item. Here, the weighted co-occurrence matrixAis deî€›ned as a combination ofğ‘˜-hop (ğ‘˜ = 1, 2, 3) item co-occurrence matrices. For eachS, theğ‘˜-hop item pair is deî€›ned as a directed <ğ‘–,ğ‘–> (ğ‘› âˆˆ N). For example, user B in Figure 2 has four interacted items, so the 1-hop item pairs are <ğ‘–,ğ‘–>, <ğ‘–,ğ‘–> and <ğ‘–,ğ‘–>, the 2-hop item pairs are <ğ‘–, ğ‘–>, <ğ‘–, ğ‘–> and the 3-hop item pair is <ğ‘–, ğ‘–>. The detailed steps of the GCE module are as follows. Firstly, theğ‘˜-hop item pairs are î€›ltered out from the historicalno interaction sequencesI=ğ‘–, ğ‘–, Â· Â· Â· , ğ‘–and the correspondingno timestamp sequenceT=ğ‘¡, ğ‘¡, Â· Â· Â· , ğ‘¡,ğ‘¢ âˆˆ U. In order to better capture the relation between items, a time interval thresholdğ¿ is set. The two items inSwith the time interval less thanğ¿ can become a co-occurrence item pair. Given the large number of items in the sequences, we only consider 1-hop, 2-hop and 3-hop item pairs to reduce the amount of calculation. For convenience, triplet collectionsR(ğ‘˜ = 1, 2, 3)(Eq. 1) are used to save the id of two items and their time interval for three types of hop. Since the user id is no longer functional here, the superscriptğ‘¢is replaced withğ‘—, representing one co-occurrence of the item pair <ğ‘–, ğ‘–>. For diî€erent ğ‘—, the time interval of <ğ‘–, ğ‘–> could be diî€erent. Figure 3: An example of Global Context Extraction Module. As illustrated in Figure 2, there are three users interacted with 9 items in total, thus initially generating 141-hop item pairs, 11 2-hop item pairs and 83-hop item pairs. Assuming that the time thresholdğ¿, the î€›nal number of item pairs would be 9, 3 and 1 for 1-hop, 2-hop and 3-hop respectively shown in Figure 3. Secondly, three co-occurrence matricesA(ğ‘˜ = 1, 2, 3)are calculated. Intuitively, the item pair will have closer relation if they appear more frequently. Therefore, the entry inAwill be higher with shorter time intervals and higher frequency. For each collectionR(ğ‘˜ = 1, 2, 3)deî€›ned above, the time interval of an item pair <ğ‘–, ğ‘–> is î€›rst calculated by Eq. 2 whereğ‘andğ‘are hyperparameters satisfyingğ‘ + ğ‘ = 1and the time interval of <ğ‘–, ğ‘–> is normalized to [0, 1]. Eq. 3 sums up all the time intervals of <ğ‘–, ğ‘–>. Eq. 4 symmetrizes each matrix to facilitate the subsequent graph convolution, which usually performs better on an undirected graph [12]. Thridly, the three co-occurrence matricesAare linearly combined to get A(Eq. 5) where ğ›¼, ğ›½ and ğ›¾ are hyperparameters. Intuitively, the item pair will have closer relation if they are nearby in the sequence. Therefore,ğ›¼is supposed to be the largest andğ›¾is supposed to be the smallest. The identity matrixğ¼is added into Ain order to facilitate the graph convolution. Finally, inspired by GCN [12], LightGCN [5] and GES [30], a simple and lightweight graph convolution operation is used to obtain the global context embedding of each item (Eq. 6-7). The Laplace matrixAis î€›rst calculated by the obtainedAand its degree matrixDand then multiplied with the embedding lookup table of itemsEto get the global context embedding of items An example of the above calculations is illustrated in Figure 3. The GCE module only runs once in the training phase and can also be used as an independent module for any other sequential recommendation models. After getting the global context embedding of items, our model further processes each userâ€™s recent item sequence and timestamp sequence to learn the userâ€™s personalized interaction preferences. Firstly, the sequence length thresholdğ¿is set to intercept each userâ€™s most recent interactions, which does not include the target item for training. If there are less thanğ¿items, the padding strategy [2] will be used. Eventually, each userğ‘¢has a î€›xed-lengthno sequenceS=ğ‘–, ğ‘–, Â· Â· Â· , ğ‘–and the corresponding times-no tamp sequence T=ğ‘¡, ğ‘¡, Â· Â· Â· , ğ‘¡. Secondly, the time interval matrix is generated. This module resetsğ‘¡asmin(|ğ‘¡âˆ’ ğ‘¡|, ğ¿)by using the thresholdğ¿(deî€›ned in GCE) to emphasize the userâ€™s recent interests. The interaction time interval of every two items in the userâ€™s recent interaction sequenceSis put into the time interval matrixğ‘‡âˆˆ R, which is a symmetric matrix. Thirdly, the time interval matrixğ‘‡is embedded to a tensor ğ‘‡âˆˆ R, whereğ‘‘is the dimension of embedding. The time interval aware attention mechanism [2] is used to get attention scoresSâˆˆ Rfor each time interval in the sequence, denoted as whereğ‘Šis a trainable parameter matrix andâŠ¤represents the transposition of the matrix. Finally, the time interval embedding of itemsEin the recent sequence for each userğ‘¢is calculated through the broadcast mechanism in TensorFlow, which combines the attention scoreSand the time interval embedding tensor ğ‘‡. This module aggregates the global context embeddings and the time interval embeddings of the userâ€™s recent interactive items to get userâ€™s personalized embedding. Firstly, the items in each userâ€™s recent interaction sequence are picked out from the matrixEto form a global context embedding matrix Efor the user ğ‘¢: Secondly, the time interval embeddings and the global context embeddings of each userâ€™s recent interactive items are combined by addition to get the hybrid embeddings: Thirdly, a GNN [21] is used to further extract the userâ€™s personalized representation from the recent sequence. To construct a graph, the adjacent items in the sequence are î€›rst connected and then a virtual central nodeeâˆˆ Ris created and linked with all item nodes to capture the relation between items under larger than 1-hop. The embedding ofeis set to the average value of all itemâ€™s hybrid embeddings: The multi-head attention mechanism is then used to aggregate the item nodesEand the central nodeeinğ¿steps. The layer-0 aggregation is the information initialization, whereğ‘–represents theğ‘–-ğ‘¡â„item, and the superscript(0)represents layer-0. The neighbor nodeğ‘, the center nodeeand the global context representationeare concatenated.ğ‘andğ‘˜ are then sent into the multi-head attention mechanism at layer-ğ‘™ (ğ‘™ = 1, Â· Â· Â· , ğ¿), ğ‘= ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘ğ´ğ‘¡ğ‘¡ (ğ¾ = ğ‘˜, ğ‘„ = ğ‘,ğ‘‰ = ğ‘˜ Similarly, the central node eis aggretated as: e= ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘ğ´ğ‘¡ğ‘¡ (ğ¾ = ğ‘˜, ğ‘„ = e,ğ‘‰ = ğ‘˜ After the total ofğ¿iterations, the î€›nal personalized embeddings of the items are stacked in sequential order into the matrix E, which will be used in the next module. This module extracts theKinterests from the î€›nal representation of recent sequence. Research has shown that the self-attention mechanism achieves better performance than capsule network in extracting userâ€™s multiinterests [1]. Therefore, our model also uses the self-attention mechanism with two trainable parameter matricesğ‘Šâˆˆ Rand ğ‘Šâˆˆ Rfor userâ€™s K interest extraction: To train the whole model, the optimal interestğ‘œof the obtained Kinterests of each user is selected based on the target item (i.e., the next interactive ground-truth item) (Eq. 22), where theğ‘ğ‘Ÿğ‘”ğ‘šğ‘ğ‘¥operation in TensorFlow is used on the multiplication ofEand the embedding of the target itemeâˆˆ To be more eî€œcient, the sampled softmax method [19] is used to train the model, with the goal of minimizing the following loss L(ğœƒ): L(ğœƒ) =âˆ’ logÃexp(ğ‘œe)(23) The optimizer used in GIMIRec is Adam [11]. After the training, GIMIRec achieves Top-ğ‘recommendation for all users in the test set. Due to the large size of the item pool, a GPU-accelerated method Faiss [9] is used to search forğ‘items that are most relevant to user interests. The recommendation results are given based on the Eobtained from Eq. 21, whereeis the embedding of the predicted item,Eis the ğ‘˜-ğ‘¡â„ interest embedding of the user ğ‘¢. This section introduces the experimental setup, evaluation methods and the performance comparison of the proposed GIMIRec model against six baselines, including the state-of-the-art multi-interest recommendation model. Moreover, the ablation study is carried out to verify the eî€ectiveness of each innovation in GIMIRec. Datasets.We conducted experiments on three real-world datasets: Amazon-Books, Amazon-Hybridand Taobao-Buy. The Amazon dataset is divided into a series of sub-datasets according to the product category, among which Amazon-Books is the largest sub-dataset. Considering users are not only interested in books, we mixed î€›ve smaller datasets (â€œ Electronics â€, â€œ Clothing, Shoes and Jewelry â€, â€œ CDs and Vinyl â€, â€œ Tools and Home Improvement â€ and â€œ Grocery and Gourmet Food â€) to form a new dataset AmazonHybrid. Taobao-Buy is the sub-dataset of the Taobao dataset î€›ltering out all purchases. We discarded the users and items with less than 5 interactions and illegal timestamps. The length of the recent interaction sequenceğ¿is set to 20 for Amazon-Books and Amazon-Hybrid, and 50 for Taobao-Buy. The statistics of the datasets are listed in Table 1. We maintained the dataset division in the model validation of ComiRec [1] and PIMI [2]. In detail, we divided each dataset into a training set, a validation set, and a test set at a ratio of 8:1:1. To evaluate the model, we predicted the most recent 20% of user behaviors in the validation and test sets and used the remaining 80% to infer user multi-interest embeddings. Baselines.Comparison methods include: (1) GRU4Rec [6] (2015), the î€›rst sequential recommendation method using recurrent neural networks; (2) Youtube DNN [3] (2016), a deep neural network sequential recommendation model; (3) MIND [13] (2019), the î€›rst sequential recommendation model proposing the multi-interest framework; (4) ComiRec-DR and (5) ComiRec-SA [1] (2020) use dynamic routing mechanism and self-attention mechanism respectively to extract userâ€™s multiple interests; (6) PIMI [2] (2021), the upto-date sequential recommendation model based on multi-interest framework. It is noted that in above models based on multi-interest framework, MIND and ComiRec introduce extra information such as user age, item category etc., which is not adopted in PIMI and our model GIMIRec. Evaluation Metrics.We used three common Top-ğ‘recommendation evaluation metrics, Hit Rate @ğ‘, Recall @ğ‘and NDCG @ğ‘, to compare the performance of models. Hit Rate @ğ‘represents the percentage that recommendation results contain at least one ground truth item in topğ‘position. Recall @ğ‘indicates the proportion of the ground truth items in the recommendation results. NDCG (Normalized Discounted Cumulative Gain) @ğ‘measures the ranking quality in theğ‘items by assigning high scores to high rankings [2]. In our experiments,ğ‘is set to 20 and 50 respectively. Implementation Details.GIMIRec is implemented with Python 3.8 in TensorFlow 2.4, and a CUDA compatible GPU is used on the Ubuntu 20.04 operating system. The embedding dimensionğ‘‘is set to 64 for all datasets. The batch size on Amazon-Book, AmazonHybrid and Taobao-Buy are set to 128,128 and 256, respectively, with a dropout rate 0.1 and learning rate 0.001. In sampled softmax loss calculation, the number of samples is set to 10. Other hyperparameter settings are shown in Table 2. Finally, we iterated the training up to 1 million rounds as the previous models did. Recommendation performance.The recommendation results of all methods on three datasets are shown in Tables 3 and 4, corresponding to the value ofğ‘=20 andğ‘=50. To be fair, all models adopt the same parameter settings. The result shows that the GIMIRec model achieves the best performance on all evaluation indicators for all datasets. The performance of GIMIRec is signiî€›cantly better than MIND and ComiRec even though there is no extra information introduced. Compared to the most advanced PIMI method, GIMIRec has great advantages, especially for Amazon data-sets. For Amazon-Hybrid, the performance improvement is the highest. The possible reason is that the multiple interests of users can be better represented from a rich variety of items. For Taobao-Buy, the performance improvement is not as good as that for Amazon datasets due to the shorter time span of dataset and sparser user interactions. The result fully veriî€›es the eî€ectiveness of integrating the global context information. Ablation study.We conduct a series of ablation studies to observe the impact of parameters settings in the GCE module on the results. GIMIRec is compared with three GIMIRec variants, GIMI-Rec(no Interval, Number and Threshold), GIMIRec (no Interval and Number) and GIMIRec(no Interval). Among them, GIMIRecchanges GIMIRec by setting a=0 and b=1 in Eq. 2, indicating that the interaction interval information is not used when calculating the global context. GIMIRecfurther changes GIMI-Recin Eq. 3 by settingğ‘= 1, indicating that neither time intervals nor the co-occurrence number of item pairs are used. The diî€erence of GIMIRecand GIMIRecis that the former does not set the time interval thresholdğ¿. The experimental results in Table 5 show that the integration of the number of interactions and time intervals in the global context extraction module in GIMIRec is eî€ective and can further improve the performance. The use of the time interval thresholdğ¿is valid to î€›lter nearby item pairs. Impact of the number of interests K.Diî€erent from the general sequential recommendation, the multi-interest framework learnsKvectors for each user. Therefore, we discussed the impact ofKon the performance. The experimental results are listed in Table 6. It shows that the recommendation result of modeling multiple vectors for user preferences is better than that of modeling only a single vector. Moreover, the optimalKvalue of the model relies on the dataset. The recommendation performance is the best whenKis 4 for two Amazon datasets. However, for Taobao-Buy, most of the recommendation results are the best whenKis 8. The possible reason is that Taobao-Buy has more product categories, which could reî€ect more interests of users. Table 6: The impact of the number of interests K . The best in each column is bolded and the second best is underlined. Impact of hyperparameters ğ›¼, ğ›½ and ğ›¾.In the GCE module, we used three hyperparameters,ğ›¼,ğ›½andğ›¾, to set the co-occurrence weight of 1-hop, 2-hop and 3-hop respectively. Table 7 lists the recommendation results of GIMIRec on Amazon-Hybrid under diî€erent combinations of three parameters. It shows that the result with all three matrices is better than that of only 1-hop matrix. The recommendation result is optimal when the proportion ofğ›¼,ğ›½and ğ›¾is 5:2.5:1. The result is consistent with expectation, that is, the 1-hop relation is the most important. Impact of hyperparameters ğ‘ and ğ‘.Theğ‘andğ‘in Eq. 2 determine how the GCE module uses the time interval of historical interactions. The value ofğ‘(the weight of time interval) has been proved to be eî€ective in above ablation experiments. In Table 8, we experimented diî€erent combinations ofğ‘andğ‘in Amazon-Hybrid. Table 7: Impact of hyperparameters ğ›¼, ğ›½ and ğ›¾. The best in each column is bolded and the second best is underlined. 5:2.5:1 7.033 10.750 13.304 11.642 17.262 21.186 Table 8: The impact of hyperparameters ğ‘ and ğ‘. Table 9: The impact of the time threshold ğ¿. As seen, the best setting isğ‘=0.5,ğ‘=0.5 and a too large value ofğ‘ would cause the deterioration in model performance, because it may weaken the impact of co-occurrence number. Impact of the time interval threshold ğ¿.Time thresholds are set in both the GCE module and the recent time interval representation module. We take Amazon-Hybrid as an example to observe the impact of diî€erentğ¿on the results (see Table 9). It shows that for Amazon-Hybrid the model withğ¿set to 64 performs best, probably because a larger time interval threshold may cause userâ€™s recent interests to not be well emphasized, while a smaller threshold may cause the loss of part of userâ€™s interests. We also found that the optimalğ¿in Taobao-Buy is 7 (see the settings in Table 2). The results demonstrate that theğ¿value is closely related to the dataset. Impact of the number of GNN layers.The aggregation module uses a multi-head attention mechanism for aggregation and the number of GNN layers directly aî€ects the performance. We tried 1 to 5 layers of aggregation on the Amazon-Hybrid dataset and found that the performance is stabilized after 3 layers. The detailed results are shown in Figure 4. Figure 4: The impact of the number of GNN layers ğ¿. Time complexity analysis of the GCE module.In GCE, we conducted a lightweight graph convolution instead of a complete GCN. The overall time complexity of the latter isO(|ğœ‰ |ğ‘‘ğ¾ +|I |ğ‘‘ğ¾) [24,26], where|I|represents the number of items,|ğœ‰ |represents the number of interactions (edges),ğ‘‘is the embedding dimension andğ¾is the depth (i.e., the number of layers) of GCN. However, in the GCE module, only 1, 2 and 3-hop relations are calculated, the process of graph convolution is simpliî€›ed and only one convolution layer is used. As a result, the time complexity of GCE is reduced to O(|ğœ‰ |ğ‘‘). This paper proposes a novel multi-interest framework for sequential recommendation, which leverages the historical interactions of all users to extract global context information, so as to better learn usersâ€™ multiple interests and provide better personalized recommendations. Speciî€›cally, the co-occurrence number of item pairs and their time intervals from all interaction sequences are î€›rst used to learn the global context embedding of items based on a simpliî€›ed graph convolution operation. Then, the global context embedding is fused with the recent time interval embedding to get the personalized embedding for each user. Finally, a multi-interest framework is used to learnKinterest vectors for each user to provide recommendations. Extensive experiments verify that the potential information contained in the historical sequences can greatly beneî€›t sequential recommendation. Since the proposed module does not use any external information and has lightweight calculations, it can be easily transplanted to any sequential recommendation model. In the future, we will further explore the multi-interest framework by focusing on the non-linear relationship between multiple interests of users and î€›ne-grained representation of items.