Cross-domain recommendation can help alleviate the data sparsity issue in traditional sequential recommender systems. In this paper, we propose the RecGURU algorithm framework to generate a Generalized User Representation (GUR) incorporating user information across domains in sequential recommendation, even when there is minimum or no common users in the two domains. We propose a self-attentive autoencoder to derive latent user representations, and a domain discriminator, which aims t o predict the origin domain of a generated latent representation. We propose a novel adversarial learning method to train the two modules to unify user embeddings generated from diï¬€erent domains into a single global GUR for each user. The learned GUR captures the overall preferences and characteristics of a user and thus can be u sed to augment the behavior data and improve recommendations in any single domain in which the user is involved. Extensive experiments have been conducted on two public cross-domain recommendation datasets as well as a large dataset collected from real-world applications. The results demonstrate that RecGURU boosts performance and outperforms various state-of-the-art sequential recommendation and cross-domain recommendation methods. The collected data will be released to facilitate future research. â€¢ Information systems â†’ Recomme nder systems; â€¢ Computing methodologies â†’ Learning latent representations. Cross-Domain Recommendation, Sequential Recommendation, Learning Representation, Autoencoder, Adversarial Learning ACM Referen ce Format: Chenglin Li, Mingjun Zhao, Huanming Zhang, Chenyun Yu, Lei Cheng, Guoqiang Shu, Beibei Kong, and Di Niu. 2022. RecGURU: Adversarial Learning of Generalized User Representations for Cross-Domain Recommendation. In Proceedings of the Fifteenth ACM International Co nference on Web Search and Data Mining (WSDM â€™22), February 21â€“25, 2022, Tempe, AZ, USA. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3488560.3498388 Sequential recommendation has achieved great success in modeling the preferences and intentions of online users by utilizing both their recent act ions and long-term behavior histor y. Existing methods oft en leverage recurrent neural networks (RNNs) and attention mechanisms to model the interests of users based on sequential data [2, 8, 29, 3 4]. However, most studies to date on sequential recommendations have been focusing on a single domain, where data sparsity issues prevailâ€”chances are that most users may only have a short behavior history in the domain of interest. Cross-domain recommendation has been proposed to alleviate the data sparsity issue by leveraging user behavior in multiple domains to help recommendation in the target domain and has attracted much attention in both academia and industry. Recent work on cross-domain recommendation focuses on the transfer learning of user and item information from diverse perspectives. For example, mapping functions have been proposed to map user representations from one domain to another, by learning from the behavior of users that appear in both domains [2 3]. Nevertheless, a common limitation of existing cross-domain recommendation methods is that they perform transfer learning primarily based on data of the overlapped users, and fail to function well when there are few or even no overlapped users in two domains [23]. However, in many real-world applications, there is often not a suï¬ƒcient number of overlapped users. In this p aper, we propose Re c GURU, w hich consists of two parts: the Generalized User Representation Unit (GURU) to obtain a single Generalized User Representation (GUR) for each user, and the Cross-Domain Sequential Recommendation (CDSRec) unit to achieve cross-domain collaboration in sequential recommendation task. Instead of mapping user embeddings from one domain to another, we propose to generalize a userâ€™s embedding, i.e., its GUR, to incorpo rate information from both do mains through an adversarial learning framework. Once a GUR is obtained for each user, we integrate the extracted GUR into the CDSRec unit using attention mechanisms to boost recommendation performance. Speciï¬cally, we make the following contribu tions: First, in the GURU module, an autoencoder is proposed to generate informative user embeddings in each domain, which are to be uniï¬ed later into a generalized embedding, the GUR, with adversarial l earning. The autoencoder consists of a self-attentive encoder with model weights shared across both the source and target domains to produce latent user embeddings and two decoders to reconstruct behavior sequences of users in the source and target domains, respe ctively. We train the autoencoder through behavior sequence reconstructions to generate meaningful preliminary embeddings for users with unsup ervised self-learning. The GURU modu le further performs adversarial training to unify domain-dependent user embeddings into a single global (GUR) for each user, which domain-independent. Speciï¬cally, the encoder part of the proposed autoencoder serves as a generator to produce user embeddings, while a discriminator is trained to identify the origin domain of a generated embedding for a user randomly sampled from either the source or the target domain. The encoder and discriminator are trained alternately with adversarial objectives until the discriminator can not distinguish which domain a given user embedding comes from. This is when the user embeddings in the two domains become statistically indistinguishable and GURs are su ppose d to be generalized global user embeddings, incorporating information from bo th domains. This method does not rely on common users present in both target and source domains, therefore eliminating the dependency on overlapped users as required by prior art. Furthermore, we introduce an eï¬€ective and stable training procedure for RecGURU, consisting of three phases. We ï¬rst pre-train the autoencoder to substantially reduce the reconstruction loss which boost-starts the subsequent adversarial learning. In the adversarial learning phase, the reconstruction loss is further jointly optimized in a multi-task fashion which prevents the encoder from generating wild representations, stabilizing adversarial learning. In the meantime, RecGURU can still l everage overlapped users as prior work does, by introducing an ğ‘™penalty in t he optimization procedure to explicitly force each common user to have the same shared embedding in diï¬€erent domains. Finally, t he CDSRec module, which incorporates the GUR with attention mechanisms, is ï¬ne-tuned in t he target domain with the next-item recommendation task t o b oost the recommendation performance. Through extensive experiments, we show that RecGURU has achieved improvement on sequential recommendation, compared to several state-of-the-art single-domain and cross-domain recommendation methods. Speciï¬cally, we outperform all the baselines on various metrics by a large margin on the Amazon dat asets including "Sp ort", "Clothing", "Movie", and "Book". Additionally, we have co llecte d a large cross-domain recommendation dataset with two domains, i.e. â€œWeseeâ€ and â€œTencent V ideoâ€, from t wo real-world applications which provide video streams to millions of users. Ablation studies on the collected datasets with various portions of overlapped users are conducted to show the eï¬€ectiveness of each proposed sub-module as well as the robustness of our method. The collected d atasets will be made public to facilitate future research in the ï¬eld of cross-domain and sequential recommendation. Sequential Recommendation. Early studies adopt Markov Chains (MCs) to capture se quential patterns from usersâ€™ historical interactions [6, 27]. Recently, researchers have been putting eï¬€ort to adapt the Recurrent Neural networks (RNN) and att ention mechanism to solve sequential recommendation problems. GRU4Rec [9] and its improved version GRU4Rec+ [8] leverage Gated Recurrent Unit (GRU) with BPR lo ss to model a userâ€™s se quential behaviors. In SAS [13], unidirectional self-attention is adopted here to encode a userâ€™s historical behavior. Bert4Rec [29] follows the idea of BERT and trained a bidirectional self-attention model. SHAN [34] adopts a hierarchical attention framework where two attention networks are used to model userâ€™s long- and short-term preferences. Cross-domain Recommendation. Cross-domain recommendation alleviates data sparsity issues posed in single domain recommendation via auxiliary information from other domains. CoNet [10] tr ansfers and combines knowledge across diï¬€erent domains through cross-connections between feed-forward neural networks. However, these methods only focus on overlapped users. CATN [36] solves the cold-start problem via aspect transfer which requires side information of both users and items. [16] leverages meta-transfer learning to address the sparsity problem, but it requires multiple contextual information such as the userâ€™s average spend. To transfer knowledge from the source d omain to the target domain, EMCDR [23] learns a mapping function on overlapped users which maps user preferences across domains. DCDCSR [37] maps the latent factors in the target domain to ï¬t the benchmark factors which co mb ines the features in both the target and source domains. To reduce the dependency on overlapped users, SSCDR [12] adopts a semi-supervised strategy. DDTCDR [18] and its improved version DOML[19] adopt dual metric learning (DML). Generative Adversarial Network (GAN) [4] is gaining popularity in cross-domain recommendation[20, 25, 31, 31, 33]. CnGAN [25] introduces the use of the GAN to learn a better mapping function of user representations from the source domain to the target domain. Since the discriminator in CnGAN is trained to distinguish between real and synthetically mapped pairs, where the real mapped pairs only come from overl ap ped users. Thus, CnGAN is critically dependent on the quantity and quality of overlapped users, which usually cannot be guaranteed in reality. Additionally, the CnGAN tries to stabilize the training of GAN by introducing more synthetically mapped pairs. However, this makes t he input data to the d iscriminator unbalanced, thus may fail t o train a good mapping function. target do main. Yan et al. [ 33] adopt adversarial samples in the training process t o improve the generalization ability of the cross-domain recommender system. ATLRec [20] transfers shareable features across domains, however, it focuses on overlapped users which show up in both the target and source domains. Cross-domain Sequential Recommendation. ğœ‹-Net [21] is able to generate recommendations for both domains through a cross-domain transfer unit. However, it requires synchronously shared timestamps and can not be applied to non-overlap ped users. Zhuang et al. [ 38] transfers usersâ€™ novelty-seeking properties learned from the sequential data in the source domain to the target domain. Yuan et al. [35] proposed a framework that is able to ï¬ne-tune a large pre-trained user embedding network to adapt to downstream tasks in the target domain. However, these studies have diï¬€erent problem settings from our work. And they can only be applied to overlapped users, b ut our method can handle both overlapped and non-overlapped users. We ï¬rst formulate the implicit feedback-based sequential recommendation problem in a single d omain. Let ğ‘ˆ = {ğ‘¢, Â· Â· Â· ,ğ‘¢} and ğ‘‰ = {ğ‘£, Â· Â· Â· , ğ‘£} denote t he sets of users and items, respectively, w here |ğ‘ˆ | and |ğ‘‰ | are the total number of users and items. For each user ğ‘¢, âˆ€ğ‘– âˆˆ {1, Â· Â· Â· , |ğ‘ˆ |}, its interactions with items, in chronological order, are denoted as ğ‘ = (ğ‘£, Â· Â· Â· , ğ‘£), where |ğ‘ | is the length of behavior sequence ğ‘ . Formally speaking, given a user ğ‘¢, sequential recommendation aims to predict the next item that the user is most likely to interact with at the next time step |ğ‘ | + 1 based on his or her past behavior sequence ğ‘ , which can be formalized as modeling the probability over all items: In cross-domain scenarios, in order to improve the recommendation performance, user information from other domains is also taken into account. Spe ciï¬cal ly, for two domains ğ´ and ğµ, given an overlapped user who appears in both domains, ğ‘¢âˆˆ (ğ‘ˆâˆ© ğ‘ˆ), cross-domain sequential recommendation tries to improve the recommendation accuracy of the next item in one domain by integrating user information from both domains. For example, for nextitem recommendation in domain ğ´, it can be formulated as modeling the probability over al l possible candidates given the behavior sequences ğ‘ and ğ‘ in domains ğ´ and ğµ: For non-overlapped users, due to a lack of their behavior in both domains, we can only exploit the implicit information from the other domain to enhance the recommendation performance in the target domain. Therefore, Equation (2) is reformulated as where ğ‘†= {ğ‘ }, âˆ€ğ‘¢âˆˆ ğ‘ˆdenotes the colle ction of behavior sequences for all users in domain ğµ and info(Â·) represent a model which is used to extract any kinds of useful information from ğ‘† to assist recommendation in domain ğ´. Inspired by previous studies in single domain sequential recommendations which try to combine the long-term (static) and shor tterm (dynamic) preferences of users for next-item recommendation [1, 32, 34], we propose a novel RecGURU framework. It consists of two parts: a generalized user representation unit (GURU) which learns the generalized user representation across domains and a cross-domain sequential recommendation (CDSRec) unit which takes the GUR as input to achieve cross-domain collaboration for the next-item recommendation task. Here, the GUR represents the static preference contains the user information in both domains. The overall architecture of the proposed model is shown in Figure 1. The GURU takes the long-term behavior sequence as input and generates generalized representations for all users from both domains. Then, the short-term user b ehavior together with the extracted general representation is fed into the CDSRec modul e to enhance sequential recommendation in each individual domain. To construct a GUR, we p ropose an adversarial training framework that involves an autoencoder for generating informative user representations and a domain discriminator to unify the representations across diï¬€erent domains. Furthermore, we apply an additional ğ‘™regularizer on overlap ped users to further ensure each overlapped user has similar representations in both domains. Note that, motivated by the idea of the cross-lingual language models [1 7], we adopt a shared encoder across domains to avoid over-parameterization and achieve further information sharing via parameter-sharing across domains. Figure 1 gives a fur ther example of how input sequences are processed to produce GURs and next-item recommendations. Speciï¬cally, the behavior sequences (ğ‘£, ğ‘£, ğ‘£, ğ‘£, [ğ‘’ğ‘œğ‘ ]) and (ğ‘£, ğ‘£, ğ‘£, [ğ‘’ğ‘œğ‘ ]) of user ğ‘¢âˆˆ ğ‘ˆand ğ‘¢âˆˆ ğ‘ˆ, where [ğ‘’ğ‘œğ‘ ] denotes the end of sequence token, are fed into two individual emb edding layers to produce the embedding vectors (ğ’†, ğ’†, ğ’†, ğ’†, ğ’†) and (ğ’†, ğ’†, ğ’†, ğ’†) containing both item information and sequence position information. Then, the GURU encoder takes the embedded sequences as inputs and generates the latent u ser representations â„and â„of usersğ‘¢andğ‘¢. To make sure the user representation is meaningful and informative, decoders are applied to reconstruct the original input sequences, such that, the encoder and decoder in each domain form an autoencoder framework. Furthermore, in order to get generalized representations which comb ines information from both the source and target domains, the domain discriminator is applied to â„and â„to pul l the d istributions o f user representation in the two domains close t o each other. Hereafter, in each domain, the sequential reco mmendation model combines the GUR and the short-term user behaviors to generate the next-item recommendation, for example, the predicted item embeddingsË†ğ‘°at the 4th timestamp of user ğ‘¢in do main B. As shown in Figure 1, in the GURU module, user representations are learned in each individual domain, and information from the other domain is incorporated to make the representation â€œgeneralâ€ Figure 1: Model st ructure of the proposed RecGU RU. User behavior sequences in both domain ğ´ and ğµ are fed into the GURU encoder to generate generalized latent user representations, i.e. ğ’‰ model for the next-item recommendation in each individual domain. through regularizer achieved by domain discriminator on latent user representation. User Representations in Single Domain. In each individual domain, we use an autoencoder to learn the latent user representation that is capable of reconstructing the original input sequence of the user. As compared to extracting user representations from the next-item prediction task, the autoencoder can produce meaningful representations through the reconstruction task to boost performance. The autoencoder in our framework consists of an embedd ing module, an encoder module, and a decoder module. Formally, to extract a representation fo r any given user ğ‘¢with its behavior sequence ğ‘ = (ğ‘£, Â· Â· Â· , ğ‘£, Â· Â· Â· , ğ‘£), we apply the following procedures: ğ’†, Â· Â· Â· , ğ’†, Â· Â· Â· , ğ’†, ğ’†= ğ¸ğ‘šğ‘ğ‘’ğ‘‘ (ğ‘£, Â· Â· Â· , ğ‘£, Â· Â· Â· , ğ‘£, [ğ‘’ğ‘œğ‘ ]), ğ’‰, Â· Â· Â· , ğ’‰, Â· Â· Â· , ğ’‰, ğ’‰= ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ (ğ’†, Â· Â· Â· , ğ’†, Â· Â· Â· , ğ’†, ğ’†),(4) ğ‘£, Â· Â· Â· , ğ‘£, Â· Â· Â· , ğ‘£= ğ·ğ‘’ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ (ğ’‰), where ğ’‰is the latent representation of item ğ‘£at position ğ‘¡, and ğ’† represents the sum of item embedding and po sitio nal embedding. ğ’†and ğ’‰are the embedding and latent representation of the [ğ‘’ğ‘œğ‘ ] token. We use bold font to indicate vector variables. The input sequence of items and the [ğ‘’ğ‘œğ‘ ] token are converted into real-valued vectors through the Embed module, which consists of an item embedding layer and a positional embedding layer to incorporate both the item information and the sequential information of behavior sequences. To this end, we create two trainable embedding matrices I âŠ‚ Rand P âŠ‚ Rfor item and positional embeddings, respectively, where ğ‘‘ represents the number of dimensions in the latent space and ğ‘ + 1 is the maximum length of input sequences including the [ğ‘’ğ‘œğ‘ ] token. By summing up the output of item embeddings and positional embeddings with and ğ’‰. Then, the generated GUR is fed into the CDSRec point-wise summation, we derive the embedding representations for all items and the [ğ‘’ğ‘œğ‘ ] token, denoted as (ğ’†, Â· Â· Â· , ğ’†, Â· Â· Â· , ğ’†, ğ’†) shown in Equation (4). After the derivation of embeddings, we adopt the autoencoder to obtain the ï¬xed-length representations of the behavior sequences of users. Speciï¬cally, we adopt an encoder with a structure similar to Transformer [30] which is compose d of a stack of identical transformer layers. Each transformer layer is composed of a multihead, bidirectional self-attention layer, and a position-wise fully connected feed-forward layer. Shown in Equation (4), the Encoder outputs a list of latent vectors (ğ’‰, Â· Â· Â· , ğ’‰, ğ’‰) for all input items in the input sequence. Then, the latent vector of the [ğ‘’ğ‘œğ‘ ] token ğ’‰is treated as the representation of the whole behavior sequence, i.e. user representation of ğ‘¢, and is further fed into the decoder. We adopt a decoder that also consists of mult iple transformer layers. In addition to the self-attention and feed -forward sub-layers in each transformer layer, another multi-head attention layer over the user representation ğ’‰is inserted right after the self-attention layer in each deco der layer. Additionally, the masking technique is applie d t o ensure that the predictions depend only on previous behavior sequences. Formally, shown in Equation (4), our Decoder takes the latent user representation ğ’‰as input and reconstructs the original input sequence in an auto-regressive manner [30]. Illustrated in Figure 1, diï¬€erent users have diï¬€erent lengths of behavior sequences, thus, we transform the input sequences of all users into ï¬ xed-length sequences with a length of ğ‘ + 1, including the [ğ‘’ğ‘œğ‘ ] token. Speciï¬cally, for users with sequences longer than ğ‘ (except for the [ğ‘’ğ‘œğ‘ ] token), we consider t he most recent ğ‘ items, for users with sequences shorter than ğ‘ we add [ğ‘ğ‘ğ‘‘] to the left of its original sequence until it has a total length of ğ‘ . Following the convention of autoencoder, by optimizing the reconstruction error between t he input samples and the reco nstructed samples, our model can learn the most important attributes of the input behavior sequence. In this paper, the autoencoder structure is applied to sequence samples, thus we formulate the reconstruction loss for input sequence ğ‘ as follows: where ğ‘ and Ë†ğ‘ are the input and reconstruction output of autoencoder, and ğ’‰= ğ¸ğ‘›ğ‘ğ‘œğ‘‘ğ‘’ğ‘Ÿ (ğ¸ğ‘šğ‘ğ‘’ğ‘‘ (ğ‘ )) denotes the latent user representation of user ğ‘¢as is shown in Equation (4). Moreover, at any position ğ‘¡, if its original item is the [pad] token, we simply ignore the reconstruction loss at this position. Generalizing User Representation Across Domains. We propose to incorporate information from other domains into the user representation learned in a single domain to achieve cross-domain collaboration. Compared to the single-domain sequential recommendation, the representations regularized by the distributional constraints are more general and contain extra information from other d omains. A commonly used technique for autoencoder to integrate prior knowledge is to add extra penalty terms onto the reconstruction o bjective function: where Î©(Â·) denotes the penalty function. For examples, sparse autoencoder (SAE) [22] encourages sparsity of latent vectors b y adding an ğ‘™or KL divergence penalty, variational autoencoder (VAE) [15] assume that the latent representation â„follows a Multidimensional Gaussian distribution. Similarly, to integrate knowledge from both domains, a Kullbackâ€“Leibler (KL) divergence penalty can be added to the reconstruction loss, where the KL divergence measures the d istance between the distributions of the learned user representations in both domains, leading to the following reconstruction objectives in both domains: where ğœŒand ğœŒdenote the learned distributions of latent user representations in domain ğ´ and domain ğµ, respectively. However, there is no closed-form expression for the distributions of latent user representations. Furthermore, the latent distributio ns are characterized by the parameters of the autoencoder, which is being constantly updated along the training process. Instead of directl y estimating the ground-truth distributions, which is extremely diï¬ƒcult, we propose to bypass this challenge by adopting an adversarial training strategy to implicitly minimize the KL divergence between the t wo learned distributions ğœŒand ğœŒ. As illustrated in Figure 1, the encoders generate latent representations ğ’‰âˆ¼ ğœŒand ğ’‰âˆ¼ ğœŒ, where the distributions are characterized by the parameters of encoders in domain ğ´ and ğµ, respectively. A discriminator is then built for a binary classiï¬cation task, where the input is a single latent representation either from domain ğ´ or domain ğµ, while t he output is the prediction on which domain the input representation originates from. We adopt the neg loglikelihood loss as the objective function for the adversarial optimization process, denoted as: L(â„) = âˆ’ğ‘¦ Â· ğ‘™ğ‘œğ‘”ğœ (ğ‘“ (â„)) âˆ’ (1 âˆ’ ğ‘¦) Â· ğ‘™ğ‘œğ‘”(1 âˆ’ ğœ (ğ‘“ (â„))), where I(Â·) is the indicator function which equals to 1 when the condition ğ‘¢âˆˆ ğ‘ˆis true, otherwise, ğ‘¦ = 0. ğœ (Â·) is the sigmoid function and the logits value ğ‘“ (â„) is calculated by the domain discriminator ğ‘“ (Â·). Following the conventional optimization scheme of GAN, we alternately update the domain discriminator and the encoder until the model reaches the point where the discriminator is unable to tell whether a given user representation is from domain ğ´ or ğµ. At this point, the distributions of user representations ğœŒand ğœŒ are su ppose d to be close to each other, i.e., which contributes to reducing the KL divergence ğ¾ğ¿(ğœŒ||ğœŒ) between the two distributions. Therefore, we have achieved information sharing through the distributional constraint on latent user representations wit hout relying on the information of overlapped users. In additio n to implicitly unifying the user representations with all u sers, we also propose an ğ‘™penalty t o enforce the explicit representation sharing over domains for each overlapped user. That is, the representations of the same user shou ld be the same or close across domains. Illustrated in Figure 1, fo r an overlapped user, i.e. ğ‘¢= ğ‘¢, we add an ğ‘™regularizer on its latent representations in domain ğ´ and ğµ: By adding the ğ‘™regularizer to the loss function for overlapped users, representations of the same user in diï¬€erent domains are pushed to be equal or close to each other, exploiting the information of overlapped users in an explicit way. The generalized user representation extracted by the proposed GURU modul e represents the overall preference of u sers in diï¬€erent domains, which is beneï¬cial to the recommendation task in a speciï¬c domain. For the next-item recommendation task in each domain, a sequential recommender is built on the derived GURs and it is composed of mul tiple unidirectional attention layers and feed-forward layers. Speciï¬cally, the GURs are passed into the model through multi-head attention mechanisms. Formally, for a given user ğ‘¢with se quence ğ‘ = (ğ‘£, Â· Â· Â· , ğ‘£), the sequential recommender takes its generalized user representation ğ’‰and short-term behaviors (ğ‘£, Â· Â· Â· , ğ‘£) as the input and outputs the current p reference vecto r of the user ğ’’at time step |ğ‘ | in the latent space, given by where ğ‘š denotes the length of the short-term behavior sequence and the GUR is extracted from the long-term behavior sequence Input: User sets ğ‘ˆ, ğ‘ˆ, item sets ğ‘‰, ğ‘‰; Behavior sets: ğ‘†= {ğ‘ }, âˆ€ğ‘¢âˆˆ ğ‘ˆ, ğ‘†= {ğ‘ }, âˆ€ğ‘¢âˆˆ ğ‘ˆ; Overlapped users ğ‘ˆ= ğ‘ˆâˆ© ğ‘ˆ. Hyper-parameter: CRITIC_ITERS. Initialization: Initiate the item and positional embedding matrices: Initiate the GURU, CDSRec and the domain Discriminator. Pre-training Phase: Train autoencoders according to (5) on ğ‘†and ğ‘†. Multi-task Adversarial Training Phase: while not converged do for ğ‘– â† 0 to CRITIC_ITERS do Sample two batches of usersÂ¯ğ‘ˆâˆ¼ ğ‘ˆ,Â¯ğ‘ˆâˆ¼ ğ‘ˆand one batch of overlapped usersÂ¯ğ‘ˆâˆ¼ ğ‘ˆ. Fix the parameters of the domain Discriminator. Train the model according to the loss deï¬ned in (13). Fine-tune: Fine-tune the CDS Rec model according to the BPR loss deï¬ned in (12) in each domain. (ğ‘£, Â· Â· Â· , ğ‘£) of user ğ‘¢. CDSRec denotes the cross-domain sequential recommender. The preference scores of t he user to all the candidate items are then computed as the inner prod uct between its current preference ğ’’and the it em embeddings of all candidates, denoted as where ğ‘°is the item embedding of ğ‘£ and the candidate set ğ‘‰âŠ‚ ğ‘‰ is a su bset of the the entire item set ğ‘‰ . Candidate items are then ranked and recommended according to the calculated preference scores. We adopt a Bayesian Personalized Ranking (BPR) loss [26] to train the recommendation model. For a given user ğ‘¢from domain ğ´, we calculate the loss of an item recommendation at time step ğ‘¡ L= âˆ’ log ğœ (ğ’’ğ‘°) âˆ’ lo g(1 âˆ’ ğœ (1|ğ‘|ğ’’ğ‘° where ğ‘£ and ğ‘°are the target item and its corresponding item embedd ing, ğ‘is the set of negative item samples and ğœ (Â·) represents the sigmoid function. We propose a three-phase t raining algorithm shown in Algorithm 1 to optimize the proposed model. In the ï¬rst phase, we pre-train the autoencoders in each domain individually with the reconstruction task. Through the pretraining process, the reconstruction loss is largely reduced, producing a boost-start for the following adversarial training. In the Table 1: Statistics for the three cross-domain scenarios. second phase, following the training process of GAN in [5], at each iteration, we ï¬rst optimize the discriminator loss Lfor CRITIC_ITERS steps (which equals to 5 in our implementation). Then, the reconstruction task, negative discriminator loss, and ğ‘™ loss on overlapped users are jointly optimized in a mult i-task fashion b y minimizing the loss: With the reconstruction task, we prevent encoders from generating wild representation stabilizing the adversarial learning. Following the common practice in GAN training [5], we also adopt the gradient penalty term in the critic optimization step. Finally, we ï¬ne-tune the CDSRec model in each individual domain with the next-item recommendation task. In this section, we conduct extensive experiments on two p ublic and one collected cross-domain sequential recommendation scenarios. Comparison with the state-of-the-art single domain and cross-domain baselines shows the eï¬€ectiveness of our proposed method. Furthermore, ablation tests demonstrate the impact of each proposed sub-modules on the recommendation result and the robustness of our model under scenarios of a diï¬€erent portion of overlapped users. Four publicly available Amazon datasets [24] and two collected datasets are used to form three cross-domain sequential recommendation datasets: â€œSport s-Clothingâ€, â€œMovie-Bookâ€, and the collected dataset. On Amazon datasets, only recent positive reviews, posted after October 1, 201 7, with a rating score higher than 2, are selected. We collect data from two popular video applications, â€œWeseeâ€ and â€œTencent Videoâ€, both with over billions of daily active users. Watching histories for three consecutive d ays,from June 26, 2020, to June 28, 2020, of these two applications are collected in this dataset. The detailed breakdown of the three cross-domain sequential recommendation datasets is shown in Table 1. Note that, the collected dataset mostly consists of overlapped users, thus, we can manually adjust the portion of overlapped users in a wide range to test the robustness of our method. Following common practices in sequential recommendation [29], for a given user the second to last item in the behavior sequence is selected as the validation item, and the last item is used for testing, while the remaining items are used for training. Table 2: Comparison between the proposed method with single-domain and cross-domain baselines on â€œSport-Clothâ€ and â€œMovie-Bookâ€ scenarios. â€œAutoRecâ€ and â€œAutoEMâ€ are two variants of our proposed metho d . All values are in percentage. Datasets MetricPOP BPRMF SAS Bert4Rec AutoRec CMF MFEM CnGAN DOML AutoEM RecGURU SportHR@10NDCG@5 ClothHR@10NDCG@5 MovieHR@10NDCG@5 BookHR@10NDCG@5 Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG) [11] are adopted to evaluate the performance of all methods. We follow the strategy used in [7] to reduce the heavy computation cost. Spe ciï¬cal ly, for users from public datasets, we sample 200 negative items in the item list with respect to their frequencies, which t ogether with the ground-truth item, form the candidates for recommendation. On collected datasets, the size of the candidate set becomes 2 0,000. HR and NDCG with ğ‘˜ = 5, 10, 20 are reported. The following single-domain and cross-domain baseline algorithms are evaluated. â€¢ POP: All items are recommended according to their popul ar ities. â€¢ BPRMF [26]: It o ptimizes the matrix factorization with the BPR loss. â€¢ SAS [13]: It adopts unidirectional self-attention to model user behaviors. â€¢ Bert4Rec [29]: It incorporates t he idea of Bert [3] to the next item recommendation task. â€¢ AutoRec: It is the single-domain version of proposed model that adopts autoencoder to generate static user representation which is used for the sequential recommendation. â€¢ CMF [28]: It simultaneously factors interaction matrices in both target and source domains. â€¢ MFEM: It learns a mapping function on overlapped users. Thus, for non-overlapped users, we can get their embeddings in the source domain through the trained mapping function as well as the cross-domain recommendation through Equation (2). Here, the user embeddings are learned with the BPRMF model. â€¢ CnGAN [25]: It adopts adversarial learning to learn a better mapping function that maps user embedd ings from the source domain to the target domain or vice versa. â€¢ DOML [19]: It adopts dual metrics learning in cross-domain recommendation when there are few overlapped users. â€¢ AutoEM: As a variation of proposed method, it also learns a mapping function on overlapped users to get the embeddings of non-overlapped users. However, here, the item and user embeddings are learned by the proposed AutoRec model. We implement the models using PyTorch with python 3.6 and train our framework on Tesla P40 GPUs with a memory size of 22.38 GiB and a 1.53 GHz memory clock rate. On Amazon datasets, 3 transformer layers are adopt ed, whereas 6 transformer layers are used on the col lected datasets in the encoder and decoder modul e. We use 2 attention heads for all the attention layers throughout the mod el. And the dimension of all feed-forward l ayers is set to 51 2. For adversarial training, we build the domain discriminator with four fully connected layers with a hidden size of 128. Furthermore, we adopt the improved W-GAN [5] framework to alternately optimize the domain discriminator and the GURU model through the discriminator loss L. For simpliï¬cation, we adopt the same length of sequence for both autoencoder and recommendation tasks, which is 100 o n all datasets. The number of dimensionality of user embedding is 64 on both public and collected datasets. Multiple Adam optimizers [14] are used to update d iï¬€erent modules of the proposed RecGURU framework. For all the transfer learning-based cross-domain baselines, we ï¬rst concatenate the user embeddings from the source and target domains. Then, a full y connecte d layer is applied to get the crossdomain user embedding which is further used for next item recommendation. More details are given in the supplementar y. Training Loss. Figure 2 shows the training losses on t he â€œMovieBookâ€ dataset. Speciï¬cally, the Wasserstein distance and the critic loss, i.e. discriminator l oss, are given in Figure 2(a). Both processes Table 3: Ablation studie s on customized collected datasets with the portion of overlapped users ranging from 10% to 75%. All values are in percentage. Figure 2: Training losses on the â€œMovie-Bo ok â€ dataset. converge around a thousand adversarial iterations. In the beginning, the discriminator is goo d at distinguishing representations from diï¬€erent domains with an increasing Wasserstein distance. After more adversarial training iterations, the Wasserstein distance is reduced and converged which in line with the training process of standard GANs [5]. The reconstruction tasks in both target and source domains converge after 400 iterations as is shown in Figure 2(b). The recommendation task in the target domain converges after only 100 steps of ï¬ne-tune iterations. Main Results. Table 2 summarizes the performances of all baselines and the propo sed method s on the two public cross-domain scenarios. AutoRec, the single domain version of o ur solution, o utperforms SAS and Bert4rec on single domain sequential recommendation tasks on most of the public datasets. The success of AutoRec can be attributed to the adoption of user representations extracted from the history user behavior through an autoencoder. The proposed RecGURU outperforms all other baselines with an improvement on HR@10 and NDCG@10 of 6.5%, 9.2%, and 22.5%, 31.3% on the â€œSportsâ€ and â€œMovieâ€ datasets, respectively, co mpared to the best given by all the other baselines. We can also outperform the state-of-the-art single-domain and cross-domain methods on â€œClothâ€ and â€œBookâ€ datasets in most cases but with a relatively smaller margin compared to the results repor ted on the â€œSportâ€ and â€œMovieâ€ datasets. This is reasonable, as is shown in Table 1, the â€œSportâ€ and â€œMovieâ€ datasets are mu ch smaller and with more sparsity than the â€œClothâ€ and â€œBookâ€ datasets. Obviously, when the data in the source domain is highly sparse, we can only get a limite d amount of information from the source domain to help with recommendations in the target domain. Therefore, AutoRec, AutoEM, and RecGURU achieve close performance on the â€œBookâ€ dataset. However, overall RecGURU outperforms other baselines in mo st cases. The superiority of RecGURU over AutoRec and AutoEM can be attributed to the generalization of user representations achieved through the adversarial training of the domain discriminator and the autoencoder. Furthermore, compared with the improvements RecGURU has achieved over single domain sequential recommendation baselines, cross-domain methods such as MFEM, CnGAN, and DOML achieved limited improvements over BPRMF. This can be attributed to the fact that these baselines either rely on overlapped users (MFEM, CnGAN) or need side information for more general user and item embeddings (DOML). Variants of the proposed method are evaluated on the collected datasets for ablation studies to show the impact of each propose d sub-modules. We incrementally accommodat e diï¬€erent modules into the single-domain sequential recommendation model SeqR ec, until we incorporate all the proposed sub-modules and features. Speciï¬cally, the following models are evaluated: â€¢ SeqRec: Sequential recommendation model without autoencoder. â€¢ +Au to: The AutoRec model intro duced in Sec. 4.1. â€¢ +GURU: The proposed RecGURU model. Furthermore, to t est the robustness of the proposed method under a variety o f overlapping rates, we manually change the number of overlapped users to form four new datasets with an overlapping rate of 10%, 30%, 50%, and 7 5%. Table 3 summarizes the results of ablation tests of all the introduced variants on all datasets wit h various overlapping rates. On the â€œWeseeâ€ dataset, each time we add a new sub-module or feature incrementally on top of the previous model, we can observe improvement in the overall recommendation performance, which illustrates the eï¬€ectiveness of autoencoder and GURU modules. Moreover, this phenomenon appears on all the four datasets with overlapping ranging from 10% to 75% which shows the robustness of our method to user overlapping rate. Similar to the â€œBookâ€ dataset, o n the â€œTencent Videoâ€ dataset, the single-domain version of our proposed method, AutoRec, is slightly better than its cross-domain version, which is also due to the sparsity issue in the source domain, i.e. â€œWeseeâ€, as we have explained before. In this pap er, we propose RecGURU, a novel cross-domain sequential recommendation framework based o n Generalized User Representations (GURs). Diï¬€erent from previous work w hich aims to transfer knowledge across domains, in the RecGURU system, we propose the GURU module that is capable of extracting a generalized user representation uniï¬ed over diï¬€erent domains via adversarial learning. Speciï¬cally, an autoencoder is adopted to generate user representations in each individual domain. Cross-domain generalization of user representations is achieved by adversarially training a discriminator and the encoder until the domain-dependent embeddings are statistically indistinguishable among diï¬€erent domains. We further propo se various schemes to stabilize and boost the learning eï¬€ectiveness of RecGURU. Experimental results on both publicly available dat asets and col lected datasets show the eï¬€ectiveness of the proposed method.