The key to personalized news recommendation is to match the userâ€™s interests with the candidate news precisely and eî€œciently. Most existing approaches embed user interests into a representation vector then recommend by comparing it with the candidate news vector. In such a workî€ow, î€›ne-grained matching signals may be lost. Recent studies try to cover that by modeling î€›ne-grained interactions between the candidate news and each browsed news article of the user. Despite the eî€ectiveness improvement, these models suî€er from much higher computation costs online. Consequently, it remains a tough issue to take advantage of eî€ective interactions in an eî€œcient way. To address this problem, we proposed an endto-endSelectiveFine-grainedInteraction framework (SFI) with a learning-to-select mechanism. Instead of feeding all historical news into interaction, SFI can quickly select informative historical news w.r.t. the candidate and exclude others from following computations. We empower the selection to be both sparse and automatic, which guarantees eî€œciency and eî€ectiveness respectively. Extensive experiments on the publicly available dataset MIND validates the superiority of SFI over the state-of-the-art methods: with only î€›ve historical news selected, it can signiî€›cantly improve the AUC by 2.17% over the state-of-the-art interaction-based models; at the same time, it is four times faster. Nowadays, people are overwhelmed with information, exhausted to seek things theyâ€™re interested in. Online news platforms e.g. MSN Newsgreatly alleviate this information overload problem by recommending news articles according to userâ€™s speciî€›c interests [16,19,31,41]. The key technology of these news platforms is personalized news recommendation [13]. Due to the particular large-scale and time-sensitive property of news, the news recommenders must be both eî€ective and eî€œcient so that it can be deployed in real production systems. Figure 1: Example of a userâ€™s behavior log in MSN. Three historical news articles of the user are shown andcis the news she actually clicked afterward. The text marke d in the same color is the î€›ne-grained (term-term) matching signals. The darker the color, the more they match. A lot of existing news recommendation approaches [1,1,16,21, 29â€“33,39] follow a representation-based matching strategy. They learn a representation vector for the candidate news and encode the userâ€™s history news into a vector to form the user representation in the same semantic space. The matching score between these vectors is calculated as the click probability. However, the user vector is an aggregation of multiple historical articles, so it hardly keeps the î€›ne-grained information and may contain noise in the articles. For example in Figure 1, the candidate news matches userâ€™s î€›ne-grained interests Biden and Hillary (î€›ne-grained interaction happens), which motivates the userâ€™s current click. Unfortunately, the aggregated user vector mixes all terms inh, handhand blurs these î€›ne-grained interests, thus degrades the capacity of user modeling. Even worse, noises such as wind and wildî€›res are also included and they are unrelated to the current click. Though some recent â€œmulti-channelâ€ methods [2,20] attempt to cover richer information by maintaining multiple representation vectors, they are still limited to modeling î€›ne-grained interaction in an explicit and reliable manner. In order to capture î€›ne-grained matching signals between the candidate news and the user, Wang et al. [26] proposed an interactionbased model. It computes similarity matrices between the candidate news and every historical news piece of the user at word level to derive the click probability. Despite the eî€ectiveness improvement, the model is especially slow. It has to recompute term-level interaction matrices with every historical article when scoring each candidate, which is far more expensive than dot product used in representation-based methods. Intuitively,we shouldnâ€™t involve all the historical articles in interaction. For example,hshould be excluded within this click because it is irrelevant to the current candidatecand there would be no interactions between them. Tailoring the user history to several recent browsed news items seems to be a straightforward solution to save eî€œciency. However, blindly interacting with only the latest browsed news limits the recommendation eî€ectiveness, mainly because: 1) When the length of the kept browsing history is short, there isnâ€™t suî€œcient news for the model to learn the userâ€™s interests well. Back to the Figure 1, if we cut oî€ earlier history news hand h, the interaction quality would greatly decrease since the most informative one, h, is lost. 2) When the capacity becomes larger, irrelevant historical news, such as h, is involved. As mentioned above, such news is harmful to the recommendation accuracy. It hardly contributes to motivate the click and would act like noise, reducing the matching score of the candidate that the userâ€™s truly interested in. To tackle the above problem, in this paper, we proposeSFI, a SelectiveFine-grainedInteraction framework. The key idea of it is toselect a small number of historical news articles with higher informativeness, then perform î€›ne-grained interaction over them only. The biggest challenge of SFI is to select the informative history news sparsely, precisely, and eî€œciently. Most previous works about feature selection employ gating operator [7,18,40]. But it cannot be directly used in SFI because gating doesnâ€™t eliminate zero entries so the total computations are not lessened. Two-stage training [8] is not desirable either, because no ground-truth label (indicates which historical news interacts with the candidate) is available to train the selector. Last but not least, weâ€™d better manipulate news-level representations to guide selection for the sake of eî€œciency. In this work, we design thelearning-to-selectmechanism to fulî€›ll all our goals. Speciî€›cally, SFI learns a selection vector for each news article, and computes the cosine similarity between candidate news and every history news in the selection space, taking the result as the informativeness of each historical article. Next, we design two successive selection networks. The hard-selection network enforces sparsity. It selectsğ¾most informative news and excludes others from following interactions. Within the output of the hardselection, the following soft-selection network masks news whose informativeness is below a given threshold and attaches diî€erent weights to the unmasked ones. This reî€›nement allows the gradient î€ow through to optimize the selection vectors, so that the model can learn to highlight valuable features for selection hence achieve higher eî€ectiveness. Extensive experiments on the publicly available dataset MIND show that SFI outperforms all baselines in terms of both eî€ectiveness and eî€œciency: with only î€›ve historical news articles selected, it signiî€›cantly improves the recommendation eî€ectiveness by 2.17% over the state-of-the-art interaction-based models with four times faster speed (almost reaches the fastest speed of representationbased methods and outperforms it by 2.71% in AUC). We also comprehensively compare SFI with its naive recentğ¾counterpart and investigate the eî€œciency eî€ectiveness trade-oî€ brought by SFI. The main contributions of this paper can be summarized into three aspects: (1) We propose SFI, a selective î€›ne-grained interaction framework, to take full advantage of the î€›ne-grained interaction in a highly eî€œcient way. (2) We design the learning-to-select mechanism to sparsely and automatically select informative historical news w.r.t. the candidate. (3) We conduct extensive ablation studies to verify the advantage of selection; and further investigate the eî€œciency-eî€ectiveness trade-oî€ that SFI achieves. In this section, we î€›rst review the traditional recommendation methods, then the neural news recommendation methods. A lot of traditional recommendations methods [4,12,15] are based on collaborative î€›ltering (CF). CF-based methods cluster users by â€œco-visitationâ€ relationships to recommend news to similar users [4]. Another line of CF studies apply Matrix Factorization [12] and Factorization Machine [23] to model the interaction between users and items. However, these methods face the problem of cold-start and sparsity, which is severe in news domain. They also require diî€œcult and labor-consuming feature engineering. As the counterpart of CF, content-based recommendation methods become the main focus of news recommendation [14,19] because of the rich text information in news articles. In recent years, deep learning techniques are widely used in news recommendation systems and achieve better results than traditional methods. They can be categorized as follows: 2.2.1 Feature-based Methods. Following traditional recommendation approaches, feature-based methods feed the model with news content together with manually designed features, then employ neural networks to model the complex interactions among all the features [3,6,17]. For example, Cheng et al. [3] propose to combine shallow and deep neural networks to extract valuable information from a variety of manual features. Guo et al. [6] add deep layers over the factorization machine to model high order interactions. 2.2.2 Representation-based Methods. More methods proposed to learn representations of news and users from raw texts and browsing histories respectively [1,10,16,21,29â€“33,39]. Numerous well designed models are proposed: multi-layer perceptron over trigrams [10], denoising auto-encoder [21], convolution neural networks [1,16,29â€“32,39], and various attention-based methods [29, 30,33]. Multi-channel structure is also explored [20]. Besides, some approaches [9,34,36] focused on using graph neural networks to represent news and users with their neighbors. Several methods [27, 28] proposed to incorporate knowledge to construct knowledgeaware representations of news and users. In spite of the improvements these methods have made, all of them embed the news and the user into one or several one-fold vectors in the semantic space, where the î€›ne-grained information is limited. And the representations can only meet each other in the prediction phase, which may impair î€›ne-grained matching signals between the user and the candidate news. 2.2.3 Interaction-based Methods. To address the above problem, interaction-based models, which match userâ€™s interests with the candidate news at more delicate levels, are proposed. Wang et al. [26] designed the state-of-the-art interaction-based method for news recommendation. They constructed segment-to-segment similarity matrices between the candidate news and every historical news article of the user from 3 diî€erent granularities. Then they use 3D-CNN to highlight salient matching signals to make recommendations. Although FIM achieves better results feature- and representation-based methods, its main drawback is the especially slow inference speed. In this work,we explore the interaction-based methods and aim to eî€œciently select fewer historical articles with higher value to perform interactions. Several works in other î€›elds have proposed to select important features for interaction [7,18,40], but they all employ the gating operator, which fails to discard the unselected items hence cannot be directly used to achieve our goal. The most related work [8] splits selection to another training stage, forbidding the model learning to select. Our proposedlearningto-select mechanism eî€ectively addresses these issues. First we formulate the news recommendation problem. Given a userğ‘¢, we have a set of historical news articles browsed by her at the platform, denoted asH = {h,h, Â· Â· Â· ,h}. For a candidate news c, our goal is to infer the probability that the user clicks this news article based on her browsing historyH, denoted asğ‘ (c|H ). The architecture of SFI is presented in Figure 2. Speciî€›cally, it contains four major modules. Thenews encoder modulelearns word- and news-level representations, the î€›ne-grained ones are intended for interaction and the coarse-grained ones are further transformed for selection. The followinghistory selector module manipulates news-level representation to eî€œciently and precisely select informative news from the userâ€™s browsing history. The î€›negrained representations of selected news are fed into thenews interactor moduleto compute interactions. The coarse-grained matching signals are also modeled in this module. Finally theclick predictor moduleincorporates all matching signals to predict the click probabilityğ‘ (c|H ). Next, we introduce each component in our model, especially the history selector. Since usersâ€™ click decisions on news platforms are usually based on the title of news articles [30], the news encoder learns the news representation from title only. Denote the word sequence of a news title asğ‘† = {w,w, Â· Â· Â· ,w}, whereğ‘is the length ofğ‘†. First of all, we transformğ‘†into a sequence of vectorsğ¸ = {e, e, Â· Â· Â· , e} by word embedding matrixWâˆˆ R.ğ‘‰is the vocabulary size and ğ· is the dimension of embeddings. Usually, the local contexts of a word across diî€erent spans play a big role in representing the word [26,29,30]. Therefore, we employ a hierarchical dilated convolution [38] to extract context features from diî€erent semantic granularities. In theğ‘™-th convolution layer, the representation of the ğ‘–-th word is calculated as: ! whereÃ‰means the concatenation operation for vectors.Fis the convolution kernel of size 2ğ‘¤ +1,ğ›¿denotes dilation rate,bdenotes bias andğ‘“denotes the number of î€›lters. A detailed description of dilated convolution can be found in [26]. By hierarchically stacking dilated convolutions with expanding dilation rate, local contexts of diî€erent distances are fused into the word representations. Afterward, the output of each convolution layer is appended to the î€›nal representation ofğ‘–-th word:r= {r}, where{Â·}denotes the vertical alignment of a matrix, andğ¿denotes the total number of the stacked convolution layers. The representation of each semantic level may contain information of diî€erent importance for matching. For example, in the news title â€œRestaurants to Satisfy Late Night Cravings in Louisville and Beyondâ€, phrase-level local contexts â€œLate Night Cravingâ€ for the word â€œNightâ€ matter more than that of sentence-level, e.g. â€œRestaurants ... Night ... Andâ€. Therefore, we use an attentive pooling technique [1,29,39] to highlight the important local contexts of a single word. Speciî€›cally, a trainable vectorsqâˆˆ Ris introduced as the query of attention. The representationrof the word wthat fuses information across every semantic level is computed as: Similarly, diî€erent words may contribute diî€erently in expressing the news. For example, â€œLouisvilleâ€ is more informative than â€œBeyondâ€ because it reveals the location. We use another query vector qâˆˆ Rto highlight the informative words in the news title and obtain the overall representation of the entire news: So far, the î€›ne-grained representation of each wordr= {r} and the coarse-grained representation of the entire newsrare generated by news encoder. We further explore other kinds of stateof-the-art encoders, and study their performance in Section 5.3. The history selector is the core component of our model. It selects the informative historical news sparsely, automatically, and eî€œciently with a learning-to-select mechanism. Then the selected news pieces are fed into next module for î€›ne-grained interactions. Denote the news-level representation ofğ‘–-th clicked news in the userâ€™s history ash, and that of candidate news asc. Recall that the news-level representation is attentively aggregated from the word vectors in the title, which are optimized for the î€›nal matching and thus arenâ€™t selection-oriented. Therefore, directly usinghandcfor selection may lead to sub-optimal results. In learning-to-select, we project all the news-level representations into the same selection space using a fully-connected network to mitigate such conî€icts: wherercould be eitherhorc. Considering selection eî€œciency, we deî€›ne the candidate-aware informativeness of every historical article as the cosine similarity between selection vectors: In selection, no supervision signals are available fors, so itâ€™s critical to optimize the parameters end-to-end to allow the model learn valuable features for selection. Besides, sparsity must be enforced, otherwise the model would still be slow. Keeping both constraints in mind, we design two complementary selection networks. 3.2.1 Hard-Selection Network. This sub-module enforces sparsity: it keeps the topğ¾most informative history news and discards others. Formally, whereargTOPK(s)gets the index of the topğ¾value in the vector s. The corresponding history news sliced byxis selected and its î€›ne-grained representations will get involved in the î€›ne-grained interaction later. To do that, we î€›rst transformxto one-hot encoding matrixX = one_hot(x) âˆˆ R, where theğ‘–-th row inXis the one-hot vector ofx, then use matrix multiplication to prune the browsing history to a smaller size: whereğ» = {{h}}âˆˆ Ris the î€›ne-grained representation tensor of userâ€™s browsed news,Ë†ğ» âˆˆ Ris that of the selected news. By regulating hyper parameterğ¾, we can elastically control the modelâ€™s eî€œciency. However, this sub-module has three defects that may limit the eî€ectiveness: 1) BecauseXis sparse, the gradient cannot be passed to optimizeW; 2) Because the informativeness distributions of diî€erent users vary greatly, some noisy news articles are not î€›ltered out among topğ¾; 3) All of the selected news articles are weighted equally even though some of them are more informative. To tackle the above problems, a soft reî€›nement is proposed. 3.2.2 Soî€œ-Selection Network. This sub-module makes the gradient î€ow through selection. It is essentially a gating operator with a threshold [40], which further rules out noise (namely authentically uninformative news) and improves eî€ectiveness. Given the output of hard-selection, the soft-selection network masks the news whose informativeness is below the threshold and attaches diî€erent importance to the unmasked ones: âŠ™is Hadamard Product, andğ›¾denotes the threshold.ğœ (Â·)is elementwise.Expand(Ëœs)repeats the elements inËœs, expanding it intoR. Ëœğ»is the reî€›nedË†ğ», where all representations of the news whose informativeness is lower than ğ›¾ are masked as 0. The number of the authentically informative news articles is î€oating per candidate, so a dynamic quantity of news items is kept. This entitles more î€exibility to the selection operation. Meanwhile, withËœğ‘ attached toË†ğ», the news interactor can attend to more informative ones, and the parameters inProj(Â·)can be optimized by the selecting step since the element-wise multiplication is diî€erentiable. This helps SFI to learn features that are important for selection and will enhance the eî€ectiveness remarkably. In back propagation, gradient from the loss function is applied to the news interactor, then to the selected î€›ne-grained representation tensorËœğ». For simpliî€›cation,Ëœğ»is reshaped into a vector ËœR âˆˆ Rwhereğ‘‘ = ğ¿ Ã— ğ‘“, together with its gradientâˆ‡= reshape(âˆ‡) âˆˆ R. The same operation is taken forË†ğ», formingË†R âˆˆ R. Then the gradient for Wis: whereDiag(Ë†R)is the matrix with the elements ofË†Ras the diagonal. Z = {h}âˆˆ Ris the news-level representation matrix of the historical news, andZ= {Proj(h)}, c= Proj(c)are the corresponding selection vectors. ğ‘”(ğ‘ ) is the derivative for ğœ (ğ‘ ): In this way, the gradient safely î€ows through the selection stage and reachess, to increase the score of the useful news pieces and vice versa. It is further spread to optimizeWto achieve the above adjustment, however, only from the selected entries. The selected historical news articles are fed into this module to perform î€›ne-grained interactions with the current candidate. We denote the representation of the words inğ‘£-th selected news asd= {t}wheret= {t}âˆˆ Ris the stacked representation of ğ‘—-th word. Similarly, the representation of each word in the current candidate news isc= {p}. Resembling FIM [26], we construct pair-to-pair similarity matrixMofğ‘™-th semantic granularity, where each entry is the scaled dot product between the î€›ne-grained representations of ğ‘£-th selected news and the candidate news: Next, the similarity matrices of each granularity across all the selected history news are fused into a 3D cubeğ‘‚ âˆˆ R, where a series of 3D CNN and 3D max pooling is applied to highlight the signiî€›cant matching signals. Outputs of the î€›nal pooling layer are î€attened as the vector containing î€›ne-grained interactive information across the user and candidate news, denoted asğœ™. Other state-of-the-art interactors are studied in Section5.3. In SFI, î€›ne-grained matching informationğœ™only engage selected news articles. However, it is important not to leave out the unselected ones. Although conducting î€›ne-grained interactions on them is unnecessary, we still value the coarse-grained matching signals of them, which come from the matching between news-level representations: ğœ“= {ğœ“,ğœ“, Â· Â· Â· , ğœ“= h ğœ“gives an overall matching degree of the user and the candidate news and is complementary toğœ™. It facilitates the model to learn more precise correspondences between the matching signals and the click probability. Another critical point is that by involvingğœ“ #impressions15,777,377#clicks24,155,470 to score the candidate, the gradient can be delivered by all of the historical news articles rather than only the selected ones. The click predictor module incorporates the output from news interactor then predicts the probability of a user clicking on a candidate news article. The news articles with higher click probability are ranked higher in the î€›nal user interface. Given vectors containing course- and î€›ne-grained matching information,ğœ“andğœ™respectively, we propose to incorporate both by:î€ˆî€‰ Following [10,30], we use negative sampling to simulate the unbalanced distribution of clicked news in an impression. For each ground-truth candidate, we randomly sampleğ‘šnews that is not clicked by her in the same impression as negative samples: Thus, it is converted to ağ‘š +1 classiî€›cation problem, and the negative log likelihood loss is going to be minimized when training:îƒ• wherecis the ground-truth news piece which the user clicked, and S denotes all training samples. Finally, we jointly train the news encoder, histor y selector, newsinteractor and click predictor through the î€›nal click signal. In such a way, the model can better learn dependencies among modules. Our experiments are conducted on MIND [35], a large-scale dataset collected from the usersâ€™ click logs of the Microsoft News platform from Oct. 12 to Nov. 22, 2019. The statistics of MIND are shown in Table 1. We use the same training-testing partition as [35]. In our experiments, the dimensionğ·of word embeddings is set to 300. We use the pre-trained Glove embeddings [22], to initialize the embedding matrixW. The maximum length of news titles is set at 20. the maximum number of clicked news for learning user representations was set to 50. In the news encoder, we stack 3 convolution layers with dilation rates[1,2,3]. The kernel size and the number of î€›lters is set to 3 and 150 respectively. We employ a 2layer composition for news-interactor module, the output channels and the window size is set at 32âˆ’ [3,3,3]and 16âˆ’ [3,3,3]. Each convolution component is followed by a max pooling layer with size[3,3,3]and stride[3,3,3]. We apply the dropout strategy [25] to the word embedding layer to mitigate overî€›tting. The dropout rate is set at 0.2. Adam [11] is used as the optimization algorithm. The batch size is set to 100 when training and 400 when predicting, and the encoding process is executed oî€Ÿine when predicting. Since there are 40 kernels in total on our machine, we set 40 parallel threads to load data in order to minimize the latency caused by processing data. We independently repeat each experiment for 5 times and report the average performance. We conduct all experiments on a machine with Xeon(R) Silver 4114 CPUs and a TITAN V GPU. Following existing studies, we use the average AUC, MRR, nDCG@5, and nDCG@10 scores over all impressions to evaluate the eî€ectiveness of the models. All results come from the oî€œcial test entry. Moreover, given the same batch size, we use the prediction speed i.e. iterations per second to evaluate the eî€œciency. In one iteration, the batch size of candidate news articles is scored. We compare SFI with the following baseline methods: (1) General Recommendation Methods:LibFM[24], a state-ofthe-art feature-based matrix factorization approach for recommendation;DSSM[10], a deep structured semantic model that uses multiple dense layers upon tri-grams. All of the usersâ€™ clicked news are concatenated as the query, and the candidate news is regarded as documents;Wide&Deep[3], a widely used recommendation method that uses the combination of a wide channel and a deep channel for memorization and generalization;DeepFM[6], a popular neural recommendation method which combines factorization machine with deep neural networks; (2) Representation-based Methods:DFM[17], which uses dense layers for diî€erent channels and attentively fuse outputs;GRU[21], which learn news representations with an auto-encoder and utilizes GRU to learn user representations;Hi-Fi Ark[20], a multi-channel representation approach for recommendation;NPA[30], which highlights informative words and news with personalized attention; NRMS[33], which learns delicate representations of news and users by multi-head self-attention;LSTUR[1], which models longand short-term user interests with GRU; (3) Interaction-based Methods:FIM[26], the state-of-the-art interaction-based approach for neural news recommendation, which encodes news by hierarchical dilated CNN and performs interaction between each of the user browsed news articles and the candidate. Recent(ğ¾), the naive counterpart of SFI, which keeps the recentğ¾ historical news for interaction only (Recent(50) equals FIM). The overall recommendation eî€ectiveness of all models is shown in Table 2. Based on the results, we have the following observations: (1)Our proposed model SFI consistently outperforms other baselines in terms of all metrics. On the one hand, SFI captures î€›ne-grained interactions to model user interests, gaining 2.71% up to 3.23% AUC improvements over all of the state-of-the-art Table 2: The performance of diî€erent metho ds for news recommendation. The number of news items involving in interactions is bolded in ( ) for interaction-based methods. The result with superscript * is referencing the one in [35] where MIND is presented. â€  indicates a signiî€›cant improvement over all baselines with paired t-test (ğ‘ < 0.01). Interaction based0.347537.8643.51 representation-based methods. On the other hand, SFI(ğ¾)outperform Recent(ğ¾) baseline by 6.4% and 2.17% whenğ¾ =5 and 50 respectively. This result substantiates the power of the learning-toselect mechanism. (2) The variant SFI(50) that keeps the entire browsing history outperforms SFI(5) that selects only 5 historical news. This is as expected because after removing noise, SFI(50) covers richer information to model the user. Interestingly, the improvement is tiny compared with the margin between Recent(50) i.e. FIM and Recent(5). We will study this phenomenon in detail in Section 5.4.1. (3) The interaction-based methods for news recommendation outperform all representation-based methods, which validates the beneî€›t of capturing î€›ne-grained matching signals. However, simply pruning the userâ€™s history to a smaller size to save speed is not feasible because it hurts the eî€ectiveness seriously. Without Bert [5], expanded SFI (with extra news abstract) ranks among the top 15 on the oî€œcial testing leaderboard. Since the motivation of SFI is mainly concerned with eî€œciency, we further compare the inference speed between SFI and several baselines. Results in Table 3 substantiates the superiority of SFI: with the selection capacity of5, it can infer almostfourtimes faster than the state-of-the-art interaction-based method, while signiî€›cantly improving the recommendation eî€ectiveness. SFI(5) also achieves comparable speed with the state-of-the-art representation-based method NRMS, and outperforms it by2.71% in AUC. The eî€œciency of SFI(ğ¾) and Recent(ğ¾) signiî€›cantly drops fromğ¾ =5 toğ¾ =25, which will be further studied in Section 5.4.1. Table 3: The inference speed comparison of diî€erent methods for news recommendation. The improvement over FIM is given in the bracket. Table 4: The eî€ectiveness of SFI with diî€erent news encoders and news interactors. Since SFI is essentially a î€exible framework, we conduct extensive ablation studies to gain comprehensive insights into every module. In each subsection, we pose our claim î€›rst before explanations. 5.3.1HDCNN and 3DCNN are the most eî€›ective encoder and interactor among a variety of state-of-the-art architectures.In the interaction-based workî€ow, the history selector can be easily inserted between any kind of news encoder and interactor. This î€exibility motivates us to study how state-of-the-art encoders and interactors would perform. We compare among 1D-CNN with Personalized Attention [30] (denoted as PCNN), Hierarchical Dilated CNN [26] (denoted as HDCNN), Multi-head Self Attention [33] (denoted as MHA), LSTM for the news encoder and 2D-CNN, 3DCNN [26], KNRM [37], Multi-Head Self Attention [33] (denoted as MHAI) for the news interactor. The AUC scores are reported in Table 4. We î€›ndHiearchical Dilated CNNcombined with3D-CNN is the best setting. 5.3.2Every sub-module in history selector is critical to improving eî€›ectiveness.The learning-to-select mechanism comprises three parts: a selection projection, a hard selection, and a soft reî€›nement. The hard selection is the cornerstone of our work so we no longer verify its impact. For the other two components, we compare SFI with the variant that applies only hard-selection, and that applies hard-selection followed by soft-selection without learning extra selection vectors. The result is reported in Figure 3. As we observe, the soft-selection network improves the eî€ectiveness. This is because it î€›lters the authentically uninformative history news, and makes the gradient î€ow through to optimize the representation vectors used for selection. However, without selection projection, these news-level representations are optimized for two incompatible goals: selecting and matching, which may decrease the recommendation accuracy. Experiments validates our claim: Figure 3: The eî€ectiveness of history selector and coarsegrained information. the model beneî€›ts a lot from selection projection. Thanks to it, SFI can encode selective features into selection vectors, leaving the news-level representations to focus on the î€›nal matching. 5.3.3The coarse matching signals of the unselected articles are also important.In Figure 3, SFI outperforms its variant that totally abandons the coarse-grained matching signals. This observation veriî€›es that the coarse-grained matching signals are complementary. Note that doing so wonâ€™t reduce eî€œciency because batched matrix multiplication is fast on GPU. 5.3.4SFI benefits from end-to-end training.When deployed in production, the news encoding process could be done oî€Ÿine to speed up inference, known as a pipeline convention. Itâ€™s natural to migrate it to the training phase, where we î€›rst pre-train SFI without the history selector to acquire coarse-and-î€›ne representations of news. Then we replace the news encoder with a lookup table constructed from these representations and î€›ne-tune them with history selector applied. End-to-end training is the counterpart, in which we jointly train the news encoder, history î€›lter, news-interactor and click predictor by the î€›nal classiî€›cation loss simultaneously. In Figure 3, the î€›rst four bars in every group are the performance of SFI trained in pipeline, and the last bar is that of trained end-to-end under the same setting. As expected, end-to-end training leads to better eî€ectiveness. Itâ€™s because optimizing the parameters rather than directly updating the vectors can make the news encoder learn more precise representations for both selection and interaction. Also, the modules can better learn the correspondence among them in end-to-end training. 5.4.1 Influence of the Interaction Capacity. The predeî€›ned interaction capacityğ¾is the most important hyper parameter since the eî€œciency-eî€ectiveness trade-oî€ is up to it. We study its inî€uence by drawing the inference speed curve against the AUC score of the model with diî€erentğ¾in Figure 4. Motivated by several observations in Section 4, we also include Recent(ğ¾), which only interacts with the latestğ¾historical news to save eî€œciency. We add Figure 4: The eî€œciency and eî€ectiveness of SFI with different numbers of selected news. The number next to the marker indicates the selected news count. Figure 5: The informativeness score of history news at each position. Smaller x-axis represents more recent history. two dashed lines to mark the best eî€ectiveness and eî€œciency that baseline models ever achieve. The optimal result would be situated at the upper right corner. According to the î€›gure, we î€›nd:First, fromğ¾ =5 toğ¾ =50, SFI(ğ¾) is far more eî€ective than its naive counterpart Recent(ğ¾), this again validates the eî€ectiveness of history selector. However, due to the time consumption of selection, SFI(ğ¾) is a bit slower. Overall, SFI(5) is the optimal setting because it greatly outperforms NRMS and all Recent(ğ¾) including FIM, while providing a much higher efî€›ciency over FIM.Second, whenğ¾is growing, the eî€ectiveness of both Recent(ğ¾) and SFI(ğ¾) is improving, which is because a bigger capacity keeps richer information to learn the userâ€™s interests. As a side eî€ect, the model becomes slower.Third, asğ¾increases, the eî€ectiveness of SFI(ğ¾) grows slower than Recent(ğ¾) and is about saturated atğ¾ =40. Intuitively, with the learning-to-select mechanism, SFI(ğ¾) can consistently select the most eî€ective articles for interaction, so increasing the capacity only brings a little more Figure 6: The eî€ectiveness of SFI with diî€erent value of ğ›¾. valuable information. In contrast, Recent(ğ¾) cannot access the informative historical news in earlier history unless the capacity is big enough. These observations motivate us to study what informativeness scores of the historical articles at diî€erent positions are learnt by the model itself. We report the informativeness score at each history position (averaged fromğ¾ =5 toğ¾ =50) in Figure 5. The blue line is the mean value of informativeness, and the shade indicates standard deviation. The horizontal black line marks the threshold of the soft-selection. According to the î€›gure, the increasing mean value of informativeness tells us that more recent reading history helps more in expectation. At the same time, the signiî€›cantly high variance conî€›rms that historical news at each position has the potential to interact with the candidate. Therefore, with a small ğ¾, Recent(ğ¾) cannot access earlier historical news that tends to be useful in interaction. Rather, SFI is quite able to inspect them and involve them in î€›ne-grained interactions intelligently. Moreover, the informativeness of the history news whose position is farther than 40 hardly reaches the threshold. So they are considered authentically uninformative and masked even though they are among the topğ¾ â‰¥40. This explains the saturation of SFIâ€™s eî€ectiveness and justiî€›es our intuition. 5.4.2 Influence of the Informativeness Threshold. Another crucial factor of SFI is the informativeness threshold in the soft-selection network. The eî€ectiveness of SFI with diî€erent threshold settings is shown in Figure 6. In summary, the threshold shouldnâ€™t be too large or too small. Whenğ›¾ <0.1, almost all history news articles are considered informative, so the selection fails. Whenğ‘”ğ‘ğ‘šğ‘šğ‘ >0.3, the history selector rules out too many history news articles, including the valuable ones. The gradient cannot be passed adequately, either. Hence the modelâ€™s eî€ectiveness declines. When it reaches 1, all î€›ne-grained representations are masked as 0, completely disabling the news interactor. Recall that the coarse-grained matching signals persist, leading to better results than random recommendation. Overall, ğ›¾ = 0.2 is the optimal conî€›guration. Capturing î€›ne-grained interactions brings more accuracy and higher online costs for news recommenders. In this work, we proposed a selective î€›ne-grained interaction framework to select a small number of valuable historical articles for interaction, drawing a good balance between eî€œciency and eî€ectiveness. With the help of thelearning-to-selectmechanism, the selection can be performed eî€œciently, sparsely, and automatically. Experimental results show SFI can signiî€›cantly improve the recommendation eî€ectiveness by 2.17% over the state-of-the-art models with four times faster speed. We experimented a lot to provide comprehensive insights of SFI and studied the eî€œciency-eî€ectiveness trade-oî€ it achieves. In the future, we will dig deeper into representing users with terms to further improve the eî€œciency while keeping the eî€ectiveness.