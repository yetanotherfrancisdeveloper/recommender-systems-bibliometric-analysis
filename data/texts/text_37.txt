How can we predict missing values in multi-dimensional data (or tensors) more accurately? The task of tensor completion is crucial in many applications such as personalized recommendation, image and video restoration, and link prediction in social networks. Many tensor factorization and neural network-based tensor completion algorithms have been developed to predict missing entries in partially observed tensors. However, they can produce inaccurate estimations as real-world tensors are very sparse, and these methods tend to overï¬t on the small amount of data. Here, we overcome these shortcomings by presenting a data augmentation technique for tensors. In this paper, we propose DAIN, a general data augmentation framework that enhances the prediction accuracy of neural tensor completion methods. Speciï¬cally, DAIN ï¬rst trains a neural model and ï¬nds tensor cell importances with inï¬‚uence functions. After that, DAIN aggregates the cell importance to calculate the importance of each entity (i.e., an index of a dimension). Finally, DAIN augments the tensor by weighted sampling of entity importances and a value predictor. Extensive experimental results show that DAIN outperforms all data augmentation baselines in terms of enhancing imputation accuracy of neural tensor completion on four diverse real-world tensors. Ablation studies of DAIN substantiate the effectiveness of each component of DAIN. Furthermore, we show that DAIN scales near linearly to large datasets. â€¢ Computing methodologies â†’ Factorization methods;Neural networks; â€¢ Information systems â†’ Data mining. Tensor, Tensor Completion, Neural Network, Data Augmentation, Data Inï¬‚uence, Recommender System, Deep Learning ACM Reference Format: Sejoon Oh, Sungchul Kim, Ryan A. Rossi, and Srijan Kumar. 2021. Inï¬‚uenceguided Data Augmentation for Neural Tensor Completion. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637. 3482267 We observe various tensors on the Web, including images and videos, numerical ratings, social networks, and knowledge bases. Most real-world tensors are very large and extremely sparse [26] (i.e., a few observed entries and many missing values). Thus, tensor completion, i.e., the task of predicting missing values in a tensor, has been actively investigated in diverse areas including recommender systems [8], social networks [6], trafï¬c analysis [32], medical questionnaires [4], and computer vision [5]. For example, consider a movie rating tensor with three dimensions, namely users, movies, and time slices. Each tensor cell(ğ‘–, ğ‘—, ğ‘˜)contains the rating score given by userğ‘–to movieğ‘—during time sliceğ‘˜. The goal of tensor completion is to predict the rating scores of unobserved tensor cells. Tensor factorization (TF) is a popular technique to predict missing values in a tensor, but most methods [15,19,25,26,28,30] exhibit high imputation error while estimating missing values in a tensor. One of the reasons is that many TF models [15,19,25,30] regard missing values in a tensor as zeros. Hence, if those models are trained with a sparse tensor, their predictions would be biased toward zeros, instead of the observed values. Other TF methods [7,26,28] improve their accuracy by focusing only on observed entries; however, they suffer from overï¬tting as the tensor is very sparse. Neural network-based tensor completion methods [2,21,36,37] have been proposed to enhance the estimation accuracy. They have strong generalization capability [24], and capture non-linearity hidden in a tensor. However, these methods still suffer from data sparsity, and this can become a bottleneck for neural tensor completion methods, which require a large amount of data for training [35]. Moreover, these existing methods cannot generate new data points for data augmentation to solve the sparsity issue. In this paper, we propose a method that leverages the strength of neural tensor completion, and improves it through the utilization of data augmentation. Data augmentation increases the generalization capability of a model by generating new data points while training and has shown success in various applications of deep learning [29,33,40]; however, it has not been explored in the task of tensor completion. Here, we propose an inï¬‚uence-guided data augmentation technique called DAIN (DataAugmentation with INï¬‚uence Functions). First, DAIN trains a neural tensor completion model with the input tensor, and utilizes inï¬‚uence functions to estimate the importance of each training cell (e.g., the importance of a rating in a movie rating tensor) on reducing imputation error. Next, DAIN computes the importance of every entity by aggregating the importance values of all its associated cells. For example, to compute the importance of a userğ‘–, the importance of all the ratings given byğ‘–are aggregated. The importance of an entity signiï¬es its impact in reducing the prediction error. Finally, DAIN generates new data Table 1: Comparison of our proposed data augmentation framework DAIN against existing methods. A checkmark indicates that a method satisï¬es a speciï¬c criterion. DAIN is the only data augmentation method satisfying all criteria. points by sampling entities proportional to their importance scores. Values of the augmented tensor cells are predicted via a trained neural tensor completion method. This inï¬‚uence-based sampling of entities augments data points using important entities, and thus, can lead to higher test prediction accuracy. We evaluate DAIN by conducting extensive experiments on four diverse real-world datasets and one synthetic dataset. Results show that DAIN outperforms baseline augmentation methods with statistical signiï¬cance, and that DAIN improves the prediction performance of a neural tensor completion method as the number of augmentation increases. Via thorough ablation studies, we conï¬rm the effectiveness of each component of DAIN. We also show that DAIN is scalable to large datasets as illustrated by a near-linear relationship between runtime and the amount of augmentation. Finally, we demonstrate the sensitivity of the performance of DAIN with respect to hyperparameter values. Our main contributions are summarized by the following: â€¢ Novel problem.We propose a novel data augmentation technique DAIN for enhancing neural tensor completion. To the best of our knowledge, this is the ï¬rst data augmentation method for tensor completion. â€¢ Algorithmic framework.We propose a new framework for deriving the importance of tensor entities on reducing prediction error using inï¬‚uence functions. With the entity importance values, we create new data points via weighted sampling and value predictions. We also provide complexity analyses of DAIN. â€¢ Performance.DAIN outperforms baseline data augmentation methods on various real-world tensors in terms of prediction accuracy with statistical signiï¬cance. The code and datasets are available on the project website: https://github.com/srijankr/DAIN. Neural Tensor Completion.After recent advances in neural networks (NN), there have been many research topics applying NN to tensor completion frameworks. Neural tensor factorization (NTF) [36] is the ï¬rst model that employs MLP and long short-term memory (LSTM) architectures for a tensor completion task. Neural Tensor Machine (NTM) [2] combines the outputs from generalized CANDECOMP/PARAFAC (CP) factorization and a tensorized MLP model to estimate missing values. Moreover, Liu et al. [21] propose a convolutional neural network (CNN)-based tensor completion model, named COSTCO, which learns entity embeddings via several convolutional layers. However, as shown in Table 1, the above methods cannot augment new data points, do not generalize to multiple neural network architectures, and do not utilize inï¬‚uence functions. DAIN can provide high-quality augmentation for these methods while having these desirable properties. Note that we exclude tensor completion methods [22,26,28,41] that do not utilize neural networks since DAIN is designed for the neural methods. Inï¬‚uence Estimation. Computing the inï¬‚uence (or importance) of data points on model predictions or decisions has been actively investigated [9,17,27,42]. Inï¬‚uence has been mainly used to explain the prediction results by identifying useful or harmful data points with respect to the predictions. Firstly, Yeh et al. [42] leverage the Representer Theorem for explaining deep neural network predictions. Koh and Liang [17] introduce a model-agnostic approach based on inï¬‚uence functions. Amirata and James [9] propose a data Shapley metric to quantify the value of each training datum in the context of supervised learning. More recently, TRACIN [27] is proposed to compute the inï¬‚uence of a training example on a prediction made by a model with training and test loss gradients. In this paper, we decide to use TRACIN for the tensor data augmentation setting since it is scalable, easy to implement, and does not rely on optimality conditions as Inï¬‚uence Functions [17] or Representer Theorem [42] does. Data Augmentation.Data augmentation is widely used to signiï¬cantly increase the data available for training models without collecting new data [12]. Basic image data augmentation methods include geometric transformations (such as ï¬‚ipping, cropping, rotation, and translation), mixing images by averaging their pixel values [13], and random erasing which randomly selects anğ‘› Ã— ğ‘špatch of an image and masks it with certain values [45]. Augmentation Network model [20] ï¬nds image augmentation that maximizes the generalization performance of the classiï¬cation model via trainable image transformation models and inï¬‚uence functions. Earlier data augmentation methods for text data are based on synonym replacement replacing a random word in a given sentence with its synonym by using external resources such as WordNet [23,44] and the pre-trained language model [14]. Yuning et al. [38] leverage machine translation to paraphrase a given text. There is a line of research that perturb text via regex-based transformation [3], noise injection [39], and combining words [10]. However, none of the above methods are applicable in a tensor completion task. To the best of our knowledge, ours is the ï¬rst work that enhances the performance of neural tensor completion via data augmentation. In this section, we deï¬ne the preliminary concepts necessary to understand the proposed method. Table 2 summarizes the symbols frequently used in the paper. Tensors are multi-dimensional data and a generalization of vectors (1-order tensors) and matrices (2-order tensors) to the higher order. An ğ‘ -way or ğ‘ -order tensor has ğ‘ dimensions, and the dimension ğ¼dimensionality/size of the ğ‘›dimension of X (ğ‘–, ..., ğ‘– ğ‘–entity of the ğ‘›dimension of X Î©set of validation cells of X Î©set of test cells of X ğ›¼, ..., ğ›¼entity importance Î˜parameters of a tensor completion model at epoch ğ‘¡ ğœ‚step size at a checkpoint Î˜ Î˜, ..., Î˜ğ¾ checkpoints saved at epochs ğ‘¡, ..., ğ‘¡ ğ‘‡, ğ‘€time and space complexity of training entity embeddings ğ‘‡, ğ‘€time and space complexity of training a value predictor ğ‘‡time complexity of a single inference of a value predictor size (or dimensionality) is denoted byğ¼throughğ¼, respectively. We denote anğ‘-order tensor by boldface Euler script letters (e.g.,X âˆˆ R). A tensor cell(ğ‘–, ..., ğ‘–)contains the valueX, and an entity of a tensor refers to a single index of a dimension. For example, in the movie rating tensor case, an entity refers to a user, movie, or time slice, and a tensor cell contains a rating. Tensor completion is deï¬ned as the problem of ï¬lling the missing values of a partially observed tensor. Tensor completion methods train their model parameters with observed cells (Î©) and predict values of unobserved cells (Î©) with the trained parameters. Speciï¬cally, given an N-order tensorX (âˆˆ R)with training dataÎ©, a tensor completion method aims to ï¬nd model parameters Î˜ for the following optimization problem. whereË†X= Î˜(ğ‘–, ..., ğ‘–)is a prediction value for a cell (ğ‘–, ..., ğ‘–)generated by the tensor completion methodÎ˜. Neural tensor completion methods utilize different neural network architectures to computeË†X(see Section 4.1 for the multilayer perceptron case). Root-mean-square error (RMSE) is a popular metric to measure the accuracy of a tensor completion method [21,26,36]. Speciï¬cally, we use test RMSE to check how accurately a tensor completion model predicts values of unobserved tensor cells. The formal deï¬nition of test RMSE is given as follows. Notice that a tensor completion model with the lower test RMSE is more accurate. We introduce the state-of-the-art inï¬‚uence estimator: TRACIN [27]. TRACIN calculates the importance of every training data point in reducing test loss. This is done by tracing training and test loss gradients with respect to model checkpoints, where checkpoints are the model parameters obtained at regular intervals during the training (e.g., at the end of every epoch). The inï¬‚uence of a training data pointğ‘§on the loss of a test data pointğ‘§is given as follows (please refer to Section 3 of TRACIN [27] for the details): whereÎ˜, 1 â‰¤ ğ‘– â‰¤ ğ¾are checkpoints saved at epochsğ‘¡, ..., ğ‘¡,ğœ‚is a step size at a checkpointÎ˜, andâˆ‡â„“ (Î˜, ğ‘§)is the gradient of the loss of ğ‘§ with respect to a checkpoint Î˜. Inï¬‚uence estimation with TRACIN has been shown to have clear advantages in terms of speed and accuracy over existing methods [27], such as Inï¬‚uence Functions [17] or Representer Point method [42]. We utilize TRACIN to create the Cell Importance Tensor in Section 4.2. In this section, we describe DAIN, a data augmentation pipeline using the importance of entities for efï¬cient tensor completion. Figure 1 shows our proposed method, and Algorithm 1 summarizes the overall data augmentation process of DAIN. We explain the steps of our method in the next subsections. First, we train a neural network model to learn embeddings for every entity in an input tensor (e.g., user, movie, and time embeddings in the movie rating tensor). The traditional technique of learning one embedding per data point is not scalable in tensors, as the number of data points in the tensor can be very large. Instead, we learn one embedding per entity of each dimension (i.e., in the movie rating tensor example, we learn one embedding for every user, movie, and time slice, respectively). Each tensor cell can then be represented as an ordered concatenation of the embeddings of its entities. For example, a tensor cell(ğ‘–, ..., ğ‘–)can be represented as [ğ¸,ğ¸,ğ¸, . . .], whereğ¸is an embedding of an entityğ‘–of theğ‘›dimension. The entity embeddings are trained to accurately predict the values of the (training) data points in the tensor. We train an end-to-end trainable neural network to learn the entity embeddings (line 1 in Algorithm 1). We choose a multilayer perceptron (MLP) model with ReLU activation since it shows the best imputation performance with data augmentation (see Figures 3(a) and 3(b)). Note that existing neural tensor completion models, such as NTF [36] and CoSTCo [21], can be also used to generate these embeddings. Our modelâ€™s prediction value for a tensor cell(ğ‘–, ..., ğ‘–) is deï¬ned by the following. Figure 1: An overview of our proposed data augmentation method DAIN for a 3-dimensional tensor. DAIN ï¬rst learns entity embeddings with training data and computes training and validation loss gradients (step 1). Next, it obtains cell importance values by the inï¬‚uence estimator TRACIN and creates the Cell Importance Tensor (step 2). DAIN uniformly distributes cell importance values to the corresponding entities, and each entity calculates its importance by aggregating the assigned cell importance values (step 3). Finally, DAIN performs data augmentation to an input tensor by weighted sampling on those entity importance arrays and value predictions (step 4). Note that DAIN can use any neural tensor completion methods to learn entity embeddings (step 1) or predict values of sampled tensor cells (step 4). Please refer to each subsection in Section 4 for detailed information on each step. Note that[ğ¸, . . . , ğ¸]represents the embedding of a cell(ğ‘–, ..., ğ‘–) obtained by concatenating embeddings of entitiesğ‘–, ..., ğ‘–,Ë†X is the imputed output value by the neural network for a cell(ğ‘–, ..., ğ‘–), andğ‘€indicates the number of hidden layers.ğ‘Š, ...,ğ‘Šand ğ‘, ..., ğ‘are weight matrices and bias vectors, respectively.ğœ™, ..., ğœ™ are activation functions (DAIN usesğ‘…ğ‘’ğ¿ğ‘ˆ). The model parameters ğ‘Š, ...,ğ‘Šandğ‘, ..., ğ‘are referred to asÎ˜. By minimizing the loss function(1)combined with Equation(4), we obtain trained entity embeddings as well as loss gradients for all training and validation cells (lines 2-3 in Algorithm 1). We utilize those gradient vectors to compute cell-level importance values in the next part. In Step 2, we create Cell Importance Tensor (CIT) which represents the importance of every tensor cell in reducing the prediction loss. In the movie rating tensor example, CIT stores the importance of ratings. Any inï¬‚uence estimator can be used to compute the CIT, but we choose TRACIN [27] introduced in Section 3.3 since it is scalable, easy to implement, and does not rely on optimality conditions. Moreover, we later show DAIN with TRACIN shows the highest prediction performance empirically compared to other inï¬‚uence estimation methods (see Figures 6(a) and 6(b)). We derive a value of a CIT cellğ‘§by the following steps. Computing the CIT is shown in step 2 in Figure 1. Equation(3)in Section 3.3 computes the inï¬‚uence of a training cellğ‘§on the loss of a test cellğ‘§. Since we cannot access the test data, we compute the inï¬‚uenceğœ¶of a training cellğ‘§on reducing overall validation loss by the following (line 4 in Algorithm 1). ğœ¶= |ğ¼ğ‘›ğ‘“ (ğ‘§, ğ‘§)| = |ğœ‚âˆ‡â„“ (Î˜, ğ‘§) Â· âˆ‡â„“ (Î˜, ğ‘§)| whereğ¾is the number of checkpoints,ğœ‚is a step size at a checkpointÎ˜, andâˆ‡â„“ (Î˜, ğ‘§)is the gradient of the loss ofğ‘§with respect to a checkpointÎ˜. We use an absolute value function to theğœ¶ calculation since cells with negative inï¬‚uence can also be important. For example, cells with large negative inï¬‚uence contribute to increasing the validation loss signiï¬cantly. We can mitigate the loss increase by our data augmentation method, where the absolute function leads DAIN to create more augmentation for these cells. Identifying important entities is crucial. The output of the previous step identiï¬es the cell importance, but the importance of each entity remains unknown. For instance, in the movie rating tensor example, Algorithm 1: Data Augmentation with DAIN the cell importance score captures the importance of the rating on the prediction loss, while it does not reï¬‚ect the importance of a user, a movie, or a time slice. If we can ï¬nd the importance of every user, movie, and time slice, we can generate new inï¬‚uential data points to minimize prediction error by combining users, movies, and time slices that have high importance values. Here, we describe an aggregation technique to calculate the entity importance values from cell importance values (shown in step 3 of Figure 1 and line 5 in Algorithm 1). First, we uniformly distribute a cellâ€™s importance value to its associated entities. For instance, given a training cellğ‘§ = (ğ‘–, ..., ğ‘–), we uniformly distributeğœ¶to itsğ‘ entities{ğ‘–, ..., ğ‘–}. After performing the allocation for all training cells, we compute the entity importance scoreğ›¼for an entityğ‘–of theğ‘›dimension by aggregating cell importance scores as follows: Recall thatğœ¶indicates the Cell Importance Tensor, andÎ© represents a set of training cells from an original tensor. In the movie rating tensor example, the above equation signiï¬es that a userâ€™s entity importance is the aggregation of the importance scores of all the ratings the user gives. Similarly, a movieâ€™s importance is the sum of the importance of the ratings it receives, and the importance of a time slice is the sum of the importance of all the ratings given during the time slice. Another way of computing the entity importance is applying rank1 CP (CANDECOMP/PARAFAC) factorization [18] on the Cell Importance Tensor. The output factor matrices from the CP model are entity importances. Speciï¬cally, a value of each output array indicates the importance of the corresponding entity on predicting values in a training tensor. The loss function of the rank-1 CP model is given as follows. ğ¿(ğ›¼, ..., ğ›¼) = Note thatğ›¼, ..., ğ›¼are entity importances,ğœ†is a regularization factor, andâˆ¥Xâˆ¥is Frobenius norm of a tensorX. We choose the aggregation scheme to compute the entity importance as the rank-1 CP model can produce inaccurate decomposition results when a given tensor is highly sparse. By conducting extensive experiments, we later show that the aggregation method over the rank-1 CP model with respect to the prediction accuracy (see Figures 7(a) and 7(b)). In the ï¬nal step, we identify the tensor cells and values for data augmentation using the entity importance scores and a value predictor, respectively. This is illustrated in step 4 of Figure 1 and lines 6â€“15 in Algorithm 1. A high entity importance score signiï¬es that the corresponding entity plays an important role in improving the validation set prediction. Thus, we create new cells using these important entities. We ï¬rst train a neural tensor completion modelÎ˜with the original training cells (line 6), which will be used later for value predictions. After that, we conduct weighted sampling on every entity importance array (line 11) and select one entity from each dimension (line 12). Mathematically, an entityğ‘–of theğ‘›dimension (e.g., a user among all users) has a probabilityto be sampled. We combine the sampled entities from all dimensions to form one tensor cell (line 13). We repeat the process several times to create the required number of data points for augmentation (lines 8â€“13). Once indices of the cells are sampled, their values need to be determined (line 14). Trivially, one can use the overall average valueÃ (i.e.,X) or ï¬nd the most similar index in the embedding space and take its value. However, these heuristics can be inaccurate and computationally expensive, respectively. An advanced way of assigning the values of the augmented data points is by predicting the values using a tensor completion model (either the previously trainedÎ˜or training another modelÎ˜). Speciï¬cally, we can use the trained embeddings fromÎ˜to predict the values, or we can train an off-the-shelf prediction methodÎ˜ with the training data and predict the values with it. Reusing the trained neural networkÎ˜is computationally cheaper since it only needs to do a forward pass for inference, which is fast. However, this can cause overï¬tting in the downstream model since the resulting augmentation cell values are likely to be homogeneous to the original tensor. On the other hand, using another off-the-shelf methodÎ˜ can increase the generalization capability of a downstream model by generating more heterogeneous data compared toÎ˜. Thus, we choose to train a new off-the-shelf modelÎ˜. We use the state-ofthe-art algorithm COSTCO [21] asÎ˜to predict the values of the augmented tensor cells, since COSTCO value predictor exhibits high prediction accuracy than other methods empirically (see Section 5.3 and Figure 5). After we obtain all tensor cell indices and values needed for augmentation, we add them to the input tensor to obtain an augmented tensorX(line 15). This augmented tensor can be used for downstream tasks. In this subsection, we analyze the time and space complexity of DAIN. Time complexity.The ï¬rst step of DAIN (Section 4.1) is training a neural tensor completion modelÎ˜to generate entity embeddings and gradients, and it takesğ‘‚ (ğ‘‡)assumingğ‘‚ (ğ‘‡)is the time complexity of trainingÎ˜as well as gradient calculations. The second step of DAIN (Section 4.2) is computing the cell importance ğœ¶for each training cellğ‘§ âˆˆ Î©. A naive computation ofğœ¶ in Equation(5)for all training cells takesğ‘‚ (ğ¾ğ·|Î©||Î©|), whereğ¾andğ·are the number of checkpoints and the dimension of the gradient vector, respectively. We accelerate the computationÃ toğ‘‚ (ğ¾ğ· (|Î©| + |Î©|))by precomputingâˆ‡â„“ (Î˜, ğ‘§) for all checkpointsÎ˜, ..., Î˜in Equation(5). The third step of DAIN (Section 4.3) is calculating the entity importance with the aggregation technique by Equation(6). It takesğ‘‚ (ğ‘ |Î©|)since we distributeğœ¶, âˆ€ğ‘§ âˆˆ Î©to its entities and aggregate the assigned values. Finally, the data augmentation step (Section 4.4) takes ğ‘‚ (ğ‘‡+ ğ‘(ğ‘ log ğ¼ + ğ‘‡)), whereğ‘‚ (ğ‘‡)andğ‘‚ (ğ‘‡)are the training and single inference time complexities of the value prediction modelÎ˜, respectively.ğ‘‚ (ğ‘ğ‘ log ğ¼ )term indicates the time complexity of weighted samplingğ‘cells without replacement [34] fromğ‘dimensions (assumingğ¼= Â· Â· Â· = ğ¼= ğ¼). The ï¬nal time complexity of DAIN isğ‘‚ (ğ‘‡+ğ‘‡+ (ğ¾ğ· + ğ‘ )|Î©| + ğ¾ğ· |Î©| + ğ‘(ğ‘ log ğ¼ + ğ‘‡)). Space complexity.The ï¬rst step of DAIN (Section 4.1), obtaining entity embeddings and gradients, takesğ‘‚ (ğ‘€+ ğ¾ğ· (|Î©| + |Î©|))space, assumingğ‘‚ (ğ‘€)is the space complexity of training a neural tensor completion modelÎ˜(including entity embeddings). ğ‘‚ (ğ¾ğ· (|Î©| + |Î©|))space is required to storeğ·-dimension gradients of training and validation cells for allğ¾checkpoints. The second step of DAIN (Section 4.2), computing the cell importance ğœ¶, takesğ‘‚ (|Î©|)space since we need to store all cell importance values. The third step of DAIN (Section 4.3), calculating the entity importance with the aggregation method, takesğ‘‚ (ğ‘ ğ¼)space since we need to store importance scores of all entities fromğ‘ dimensions (assumingğ¼= Â· Â· Â· = ğ¼= ğ¼). Finally, the data augmentation step (Section 4.4) takesğ‘‚ (ğ‘€+ğ‘ğ‘ )space since we need ğ‘‚ (ğ‘€)space for training the value predictorÎ˜andğ‘‚ (ğ‘ğ‘ ) space for storing the data augmentation withğ‘cells. The ï¬nal space complexity of DAIN isğ‘‚ (ğ‘€+ğ‘€+ğ¾ğ· (|Î©|+ |Î©|) + ğ‘ (ğ¼ + ğ‘)). In this section, we evaluate DAIN to answer the following questions. (1) Effectiveness of DAIN (Section 5.2).How much does our proposed data augmentation technique enhance the accuracy of neural tensor completion compared to the baseline augmentation methods? (2) Ablation studies of DAIN (Section 5.3).DAIN consists of three major components: training entity embeddings, generating new tensor cells, and predicting values of the new cells. How much does each component of DAIN contribute to boosting the neural tensor completion accuracy? (3) Comparisons of inï¬‚uence estimators (Section 5.4).How much do different inï¬‚uence estimators impact the prediction accuracy improvements of DAIN? (4) Comparisons of entity importance algorithms (Section 5.5). How much do different entity importance methods affect the prediction accuracy improvements of DAIN? (5) Scalability of DAIN (Section 5.6).Does the running time of DAIN linearly scale with the number of data augmentation? (6) Hyperparameter sensitivity (Section 5.7).How much do model hyperparameters of DAIN, such as embedding dimension and layer structures, affect the prediction accuracy? We ï¬rst describe the datasets and experimental settings in Section 5.1, and then answer the above questions. Table 3: Summary of real-world tensor datasets used in our experiments. 5.1.1 Datasets. We use four real-world tensor datasets to evaluate the performance of our proposed data augmentation method and baselines. As summarized in Table 3, we use MovingMNIST [31], Foursquare [43], Reddit [1], and LastFM [11]. MovingMNIST is a video tensor represented by (sequence, length, width, height; intensity), and we use 10% of the total data. Foursquare is a pointof-interest tensor represented by (user, location, timestamp; visit). Reddit includes the posting history of users on subreddits represented by (user, subreddit, timestamp; posted). LastFM contains the music playing history of users represented by (user, music, timestamp; played). For all tensors having timestamps, we converted their timestamps to unique days, so that all timestamps in the same day would have the same converted number. We randomly added negative samples with random indices and zero values to original data for Foursquare, Reddit, and LastFM tensors. 5.1.2 Baselines. To the best of our knowledge, there are no existing methods for data augmentation designed speciï¬cally for tensors. Therefore, we create a few baselines based on the broader literature in data augmentation by the following: â€¢ Duplication or Oversampling:This simple data augmentation method randomly copies tensor cells and values from the existing training data, and adds them to the original tensor; there can be multiple tensor cells with the same values. â€¢ Entity Replacement:This data augmentation method randomly selects existing tensor cells and replaces their indices one by one with one of the top-10 closest entities in the embedding space, while keeping their tensor values. Figure 2: Effectiveness of DAIN. Test RMSE of the neural tensor completion method (MLP) after applying various data augmentation methods on real-world datasets. For all tensors, DAIN outperforms all baselines and shows the lowest test RMSE value at the maximum augmentation with statistical signiï¬cance. â€¢ (Random, MLP) and (Random, COSTCO):These baselines generate data augmentation by randomly sampling missing tensor cells and predicting their values via the MLP and COSTCO [21] models trained with the original tensor, respectively. Recall that neural tensor completion methods such as COSTCO [21] and NTF [36] cannot generate new data points by themselves. Thus, these methods are not included in our baselines. The inï¬‚uence estimator TRACIN [27] also only computes the cell importance but cannot create new data augmentation. We exclude non-neural tensor completion algorithms [22,26,28,41] to test the data augmentation since DAIN is optimized for neural tensor completion methods. 5.1.3 Experiment Setup. We randomly split our datasets to use 90% data for training and 10% for test. We randomly sample 20% of training data as the validation set. We stop the training of a neural network when the validation accuracy is not improving anymore, with a patience value of 10. We repeat all experiments 10 times and report the average test RMSE. To measure statistical signiï¬cance, we use a two-sided test of the null hypothesis that two independent samples have identical average (expected) values. If we observe a small p-value (e.g., < 0.01), we can reject the null hypothesis. We ï¬ne-tune neural network hyperparameters of DAIN with validation data. Speciï¬cally, embedding dimension, batch size, layer structure, and learning rate are set to 50, 1024, [1024,1024,128], 0.001, respectively. We use Adaptive Moment Estimation (Adam) [16] optimizer for the neural network training, and the maximum number of epochs for training a neural network is set to 50. To compute the Cell Importance Tensor, we set checkpoints and step size to every epoch and 0.001, respectively. We take gradients with respect to the last fully connected layer. We execute all our experiments on Azure Standard-NC24 machines equipped with 4 NVIDIA Tesla Figure 3: Ablation study of entity embedding generators of DAIN. We test the effectiveness of the MLP embedding model of DAIN on MovingMNIST and Foursquare tensors. The MLP model outperforms the COSTCO embedding model in terms of test RMSE at the maximum augmentation with statistical signiï¬cance. Figure 4: Ablation study of cell creation methods of DAIN. We test the effectiveness of the cell creation component of DAIN on MovingMNIST and Foursquare tensors. The entity importance-based cell creation of DAIN outperforms the random cell creation baseline in terms of test RMSE at the maximum augmentation with statistical signiï¬cance. Figure 5: Ablation study of value prediction methods of DAIN. We test the effectiveness of the value prediction component of DAIN on MovingMNIST and Foursquare tensors. The value prediction module of DAIN outperforms the MLP baseline in terms of test RMSE at the maximum augmentation with statistical signiï¬cance. We choose the MLP baseline since it shows higher prediction accuracy than the random one. K80 GPUs with Intel Xeon E5-2690 v3 processor. The data augmentation module is implemented in Python with the PyTorch library. To test the effectiveness of our proposed data augmentation method, we measure and compare test RMSE values of the neural tensor completion model (MLP) after applying DAIN and baselines on real-world tensors. Figure 2 shows their performance. We omit error bars since they have small standard deviation values. Both the baseline methods, Duplication and Entity Replacement, lead to a reduction in performance, i.e., an increase in test error, with any amount of augmentation. More augmentation leads to a greater reduction in performance. A potential reason is that the feature spaces of original and augmentation are very similar, so the augmentation cannot increase the generalization capability of a model. Although the other baselines (Random, MLP) and (Random, COSTCO) outperform Duplication or Entity Replacement, they have limitations in reducing test RMSE compared to DAIN. Our proposed method DAIN leads to improvements in performance across all datasets with augmentation. DAIN performs the best across all ï¬ve methods and presents the lowest test RMSE at the maximum augmentation with statistical signiï¬cance, which clearly demonstrates the effectiveness of DAIN. The performance improves as more data is added to the original one. A key reason for the high performance of DAIN is that it combines important entities from each dimension, which is leading to the augmentation of more inï¬‚uential and heterogeneous data points to the original tensor. At a high-level view, DAIN consists of three componentsâ€”the ï¬rst is training entity embeddings with a neural tensor completion methodÎ˜(step 1 in Figure 1), the second component is selecting new tensor cells for augmentation based on entity importance (steps 2 and 3), and the ï¬nal is predicting values of the augmented cells using a neural tensor completion methodÎ˜(step 4). As mentioned previously, in DAIN, we use MLP, entity importance, and COSTCObased predictor in the three components, respectively.We perform ablation studies to investigate the contribution of each component in DAINâ€™s performance by replacing the component with the baseline component, while ï¬xing all the others. Figures 3, 4, and 5 show ablation study results of the three components of DAIN on the two largest tensor datasets, namely MovingMNIST and Foursquare. In Figure 3, DAIN with MLP embedding generator presents lower test RMSE than a baseline of DAIN with COSTCO embedding generator at 50% augmentation ratio. A potential reason is that using COSTCO in both step 1 and step 4 (for augmented cellâ€™s value prediction) can lead to overï¬tting. On the other hand, using MLP in step 1 and COSTCO in step 4 gives generalization capability to the model. The entity embedding module has a huge impact on performance improvements since entity embeddings and gradients generated by the module affect all the subsequent modules. In Figure 4, DAIN with entity importance-based cell creation beats the model with random cell creation. The result substantiates the usefulness of using entity importance in creating tensor cells for augmentation. In Figure 5, DAIN with COSTCO for augmented cellâ€™s value prediction outperforms DAIN with a MLP-based value predictor. This is again because of overï¬tting where the MLP predictor produces homogeneous data points since the entity embedding model in step 1 Figure 6: Comparisons of inï¬‚uence estimators. We compare two inï¬‚uence estimators with respect to test RMSE improvements on MovingMNIST and Foursquare tensors. We ï¬nd the TRACIN inï¬‚uence estimator is more suitable for our data augmentation framework. Figure 7: Comparisons of entity importance algorithms. We compare two entity importance algorithms with respect to test RMSE improvements on MovingMNIST and Foursquare tensors. We ï¬nd entity importance calculation by the aggregation scheme is more suitable for our data augmentation framework. is an MLP as well. On the other hand, using a COSTCO-based value predictor produces heterogeneous data points giving generalizability. In summary, all of the components of DAIN prove useful by reducing test RMSE values signiï¬cantly compared to baseline components. In this subsection, we explore how different inï¬‚uence estimators affect the prediction accuracy improvements of DAIN on the two largest real-world tensors, namely MovingMNIST and Foursquare datasets. We compare two inï¬‚uence calculation methods: TRACIN [27] and Representer Theorem [42]. We exclude inï¬‚uence function [17] since it shows worse prediction accuracy than the Representer Theorem method. As shown in Figures 6(a) and 6(b), TRACIN consistently outperforms Representer Theorem with any amount of augmentation (with statistical signiï¬cance at 50% augmentation). One possible explanation for this gap is that TRACIN directly computes the inï¬‚uence of a training cell in reducing validation loss, while Representer Theorem cannot measure the direct contribution. Similar to Section 5.4, we conï¬rm how different entity importance calculators affect the prediction accuracy improvements of DAIN on the two largest real-world tensors, namely MovingMNIST and Foursquare datasets. Between two entity importance calculators introduced in Section 4.3, the aggregation algorithm shows superior Figure 8: Scalability of DAIN. This plot measures the running time of DAIN at different augmentation levels. The sudden jump in total running time from 0% to 10% augmentation occurs due to the cost for the cell and entity importance calculation (see time complexity analysis in Section 4.5); the runtime increases linearly thereafter. performance than the rank-1 CP factorization (see Figures 7(a) and 7(b)) with extensive test RMSE differences at 50% augmentation. The main reason for this gap is that rank-1 CP factorization cannot decompose the Cell Importance Tensor accurately due to its high sparsity, while the aggregation algorithm does not suffer from the sparsity issue. In this subsection, we measure the runtime of DAIN on the largest real-world, namely MovingMNIST, and a synthetic tensor, namely Synthetic-1M. We construct the synthetic tensor with dimensionality(1000, 1000, 1000, 1000)and1, 000, 000non-zero cells. We randomly generated factor matrices and reconstructed a synthetic tensor from them. Note that the total running time for0% augmentation is equivalent to the neural network training time. On the other hand, our proposed data augmentation method involves four runtime-related components: (1) initial neural network training, (2) cell importance calculation, (3) entity importance calculation, and (4) weighted sampling and value inference. Therefore, there are static costs (i.e.,ğ‘‚ (ğ‘‡+ğ‘‡+ (ğ¾ğ· + ğ‘ )|Î©| + ğ¾ğ· |Î©|in Section 4.5) resulted from steps (1), (2), and (3), and linearly increasing costs (i.e.,ğ‘‚ (ğ‘(ğ‘ log ğ¼ +ğ‘‡))in Section 4.5) proportional to the amount of augmentation for step (4). Figure 8 exhibits the total running time of DAIN on MovingMNIST and Synthetic-1M tensors. As expected before, we ï¬nd a sudden increase in total running time from 0% to 10% augmentation on both tensors due to steps (2) and (3) above. After that, the running time increases linearly. This shows that DAIN is highly scalable and can be applied to large datasets. In this subsection, we investigate how much model hyperparameters affect the prediction accuracy of a model. Our neural network hyperparameters include the length of entity embeddings, the number of hidden layers, the size of a hidden layer, learning rate, and batch size. We vary one hyperparameter while ï¬xing all the others to default values mentioned in Section 5.1.3. Figure 9 shows the hyperparameter sensitivity of DAIN with respect to the prediction accuracy on the Foursquare dataset with 50% augmentation ratio. The RMSE improvement indicates how much validation RMSE values are enhanced after the augmentation (higher is better). As the embedding size increases, we observe Figure 9: Hyperparameter sensitivity plots. We vary one of the hyperparameters of DAIN, namely embedding length, hidden layer size, number of hidden layers, batch size, and learning rate, while ï¬xing all the others to default values. We measure the validation RMSE improvements after 50% augmentation on the Foursquare dataset. We ï¬nd medium-sized layers with large embedding dimensions, and proper learning rates are key factors for the prediction accuracy. performance improvements since large embeddings contain more useful information about training data. Regarding the number of hidden layers, 3 hidden layers are appropriate since 1 or 2 layers may not fully learn the augmented tensor, and 4 layers may overï¬t. Too small or too large of learning rates degrade the prediction accuracy since a small one leads to slower convergence, and a large one can ï¬nd a low-quality local optimum. In this paper, we proposed a novel data augmentation framework DAIN for enhancing neural tensor completion. The key idea is to augment the original tensor with new data points that are crucial in reducing the validation loss. Experimental results on real-world datasets show that DAIN outperforms baseline methods in various augmentation settings with statistical signiï¬cance. Ablation studies of DAIN demonstrate the effectiveness of the key components of DAIN. We also verify that DAIN scales near linearly to large datasets. Future directions of this work include exploring the effectiveness of DAIN in downstream tasks such as anomaly detection and dataset cleanup. Deriving theoretical guarantees of the performance boost from DAIN is worth exploring as well. This research is supported in part by Adobe, Facebook, NSF IIS2027689, Georgia Institute of Technology, IDEaS, and Microsoft. S.O. was partly supported by ML@GT, Twitch, and Kwanjeong fellowships. We thank the reviewers for their feedback.