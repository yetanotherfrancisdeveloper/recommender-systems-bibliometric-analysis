Modern deep neural networks (DNNs) have greatly facilitated the development of sequential recommender systems by achieving state-of-the-art recommendation performance on various sequential recommendation tasks. Given a sequence of interacted items, existing DNN-based sequential recommenders commonly embed each item into a unique vector to support subsequent computations of the user interest. However, due to the potentially large number of items, the over-parameterised item embedding matrix of a sequential recommender has become a memory bottleneck for eî€œcient deployment in resource-constrained environments, e.g., smartphones and other edge devices. Furthermore, we observe that the widely-used multi-head self-attention, though being eî€ective in modelling sequential dependencies among items, heavily relies on redundant attention units to fully capture both global and local item-item transition patterns within a sequence. In this paper, we introduce a novel lightweight self-attentive network (LSAN) for sequential recommendation. To aggressively compress the original embedding matrix, LSAN leverages the notion of compositional embeddings, where each item embedding is composed by merging a group of selected base embedding vectors derived from substantially smaller embedding matrices. Meanwhile, to account for the intrinsic dynamics of each item, we further propose a temporal context-aware embedding composition scheme. Besides, we develop an innovative twin-attention network that alleviates the redundancy of the traditional multi-head self-attention while retaining full capacity for capturing long- and short-term (i.e., global and local) item dependencies. Comprehensive experiments demonstrate that LSAN signiî€›cantly advances the accuracy and memory eî€œciency of existing sequential recommenders. ACM Reference Format: Yang Li, Tong Chen, Peng-Fei Zhang, and Hongzhi Yin. 2021. Lightweight Self-Attentive Sequential Recommendation. In Proce edings of the 30th ACM International Conference on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 11 pages. https://doi.org/10.1145/3459637.3482448 Modelling sequential user behaviours has received great attention in contemporary applications, such as e-commerce, online services, and smart transport [49]. Among these applications, sequential recommender systems (SRSs) have become a prominent solution to information overload on the web. The main goal of SRSs is to make a proactive recommendation on the next item a user may be interested in by mining the userâ€™s recent preferences from the sequence of her/his interacted items. Early SRSs incorporate Markov chain-based models [7,10,33] to capture high-order sequential patterns based on Markov chain (MC), which essentially factorises a user-speciî€›c item-item transition tensor by considering î€›rst-order Markov chain. However, these methods primarily learn the transition patterns based on the most recent item interactions, neglecting long-term (i.e., global) user preferences. With the revolution of deep neural networks (DNNs), various deep methods have been proposed for the sequential recommendation [21,28â€“31], especially recurrent neural network-based [3,4,6,22,53] sequential recommenders. Notably, the majority of state-of-the-art SRSs are latent factor models, where each item is mapped into a unique vector representation (a.k.a. embedding), and the item embedding is then used to calculate the sequential preferences of the target user. With the fast pace of digitisation and hardware revolution, there has been a recent surge of moving data analytics from cloud servers to edge devices [35] to ensure timeliness and privacy. As sequential recommendation involves frequent updates on a userâ€™s behaviour records, developing lightweight SRSs appears to be an ongoing trend, because such on-device computation capability can prevent potential latency caused by communications with the cloud and eî€ectively retains usersâ€™ personal data on their own devices. Due to the sheer volume of diî€erent items (e.g., Alibabaâ€™s billion-scale item set [41]), the item embeddings in latent SRSs are the main source of memory consumption [42] rather than other parameters like weights and biases of DNNs. In this regard, recent studies on lightweight recommenders [5,23,24,34] are predominantly focused on compressing the originally large item embedding matrix to improve the memory eî€œciency of recommenders. Their core idea of such compression is compositional embeddings, where a recommender consists of a small number (substantially smaller than the number of all items) of base embeddings, such that an item can be represented as a distinct combination of selected base embeddings. However, most compositional embedding-based recommenders are designed for static recommendation settings, where the recommendation results for each user are purely conditioned on the static user-item aî€œnity instead of her/his interest dynamics. As a common practice, the aforementioned lightweight recommenders usually combine compositional embeddings with oî€-theshelf deep recommendation modules (e.g., the DLRS in [34] and the DeepFM in [25]). Though a similar solution can be sought for lightweight SRSs by straightforwardly feeding compositional item embeddings into sequential DNNs (e.g., recurrent neural networks), an over-parameterised network structure will in fact defeat the purpose of a memory-eî€œcient model. Also, the excessive computations may impede the timeliness of a modelâ€™s on-device inference. Hence, in addition to a lightly parameterised item embedding scheme, a lightweight SRS should also be able to thoroughly discover the temporal signals from all interacted items with a carefully designed, compact, yet eî€ective sequence mining paradigm. To this end, we proposelightweightself-attentivenetwork (LSAN), a novel solution to memory-eî€œcient sequential recommendation that simultaneously addresses those two key challenges. Speciî€›cally, LSAN aggressively replaces the item embedding matrix with ğ‘base embedding matrices, each of which contains substantially fewer embedding vectors (i.e., base embeddings) than the total amount of items, i.e.,ğ‘ â‰ª |V |for item setV. Then, compositional item embeddings are generated by fusingğ‘base embeddings respectively selected from each base embedding matrix. To ensure the uniqueness of each composed item embedding, we design a context-aware temporal compositional embedding scheme, where base embeddings are located via a quotient-remainder [34] operation. Unlike traditional compositional item embeddings that stay î€›xed regardless of any temporal information in a sequence, we propose to dynamically alter the generated item embedding according to sequence-speciî€›c contexts by attentively merging the base embeddings for each item. The rationale is that in a sequence, every itemâ€™s relevance is sensitive to factors like seasonal changes and adjacent items [20,43], hence we let LSAN account for such information when generating compositional item embeddings. In LSAN, we resort to self-attention for modelling the temporal patterns among the interacted items. However, though the self-attention [17,27,55] is widely acknowledged as a lightlyparameterised and eî€ective approach for capturing sequential information in SRSs compared with the popular alternative â€“ RNNs, our observation is that recommenders using self-attention can still incur parameter redundancy. For sequential recommendation tasks, the attention module should be able to capture both long- and short-term (i.e., global and local) preferences of a user. Unfortunately, recent studies [44,46] point out that self-attention tends to over-emphasise local relationships between adjacent items, making it diî€œcult to learn the correlations between items that are far from each other in a sequence. Hence, existing recommenders commonly employ multi-head self-attention, so as to enhance the modelling capacity and acquire suî€œcient global information. Such redundancy can be tolerated and may beneî€›t the recommendation accuracy when the computing resource allows, but it fails to meet the highly constrained deployment environment in the context of memoryeî€œcient SRSs. In light of this, we introduce a novel twin-attention paradigm in LSAN, where the global and local preference signals are separately captured via two specialised modules instead of a group of general attention units. As such, coupled with the dynamic compositional item embedding scheme, LSAN eî€ectively learns local and global user preference signals for accurate sequential recommendation without the need for an excessively complex and large model. With the proposed LSAN, our main contributions to lightweight sequential recommendation are three-fold: â€¢We devise a dynamic context-aware compositional embedding scheme, which largely decreases the memory footprint of item embedding matrix â€“ the major consumer of memory space of SRSs â€“ and ensures the uniqueness and dynamics of generated item embeddings at the same time. â€¢We propose a novel twin-attention sequential framework, which specialises the learning of long- and short-term user preference signals via a dedicated self-attention and convolution operation, respectively. This facilitates explicit modelling of both global and local patterns while avoiding the redundancy of multi-head self-attention modules. â€¢Extensive experiments are conducted on three benchmark datasets. The results demonstrate the advantageous eî€ectiveness and memory eî€œciency of LSAN against state-of-the-art baseline methods. LetV,Ube the sets of items and users, respectively. We useS= {ğ‘£, ğ‘£, ..., ğ‘£}to denote a sequence ofğ‘‡chronologically ordered items that userğ‘¢ âˆˆ Uhas interacted with. Each itemğ‘£âˆˆ Sis assigned an order indexğ‘– = 1, 2, ...,ğ‘‡which reî€ects the position of an item in the sequence. To locateğ‘£among the set of|V|items, we deî€›ne a functionindex(ğ‘£)that mapsğ‘£to a unique and î€›xed global index1, 2, ..., |V|. Then, given a sequence of interactionsS, our goal is to compute a ranking list consisting of topğ¾items that ğ‘¢ is most likely to visit at the next time step ğ‘‡ + 1. In this section, we introduce our proposed memory-eî€œcient sequential recommender, namely LSAN. LSAN consists of two main components: (1) a dynamic context-aware compositional embedding layer that enables a lightweight yet highly expressive item embedding paradigm; and (2) a twin-attention network that eî€ectively learns global and local user preferences without the need for redundant multi-head self-attention modules. In what follows, we present the design of LSAN in detail. Recall that in a typical latent factor-based recommender system, each itemğ‘£is associated with a uniqueğ·-dimensional embedding vector, which corresponds to theğ‘£)-th row in an embedding table ğ‘¬ âˆˆ R. The memory complexity of maintaining such an embedding table isO(|V|ğ·), where the memory cost will become impractical for edge devices whenVis large-scale. To reduce the size ofğ‘¬for better memory eî€œciency, we replaceğ‘¬with a set ofğ‘base embedding tables denoted as {eğ‘¬,eğ‘¬, ...,eğ‘¬}, whereeğ‘¬âˆˆ R forğ‘› = 1, 2, ..., ğ‘. Here,ğ‘šindicates the number of base embeddings in theğ‘›-th base embedding tableeğ‘¬andğ‘šâ‰ª |V |. For each item, its compositional embedding is produced by î€›rst selecting one base embedding vector from eacheğ‘¬, and then attentively fusing all selected base embeddings into a single vector. It can be concluded that there areÎ ğ‘šdiî€erent combinations of base embeddings. Thus, we only need to makeÎ ğ‘šâ©¾ |V|to ensure the uniqueness of constructed embeddings for each item. To guarantee that each item receives a distinct combination of base embeddings, we resort to the quotient-remainder trick [34] that does not introduce any additional learnable parameters. Particularly, taking the î€›rst base embedding tableeğ‘¬as an example, the corresponding base embedding indexğ‘(i.e., the row index ineğ‘¬) for itemğ‘£âˆˆ Scan be computed by a remainder function over the base embedding table size ğ‘š: Then, the î€›rst base embeddingeğ’†forğ‘£can be retrieved by a lookup operation oneğ‘¬w.r.t. row indexğ‘. Mathematically, letğ’‡âˆˆ R be the one-hot encoding ofğ‘£, then a hash matrixğ‘¹âˆˆ R foreğ‘¬can be computed element-wise via: Therefore, the look-up operation ofğ‘£â€™s î€›rst base embeddingeğ’† can be mathematically formulated as: Analogously, forğ‘› = 2, 3, ..., ğ‘,ğ‘£â€™s hash matrixğ‘¹for theğ‘›-th base embedding table can be generalised as: ğ‘¹=1 if ğ‘— mod ğ‘š= index(ğ‘£) \ Î ğ‘š0 otherwise where the index in theğ‘›-th base embedding table for itemğ‘£is determined by the resulting quotient from the prior base embedding tables, i.e.,index(ğ‘£)\Î ğ‘š. Then, we obtain the base embedding eğ’†from ğ‘¹via: Through the quotient-remainder trick, we now have acquired a set of base embeddings{eğ’†,eğ’†, ...,eğ’†}for itemğ‘£. Intuitively, a uniî€›ed item embedding can be easily formed by an ensemble operation, such as element-wise addition/multiplication. However, such constructed embeddings are î€›xed, and are insuî€œcient in capturing the intrinsic dynamics of an itemâ€™s properties. As pointed out by [20,43], learning context-aware temporal item embeddings is beneî€›cial for mining a userâ€™s preferences from her/his interaction sequences. In order to bring such temporal contexts into the generated compositional item embeddings, we propose to attentively assign diî€erent weights to the selected base embeddings conditioned on the context around the target item. Speciî€›cally, for each itemğ‘£âˆˆ S(ğ‘– = 1, 2, ...,ğ‘‡), we have a contextğ‘Ÿrepresenting this itemâ€™s situation within the sequence. The construction ofğ‘Ÿcan be highly î€exible, where in our work, based on the side information shared by all of our experimental datasets (see Section 4), we deî€›ne ğ‘Ÿ= (ğ‘, ğ‘, time(ğ‘–))as a triplet of the categories of the previous and current items and the discrete time slot (i.e., every hour of a day). We denoteğ‘… = {ğ‘Ÿ, ğ‘Ÿ, ğ‘Ÿ}be the set of unique context tuples. Each tupleğ‘Ÿ âˆˆ ğ‘…is assigned with a one-hot vector. Then, we can map the one-hot encoding vectorğ‘Ÿinto a dense context embeddingğ’“âˆˆ R. Note that a padding label for the category information is adopted whenğ‘¡ = 1. Under a given contextğ’“, we can calculate an attention weight for each base embedding: whereSiLU(ğ‘¥) = ğ‘¥ Â· sigmoid(ğ‘¥)is an activation function that is an alternative to ReLU providing non-linearity to the model with faster convergence speed [9]. The attention weight is then used to compute the compositional embedding ğ’‰for item ğ‘£: Finally, to facilitate side information modelling, we inject the context embedding of the itemğ‘–(i.e.,ğ’“) into the above computed compositional embedding â„via a non-linear operation: where[; ]is the concatenation operation andMLP(Â·) : 2ğ· â†’ ğ· denotes a multi-layer perceptron. For a sequence ofğ‘‡interacted items, we can obtain an embedding matrixğ‘¯ = [Ëœğ’‰;Ëœğ’‰; ...;Ëœğ’‰]âˆˆ Rby sequentially stacking all compositional item embeddings. Self-attention has been a predominant approach in recent SRSs owing to its simplicity and capability of learning sequential dependencies among items. As discussed in Section 1, existing self-attentive sequential recommenders mostly deploy multiple attention heads in parallel to capture both global and local user preference signals from the sequence, resulting in unnecessary redundancy in both the network structure and parameter size. To alleviate such problem, we propose a twin-attention neural network to better capture the sequential information while maintaining the lightweight nature of LSAN. As depicted in Figure 1, it has two branches: a self-attention branch and a convolution branch specialised for global and local preference modelling, respectively. 3.2.1 Convolution Branch for Local Paî€erns. Diî€erent from the self-attention branch which attends to all items in a sequence, convolution operations have shown success in extracting local features for image recognition and text classiî€›cation. Their strong capacity in extracting regional features makes them an ideal component for capturing the short-term preferences among items that co-occur in a short time period. As such, with the matrixğ‘¯ âˆˆ Rcarrying allğ‘‡item embeddings in the sequence, we perform 1D convolution over the embedding matrix. Assuming the sliding window size isğ¿ and the output size isğ·, the standard convolution operation needs ğ¿ğ·trainable parameters. To decrease the parameter size, we resort to a lightweight version of convolution [45]. In particular, a depth-wise convolution operation is introduced, which applies a shared kernel of sizeğ¿for each channel (i.e., each item embedding dimension). This reduces the number of required parameters from O(ğ¿ğ·)toO(ğ¿ğ·). Mathematically, theğ‘‘-th element (ğ‘‘ = 1, 2, ..., ğ·) ofğ‘–-th embedding in the resulted output matrixğ‘¯âˆˆ Rcan be formulated as: whereğ‘¾âˆˆ Ris the kernel, andâŒˆÂ·âŒ‰denotes the ceiling operation. It is worth noting that, each row in the resulted matrixğ‘¯ encodesğ‘–-th itemâ€™s interaction with items closely surrounding it within theğ¿-sized sliding window, hence is a representation of all the local dependencies within the item sequence. 3.2.2 Self-aî€ention Branch for Global Paî€erns. The rationale of coupling self-attention with convolution is that, by having a convolution branch dedicated to extracting local sequential patterns, the self-attention branch can now better specialise in learning global patterns, thus reducing the need for using an excessive amount of self-attention units for optimal performance. As for self-attention, it has become one of the most prevalent means in various natural language processing (NLP) and sequential tasks as it can eî€ectively capture relationships among items regardless of their distances (i.e., multi-hop). However, the plain self-attention fails to preserve the inherent orders of items in the sequence, impeding its eî€œcacy for sequential recommendation tasks [17]. In this sense, we make the self-attention branch order-aware. Firstly, we deî€›neğ‘‡learnable position embeddingsğ’‘, ğ’‘, ..., ğ’‘âˆˆ R, which are stacked into a matrixğ‘· âˆˆ R. Then, we fuse the positional information into the original item embeddings: where each item is essentially paired with its corresponding positional context in the sequence. After that, a scaled dot-product selfattention is applied to compute item representationsğ‘¯âˆˆ R by mining the long-range dependencies: whereğ‘¸ = ğ‘¾eğ‘¯,ğ‘² = ğ‘¾eğ‘¯andğ‘½ = ğ‘¾eğ‘¯are transformed item representations that are projected into query, key and value spaces, respectively. 3.2.3 Enhancing Expressiveness with Parallelism. Similar to pure self-attention-based methods, one can employ more than one attention heads for both branches in the twin-attention. For simplicity, we assume the convolution and self-attention modules each have ğ»heads in parallel. Then, the î€›nal output of the twin-attention can be obtained by concatenating2ğ»learned representation matrices followed by : ğ‘¯= [ğ‘¯; ...; ğ‘¯; ğ‘¯; ...; ğ‘¯ whereğ‘¯âˆˆ Ris the î€›nal output. Note that in LSAN, it is not strictly necessary to setğ» > 1as the design of twin-attention can already facilitate comprehensively learning both global and local user preferences. One beneî€›t of such parallelism over the traditional multi-head attention is that, with the same total amount of2ğ»attention heads, twin-attention consumes fewer parameters (i.e.,O(ğ» (ğ¿ğ· + 3ğ·))in twin attention versusO(6ğ»ğ·)in selfattention,ğ¿ â‰ª ğ·) and is able to yield stronger performance, as will be illustrated in Section 4. 3.3.1 Point-wise Feed-forward Network. To further enhance the representation capacity of LSAN, we incorporate non-linearity into the output of the twin-attention. Speciî€›cally, we employ a pointwise feed-forward network (FFN) as follows: whereğ‘¾âˆˆ R, ğ‘¾âˆˆ Rare weight matrices, ğ’ƒâˆˆ R, ğ’ƒâˆˆ Rare bias vectors, andbğ‘¯âˆˆ ğ‘ ğ‘…is the output of the point-wise FFN. Meanwhile,GeLU(Â·)denotes the Gaussian error linear unit [8,12] that we use for non-linearity. 3.3.2 Generating Rankings. With the î€›nal representationbğ‘¯ that encodes both the userâ€™s long- and short-term interests, we generate the rankings for all items to facilitate top-ğ¾recommendation. This is achieved by estimating the likelihood of having userğ‘¢interact with each item, which is formulated as learning a |V|-dimensional probability distribution by: whereğ‘¾âˆˆ Randğ’ƒâˆˆ Rare the learnable weight matrix and bias vector, respectively. By sorting eachğ‘£according to its corresponding probability scorebğ‘¦âˆˆ byin a descending order, we will be able to truncateğ¾items from the top of the list as our recommendation results. With the estimated probability vectorby, we then employ crossentropy loss function to quantify the error of predicting the next item for LSAN: whereğ‘  â‰¤ ğ‘†is the index of training samples,yis the one-hot vector representing the ground truth of the next item, andÎ¨is the set of all trainable parameters under theğ¿2regularisation term with coeî€œcient ğœ†. In this section, we evaluate the recommendation eî€ectiveness and memory eî€œciency of our LSAN model for sequential recommendation. Speciî€›cally, we î€›rst analyze the performance of LSAN by comparing it with state-of-the-art sequential recommenders from both accuracy and model size perspectives. After that, we further investigate the impact of the key components and hyperparameters in LSAN. Table 1: Statistics of experimental datasets. Avg. Int. per Item 16.4038 14.0554 16.1430 14.7235 4.1.1 Dataset. We conduct experiments on four commonly-used benchmark datasets. The statistical details of all datasets after preprocessing are reported in Table 1, including the number of users, items, interactions, categories, average interactions per user (Avg. Int./User) and average interactions per item (Avg. Int./Item). All the experimental datasets are highly sparse. We brieî€y introduce their properties below. â€¢Beauty, Sports and Toys: These three datasets are provided by [11], which are collected from Amazon and contain product reviews and abundant metadata. â€¢Yelp: The dataset contains user check-in data provided by Yelp, where businesses are viewed as items. The data we use for our experiments span across 2019. For each dataset, we group interactions by user IDs, and then generate one chronological item sequence for each user. The inactive users and unpopular items with less than 5 interactions are discarded. We adopt the leave-one-out evaluation approach, i.e., for each user interaction sequence, we use the last item as the test instance, the second last item as a validation sample, and the remaining items for training. We choose Hit Ratio at Rankğ¾(HR@ğ¾) and Normalised Discounted Cumulative Gain at Rankğ¾(nDCG@ğ¾) on top-ğ¾ranked items, which are widely used in recommender systems [3,17] for top-ğ¾performance and overall ranking performance evaluation. We report the performance results on HR@{5, 10, 20} and nDCG@{5, 10, 20}, respectively. As suggested by [19], to eliminate potential biases, we rank each ground truth item along with the whole item set (i.e.,V) to compute all metrics, and report the average scores over all users. We compare LSAN with the most representative, state-of-the-art sequential recommendation methods below: â€¢FPMC [33]: It is a combination of matrix factorisation with Markov chain, which can simultaneously capture sequential information and long-term user preferences. â€¢GRU4Rec [13]: It is an RNN-based sequential recommender with session-wise mini-batch training strategy. The model is optimised by a pair-wise ranking loss. â€¢Caser [39]: This is a CNN-based method that models highorder Markov-chain probability by performing convolutional operations on the item embedding matrix. â€¢SASRec [17]: It is a next-item sequential recommendation method based on the Transformer architecture, which employs multi-head self-attention mechanism to explore implicit user interactions. â€¢BERT4Rec [37]: It is an improvement of SASRec, which contains an additional Cloze objective and bidirectional selfattention structure. LSAN is implemented using PyTorch with Nvidia GTX 2080 Ti. In LSAN, we set the dimension sizeğ·to 128, CNN kernel sizeğ¿to 5, the number of attention headsğ»to 2 for each branch, and the number of stacked twin-attention layers to 1 on all datasets. All the trainable parameters in our model are optimised using Adam optimiser [18] with the batch size of 256, learning rate of 0.001 and ğ¿2regularisation strengthğœ†of1ğ‘’ âˆ’ 5. For a fair comparison on accuracy and model size, we apply the same dimension sizeğ·for all methodsâ€™ embeddings. Note that in LSAN, altering eitherğ‘orğ‘š can lead to diî€erent compression rates on the original embedding table. Hence, we î€›xğ‘ = 2and varyğ‘š(ğ‘š=) for the ease of hyper-parameter tuning. In Section 4.5, we will î€›rst test LSANâ€™s performance withğ‘š= 2, while we will further discuss how LSAN performs when we compress the model size more aggressively with a larger ğ‘šin Section 4.6. We summarise the results of all models on four benchmark datasets in Table 2. From the table, we can draw the following observations: Among all sequential baseline methods, FPMC receives the worst results over all evaluation metrics. This is mainly because FPMC only exploits î€›rst-order dependencies where the higher-order relationships among items are neglected. In comparison, Caser utilises convolutional kernels to extract k-hop adjacent item dependencies, thus, obtaining better performance results than FPMC. However, since the sliding window size of Caser could only cover a small number of items, which lacks the ability on handling sparse datasets with long sequences, e.g., Yelp. As a result, it can be observed that the RNN-based method, GRU4Rec, has better performance than Caser on those datasets. SASRec and BERT4Rec are state-of-the-art self-attentive approaches, which have clear margins with the other baseline methods. This shows the superiority of the self-attention architecture in sequential behaviour modelling. It is worth noting that BERT4Rec does not outperform SASRec under our evaluation settings (i.e., full test sample set), which is diî€erent from their original paper. We think this maybe because there may exist improper biases in the naive negative sampled test sets according to their original implementation details. We present the performance results and relative improvements over the best baseline of two versions of our proposed LSAN in Table 2, i.e., LSANand LSAN. The only diî€erence between them is that LSAN is our proposed model, while LSANis trained using a full-sized embedding table. From the results, using either full-sized embedding table or our proposed dynamic context-aware compositional embedding can surpass the best baseline method SASRec with signiî€›cant margins on most evaluation metrics. This proves the eî€ectiveness of our twin-attention structured model on handling long-term and short-term user preferences. Moreover, though a large number of parameters are reduced via our compositional embeddings, LSAN performs even much better than LSANon all datasets. We believe that by incorporating seasonal and categorical factors (i.e., temporal dynamics) within the model can largely enhance the expressiveness of LSAN with fewer trainable parameters. In addition, an obvious model size reduction can be observed from both LSAN and LSAN. We provide a more detailed discussion on memory usage in the following section. In this section, we study how the compression rate aî€ects the model performance. Intuitively, the model will have worse performance when the compression rateğ‘šincreases. We compare the model size and next-item recommendation performance of LSAN at diî€erent compression rates with the best baseline method, SASRec. From the results shown in Table 3, we can observe that whenğ‘š= 2, our LSAN model surpasses SASRec over most metrics with only around 60% of parameters in SASRec on all datasets. Whenğ‘šincreases to 3 (i.e., around 45% of SASRec parameters), LSAN still produce comparable recommendation results on Beauty, Toys and Yelp datasets, which further approves the outstanding memory eî€œciency and recommendation eî€ectiveness of our model. However, whenğ‘š goes up greater than 3, we can see a clear performance drop. It is understandable that when the model contains a very small number of parameters, the model is under-parameterised resulting in the failure of capturing meaningful information from items. Comparing with the existing self-attentive methods, LSAN mainly contains two novel components: dynamic context-aware compositional embeddings and twin-attention layers. To verify the effectiveness of each component, we conduct ablation studies on all benchmark datasets. Table 4 shows the performance of our default model and its two degraded variants (ğ· = 128). We give a brief description and detailed analysis of each variant in the following: Speciî€›cally, we compare LSAN with the following degraded variants: â€¢LSAN: This variant removes the dynamic compositional embedding component. The embedding part becomes exactly the same as the QR embedding introduced in [34]. More concretely, we create this variant by modifying Eq.Ã (7) toğ’‰=eğ’†. We can see a signiî€›cant performance drop on most datasets when the temporal information is removed from the composited embeddings. This suggests that respecting temporal dynamics is of great importance in user preference modelling. â€¢LSAN: This variant replaces the twin-attention layers with self-attention layers. There is a clear performance drop on three datasets when only self-attention is applied. This reveals that the self-attention does not have suî€œcient capability to uncover both long-term and short-term user preferences with limited attention heads. We further examine the impact of four various hyper-parameters, including dimension sizeğ·, partition sizeğ‘š, number of attention headsğ», and number of stacked twin-attention layers. For each test, we vary the value of one hyper-parameter, while keep the others be the optimal settings. The results are demonstrated in Figure 2. 4.8.1 Impact of Dimension Size. the dimension of The value of dimension size is examined from 16 to 256. We can observe that a small dimension size (i.e., 16) cannot preserve suî€œcient latent information of items for user sequential behaviour modelling. The model performance increases steadily when the dimension size Table 2: Comparison on sequential recommendation accuracy and model sizes. In each row, the best and second best results are highlighted in boldface and underlined, respectively. The parameter size of each model is obtained when ğ· = 128. Yelp grows up on all datasets. However, we also î€›nd that a larger hidden dimensionality (e.g., 256) may not contribute to better performance, which may be mainly caused by the over-î€›tting problem especially when the data is extremely sparse. This also proved by the performance results on Sports dataset that has highest sparsity among all datasets: LSAN becomes sensitive to the choice of appropriate dimensionality, i.e.,ğ· = 64î€›ts the model best. On the other datasets, LSAN reaches the best performance when ğ· = 128. 4.8.2 Impact of Aî€ention Head Number. The results in the second graph in Figure 2 show that more attention heads contribute better to the model performance. It is worth noting that the actual number of our twin-attention heads are doubled. Thus, LSAN model reaches best performance on most datasets, when it is equipped with 2 selfattention heads and 2 convolution heads. Table 3: A comparison of performance results and number of model parameters using diî€erent embedding compression rate ğ‘šon four datasets. Datasets Metrics SASRec LSAN(2x) LSAN(3x) LSAN(4x) LSAN(5x) Table 4: Ablation study of diî€erent variants on four datasets. 4.8.3 Impact of Twin-aî€ention Layer Number. LSAN receives the best performance with 1-layer architecture. Diî€erent from SASRec and BERT4Rec, which usually require 2 or 3 layers to fully capture various-order item dependencies. With the help of our proposed twin-attention structure, our model is capable of capturing various sequential information with only one layer. We also observe that the model performance drops obviously when more twin-attention layers are stacked on all dataset. This may be because of the over-î€›tting problem when LSAN is launched on extremely sparse datasets. Recall that in Section 1, we argue that self-attention-based models put too much emphasise on local patterns, which is the motivation of our design on twin-attention architecture. To more intuitively demonstrate how twin-attention performs eî€ective user behaviour modelling, we examine three randomly selected user interaction sequence samples and calculate the averaged attention weights on the last 10 items from each sample over all attention heads. The heat maps of the normalised attention weights from LASN and SASRec are illustrated in Figure 3. From the î€›gure, it can be easily distinguished that the attention module in LSAN focus on item Figure 3: Heat maps of average attention weights on Yelp dataset. (a-c) are from SASRec, while (d-f) are from LSANâ€™s self-attention branch. global patterns (i.e., no diagonal pattern shown in the î€›gure), thus leaving local pattern modelling to the convolution branch in our twin attention. In comparison, the heat maps of SASRec show a clear concentration on local patterns, which fails to model global patterns eî€ectively. Early work on sequential recommendation is mainly based on Markov chains. Rendle et al. [33] propose FPMC that combines the power of matrix factorisation and Markov-chain to learn an item-toitem transition probability matrix, which is then used to make next item prediction based on the userâ€™s latest interaction. After that, several models built upon high-order Markov chains are introduced [10,11]. The advances in recurrent neural networks (RNNs) have brought signiî€›cant performance boost in sequential recommendation. Hidasi et al. [13] propose a sequential recommender based on RNNs, which employs gated recurrent units (GRUs) to extract the high-order sequential information from the userâ€™s interaction history. Subsequent RNN-based approaches leverage attention networks [38], memory networks [14,15], copy mechanism [32], or reinforcement learning scheme [47], to improve the eî€ectiveness of sequential user interest modelling. Another line of work [39] treats a sequence of item embeddings as a feature map of an image, and performs convolution operation upon embeddings to capture local dependencies among items. Owing to the promising capability in sequential data modelling, attention mechanism has become popular and widely studied in various domains, such as text classiî€›cation [48] and machine translation [1]. However, these approaches treat attention mechanism as an additional module upon the RNN backbone, resulting in higher computational cost. To solve this issue, a new attention architecture, transformer, is proposed in [8,40]. Its main building block is multi-head self-attention, which allows faster parallel computation and achieves state-of-the-art performance in a wide range of sequence modelling tasks. In light of self-attention, Kang et al. [17] propose a self-attentive framework named SASRec, which adopts a multi-head self-attention layer to capture the userâ€™s sequential behaviours and achieves state-of-the-art performance on various sequential datasets. Later on, Sun et al. [37] encode sequence data in bidirectional manner by introducing BERT4Rec together with a masked training scheme. However, all aforementioned sequential recommendation methods suî€er from high memory cost in two aspects, which are infeasible for on-device applications. First, the large item embedding table brings high memory complexity. Second, as discussed in [46], the multi-head self-attention architecture tends to pay more attention on local dependencies, resulting in weak global preference modelling. In contrast, our proposed LSAN largely reduces the memory cost from the embedding table by a dynamic compositional embedding scheme. Besides, LSAN eî€ectively learns global and local dependencies by a novel twinattention, where the heavy redundancy in traditional self-attentive recommenders are resolved by two specialised branches for longand short-range pattern mining. DNN-based methods have demonstrated strong capability in various recommendation tasks. However, with the rapid development of edge devices, there has been an increasing demand on adopting DNN-based models on mobile phones and even smaller edge devices for stability and reliability. In this line of research, the methodologies can be roughly categorised into four types: pruning, quantisation, knowledge distillation, and compositional embedding. Network pruning approaches manage to reduce the overî€›tting parameters by discarding unnecessary ones from the neural model. Zhou et al. [54] introduce a group-sparse regularisation upon CNN kernel to produce a compact version without losing accuracy. However, most pruning methods require more iterations to reach convergence leading to extreme time cost. The second line of work aim to create one or more codebooks for a group of similar item representations. JÃ©gou et al. [16] propose to decompose the item representation space into multiple subspaces, and then a codebook of each subspace can be obtained by clustering items in each subspace. The recent work, LightRec [23], develops a recurrent composite embedding encoder to learn diversiî€›ed codebooks in a recursive manner. However, the learning of codebooks is independent of learning the item representations. Thus, the model cannot be trained end-to-end. Recently, knowledge distillation has gained popularity due to its high adaptivity to various complex models. This line of work primarily trains a large complex teacher network at the î€›rst place, and subsequently utilises soft labels obtained from the teacher model to train a lightweight student model. Wang et al. [42] devise a tensor-train decomposed lightweight RNN model, and train the model using a well-tuned state-of-the-art teacher model via knowledge distillation for next POI recommendation. With the observation that the embedding matrices in recommender systems are the major source of memory consumption, some recent studies resort to embedding compression. A number of studies [2,26,36,50â€“52] introduce the idea of converting a continuousvalued embedding vector to a discrete code, where each bit refers to the learned index of a base embedding table. However, this still requires the model to store extra discrete code for each item. To solve the limitation, Shi et al. [34] propose a quotient-remainder indexing technique, which is able to obtain a unique set of base embeddings without allocating extra embedding space. Nevertheless, all the above-mentioned embedding compression work is designed for static recommendation scenario, which neglects the dynamics of user interests in sequential recommendation. To address this issue, we design a context-aware temporal compositional embedding scheme that incorporates temporal information by attentively merging base embeddings for each item. As such, our proposed LSAN is capable of preserving the temporal dynamics and optimising memory eî€œciency simultaneously. In this paper, we introduce a lightweight twin-attention sequential recommender named LSAN, where two parallel branches are respectively specialised for short-term and long-term user preference modelling. To overcome the common bottleneck of large memory cost in existing DNN-based sequential recommender, we introduce temporal context-aware compositional embedding scheme, which largely reduces the memory cost and preserves intrinsic temporal dynamics of sequential data. Extensive experiments conducted on four real-world datasets clearly demonstrate the eî€ectiveness and eî€œciency of our proposed model. This work is partially supported by the Australian Research Council under the streams of Discovery Project (No. DP190101985), Future Fellowship (No. FT210100624), Centre of Excellence (No. CE200100025), and Industry Transformation Training Centre (No. IC200100022).