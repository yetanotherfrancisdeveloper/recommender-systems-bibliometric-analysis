University of Novi Sad, SerbiaGoogle Research, United Kingdom dusan.stamenkovic@dmi.uns.ac.rsalexkz@google.com Since the inception of Recommender Systems (RS), the accuracy of the recommendations in terms of relevance has been the golden criterion for evaluating the quality of RS algorithms. However, by focusing on item relevance, one pays a signiî€›cant price in terms of other important metrics: users get stuck in a "î€›lter bubble" and their array of options is signiî€›cantly reduced, hence degrading the quality of the user experience and leading to churn. Recommendation, and in particular session-based/sequential recommendation, is a complex task with multiple - and often conî€icting objectives that existing state-of-the-art approaches fail to address. In this work, we take on the aforementioned challenge and introduce Scalarized Multi-Objective Reinforcement Learning (SMORL) for the RS setting, a novel Reinforcement Learning (RL) framework that can eî€ectively address multi-objective recommendation tasks. The proposed SMORL agent augments standard recommendation models with additional RL layers that enforce it to simultaneously satisfy three principal objectives: accuracy, diversity, and novelty of recommendations. We integrate this framework with four stateof-the-art session-based recommendation models and compare it with a single-objective RL agent that only focuses on accuracy. Our experimental results on two real-world datasets reveal a substantial increase in aggregate diversity, a moderate increase in accuracy, reduced repetitiveness of recommendations, and demonstrate the importance of reinforcing diversity and novelty as complementary objectives. â€¢ Information systems â†’ Recommender systems;â€¢ Retrieval models and ranking;â€¢ Diversity and novelty in information retrieval; Recommendation; Reinforcement Learning; Multi-Objective Reinforcement Learning; Diversity; Novelty ACM Reference Format: DuË˜san StamenkoviÄ‡, Alexandros Karatzoglou, Ioannis Arapakis, Xin Xin, and Kleomenis Katevas. 2018. Choosing the Best of Both Worlds: Diverse and Novel Recommendations through Multi-Objective Reinforcement Learning. In Woo dstock â€™18: ACM Symposium on Neural Gaze Detection, June 03â€“05, 2018, Woodstock, NY . ACM, New York, NY, USA, 9 pages. https://doi.org/10. 1145/1122445.1122456 Whether in the context of entertainment, social networking or ecommerce, the sheer number of choices that modern Web users face nowadays can be overwhelming. Contrary to the common belief that more options are always better, selections made from large assortments can lead to a choice overload [17] and impair usersâ€™ capacity for rational decision making. Simply put, when presented with large array situations (e.g., limitless products to purchase from or media content to consume), users are at higher risk of feeling like they made the wrong decision and experience regret, which can degrade the quality of experience with an online service or platform. The problem becomes further aggravated when one is inclined to consider the costs and beneî€›ts of all alternative options. Recommender Systems (RS) alleviate this paradox of choice [32] by acting as second-order strategies [35] that facilitate access to relevant information and improve the browsing experience [16,43]. Hence, in settings where the abundance of options can result in unsatisfying choices or, even worst, abandonment, the user experience is ultimately determined by the RS capacity to î€›lter irrelevant content and recommend only items regarded as desirable. So far, the main focus of the research community in the area of RS has been placed on designing algorithms that can identify and recommend relevant content. However, while doing so, they tend to optimise (for the most part) mainstream metrics such as accuracy, at the expense of other content-derived qualitative aspects. In this work, the term â€œaccuracyâ€ denotes the performance of the RS in terms of ranking relevant items in the oî€Ÿine test set, and it should not be mistaken for accuracy in classiî€›cation tasks. In recent years, diversity and novelty of recommendations have been recognized as important factors for promoting user engagement, since recommending a diverse set of relevant items is more likely to satisfy usersâ€™ variable needs. For example, Hu and Pu[15] report a strong positive correlation between diversity of recommendations and ease of use, perceived usefulness, and intentions to use the system. Therefore, a RS that suggests strictly relevant items to a user who just purchased an espresso machine will, most likely, end up recommending more coî€ee machines, while the preferred set of recommendations would include coî€ee mugs, cleaning equipment, coî€ee beans, so to speak. In the former case, users will get to interact only with a small subspace of the available item space [28] and, according to the â€œlaw of diminishing marginal returnsâ€, the utility of the recommendations will eventually degrade as users are exposed to similar content, over and over again. Session-based recommendation has been introduced as an alternative, industry-relevant approach to RS. In session-based recommendation, a sequential model (e.g., a RNN [14] or a transformer [19,37]) is trained in a self-supervised fashion to predict the next item in the sequence itself, instead of some â€œexternalâ€ labels [14,19,43]. This training process was inspired by language modelling tasks where, given a word sequence input, the language model predicts the most likely word to appear next [24]. However, this training method can also produce sub-optimal recommendations, since the loss function is deî€›ned purely by the mismatch between model predictions and the actual items in the sequence. Models trained under such a loss function focus only on matching the sequence of clicks a user may generate, while forfeiting other desirable objectives. For example, a service provider may want to promote recommendations that will converge to purchases, increase user satisfaction, diversify user-item interactions and promote long-term engagement. Nevertheless, in order to optimize an RS towards said objectives, one needs to capture them with a diî€erentiable function, which is not a trivial task. Therefore, the use of multi-objective optimization (MOO) is heavily limited in areas where important objectives can only be presented in a form of non-diî€erentiable functions/metrics. Diversity and novelty of recommended item lists are correlated with increased diversity of sales [9], and address the â€œwinner-takesallâ€ problem by recommending less popular items from the so-called â€œlong-tailâ€™. An item from a diverse recommendation list is more likely to be novel, i.e., an item that the user would not normally interact with. This is supported by prior work, which suggests that most users appreciate novel and less popular recommendations [21,44]. Recommendation models trained with simple supervised learning may encounter diî€œculties in addressing the above recommendation expectations and the multi-objective nature of many online tasks. To address the current challenges, we expand on the idea of utilising RL in the RS setting and introduce a Scalarized MultiObjective RL (SMORL) approach. SMORL uses a single RL agent to simultaneously satisfy three, potentially conî€icting, objectives: i) promote clicks, ii) diversify the set of recommendations, and iii) introduce novel items, while at the same time optimising for relevance. The model focuses on the chosen rewards while maintaining high relevance ranking performance. More speciî€›cally, given a generative sequential or session-based recommendation model, the (î€›nal) hidden state of the model can be seen as itâ€™s output layer, since it is multiplied with the last (dense softmax) layer to generate the recommendations [14,19,43]. We augment these models with multiple î€›nal output layers. The conventional self-supervised head, is trained with the cross-entropy loss to perform ranking, while the SMORL part is simultaneously trained to modify the rankings of the self-supervised head. The RL heads can be seen as regularizers that introduce more diverse and novel recommendations, while the ranking-based supervised head can provide more robust learning signals (including negative signals) for parameter updates. One of the main advantages of using MORL instead of MOO in the context of RS is the possibility of using non-diî€erentiable functions for reward system that the RL agent uses to regularize the base model. Previous attempts of balancing accuracy with diversity and novelty included re-ranking of the î€›nal set of recommendations or training of multiple models and the use of genetic algorithms to aggregate those models [31], whereas our approach relies on training a single model and using the SMORL framework to balance the principal recommendation objectives. We argue that this framework can be easily extrapolated to other domains such as music, video, and news recommendations (by using embedding systems [3,23]), where diversity and novelty are high-value metrics. In summary, our work makes the following contributions: â€¢We devise a novel diversity reward that utilises the item embedding space. â€¢We devise a novel metric for evaluation of RS that measures repetitiveness of recommendations. â€¢To the best of our knowledge, we apply Multi-Objective Reinforcement Learning (MORL) in the setting of RS for the î€›rst time and explore some of the many possibilities and future research directions that this approach oî€ers. â€¢We introduce SMORL that drives the self-supervised RS to produce more accurate, diverse and novel recommendations. We integrate four state-of-the-art recommendation models into the proposed framework. â€¢We conduct experiments on two real-world e-commerce datasets and demonstrate less repetitive recommendations sets, signiî€›cant improvements in aggregate diversity metrics (up to 20%), all while maintaining, or even improving accuracy for all four state-of-the-art models. Several deep learning-based approaches that model the user interaction sequences eî€ectively have been proposed for RS. Hidasi et al. [14]used gated recurrent units (GRU) [8] to model user sessions, while Tang and Wang[36]and Yuan et al. [43]used convolutional neural networks (CNN) to capture sequential signals. Kang and McAuley[19]exploited the well-known Transformer [37] in the î€›eld of sequential recommendation, with promising results. All of these models can serve as the base model whose input is a sequence of user-item interactions and the output is a latent representation that describes the corresponding user state. Several attempts to use RL for RS have also been made. In the oî€-policy setting, Chen et al. [5]and Zhao et al. [45]proposed the use of propensity scores to perform oî€-policy correction, but with training diî€œculties due to high-variance. Model-based RL approaches [6,34,47] î€›rst build a model to simulate the environment, in order to avoid any issues with oî€-policy training. However, these two-stage approaches depend heavily on the accuracy of the simulator. Xin et al. [42]introduced SQN and SAC, two self-supervised RL frameworks for RS that augment the recommendation model with two output layers (heads). First head is based on the cross-entropy supervised loss, while the other RL head is based on the Double Q-learning [11]. Although SQN and SAC improve performance, they only increase accuracy by promoting clicks and purchases that a user might make. However, an accurate RS is not necessarily a useful one: real value lies in suggesting items that users would likely not discover for themselves, that is, in the novelty and diversity of recommendations [12]. Improving accuracy typically decreases diversity and novelty, which can occur when RL is deployed to regularize session-based RS (see discussion in Section 5). A decrease of aggregate diversity can impact the user experience and satisfaction with the RS [15]. Anderson et al. [1]also report that current recommendations discourage diverse user-item interactions. Diversifying recommendations and introducing novel recommendations were recently recognized as important factors for improving RS. Early eî€orts focused on post-processing methods that aimed to balance accuracy and diversity [2,29,33]. In order to mitigate issues with signiî€›cant cumulative loss on the ranking function, personalized ranking methods were proposed [7]. Chen et al. [4] tried to address the issues of post-processing methods that consider only pairwise measures of diversity and ignore correlations between items, by proposing the probabilistic model Determinantal Point Process [22] that captures the correlation between items using a kernel matrix. Once this matrix is learned, many sampling techniques can generate a diverse set of items [4,38,40]. These models achieve a trade-oî€ between accuracy and diversity at best. On the other hand, SMORL signiî€›cantly increases diversity and slightly improves the accuracy. In the RL setting, Zheng et al. [46]focused on explorationexploitation strategies for promoting diversity, by randomly choosing random item candidates in the neighborhood of the current recommended item. Hansen et al. [10]proposed a RL samplingbased ranker that produces a ranked list of diverse items. This model is a simple ranker and the model itself doesnâ€™t learn to produce diverse set of items, while the learning process utilizes the REINFORCE algorithm [41] which is known to suî€er from highvariance. Finally, prior attempts to optimize multiple objectives in the setting of RS relied on Pareto-Optimization using grid search [31] or multi-gradient descent [25]. However, by deî€›nition, one Pareto optimal solutions is not necessarily better than other Pareto optimal solution with respect to all objectives. LetIdenote the whole item set, then a user-item interaction sequence can be represented asx= {ğ‘¥, ğ‘¥, ..., ğ‘¥, ğ‘¥}, where ğ‘¥âˆˆ I (0< ğ‘– â‰¤ ğ‘¡)is the index of the interacteditem at timestamp ğ‘–. The goal of next item recommendation is recommending the item xto users that will best suit their current interests, given the sequence of previous interactions x. From the perspective of MORL, the next item recommendation task can be formulated as a Multi-Objective Markov Decision Process (MOMDP) [39], in which the recommendation agent interacts with the environmentsE(users) by sequentially recommending items to maximize the discounted cumulative rewards. The MOMDP can be deî€›ned by tuples of(S, A, P, R, ğœŒ,ğ›¾) where: â€¢ S: a continuous state space that describes the user state. The state of the user at timestampğ‘¡can be deî€›ned ass= ğº (x) âˆˆ S(ğ‘¡ >0), whereğºis a sequential model that will be discussed in Section 4. â€¢ A: a discrete action space that contains candidate items. The actionğ‘of the agent is to recommend the selected item. In the oî€Ÿine RL setting, we either extract the action at timestamp ğ‘¡from the user-item interaction, i.e.,ğ‘= ğ‘¥, or by setting it to a top prediction obtained from the self-supervised layer. The â€œgoodnessâ€ of a state-action pair(s, ğ‘)is described by its multi-objective Q-value function Q(s, ğ‘). â€¢ P:S Ã— A Ã— S â†’ Ris the state transition probability ğ‘ (s|s, ğ‘), i.e., a probability of state transition fromsto swhen agent takes action ğ‘. â€¢ R:S Ã— A â†¦â†’ Ris the vector-valued reward function, wherer(s, ğ‘)denotes the immediate reward by taking action ğ‘ at state s. â€¢ ğœŒis the initial state distribution with sâˆ¼ ğœŒ. â€¢ ğ›¾ âˆˆ [0,1]is the discount factor for future rewards. Forğ›¾ =0, the agent only considers the immediate reward, while for ğ›¾ =1, all future rewards are regarded fully except the one of the current action. The goal of the MORL agent is to î€›nd a solution to a MOMDP in a form of target policyğœ‹(ğ‘|s)so that sampling trajectories according toğœ‹(ğ‘|s)would lead to the maximum expected cumulative reward: whereğ‘“:Râ†¦â†’ Ris a scalarization function, whileğœƒ âˆˆ Rdenotes the policy parameters. The expectation is taken over trajectories ğœ = (s, ğ‘, s, ğ‘...), obtained by performing actions according to the target policy: sâˆ¼ ğœŒ, ğ‘âˆ¼ ğœ‹(Â·|s), sâˆ¼ P(Â·|s, ğ‘). A scalarization functionğ‘“maps the multi-objective Q-values Q(s, ğ‘)and a reward functionr(s, ğ‘)to a scalar value, i.e., the user utility. In this paper, we focus on linearğ‘“; each objectiveğ‘–is given an importance, i.e. weightğ‘¤, ğ‘– =1, ...,ğ‘šsuch that the scalarization function becomes ğ‘“(x) = wx, where w = [ğ‘¤, .., ğ‘¤]. We cast the task of next item recommendation as a (self-supervised) multi-class classiî€›cation problem and build a sequential model that receives user-item interaction sequencex= [ğ‘¥, ğ‘¥, ..., ğ‘¥, ğ‘¥] as an input and generatesğ‘›classiî€›cation logitsy= [ğ‘¦, ğ‘¦, ...,ğ‘¦] âˆˆ R,whereğ‘›is the number of candidate items. We can then choose the top-ğ‘˜items fromğ‘¦as our recommendation list for timestamp ğ‘¡ + 1. Each candidate item corresponds to a class. Typically one can use a generative sequence modelğº (Â·)to map the input sequence into a hidden states= ğº (x). This serves as a general encoder function. Based on the obtained hidden state, we can utilize a simple decoder to map the hidden state to the classiî€›cation logits asğ‘¦= ğ‘‘ (s). One can deî€›ne the decoder function Figure 1: The SMORL for Recommender Systems (SMORL4RS) training routine for sequential or session-based RS. Generative model G maps user-item interaction sequence xto the latent state s to logits yand to 1-dimensional Q-values: ğ‘„, ğ‘„, and ğ‘„ prediction obtained by the logits. The vector value d Q-value function is set to: Q = the scalarization function and SDQL algorithm, and used for training the base model along with the cross-entropy loss. ğ‘‘as a simple fully connected layer or the inner product with candidate item embeddings [14,19,43]. In this work, we make use of the fully connected layer. Finally, we train our recommendation model by optimizing the cross-entropy lossğ¿based on the logits ğ‘¦. Optimization of the cross-entropy loss will push the positive logits to high values, while the items that user did not interact with will be â€œpenalisedâ€, which will result in a strong negative learning signal. This negative signal is essential for learning in the base model, since the SMORL head provides strong gradients only for positive actions, i.e., top-1 items. Furthermore, due to the fact that the sequential recommendation modelğºhas already encoded the input sequence into a latent representations, we directly uses as the current state for the RL head without the need to introduce a separate RL model. We stack additional fully connected layers to calculate one-dimensional Q-values on top of ğº: ğ‘„(s, ğ‘) = ğ›¿ (sH+ ğ‘) = ğ›¿ (ğº (x)H+ ğ‘) whereğ‘§ âˆˆ {acc, div, nov},ğ›¿denotes the activation function, while Handğ‘are learnable parameters of the Q-learning output layer. The SMORL part then stacks computed accuracy, diversity, and novelty Q-values into a vector-valued Q-value function: In order to learn vector-valued Q-functions and tackle MORL tasks, Scalarized Deep Q-learning (SDQL) [27] extends the popular DQN algorithm [26], by introducing a scalarization functionğ‘“. At every time stepğ‘¡, Q-network is optimized on the lossğ¿computed on a mini-batch of experience tuples(s, ğ‘, ğ‘Ÿ, ğ‘ )obtained from experience buî€er ğ·: wherey(s, ğ‘) = r+ ğ›¾Q(s, argmax[wQ(s, ğ‘)]), andQbeing the target network. Training towards a î€›xed target network prevents approximation errors from propagating too quickly from state to state, and sampling experiences to train on (experience replay) increases sample eî€œciency and reduces correlation between training samples. . With the use of fully-connected layers, sis mapped . Diversity and novelty rewards are calculated using the topî€‚î€ƒ When generating recommendations, we still return the top-ğ‘˜ items from the supervised head. The SMORL head acts as a regularizer of the base recommendation modelğºthat î€›ne-tunes it by assessing the quality of recommended top item, according to the predeî€›ned reward setting and scalarization functionğ‘“, i.e., importance of objectives. For the base modelğºto learn to provide more relevant recommendations, we expand on [42] and deî€›ne accuracy reward as ğ‘Ÿ(s, ğ‘) = ğ‘Ÿ(ğ‘) = 1, ğ‘ From the deî€›nition of the reward, the model is rewarded when it matches the next clicked item in the sequence. Xin et. al. [42] suggested using the reward for both clicks and purchases. However, in this work, we introduce a method that can be easily extrapolated from e-commerce to other relevant areas of RS. We note that, by reinforcing the relevance of recommended items, one can signiî€›cantly hinder the userâ€™s ability to explore the platform due to the similarity of the recommendations to the userâ€™s recent interests. We explore this claim in Section 5. Therefore, it is crucial for a model to also learn how to recommend diverse sets of items, as well as items that are more probable to never be discovered by the user. For the SMORL head to promote diverse sets of recommendations, we î€›rst train a GRU4Rec model [14], and save the embedding layer E. We then freeze the weights ofEto stop further updates of the parameters. We deî€›ne the reward ğ‘Ÿas ğ‘Ÿ= ğ‘Ÿ(s, ğ‘) = 1 âˆ’ cos(ğ‘™, ğ‘) = 1 âˆ’âˆ¥eâˆ¥âˆ¥eâˆ¥(4) whereğ‘™is the last clicked item in the session,ğ‘is a top prediction obtained from self-supervised layer, andeis the embedding of the itemğ‘¥, obtained fromE. We do not use the embedding of a model that is currently trained for calculationğ‘Ÿ. It would be unstable at the beginning of the training process, which would produce unreliable diversity rewards. This reward reinforces diversity across a session of recommendations rather than just over a single slate. Input : user-item interaction sequence set X, = ğº (x), s= ğº(x) Basing the diversity reward system on top predictionğ‘and topğ‘˜recommendations instead of only the last clicked itemğ‘™was considered, but we observed no improvement in performance. Given an item, a user may have previously seen it in another set of recommendations but chose not to click it, or already encountered it on some other platform. Therefore, in a real-world use case, it is impossible to track the items that a user may have already seen, and to suggest items that are certain to be novel. To address this issue and introduce novelty and serendipity into the set of recommendations, we take a probabilistic approach. Less popular items are more likely to be novel and lead to a more balanced distribution of item popularity. We use binarized item frequency as a novelty reward for our MORL head, which we deî€›ne as follows: ğ‘Ÿ= ğ‘Ÿ(s, ğ‘) =0.0 ğ‘in top ğ‘¥% of most popular items1.0 otherwise whereğ‘is the top predicted item obtained from the self-supervised layer. The choice ofğ‘¥is based on the empirical distribution of the item popularity inferred from the training set, i.e., we set it to the approximate percentile where the long tail starts. Both datasets used in this work have a similar distribution, so we setğ‘¥ B10. As can be seen, accuracy reward depends on the next item in the session, while diversity and novelty rewards depends on the top prediction from self-supervised layer. Recommendation is by nature a multi-objective problem and, as such, stock self-supervised learning, or even single-objective RL methods, cannot satisfy all desirable (or necessary) goals. We integrate the three proposed objectives into a single SMORL method that at each timestamp î€›nds an optimal action that takes into consideration all objectives according to a predeî€›ned user utility function, or in this case, according to the conî€›guration ofwfrom Eq.(2). SMORL is highly customizable and adaptable to a speciî€›c providerâ€™s goals - one can deî€›ne diî€erent reward systems that can result in a RS that provides more relevant, novel, diverse, unexpected, or serendipitous recommendations. The î€›nal loss that we optimize is: whereğ¿is a cross-entropy loss, andğ›¼is a hyperparameter that enables us to control the inî€uence of SMORL part. In order to enhance the learning stability, we alternately train two copies of learnable parameters. Algorithm 1 describes the training procedure of SMORL. It should be noted that after the training is î€›nished, only the self-supervised part of the base model is used to produce recommendations, while the eî€ects with respect to diî€erent metrics are observed from the regularization by the SMORL part. This training framework can be integrated in existing recommendation models, provided they follow the general architecture discussed earlier. This is the case for most session-based or sequential recommendation models introduced over the last years. In this work, we use the cross-entropy loss for the self-supervised part but other models can incorporate diî€erent loss functions [13, 30]. In addition, SMORL is a highly modular framework, where one can re-weight and â€œdeactivateâ€ speciî€›c RL objectives, or add more of them with the help of a carefully designed reward schema. Ultimately, this mechanism allows the RS to focus on providersâ€™ speciî€›c short-term and long-term goals. However, our experimental results show that models regularized by all three RL objectives perform the best in most cases, with respect to all quality metrics. We report the results of our experimentson two real-world sequential e-commerce datasets. For all base models, we used the self-supervised head to generate recommendations. We address the following research questions: RQ1:When integrated, does the proposed method increase the performance of the base models? RQ2:Can we control the balance between accuracy, diversity and novelty? RQ3:Can we increase the inî€uence of SMORL part by adjusting the intensity of its gradient? 5.1.1 Datasets: RC15and RetailRocket, Table 1. RC15.This dataset is based on the RecSys Challange 2015. The dataset is session-based and each session contains a sequence of clicks and purchases. We discard sessions whose length is smaller than 3 and then sample a subset of 200K sessions. RetailRocket.This dataset is collected from a real-world ecommerce website. It contains session events of viewing and adding to cart. To keep in line with the RC15 dataset, we treat views as clicks. We remove the items which are interacted less than three times (3), and the sequences whose length is smaller than three (3). 5.1.2 î€ality of Recommendation Metrics. Accuracy metrics.Relevance of the recommended item set is usually measured with two metrics: hit ration (HR) and normalized discounted cumulative gain (NDCG). HR@ğ‘˜is a recall-based metric, measuring whether the ground-truth item is in the top-ğ‘˜positions of the recommendation list. We deî€›ne HR for clicks as: On the other hand, NDCG is a rank sensitive metric that assigns higher scores to top positions in the recommendation list [18] Diversity & Novelty metrics.Diversity in RS can be viewed at either individual or aggregate level. For example, if the RS was to provide the same set of ten dissimilar items to all users, the recommendation list for each user would be diverse, i.e., it would have high individual diversity. However, the system can only recommend ten items out of the entire item pool and, thus, the aggregate diversity would be negligible. Therefore, in our experiments, we measure aggregate diversity using Coverage@ğ‘˜(CV@ğ‘˜),ğ‘˜ âˆˆ {1,5,10,20}. More speciî€›cally, we measure CV@ğ‘˜on two sets: set of all items and a set of less popular items. Coverage can be computed as percentage of all items (less popular items) covered by all top-ğ‘˜recommendations of the validation or test sequences. Repetitiveness of Recommendations.We introduce Repetitiveness (R), a novel metric for evaluating the usefulness of recommendations. We consider this metric a good proxy as to how easily a RS can create a î€›lter bubble, as it measures the per session average of repetitions in the top-ğ‘˜positions of recommendations lists. We measure R@ğ‘˜, ğ‘˜ âˆˆ {5, 10, 20} and deî€›ne it as: where ğ‘ is the total number of sessions in test (or validation) set. 5.1.3 Evaluation Protocols. We use 5-fold cross-validation for our performance evaluation, with a ratio of 8:1:1 for training, validation, and testing. We report average performance across all folds. 5.1.4 Baselines. We integrated SMORL in four state-of-the-art (generative) sequential recommendation models: â€¢GRU4Rec [14]: This method uses a GRU to model the input sequences. The î€›nal hidden state of the GRU4Rec is treated as the latent representation for the input sequence. â€¢Caser [36]: This recently introduced CNN-based method captures sequential signals by applying convolution operations on the embedding matrix of previous items. â€¢NextItNet [43]: This method enhances Caser by using dilated CNN to enlarge the receptive î€›eld and residual connection to increase the network depth. â€¢ SASRec [19]: This baseline is motivated from self-attention and uses the Transformer [37] architecture to encode sequences of user-item interactions. The output of the Transformer encoder is treated as the latent representation. 5.1.5 Parameter seî€ings. For both datasets the input sequences comprise of the last 10 items before the target timestamp. If the sequence length is less than 10, we complement it with a padding item. We train all models with the Adam optimizer [20]. The minibatch size is set as 256. The learning rate is set as 0.01 for RC15 and 0.005 for RetailRocket. We evaluate on the validation set every 5,000 batches of updates on RC15, and every 10,000 batches of updates on RetailRocket. To ensure a fair comparison, the item embedding size is set as 64 for all models. For the GRU4Rec model, the size of the hidden state is set as 64. For Caser, we use one vertical convolution î€›lter and 16 horizontal î€›lters, whose heights are set from {2, 3, 4}. The drop-out ratio is set as 0.1. For NextItNet, we use the same parameters reported by authors. For SASRec, the number of heads in self-attention is set as 1, according to its original paper [19]. We set the discount factorğ›¾to 0.5, as recommended by Xin et al. [42]. For both datasets, the SQN method [42] outperforms the baselines with respect to recommending relevant items to the users. However, by increasing the accuracy of the baseline model, it causes it to â€œdriftâ€ from diversity and novelty. This results in a substantial decrease (up to 20%) of coverage metrics for the baseline model, both on all and less popular items. Together with this fact, increased repetitiveness of recommendations suggests that reinforcing accuracy alone may hinder signiî€›cantly the perceived quality of experience. Furthermore, it is evident that one should simultaneously optimize the model towards diversity and novelty to achieve a balance between opposing metrics. In Table 2 and Table 3, we see that by using the SMORL method we not only obtain a balance between accuracy, diversity and novelty, but we consistently outperform the corresponding baselines across all metrics and, to some extent, we also improve their accuracy power. The increase in diversity and novelty is up to 20% relative to the baseline model, and up to 40% relative to the SQN model. Increases in the accuracy of the baseline models can be attributed to most users having diverse interests that cannot be satisî€›ed by the recommendations produced by an RS [1]. Figure 2 displays the diî€erence in cumulative diversity and novelty Table 2: Recommendation performance on RC15 dataset. NG is NDCG. CV is Coverage. Boldface denotes highest score. NtItNet-SMORL 0.4116 0.2505 0.4898 0.2703 0.3385 0.5639 0.6518 0.7283 0.2720 0.5156 0.6131 0.6981 9.97 21.73 45.49 SASRec-SMORL 0.4315 0.2651 0.5104 0.2851 0.3380 0.5755 0.6508 0.7158 0.2698 0.5285 0.6120 0.6842 10.38 22.79 48.48 Table 3: Recommendation performance on RetailRocket dataset. NG is NDCG. CV is Coverage. Boldface denotes highest score. Caser-SMORL 0.2657 0.1898 0.3052 0.1998 0.2855 0.5411 0.6324 0.7138 0.2224 0.4917 0.5925 0.6827 15.90 32.47 66.76 NtItNet-SMORL 0.3183 0.2222 0.3659 0.2342 0.3429 0.6335 0.7351 0.8129 0.2800 0.5938 0.7062 0.7924 10.92 22.89 47.73 SASRec-SMORL 0.3521 0.2477 0.4028 0.2605 0.3037 0.5724 0.6672 0.7476 0.2366 0.5261 0.6311 0.7202 12.58 26.69 56.14 Figure 2: Comparison of cumulative rewards on RC15 dataset with base models regularized by SMORL and SQN frameworks. rewards obtained on the RC15 test set. When a base model is trained with the SMORL framework, we note a signiî€›cant increase in the cumulative diversity and novelty rewards. Also, the results in Tables 2 and 3 suggest that reinforcing diversity and novelty introduces a notable improvement in these metrics, which are highly correlated with perceived quality of experience and engagement. One of the advantages of using SMORL is its objective-balancing capability, which works by re-weighting the objectives using different conî€›gurations ofwâ€™s in Eq.(2). In our setting, the î€›rst entry (c) NextItNet(d) SASRec ofwcorresponds to the strength of accuracy objective, the second to diversity, and the third to novelty objective. We conduct experiments with the following conî€›gurations of the parameterw: Here, we aim to demonstrate the diî€erence in performance when reinforcing a subset of three important objectives. We do not include w = (1,0,0)in this analysis, since SMORL becomes equivalent to SQN method from [42] and our results show exactly the same behaviour across all models. The objectives that we address in this work have a complex relationship. For example, relevance and diversity at the beginning Figure 3: Performance comparison when reinforcing a subset of objectives (achieved by using diî€erent conî€›gurations of w from Eq.(7)). Re d lines denote relevant metrics of the stock NextItNet model - not trained using the SMORL4RS framework. Figure 4: NextItNet with diî€erent intensity of SMORL gradient on the RC15 (4a) and RetailRocket (4b) datasets of the training process are correlated, i.e., more diverse recommendations produce more relevant recommendations, while their correlation becomes negative as the training progresses. Diversity and novelty are intertwined objectives, e.g., a diverse set of recommended items is more likely to contain novel items. On the other hand, the popularity of items follows a power distribution and, therefore, less popular items make up to 90% of the dataset, which means that items likely to be novel are inherently diverse. Given that the proposed method is not a pure MORL model, but rather a regularizer that forces the base model to capture diî€erent (and often competing) objectives, the intricacies of optimizing and balancing multiple objectives pose a signiî€›cant research challenge. In this section, our goal is to demonstrate that we can control how much inî€uence each objective has, and not how to î€›nd an ideal balance. With the ability of control, many engineering possibilities arise, such as deploying multiple SMORL4RS agents and deciding in an online fashion if a user should receive recommendations from an agent that is optimized towards novelty, diversity, or accuracy. Figure 3 shows the comparison of NextItNet-SMORL model regularized by the SMORL agent that uses mentioned weight conî€›gurationswon RetailRocket dataset, while similar behaviour can be observed for the RC15 dataset and other models. More speciî€›cally, Figure 3a indicates that if we regularize the model only towards novelty, we will sacriî€›ce its ability to recommend relevant items. This phenomenon is also present if we only reinforce towards diversity, but the drop in NDCG@20 metric is not as notable. On the other hand, if we optimize jointly towards diversity and novelty, we do not observe a drop in the accuracy of the base model. Additionally, if we include the accuracy objective to any of the two other, we observe an increase in the relevant metric. From Figures 3b and 3c, we note that including the accuracy objective comes at the cost of diversity and novelty, while combined optimization towards diversity and novelty produce the best results with respect to these metrics. Similarly, by including the accuracy objective, we increase the repetitiveness compared to the NextItNet-SMORL model that optimizes towards a combination of diversity and novelty. Across all base models and both datasets, the SDQL loss is dominated by self-supervised loss, which suggests that the optimization of parameterğ›¼from Eq.(5) might improve the eî€ect of SMORL part on the base model. Figure 4 shows the behaviour of NextItNetSMORL model with respect to NDCG@20 and CV@20 metrics on both datasets when we change the intensity of SDQL gradient. As expected, when multiplying SDQL withğ›¼ <1, the eî€ects are decreased and we do not improve dramatically compared to the base model. Increase in both metrics can be seen forğ›¼ âˆˆ {1,2,3,5,10}, with the best balance obtained forğ›¼ =5. For higher values ofğ›¼, we observe a notable drop in quality due to the loss of gradient signal obtained from the self-supervised loss, which indicates that it is necessary to have a self-supervised part to learn basic ranking. Similar analysis can be made for RC15 dataset. The optimal value of theğ›¼parameter is equal to 1 for most cases - SASRec on RC15 dataset, GRU4Rec on RetailRocket, and Caser on the RetailRocket dataset. However, for GRU4Rec and Caser on RC15, the optimal value is equal to 0.75, for SASRec and NextItNet on RC15 to 3, while for SASRec on RetailRocket is equal to 10. Hence for real-world use-cases, when datasets usually contain millions of items, higher values ofğ›¼might be optimal. More complex models, such as NextItNet and SASRec require higher value of ğ›¼. We î€›rst formalized the next item recommendation task and presented it as a Multi-Objective MDP task. The SMORL method acts as a regularizer for introducing desirable properties into the recommendation model, speciî€›cally to achieve a balance between relevance, diversity and novelty of recommendations. We integrated SMORL with four state-of-the-art recommendation models and conducted experiments on two real-world e-commerce datasets. Our experimental î€›ndings demonstrate that the joint optimization of three conî€icting objectives is essential for improving metrics that are strongly correlated with user satisfaction, while also preserving content relevance. Future work brings vast possibilities for exploring the use of SMORL paradigm in the setting of RS, and it will include further experiments with diî€erent objectives and application of SMORL in diî€erent areas, such as music platforms. Also, the joint optimization of supervised and SDQL loss is a research problem on its own. Finally, we plan on exploring the use of non-linear and personalized scalarization functions.