Counterfactual explanation methods interpret the outputs of a machine learning model in the form of "what-if scenarios" without compromising the î€›delity-interpretability trade-oî€. They explain how to obtain a desired prediction from the model by recommending small changes to the input features, aka recourse. We believe an actionable recourse should be created based on sound counterfactual explanations originating from the distribution of the ground-truth data and linked to the domain knowledge. Moreover, it needs to preserve the coherency between changed/unchanged features while satisfying user/domain-speciî€›ed constraints. This paper introduces CARE, a modular explanation framework that addresses the model- and user-level desiderata in a consecutive and structured manner. We tackle the existing requirements by proposing novel and eî€œcient solutions that are formulated in a multi-objective optimization framework. The designed framework enables including arbitrary requirements and generating counterfactual explanations and actionable recourse by choice. As a modelagnostic approach, CARE generates multiple, diverse explanations for any black-box model in tabular classiî€›cation and regression settings. Several experiments on standard data sets and black-box models demonstrate the eî€ectiveness of our modular framework and its superior performance compared to the baselines. Interpretable Machine Learning, Actionable Recourse, Counterfactual Explanations, Black-box Models ACM Reference Format: Peyman Rasouli and Ingrid Chieh Yu. 2021. CARE: Coherent Actionable Recourse based on Sound Counterfactual Explanations. In Proceedings of ACM Conference (Conferenceâ€™17). ACM, New York, NY, USA, 10 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn "Why was my loan application denied?" is an example of a follow-up question when a Machine Learning (ML) system does not provide a userâ€™s desired outcome. The user often expects more information beyond merely an explanation to know "What do I need to change for the bank to approve my loan?". Given the userâ€™s preferences, an actionable recourse is a list of possible changes for achieving the desired decision. By recommending the alterations, it also explains the reasons behind the current outcome. Counterfactual explanation is a technique for identifying the necessary changes in the original input that leads to the desired prediction of the ML model [24]. For example, a possible counterfactual for a user who is denied a loan by the model may be like this: â€œif your balance was 5000$, your loan application would have been acceptedâ€. This explanation justiî€›es the denial prediction of the model, gives a guideline to reverse the prediction, reveals potential fairness issues, and demonstrates the correlations and patterns among the domainâ€™s data. For having an actionable and realistic recourse, the counterfactual explanation should be personalized with respect to the user and be sound in the sense that it originates from the domainâ€™s observations. We claim that the solution should satisfy several important desiderata in addition to basic properties such as feature value similarity and opposite prediction. In the following, we discuss these requirements in conjunction with the relevant existing studies. Proximity. A counterfactual should provide plausible changes to the original input that are in accordance with the observations of the model. For example, "earning negative money for granting the loan" is an outlier counterfactual which is untrustworthy and unrealistic. This notion is called proximity, which indicates a counterfactual instance should lie in the neighborhood of the ground-truth data [12]. Recent papers have proposed various ways of handling proximity. DACE uses Local Outlier Factor [1] in the cost function of the optimization algorithm [10]. Several works employ generative models (e.g., Variational Auto-Encoders) for approximating the data distribution, and then sample realistic counterfactual instances [9,15,23]. FACE respects the underlying data distribution by connecting the counterfactual to the input via high-density paths [18]. MOC uses a weighted average distance metric between the original input and its ğ¾ nearest instances in the training data [4]. Connectedness. A counterfactual should be the result of existing knowledge and not a consequence of an artifact of the ML model. Artifacts can be created due to lack of training data for some regions in the feature space that diminish the robustness of the model and provoke misbehavior. Having an explanation caused by an artifact is not associated to the domain knowledge and is undesirable in the context of interpretability and feasibility [12]. Therefore, there is a need to deî€›ne a relationship between a counterfactual instance and existing knowledge (training data) using a path. This notion is called connectedness, which implies that an interpretable counterfactual should be continuously connected to data points from the same class [12]. This property is thus complementary to proximity, as two instances can be close but not linked by a continuous path. It leads to interpretable counterfactuals that comply with the domain knowledge and, therefore, more actionable. Coherency. A counterfactual should preserve the correlation among features to create a coherent explanation. For example, a counterfactual that provides the desired outcome via "increasing the education level from Bachelor to Master" without increasing "age" is not feasible because "age" and "education" are two correlated features, and they need to be changed jointly. This counterfactual may still satisfy soundness properties such as proximity and connectedness (as there can be found samples in the training data with the same age as the input but having a Masterâ€™s degree), however, from the actionability point of view, it is an inconsistent explanation. Considering this property is even more essential when a user imposes constraints over some features regardless of the status of other features. In this case, the counterfactual needs to implicitly preserve the consistency of changed/unchanged features. Although there are several research works around creating statistically sound explanations [4,9,10,15,18,23], to the best of our knowledge, the coherency property has been remained unexplored. Actionability. A counterfactual should satisfy some global and local preferences that are domain-speciî€›c and deî€›ned by the enduser. Global preferences are constraints that should be satisî€›ed by every counterfactual, for instance, î€›xing immutable features like raceandgender, while local preferences are related to individual instances. For example, a possible range for featurebalancefor an individual is[3000$,6000$]while for another one is[5000$,10000$]. This notion is called actionability, implying a recourse that meets the userâ€™s preferences [22]. Recent works address the actionability property either by classifying the features into immutable and mutable types [4,11,17], or by deî€›ning a range/set of values for numerical and categorical features [21, 22]. According to our literature review on counterfactual explanation methods, the majority of works focus on proximity and actionability properties, while connectedness and coherency received little attention. A likely reason for neglecting connectedness as an objective goal in the counterfactual generation process can be its computational burden. Moreover, many state-of-the-art works use proximity as an interchangeable property for connectedness, which is arguable, as proximity prevents generating outlier counterfactuals while connectedness results in counterfactuals that are connected to the existing domain knowledge. Disregarding connectedness results in instances being created from areas where the model has no information about (artifacts) and makes questionable improvisations (decisions) [13]. In our opinion, both proximity and connectedness properties are necessary for deriving statistically sound counterfactual explanations. Most of the stated works establish proximity by î€›nding a counterfactual that is connected to the entire training data. Indeed, this can be problematic, because a sparse counterfactual instance (changes in one/two features) lies fairly close to the overall training data, however, it can be out-of-distribution with respect to the subset of data that share the same values as the changed features and belong to the same class [23]. In contrast, our deî€›nition of proximity is inline with [23] which states that a counterfactual instance should be an inlier with respect to a subset of data that are similar and belong to the same class as the counterfactual. The mentioned statistical properties do not preserve the consistency among features, especially when userâ€™s preferences come into play. Often, a user deî€›nes constraints for some features irrespective of the status of others. When an algorithm only satisî€›es the speciî€›ed constraints and neglect their eî€ects on the other features, it will generate an unrealistic and impractical recourse. Hence, we view coherency and actionability as two intertwined properties that should be satisî€›ed simultaneously. In this paper, we propose a method for generatingCoherent ActionableRecourse based on sound counterfactualExplanations (CARE). CARE provides actionable recourse by fulî€›lling the mentioned desiderata through objective functions organized in a modular hierarchy structure and optimized using Non-dominated Sorting Genetic Algorithm III (NSGA-III) [5]. The optimization choice leads to a model-agnostic explanation method that generates multiple (diverse) counterfactuals for every input, applies to both classiî€›cation and regression tasks, and handles mixed-feature data sets. We propose a novel approach to preserve consistency between features and introduce a novel notion of actionability that can cover various constraints and prioritize diî€erent preferences. Our main contributions are as follow: â€¢We propose a modular explanation framework that handles the model- and user/domain-level desiderata in a consecutive and structured manner. â€¢We devise novel and eî€œcient solutions for every requirement that are formulated in a multi-objective optimization framework. â€¢We introduce a model-agnostic approach to generate multiple, diverse explanations for any black-box model in tabular classiî€›cation and regression settings. â€¢We demonstrate the importance of various requirements in counterfactual generation and the eî€œcacy of our modular framework in addressing them through extensive validation and benchmark experiments. â€¢We provide a multi-purpose and î€exible benchmark for the research community: https://github.com/peymanrasouli/ CARE. State-of-the-art works take into account a subset of the stated properties for counterfactual generation. However, as we later demonstrate by an illustrative example, every property plays a unique and crucial role in creating feasible and actionable explanations. We propose a modular framework that formulates the necessary properties in a consecutive and structured manner. Figure 1 illustrates the overall framework of CARE. It consists of four modules each fulî€›lls some speciî€›c properties for actionable recourse including VALIDIT Y, SOUNDNESS, COHERENCY, andACTIONABILITY. Low-level modules mostly contain model-related objectives, while high-level modules target user/domain-related goals. VALIDITY module acts as a basis for other modules, and counterfactuals can be generated regardless of the presence of other intermediate modules. In other words, it is possible to disable/enable desired modules in Figure 1: CAREâ€™s framework: modular hierarchy. the hierarchy. Using this structure, we can observe the eî€ect of the diî€erent properties on the generated counterfactuals. To provide an insight on how the diî€erent properties aî€ect the generated counterfactual explanations, we explain an instance of the Adult Income data set [6], denoted byğ‘‹, using incremental conî€›gurations of CAREâ€™s modules. The results are reported in Table 1. It can be seen that the valid counterfactual (ğ¸) is a minimally modiî€›ed variant of the original input that only provides the desired outcome irrespective of feasibility and actionability of changes. In contrast, the sound counterfactual (ğ¸) lies on the data manifold and appears more realistic, however, it has not established the correlation between features perfectly (the values of featuresrelationshipandsexdo not conform with each other). The coherent counterfactual (ğ¸) resolves the consistency shortcoming of sound counterfactuals and creates a coherent state for input features (featuresrelationshipandsexare consistent). Nevertheless, a statistically sound and coherent counterfactual may not be actionable from the userâ€™s perspective (changing sensitive features likesexis impractical). Hence, an actionable explanation (ğ¸) takes into account the userâ€™s preferences containing the name of mutable/immutable features, possible values, and their importance to create a realistic and actionable recourse. The stated example demonstrates the importance of every property as well as the exclusive role of CAREâ€™s modules in handling these properties. Our framework can be used as a benchmark for explanation methods sharing similar properties, as it allows proper comparison by enabling speciî€›c modules in the hierarchy. CARE has two main phases: î€›tting and explaining; the former creates an explainer based on the training data and the ML model, while the latter generates counterfactuals for every input instance using the created explainer. In the following, we describe the CAREâ€™s modules and the optimization algorithm for generating explanations. We deî€›ne a valid counterfactual explanation as an instance similar to the input with minimum changes to features that results in the desired outcome. Consider(ğ‘‹, ğ‘Œ)as a data set, whereğ‘‹is an ğ‘šdimensional feature space andğ‘Œis a target set. Letğ‘“be a blackbox model trained on(ğ‘‹, ğ‘Œ)that maps an input to a target, i.e., ğ‘“:ğ‘‹â†’ ğ‘Œ. Targets can be either discrete classes or continuous values depending on the prediction task (classiî€›cation or regression). For a given inputğ‘¥withğ‘“ (ğ‘¥) = ğ‘¦and a desired outcomeğ‘¦, our goal is to î€›nd a counterfactualğ‘¥as close toğ‘¥as possible such thatğ‘“ (ğ‘¥) = ğ‘¦. VALIDITY module satisî€›es three main objectives: desired outcome, minimum feature distance, and maximum sparsity. Maximum sparsity enhances the interpretability of the generated counterfactual explanations and is inline with minimum feature distance. We deî€›ne three cost functions for the stated objectives that are minimized during the optimization process. 2.1.1 Desired Outcome. We evaluate the prediction ofğ‘“for a generated counterfactualğ‘¥with respect to the desired outcome. Accessing the black-box prediction function allows us to deî€›ne diî€erent measurements for the desired outcome. For the classiî€›cation task, we use the Hinge loss function: whereğ‘“(ğ‘¥)is the prediction probability ofğ‘¥for the desired class ğ‘andğ‘is a probability threshold that leads to a counterfactual with a desired level of conî€›dence. This cost function considers all counterfactuals above the thresholdğ‘as valid counterfactuals. The desired outcome for the regression task is evaluated as follows: ğ‘‚(ğ‘¥, ğ‘Ÿ) =min|ğ‘“ (ğ‘¥) âˆ’ ğ‘Ÿ|, otherwise(2) whereğ‘“ (ğ‘¥)is the predicted response for the counterfactual and ğ‘Ÿ = [ğ‘™ğ‘, ğ‘¢ğ‘]is a desired response range. The devised cost function considers any predicted response within the rangeğ‘Ÿas valid (zero cost), otherwise the absolute distance between the prediction and the closest bound (ğ‘™ğ‘ or ğ‘¢ğ‘) is considered as the cost of ğ‘¥. 2.1.2 Feature Distance. We employ Gower distance [8] to calculate the distance between features in a mixed-feature setting (i.e., data set contains both categorical and numerical features). Given an original inputğ‘¥and a counterfactual instanceğ‘¥, we measure their feature value distance by summation over the diî€erence between feature values: whereğ‘šis the number of features, andğ›¿is a metric function that returns a distance value depending on the type of the feature: whereğ‘…is the value range of featureğ‘—that is extracted from the training data, and I is a binary indicator function. 2.1.3 Sparsity. To have a highly interpretable explanation, a counterfactual should alter a minimum number of features. Minimum feature distance is not equivalent to the minimum number of changed features. Therefore, we deî€›ne the cost functionğ‘‚for counting the number of altered features: Table 1: Counterfactual explanations generated using incremental conî€›gurations of CAREâ€™s modules for an instance from the Adult Income data set. whereğ‘šis the number of features in the data set. This function penalizes every change in features regardless of their type. As we mentioned earlier, a sound counterfactual should originate from the observed data (proximity) and connect to the existing knowledge (connectedness). Meeting these two conditions results in an inlier instance located in a region in the decision surface where the model has a high level of conî€›dence. Therefore, our SOUNDNESS module has two î€›tness functions that are maximized during the optimization process. 2.2.1 Proximity. Proximity indicates that the counterfactual instance lies in the neighborhood of the ground-truth samples that are predicted correctly by the model and have the same target value as the counterfactual. We utilize the proximity evaluation metric introduced in [12] as an objective function for counterfactual generation. Letğ‘¥be our original input andğ‘¥be a generated counterfactual. We refer toğ‘‹as the set of instances in the data set that are predicted correctly byğ‘“and belong to the same class (in classiî€›cation task) or response range (in regression task) asğ‘¥. Considerğ‘âˆˆ ğ‘‹is the closest instance toğ‘¥, i.e., ğ‘= arg minğ· (ğ‘¥, ğ‘), whereğ·is a distance metric (e.g., Minkowski). The counterfactualğ‘¥fulî€›lls the proximity criterion if it has the same distance toğ‘asğ‘has to the rest of the data (ğ‘‹). The formal deî€›nition of proximity is as follows: A lower value ofğ‘ğ‘Ÿğ‘œğ‘¥ğ‘–ğ‘šğ‘–ğ‘¡ğ‘¦ (ğ‘¥)refers to an inlier counterfactual that is located at a reasonable distance from the training data that are predicted identically. According to [1], the formal deî€›nition of proximity (Eq. 6) corresponds to the Local Outlier Factor (LOF) with a neighborhood size ofğ¾ =1, which is a well-known model for outlier detection. In the î€›tting phase of the explainer, we create an LOF model for every class/response range of the samples in the training data that are predicted correctly by the modelğ‘“. During the explaining phase and via the objective functionğ‘‚, we invoke the LOF model related to the class/response range of the counterfactualğ‘¥to identify its status; ifğ‘¥is an inlier, the model outputs 1, otherwise, it returns 0 that refers to an outlier. Thus, the goal is to maximize ğ‘‚for every counterfactual instance. 2.2.2 Connectedness. Connectedness implies the counterfactual instance is the result of existing knowledge and not a consequence of an artifact of the ML model. Such a counterfactual is continuously connected to the observed data (knowledge) using a topological notion of path. Along this path, features change smoothly and coherently, and each instance in the path is correlated with the preceding and succeeding instances. This property is thus complementary to proximity, in which the counterfactual is close to a real data point but is not necessarily linked to the majority of the data. Similarly, we beneî€›t from the connectedness evaluation metric proposed in [12] as an objective function for counterfactual generation. The continuous path can be approximated by the notion of ğœ–-chainability (withğœ– >0) between two instancesğ‘’andğ‘, meaning that a î€›nite sequenceğ‘‹= ğ‘’, ğ‘’, .., ğ‘’, whereğ‘‹âŠ‚ ğ‘‹, exists such thatğ‘’= ğ‘’,ğ‘’= ğ‘, andâˆ€ğ‘– < ğ‘ , ğ· (ğ‘’, ğ‘’) < ğœ–. Letğ‘¥be a counterfactual instance for an inputğ‘¥. We say counterfactualğ‘¥ isğœ–-connected toğ‘ âˆˆ ğ‘‹ifğ‘“ (ğ‘¥) = ğ‘“ (ğ‘)and there exist anğœ–-chain ğ‘‹between ğ‘¥and ğ‘ such that âˆ€ğ‘’ âˆˆ ğ‘‹, ğ‘“ (ğ‘’) = ğ‘“ (ğ‘¥). Although assessingğœ–-connectedness seems complex, its deî€›nition resembles the DBSCAN clustering algorithm [7]. We can acknowledge thatğ‘¥isğœ–-connected toğ‘ âˆˆ ğ‘‹, ifğ‘¥andğ‘belong to the same cluster of DBSCAN algorithm with parametersepsilon= ğœ– (maximum distance between two samples) andmin_samples=2 (number of samples in a neighborhood). Using DBSCAN clustering for every counterfactual instance is not computationally eî€œcient. Moreover, î€›nding an optimalepsilonparameter, which highly impacts the clustering results, for every class/response range is challenging. To remedy the stated issues, we employ a generalized version of DBSCAN, called HDBSCAN [2]. This algorithm adaptively selects the bestepsilonvalue to adaptively produce stable clusters. We avoid computational complexity by creating a HDBSCAN model on a large amount of samples and then querying the model for predicting the cluster of a potential counterfactual instance. Since one sample is not likely to alter the shape of the created clusters for the ground-truth data, we achieve a fairly accurate measurement of connectedness. Moreover, it does not require updating the clustering model, making the assessment procedure computationally eî€œcient. We deî€›ne the objective functionğ‘‚to connect the generated counterfactuals to the existing knowledge. We categorize the ground-truth samples w.r.t every class/response range that are predicted correctly by the modelğ‘“. A clustering model for every category is then constructed within the î€›tting phase of the explainer. In the explaining phase and using objective functionğ‘‚, the clustering model corresponding to the class/response range of the counterfactualğ‘¥is queried. Ifğ‘¥is assigned to a cluster, the function returns 1, otherwise, it returns 0, which indicatesğ‘¥is not Algorithm 1 Correlation Models Input:observed datağ‘‹, correlation thresholdğœŒ, score thresholdğœ, number of features ğ‘š, type of features ğ¹ Output: correlation models M connected to the known knowledge. Hence, the goal is to maximize ğ‘‚for every counterfactual instance. Preserving the relationship between changed/unchanged features is essential for creating a coherent and actionable counterfactual explanation. Fulî€›lling this property is even more important when the output is a personalized, actionable recourse. By knowing the relationship of the domainâ€™s features, one can formulate unary and binary constraints in simple terms. For example, "age does not decrease" or "education level increment causes age increment". However, relationships over multiple features can lead to complex constraints that is hard to specify manually. We introduce a novel approach to impose high-order correlation constraints over multiple features. Using correlation information extracted from the training data, we construct predictive models (also referred as correlation models) that are exploited to preserve the coherency of counterfactual features. This technique is suitable when the input features are unknown, and knowledge about the domain is limited. The COHERENCY module uses a two-phase approach: 1) constructing correlation models in the î€›tting phase (Algorithm 1), and 2) exploiting the created models in the objective function ğ‘‚during the explaining phase (Algorithm 2). Algorithm 1 starts by extracting feature correlations from the training datağ‘‹using Pearsonâ€™s R, Correlation Ratio, and Cramerâ€™s V for pairs of numerical-numerical, numerical-categorical, and categorical-categorical features, respectively [3]. This creates a symmetric correlation matrix with scaled valuesğ‘ğ‘œğ‘Ÿğ‘Ÿâˆˆ [0,1]. We consider every featureğ‘—,ğ‘— âˆˆ {1..ğ‘š}, as a dependent variable that is predicted by its correlated features (independent variables) denoted byğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ğ‘ (Line 5). Our goal is to create computationally eî€œcient classiî€›cation and regression models (e.g., CART [14] and Ridge [16]) for every categorical and numerical features, respectively. Features are considered correlated if they have a correlation value above thresholdğœŒ âˆˆ [0,1]. We only consider reliable models that have a predictive score (F1-score or R-score) above threshold ğœ âˆˆ [0,1](Line 10). This hyper-parameter has a major role in the overall performance of coherency-preservation and determines the magnitude of correlation constraints. At the end (Line 11), if there exist a reliable model for a featureğ‘—,ğ‘— âˆˆ {1..ğ‘š}, the correlation modelMwill include a triple{ğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ğ‘ , ğ‘šğ‘œğ‘‘ğ‘’ğ‘™, ğ‘ ğ‘ğ‘œğ‘Ÿğ‘’}containing the inputs of the model, trained model, and score of the model for feature ğ‘—. Algorithm 2 Coherency Objective (ğ‘‚) Input: input ğ‘¥, counterfactual ğ‘¥, correlation models M Output: coherency cost ğœ‰ In Algorithm 2, the created models M are used to establish the coherency among the counterfactualâ€™s features. First we identify the changed features in the counterfactualğ‘¥, denoted byğ¿. For a featureğ‘— âˆˆ ğ¿, if correlation modelMexists, it is used to predict the value of the feature, creating a temporary counterfactual ğ‘¥(i.e., a copy ofğ‘¥with the predicted value for featureğ‘—). The modelMmakes prediction based on the values of the correlated featuresğ‘–ğ‘›ğ‘ğ‘¢ğ‘¡ğ‘ (Line 7). Therefore, it takes into account complex relationships between multiple features. We measure the preserved coherency by calculating the distance between the counterfactual ğ‘¥and the temporary counterfactualğ‘¥(Line 8). The intuition is that if the values of correlated features conform with each other, the prediction of the model for featureğ‘—will be close to the counterfactualâ€™s value for the feature, i.e.,ğ‘¥[ğ‘—] â‰ƒ ğ‘¥[ğ‘—], leading to a low ğ‘‘ğ‘–ğ‘ ğ‘¡ğ‘ğ‘›ğ‘ğ‘’betweenğ‘¥andğ‘¥. Although we î€›ltered out unreliable models in Algorithm 1, here, we weigh the distance according to the score of the correlation model to have a truthful measurement of the coherency cost (Line 9). This procedure is repeated for every feature, and the output ofğ‘‚is the total coherency costğœ‰ that is minimized during the optimization process. Although previous modules provide a sound and coherent counterfactual explanation, it may not be necessarily actionable from the userâ€™s perspective (for example, recommending a change torace orgender). The objective of ACTIONABILITY module (Figure 1) is to allow users to deî€›ne preferences over features to guarantee the actionability of a recourse. We propose a constraint language (outlined in Table 2) to provide the user with a î€exible set of constraints for features. The language provides diverse operators for numerical and categorical features. We also present the Categorical{ğ‘£, .., ğ‘£} a set of categorical values notion of constraint importance to weigh the constraints according to their importance for the user. For example, two constraints "î€›x the race" and "constrain balance between[5000$,10000$]" obviously have diî€erent importances, as exceeding the balance range may be acceptable, but changing race is not tolerable. Without considering constraint importance, the optimization algorithm makes no diî€erence between a set of constraints. By weighing constraints, we can prioritize them, and therefore, the optimization algorithm avoids to overstep the highly important ones. Thus, we deî€›ne the userâ€™s preferencePas a set of constraint triples in the form ofC= (ğ‘“ ğ‘’ğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’, ğ‘ğ‘œğ‘›ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›ğ‘¡, ğ‘–ğ‘šğ‘ğ‘œğ‘Ÿğ‘¡ğ‘ğ‘›ğ‘ğ‘’)for a desired featureğ‘—,ğ‘— âˆˆ {1..ğ‘š}, whereğ‘šis the total number of features, i.e., P = {C, C, .., C}, ğ‘¥, ğ‘¦, ğ‘§ âˆˆ {1..ğ‘š} âˆ§ğ‘¥ â‰  ğ‘¦ â‰  ğ‘§. An example preference can beË†P = {(ğ‘ğ‘”ğ‘’, ğ‘”ğ‘’,4), (ğ‘Ÿğ‘ğ‘ğ‘’, ğ‘“ ğ‘–ğ‘¥,10)}. Algorithm 3 outlines our proposed objective functionğ‘‚which computes the actionability costğœ‚for a particular counterfactualğ‘¥according to the userâ€™s preferenceP. For every constraint tripleâˆ€C âˆˆ P, Algorithm 3 checks the satisî€›ability ofC; ifCis not satisî€›ed, thenCis added to the actionability costğœ‚. The output ofğ‘‚is the total incurred actionability cost ğœ‚ that is minimized during the optimization process. Algorithm 3 Actionability Objective (ğ‘‚) Input: input ğ‘¥, counterfactual ğ‘¥, userâ€™s preference P Output: actionability cost ğœ‚ In this section, we adopt Non-dominated Sorting Genetic Algorithm III (NSGA-III) [5] to solve our multi-objective optimization problem. Compared to other evolutionary algorithms, NSGA-III performs well with diî€erently scaled objective values and generates diverse solutions. The î€›rst property is essential for a multi-objective counterfactual explanation method where there exists a combination of î€›tness and cost functions with diî€erent ranges of output and Table 3: Summary of information of the used data sets. conî€ict goals. The second property is useful in the sense of actionability, as providing a diverse set of solutions increases the chance of obtaining a recourse complying with the userâ€™s circumstance. The objective set ğ‘‚ğ‘ ğ‘— deî€›nes the CAREâ€™s hierarchy: ğ‘‚ğ‘ ğ‘— ={â†“ ğ‘‚, â†“ ğ‘‚, â†“ ğ‘‚}, Every set inğ‘‚ğ‘ ğ‘—corresponds to a module annotated by a subscript number, i.e.,1,2,3, and4(also shown in Figure 1). Arrows indicate the type of objectives, either cost or î€›tness function, which should be minimized or maximized, respectively. As we mentioned earlier, the î€›rst set (VALIDITY module) is always present, while other sets (modules) can be arbitrarily included. For example, we may create the conî€›guration{1, 2}which only contains the VALIDITY and SOUNDNESS modules. This modular structure allows our method to be used for both counterfactual explanation (conî€›guration{1, 2}) and actionable recourse generation (conî€›guration{1, 2, 3, 4}). Since the deî€›ned objective functions make no assumption about the nature and internal structure of the prediction modelğ‘“, CARE is applicable on any ML model created for tabular classiî€›cation and regression tasks. Moreover, the NSGA-III algorithm can automatically handle mixed-feature data dispensable of auxiliary operations (such as one-hot encoding and imposing hard-constraints), making CARE suitable for mixed-feature tabular data sets. In this section, î€›rst, we describe the evaluation setup including data sets, models, and hyper-parameters. Second, we validate the importance of the stated desiderata and the eî€ectiveness of CAREâ€™s modules in handling them using multiple experiments. Finally, we demonstrate the overall performance of CARE by benchmarking against two state-of-the-art explanation methods. We mainly used standard data sets from the UCI Machine Learning Repository [6], except the COMPAS data set that can be found at [19]. Summary of information of the used data sets is reported in Table 3. The numerical features were standardized by removing the mean and scaled to unit variance. We converted original categorical features to ordinal encoding and used their corresponding one-hot encoding for creating black-box models. We split the data sets into 80% train set and 20% test set. The train set was used for creating black-box and the explainer models while Table 4: Performance of the black-box models. the test set was used for generating explanations. We created a Multilayer Perceptron Neural Networks (NN) consisted of two hidden layers each with 50 neurons and a Gradient Boosting Machines (GB) comprised of 100 estimators as black-box classiî€›ers and regressors. Table 4 reports the performance of created models on test set in terms of F1-score (classiî€›cation task) and R-score (regression task). We used 4-quantiles as cutting points for intervals in the regression task and generated counterfactuals from the neighbor interval for every input. In addition, for the classiî€›cation task, we set the probability threshold asğ‘ =0.5 and generated counterfactuals from the opposite class. We used CART decision trees [20] for categorical features and Ridge regression models [16] for numerical features as correlation models (refer to Algorithm 1). We set the minimum correlation threshold asğœŒ =0.1. To adjust the threshold of the modelâ€™s score (i.e.,ğœ) with respect to each data set, we setğœas the median value of all modelsâ€™ scores and î€›ltered out the correlation models that have a score belowğœ. Accordingly, we achievedğœ =0.72, ğœ =0.59,ğœ =0.84, andğœ =0.64 for Adult Income, COMPAS, Default of Credit Card Clients, and Boston House Prices data sets, respectively. For NSGA-III optimization algorithm, we used the two-point crossover operator with percentageğ‘ğ‘ =60% and the polynomial mutation operator with percentageğ‘ğ‘š =30%. The number of generations was set toğ‘›=10. Although it is possible to deî€›ne arbitrary population sizeğ‘›, we determined it adaptively with respect to the number of objectives using a standard formula described in the original paper of NSGA-III [5]. This approach is more suitable for our modular framework in which the number of objectives would vary with respect to diî€erent conî€›gurations. Regarding user preferences for actionable recourse, we only set global constraints according to our basic knowledge about the domains for all inputs. For example, constraint î€›x (î€›x the current value) was set for features likeraceandsexwhile constraint ge (greater than or equal to the current value) was set for featureage. Thus, actionable recourses are not locally personalized with respect to every speciî€›c input. We set equal importance values for all constraints, i.e.,ğ‘–ğ‘šğ‘ğ‘œğ‘Ÿğ‘¡ğ‘ğ‘›ğ‘ğ‘’ =1.0, to have a fair comparison regarding the actionability objective function (i.e.,ğ‘‚) among baseline methods. CARE has been developed using Python programming language, and experiments were run on a system with Intel Core i78650U processor and 32GB of memory. A complete implementation of the method, including validation and benchmark experiments, is available at: https://github.com/peymanrasouli/CARE. We visualize the impact of SOUNDNESS module in generating sound and realistic counterfactuals on Iris and Moon data sets [6]. We created Gradient Boosting (GB) classiî€›ers on these data sets and generatedcfandcfcounterfactuals using conî€›gurations{1}and {1, 2}, respectively. Figure 2(a) demonstrates counterfactuals from the furthest class for a sample from Iris data set, and Figure 2(b) depicts counterfactuals from the opposite class for an instance from Moon data set. The counterfcatuals are annotated with the values ofğ‘‚andğ‘‚denoted byğ‘andğ‘, respectively. By observing the location of the generated counterfactuals in Figures 2(a) and 2(b), the importance of SOUNDNESS module is revealed. It can be seen that minimum distance is the only important criterion forcfcounterfactuals whilecfcounterfactuals are associated to the same-class training data (ğ‘ =1 andğ‘ =1), therefore they are connected to the previous knowledge and achieve a high prediction probability from the classiî€›er. We stated that proximity is diî€erent than connecte dness and they are complementary criteria for a sound counterfactual explanation. Figure 2(a) demonstrates this distinction clearly ascfis located at the proximity of the training data (ğ‘ =1), but it is not connected to a high density region (ğ‘ = 0) which declines its interpretability and actionability. Figure 3: Evaluation of coherency-preservation. To validate the impact of COHERENCY module, we have conducted an experiment using Adult data set [6]. In this data set, categorical featureseducationandeducation-numare fully correlated to each other, as there is a one-to-one correspondence between their values. Moreover, there is a strong relationship among features marital-status,relationship, andsex. We refer to the former set of correlated features as "Education" and the latter as "Relationship." A coherent counterfactual explanation should change all features in a set accordingly; otherwise, it creates an inconsistent feature state. We deî€›ne the coherency rate as the normalized number of counterfactuals in which the consistency between features in every correlation set ("Education" and "Relationship") is preserved. Figure 3 depicts the results of the coherency validation experiment. We created anNNmodel using the train set and generated a set of ten counterfactual explanations for 500 samples form the test set using diî€erent conî€›gurations of CARE. According to Figure 3, a sound explanation method (CARE) considerably resolves the inconsistency existing in valid explanations (CARE), however, it does not guarantee generating coherent explanations for all inputs. On the contrary, conî€›gurations equipped with the COHERENCY module (i.e., CAREand CARE) substantially preserve the correlation among features, leading to maximum coherency rate. The eî€œciency of our algorithm in handling complex relationships will be validated using a user study in future work. Modules in the CAREâ€™s hierarchy are rigorously designed to handle important desiderata for the counterfactual generation. Earlier, we demonstrated the role of CAREâ€™s modules in approaching diî€erent properties via an illustrative example (Table 1). In this section, we quantitatively evaluate the impact of each module by conducting the following experiment. We consider four conî€›gurations{1}, {1, 2},{1, 2, 3}, and{1, 2, 3, 4}. We are interested to know the behavior of CARE when some modules are absent. For example, how much a counterfactual generated by conî€›guration {1} will satisfy objectives deî€›ned in conî€›guration{1, 2}. Using this information we can determine the importance of the devised modules. To this end, we createdGBmodels using the train set of Adult Income and Boston House Prices data sets [6] and generated counterfactual explanations using the speciî€›ed module combinations for 500 and 100 samples of their test set, respectively. Eventually, their results regarding the objectives deî€›ned in the last conî€›guration (i.e., {1, 2, 3, 4}), which contains all CAREâ€™s modules, were measured. By observing the results demonstrated in Table 5, we can conclude several points about the performance of diî€erent modules: â€¢CARE generates valid counterfactuals for all conî€›gurations and data sets since validity is the basis of our methodology. These counterfactuals are best at fulî€›lling theğ‘‚and ğ‘‚objectives because closeness and sparsity are the only essential goals in this setting. Moreover, their cost for theğ‘‚objective is usually low since changing immutable features leads to a counterfactual with a dramatic distance to the original input, therefore, the optimization algorithm usually avoids manipulating such features. ğ‘‚andğ‘‚objectives in conî€›gurations {1, 2},{1, 2, 3}, and{1, 2, 3, 4}. Meanwhile, since sound counterfactuals originate from high-density regions, theirğ‘‚ and ğ‘‚costs are expectedly increased. â€¢The devised COHERENCY module has eî€œciently preserved the relationship among counterfactual features, resulting in ğ‘‚â‰ˆ0 for conî€›gurations equipped with the module (i.e.,{1, 2, 3}and{1, 2, 3, 4}). This shows the generalization of our method for high dimensional data sets where there exist complex correlations between features. â€¢By comparing the results of two conî€›gurations{1, 2, 3}and {1, 2, 3, 4}we realize that given a sound and coherent counterfactual, generating an actionable recourse does not negatively inî€uence the other objectives. CARE creates actionable recourse based on sound and coherent counterfactuals. Though, for some inputs and their corresponding constraints, it is impossible to î€›nd an actionable recourse that fulî€›lls the objectives in the precedent modules (modules2and3), causing ğ‘‚cost. To evaluate the performance of CARE, we compared it with state-ofthe-art explanation methods CFPrototype [23] and DiCE [17]. We chose CFPrototype because it uses a loss function called prototype loss to generate interpretable counterfactuals located in the proximity of the same-class training data. We selected DiCE due to its diverse counterfactual generation property and its ability to impose actionability constraints on the input features. To balance between diversity and proximity of the generated counterfactuals in DiCE, we set the corresponding hyper-parameters asğœ†=1.0 andğœ†=1.0. We used CFPrototype with the default hyper-parameters stated in the paper. The methods were applied on three classiî€›cation data sets Adult Income, COMPAS [19], and Default of Credit Card Clients (Default of CCC for short) [6]. We created anNNblack-box model for every data set using their train set and explained 500 samples from their test set. The number of generated counterfactuals in CARE and DiCE was set to ğ‘ = 10. 3.5.1 Performance Evaluation. Table 6 reports the evaluation of counterfactuals with respect to the CAREâ€™s objective functions. CAREâ€™s counterfactuals originate from the distribution of the sameclass training data and are connected to the existing knowledge by a continuous path; therefore, it achieved the highest values for ğ‘‚andğ‘‚î€›tness functions. Moreover, the coherency in CAREâ€™s explanations is fully respected in both data sets (zero cost forğ‘‚). It is noteworthy that the amount of coherency-preservation is directly related to the number of correlation models that we had generated; the more reliable correlation models exist, the more complex correlations are preserved. As we explained in Section 3.4, we can eî€ortlessly generate an actionable recourse if we only consider its distance to the original input. But, CARE is designed to follow the order of the modules in its hierarchy and prioritize sound and coherent explanations. For some inputs and their corresponding constraints, it is impossible to meet the two mentioned properties, resulting in non-actionable recourses (nonzero cost forğ‘‚). Last but not least, CARE generates valid counterfactuals for every input (zero cost for ğ‘‚). 3.5.2 Coherency Evaluation. To justify the eî€ectiveness of the COHERENCY module and to demonstrate the behavior of baseline methods regarding coherency-preservation, we benchmarked CARE against CFPrototype and DiCE on the experiment deî€›ned in Section 3.3. Figure 4(a) illustrates the coherency rate of explanations with respect to the correlation sets "Education" and "Relationship." It can be seen that CARE outperforms baseline methods by generating coherent counterfactual explanations for every input. This experiment highlights that the statistically soundness objectives (like proximity which is considered by CFPrototype and DiCE methods) do not necessarily guarantee to generate coherent explanations. 3.5.3 Diversity Evaluation. We benchmarked CARE versus DiCE regarding the diversity of the generated counterfactuals. A diverse set of counterfactual explanations for a particular input provides the user with more alternatives for obtaining the desired outcome. We measure the diversity with respect to feature variation as follows: whereğ¶is combination operator,ğ‘is the total number of counterfactuals, andSis the the set of explanations. We calculate the Jaccard index between the feature names in every pair of explanations for calculatingğ‘‘. In our opinion, this is a representative metric for diversity because we are interested in a set of counterfactuals that provides various options for a user, not only changes in one or a few speciî€›c features. For some inputs, a few important features determine the decision, which in this case, recommending diî€erent values for the features is acceptable. We propose a fair metric for value-based diversity as follows: whereIis a binary indicator function. Instead of computing the diî€erence between feature values for every pair of counterfactuals inS, which is a biased measurement due to the diî€erent number of changed features (i.e., sparsity degree) by every baseline method, we measure the diî€erence between the feature values of common features in every pair of explanations. Figure 4(b) illustrates the results of this evaluation. Although we have not formulated diversity as an objective function, the choice of the optimization algorithm (i.e., NSGA-III) has led to promising outcomes. We observe that CARE tends to generate counterfactuals that are diî€erent concerning the feature names (highğ‘‘). It also performs solid when changing multiple features is not applicable (ğ‘‘â‰ˆ 0.6). 3.5.4 Computational Complexity Evaluation. Computational complexity is an essential matter for a counterfactual explanation method. The overall complexity of the NSGA-III algorithm isğ‘‚ (ğ‘€ğ‘)where ğ‘€is the number of objectives, andğ‘is the population size. CARE does not require creating a new model for explaining every instance; once a CAREâ€™s explainer for a black-box model and its corresponding data set is built, it takesğ‘‚ (ğ‘€ğ‘)to explain every input. We have used an adaptive mechanism deî€›ned in the original paper of NSGA-III [5] for setting the population size. It determines the population size based on the number of objective functions. This approach is suitable for our modular framework in which modules are arbitrarily added/removed. Figure 4(c) shows the average time spent by each algorithm for explaining a single instance. Although the used data sets have diî€erent feature dimensionality, CARE generates explanations in a reasonable and similar amount of time compared to the baseline methods. Furthermore, the computational complexity of CARE reduces when fewer modules are employed. In this paper, we presented CARE, a modular explanation framework for generating actionable recourse. We demonstrated that a sound counterfactual instance is located in the neighborhood and continuously linked to the same-class ground-truth data points. An actionable recourse can be constructed based on such counterfactuals while preserving the correlation between changed/unchanged features and satisfying user/domain-speciî€›c constraints. Modules Figure 4: Comparison of explanation methods with respect to coherency, diversity, and computational complexity. in the CAREâ€™s hierarchy are rigorously designed to address the stated desiderata. The low-level modules handle fundamental and statistical properties related to the model and observed data, while the high-level ones manage the coherency between features and user preferences. Through several experiments, we demonstrated the eî€œcacy of our devised approach in creating feasible and realistic explanations and its superior performance compared to the baselines. CAREâ€™s framework can be viewed as a î€exible benchmark for counterfactual and actionable recourse generation techniques sharing similar desiderata. In future work, we will apply our approach to a large-scale, real-world use case and evaluate its performance using a user study.