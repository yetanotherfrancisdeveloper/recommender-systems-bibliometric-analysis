E-commerce platforms usually display a mixed list of ads and organic items in feed. One key problem is to allocate the limited slots in the feed to maximize the overall revenue as well as improve user experience, which requires a good model for user preference. Instead of modeling the inî€uence of individual items on user behaviors, thearrangement signalmodels the inî€uence of the arrangement of items and may lead to a better allocation strategy. However, most of previous strategies fail to model such a signal and therefore result in suboptimal performance. In addition, the percentage of ads exposed (PAE) is an important indicator in ads allocation. Excessive PAE hurts user experience while too low PAE reduces platform revenue. Therefore, how to constrain the PAE within a certain range while keeping personalized recommendation under the PAE constraint is a challenge. In this paper, we propose Cross Deep Q Network (Cross DQN) to extract the crucial arrangement signal by crossing the embeddings of diî€erent items and modeling the crossed sequence by multichannel attention. Besides, we propose an auxiliary loss for batchlevel constraint on PAE to tackle the above-mentioned challenge. Our model results in higher revenue and better user experience than state-of-the-art baselines in oî€Ÿine experiments. Moreover, our model demonstrates a signiî€›cant improvement in the online A/B test and has been fully deployed on Meituan feed to serve more than 300 millions of customers. â€¢ Information systems â†’Computational advertising ; Online advertising; Ads allocation. Ads Allocation, Deep Reinforcement Learning, Arrangement Signal, Adaptive Ads Exposure ACM Reference Format: Guogang Liao, Ze Wang, Xiaoxu Wu, Xiaowen Shi, Chuheng Zhang, Yongkang Wang, Xingxing Wang, Dong Wang. 2022. Cross DQN: Cross Deep Q Network for Ads Allocation in Feed. In Proceedings of the ACM Web Conference 2022 (WWW â€™22), April 25â€“29, 2022, Virtual Event, Lyon, France. ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/3485447.3512109 Feed, mixed with organic items and ads, is a popular product on many e-commerce platforms nowadays [7]. Platforms serve users and gain revenue via feed. In general, there are two ways for platforms to get revenue. Firstly, once users consume organic items or ads, the e-commerce platform will gain the platform service fee (hereinafter referred to as fee) according to the orders. Secondly, as an ad is clicked by a user, the platform will charge the corresponding advertiser. For the sake of the platform, displaying more ads is beneî€›cial to ads revenue but harmful to fee since ads are less likely engaging than organic items [23]. Usually, the number of ads is limited in feed to ensure good user experience and engagement. Hence, how to allocate limited slots reasonably and eî€ectively to maximize overall revenue has become a very meaningful and challenging problem [11, 16, 22]. The structure of an industrial ads allocation system is shown in Figure 1. Blending Server takes ads sequence and organic items sequence as input and outputs a mixed sequence of the two. For Blending Server, there are two common strategies: î€›xed slots strategy and dynamic slots strategy. Most platforms simply allocate ads to pre-determined slots [10,13]. Such strategies may lead to suboptimal overall performance. Dynamic slots strategy adjusts the number and slots of ads according to the interest of users. For instance, if a user has a higher tendency to consume commercial ads, the platform will allocate more ads at conspicuous slots to maximize possible beneî€›ts. Except for personalization, dynamic slots strategies have lower ads blindness [21] and better adaptability, signiî€›cantly outperforming î€›xed slots strategy and gradually becoming todayâ€™s trend. Early dynamic slots strategies use some Figure 1: Structure of an ads allocation system. The process of ads allocation takes place in the Blending Server. Figure 2: While inserting ğ‘ğ‘‘into feed, the CTR of organic items increases while the CTR of ğ‘ğ‘‘decreases. classic algorithms (e.g., Bellman-Ford, uniî€›ed rank score) to allocate ads slots. Since the feed is presented to the user in a sequence, recent dynamic ads allocation strategies usually model the problem as Markov Decision Process [14] and solve it using reinforcement learning (RL) [4, 22, 25]. However, existing RL-based dynamic slots strategies encounter several major limitations: i) Most approaches ignore the crucial arrangement signalwhich is the inî€uence of the arrangement of displayed items on user behaviors. For example, as illustrated in Figure 2, once an ad is inserted into feed, the click-through rate (CTR) of surrounding organic items and ads î€uctuate. This signal receives attention in the scenario of Re-Rank recently [2,5,6,19] but is largely neglected in ads allocation. ii) Most of existing methods lack an eî€œcient balance between the personalization of diî€erent requests and the constraint on the percentage of ads exposed (PAE) in a period. PAE is the most important constraint in ads allocation, which balances the user experience and platform revenue. Previous methods constrain all the requests or the requests within the same hour [17] with the same target PAE, resulting in a lack of personalization and diî€erentiation in the allocation of ads between diî€erent requests. To address the limitations of existing methods, we present a novel framework called Cross Deep Q Network (Cross DQN) based on deep reinforcement learning. Speciî€›cally, we design two novel units called State and Action Crossing Unit (SACU) and Multi-Channel Attention Unit (MCAU) to explicitly extract the arrangement signal. Besides, we propose an auxiliary loss for batch-level constraint to balance the personalization of diî€erent requests and the constraint on PAE in a period. The contributions of our work are summarized as follows: â€¢ A superior ads allocation strategy. In this paper, we propose a novel RL-based framework named Cross DQNto dynamically adjust the number and the slots of ads in feed, which can eî€ectively extract the arrangement signal and reasonably balance personalization of diî€erent requests and the constraint on PAE. â€¢ Detailed industrial and practical experience. We successfully deploy Cross DQN on the Meituan feed and obtain signiî€›cant improvements in both platform revenue and user experience. Traditional strategy for ads allocation in feed is to display ads at î€›xed slots. Recently, dynamic ads allocation strategies gains growing attention. According to whether RL is used, existing dynamic ads allocation strategies can be roughly categorized into two categories: non RL-based and RL-based. Non RL-based methods usually use classical algorithms to allocate ads slots. Koutsopoulos[9]deî€›ne ads allocation as a shortestpath problem on a weighted directed acyclic graph where nodes represent ads or slots and edges represent expected revenue. The shortest path can be found by running Bellman-Ford algorithm. Furthermore, Yan et al. [21]takes the impact of the interval between ads into consideration and re-ranks ads and organic items jointly via a uniform ranking formula. RL-based methods model the ads allocation problem as an MDP and solved it with diî€erent RL techniques. Zhao et al. [25]proposes a two-level RL framework to jointly optimize the recommending and advertising strategies. Zhao et al. [24]proposes a DQN architecture to determine the optimal ads and ads position jointly. Xie et al. [20]proposes a hierarchical RL-based framework to î€›rst decide the channel and then determine the speciî€›c item for each slot. In contrast to the previous work, we incorporate the arrangement signal into a RL-based dynamic ads allocation model to improve the performance. In our scenario, we presentğ¾slots in one screen and handle the allocation for each screen in the feed of a request sequentially. The ads allocation problem is formulated as a Constrained Markov Decision Process (CMDP) [1] (S,A,ğ‘Ÿ,ğ‘ƒ,ğ›¾,C), the elements of which are deî€›ned as follows: â€¢ State space S. A stateğ‘  âˆˆ Sconsists of the information of candidate items (i.e., the ads sequence and the organic items sequence which are available on current stepğ‘¡), the user (e.g., age, gender and historical behaviors), and the context (e.g., order time). â€¢ Action space A. An actionğ‘ âˆˆ Ais the decision whether to display an ad on each slot on the current screen, which is formulated as follows: whereğ‘¥=1 display an ad in the ğ‘˜-th slot0 otherwise,âˆ€ğ‘˜ âˆˆ [ğ¾]. In our scenario, we do not change the order of ads sequence and organic items sequence in Blending Server. â€¢ Reward ğ‘Ÿ. After the system takes an action in one state, a user browses the mixed list and gives a feedback. The reward is calculated based on the feedback and consists of ads revenue ğ‘Ÿ, fee ğ‘Ÿand user experience ğ‘Ÿ: â€¢ Transition probability ğ‘ƒ.ğ‘ƒ (ğ‘ |ğ‘ , ğ‘)is deî€›ned as the state transition probability fromğ‘ toğ‘ after taking the actionğ‘, whereğ‘¡is the index for the screen/time step. The action taken in the state aî€ects user behaviors. When the user pulls down, the stateğ‘ transits to the state of next screenğ‘ . The items selected to present byğ‘will be removed from the state on the next stepğ‘ . If the user no longer pulls down, the transition terminates. â€¢ Discount factor ğ›¾. The discount factorğ›¾ âˆˆ [0,1]balances the short-term and long-term rewards. â€¢ Constraint C. The platform-level constraint is that the absolute diî€erence between the total PAE in a period and the the target valueğ›¿should be less than a thresholdğœ€to ensure stable ads revenue. The PAE is formulated as follows: whereğ‘is the number of requests in a period,Numand Nummean the number of ads and organic items in theğ‘–-th request. In this work, we choose one week as the period. Consequently, the platform-level constraint can be formulated as follows: Given the CMDP formulated as above, the objective is to î€›nd an ads allocation policyğœ‹:S â†’ Ato maximize the total reward under the platform-level constraint. The structure of popular RL model, Dueling DQN, [18] is shown in Figure 3(a), which receives the state only as the input. Such a structure fails to extract cross information between the action and the state, making it diî€œcult to model the arrangement signal of mixed lists. A common solution is to concatenate state and action directly, but the model remains hard to extract the information between the items in the mixed list designated by the action. To this end, we propose a novel structure Cross DQN (cf. Figure 3(b)). The State and Action Crossing Unit (SACU) is designed to cross the embeddings of the items in a mixed list designated by the action. The Figure 3: Architectures of Dueling DQN and Cross DQN. Multi-Channel Attention Unit (MCAU) is designed to eî€ectively extract arrangement signal from diî€erent channel combinations. Speciî€›cally, we show the detailed structure of Cross DQN in Figure 4. The model takes a state (including the organic items/ads sequence, context information, etc.) and the corresponding candidate actions as the input. Then, Item Representation Module (IRM) generates the representations (especially the representations of ads and organic items). Next, Sequential Decision Module (SDM) generates Q-values of diî€erent actions with the help of SACU, MCAU and an auxiliary loss for batch-level constraint. In SACU, the state embeddings are intersected according to the action to form a uniî€›ed matrix representation. In MCAU, the crossing matrix generated from SACU are split into diî€erent channels to calculate the multichannel attention weight. Finally, SDM chooses the action with the largest Q-value. We will introduce them in detail in the following subsections. Item Representation Module (IRM) generates the state embedding from the raw state. To eî€œciently process the information from different sources, IRM generates two sequences of mixed embeddings: one for ads and one for organic items. The embedding for each item encodes not only the information of the item itself but also the information of the user proî€›le, the context, and the interaction with historical user behaviors. First, we use embedding layers to extract the embeddings from raw inputs. We denote the embeddings for ads, organic items, historical behaviors of the user, the user proî€›le, the context as{e}, {e},{e},e, anderespectively, where the subscriptğ‘–denotes the index within the sequence andğ‘,ğ‘, andğ‘are the number of ads, organic items, and historical behaviors. Then, we use a target attention unit [15] to encodes the interaction between the historical behaviors of the user and the corresponding item, which is similar to Zhou et al. [26]: Afterwards, we append the embeddings of the user proî€›le and the context to the embedding of each item: where||denotes concatenation. Notice that there are some strong features for ads and organic items in our scenario (e.g., discount, delivery fee, delivery time), which are concatenated with the embedding of each item and input into SDM. For ease of notation, we can also write the embeddings for ads and organic items in matrix form, each row of which represents one item in the sequence, i.e., Figure 4: Network architecture of Cross DQN. Item Representation Mo dule (IRM) generates the state embedding based on the raw state. Sequential De cision Module (SDM) generates Q-values of diî€erent actions with the help of State and Action Crossing Unit (SACU), Multi-Channel Attention Unit (MCAU) and auxiliary loss for batch-level constraint. Involving several attention units, IRM may be time-consuming upon deployment. However, IRM is an independent module within Cross DQN so that we can invoke IRM in parallel to other modules preceding Cross DQN. See more details in Section 4.8. To evaluate the Q-value of a certain state-action pair, we need an eî€œcient representation of the mixed list designated by the corresponding action. State and Action Crossing Unit (SACU) helps us to construct a sequence of embeddings corresponding to the mixed list from the state embedding. First, given an action, we generate the corresponding action oî€set matrices for ads and organic items:Mâˆˆ {0,1}and Mâˆˆ {0,1}, where the(ğ‘–, ğ‘—)-th element represents whether theğ‘—-th ad/organic item is presented on theğ‘–-th slot. Recall thatğ¾ is the number of slots in one screen. For example, given the action ğ‘ = (0, 1, 0, 0, 1) with ğ¾ = 5, the action oî€set matrix Mis Then, we can calculate the cross matrixM, which is the embedding of the mixed list corresponding to the given action, using the action oî€set matrices. With SACU, we generate the embedding for the mixed list which enables us to eî€œciently extract the arrangement signal in the next module. The user may focus on one or more aspects (e.g., discount, delivery fee, delivery time) of the mixed sequence at the same time. Accordingly, we propose MCAU to simultaneously model the userâ€™s attention to diî€erent aspects of the mixed sequence. The cross matrixMâˆˆ Rgenerated by SACU contains ğ‘diî€erent channels. Each channel represents an information dimension in the latent space and can be used to model one aspect of the mixed sequence. Meanwhile, the user may pay attention to more than one aspect of the mixed sequence at the same time. So the sequence information of two or more channels need to be combined for modeling. Next we will detail that how the sequence information of multiple channels is combined and modeled. Forğ‘channels, we formulate the number of channel combinations as ğ‘, which is calculated as follows: We formalize the mask matrix for theğ‘–-th combination asM. For example, the mask matrix for combination of the î€›rst channel and the last channel is Nextly, signal matrix calculated by cross matrix and mask matrix are input into corresponding self-attention network [15] to model the attention across theğ¾items and generate a latent vector, as follows: Latent vectors output by diî€erent self-attention network are concatenated together to represent the arrangement signal extracted from diî€erent channels, as follows: With the help of SACU and MCAU, Sequential Decision Module (SDM) takes the embeddings generated by IRM and candidate actions as the input, and outputs Q-values corresponding to diî€erent actions. Given a set ofğ‘candidate actions{ğ‘}, SACU generates a cross matrixMfor each action and MCAU generates corresponding arrangement signal representation for each action. Subsequently, the outputs of the V network and the A network [18] can be calculated as follows: where pool indicates average pooling over diî€erent rows (i.e., different items in the ads/organic items sequence). Finally, SDM outputs the Q-valueğ‘„ (ğ‘ , ğ‘)corresponding to the ğ‘–-th candidate action on the current screen as follows: Recall that our objective is to maximize cumulative reward under the constraint on the average ads exposure. The key for a successful strategy is to satisfy the constraint while maintaining a diî€erentiated recommendation for diî€erent users/scenarios. For example, if the user is prone to be annoyed by ads, we should expose less ads to the user, and vice versa. Diî€erent auxiliary losses to constrain the percentage of ads exposure (PAE) can result in diî€erent level of differentiation. A common solution is to use a request-level constraint, i.e., constraining the PAE of each request to be close to the PAE targetğ›¿. Such a solution may result in poor diî€erentiation since the PAE of each request is constrained to the same targetğ›¿regardless of the context. To allow for diî€erentiation, Wang et al. [17]propose to use an hour-level constraint that allows for using diî€erent PAE targets in diî€erent hours. However, the level of diî€erentiation is still limited within an hour. To this end, we propose a batch-level constraint to constrain the average PAE of the requests in a batch instead of constraining the PAE of each request. We denote the PAE associated with the actionğ‘asPAE(ğ‘). For example, the PAE ofğ‘ = (0,1,0,0,1)is 0.4. Given a batch of transitions ğµ, our batch-level constraint can be written as: ğ¿(ğµ) =ğ›¿ âˆ’1|ğµ|PAE(arg maxğ‘„ (ğ‘ , ğ‘)) However, the argmax function is not diî€erentiable. Therefore, we use a soft version of argmax instead, i.e., we use PAE(arg maxğ‘„ (ğ‘ , ğ‘)) â‰ˆ1ğ‘expî€‚ğ›½ğ‘„ (ğ‘ , ğ‘)î€ƒPAE(ğ‘ whereğ‘ =Ãexp[ğ›½ğ‘„ (ğ‘ , ğ‘)]is the normalization factor andğ›½ is the temperature coeî€œcient. Unlike previous request-level or hour-level constraints that limit the PAE for each request, we only limit the average PAE estimated using randomly sampled batches. Such a weaker form of constraint encourages the model to choose the action with a PAE that is deviated from ğ›¿ but may better adapt for the current context. We show the process of oî€Ÿine training in Algorithm 1. We train Cross DQN based on an oî€Ÿine datasetğ·generated by an online exploratory policyğœ‹. For each iteration, we sample a batch of transitionsğµfrom the oî€Ÿine dataset and update the model using gradient back-propagation w.r.t. the loss: whereğ¿is the same loss function as the loss in DQN [12],ğ¿ is the auxiliary loss for the constraint, andğ›¼is the coeî€œcient to balance the two losses. Speciî€›cally, ğ¿(ğµ) =1|ğµ|ğ‘Ÿ + ğ›¾ maxğ‘„ (ğ‘ , ğ‘) âˆ’ ğ‘„ (ğ‘ , ğ‘). (21) Oî€Ÿine datağ· = {(ğ‘ , ğ‘, ğ‘Ÿ, ğ‘ )}(generated by an online exploratory policy ğœ‹) We show the process of online serving in Algorithm 2. In the online serving system, Cross DQN selects the action with the highest reward based on current state and converts the action to ads slots set for the output. When the user pulls down, the model receives the state for the next screen, and then makes a decision based on the information on the next screen. In our scenario, current state will transit to the next state or terminate depending on whether the user pulls down. However, the next possible state corresponding to a given action is deterministic if the interaction does not terminate. Based on this observation, we can cache the decisions for multiple screens and transmit to Figure 5: Model de composition for online service. the client at once to reduce the time cost for the communication between the server and the client. Model Decomposition. Cross DQN will be calledğ‘‡times for one cache, which is time consuming for industrial scenarios where latency is a major concern. Fortunately, the outputs of IRM can be reused across diî€erent calls of Cross DQN in one cache, which saves up to about 80% computation time. Since the generation of item representations does not rely on the previous modules (such as ranking and ads bidding), we calculate the representation of the items parallel to the previous modules, which further reduces the latency. As shown in Figure 5, the Cross DQN is decomposed into IRM and SDM for deployment. The two parts are trained end-to-end but deployed on diî€erent services for real-time prediction. The IRM is calculated parallel to Ad Ranking and Organic Ranking systems. Hence, it is latency-free for real-time inference of SDM. Parameter Sharing. Both of IRM and SDM use parameter sharing (cf. Figure 4) across diî€erent ads/organic items. As for IRM, we can calculate the representations for all recalled items at the same time without ranking information through parameter sharing and parallel computing. Meanwhile, in SDM, we use parameter sharing across diî€erent actions to guarantee the consistency of the reward evaluation of the actions and ensure that the batch-level constraint is eî€ective. In addition, parameter sharing can reduce the scale of parameters, accelerate model training and reduce memory usage. We will evaluate Cross DQN model through oî€Ÿine and online experiments in this section. In oî€Ÿine experiments, we will compare Cross DQN with existing baselines and analyze the role of diî€erent designs in Cross DQN. In online experiments, we will compare Cross DQN with the previous strategy deployed on the Meituan platform using an online A/B test. 5.1.1 Dataset. We collect the dataset by running an exploratory policy on the Meituan platform during March 2021. We present the detailed statistics of the dataset in Table 1. Notice that each request contains several transitions. The features for the ads/organic items include the identity, the category, the comment score, etc. The features for the user proî€›le include the identity, the gender, etc. 5.1.2 Evaluation Metrics. We evaluate the model with revenue indicators and experience indicators. As for revenue indicators, we use ads revenue and service fee in a period to measure platform revenue. Speciî€›cally, the ads revenue is gained from advertisers calculated using Generalized Second Price (GSP) [3] and chargedÃ per click. The total ads revenue is calculated asğ‘…=ğ‘Ÿ. The service fee is charged from merchantsâ€™ orders according to a certainÃ percentage. and the total service fee is calculated asğ‘…=ğ‘Ÿ. In our platform, user experience is measured by whether the user demand (e.g., î€›nding a satisfying product) can be fulî€›lled. As for experience indicators, we use the average conversion rate and average experience score to measure user experience. The conversionÃ rate calculated asğ‘Ÿ=CTR Ã— CVRis the ratio of the number of orders placed by the user to the number of his/her requests. The experience scoreğ‘Ÿdeî€›ned in Section 3 reî€ects the degree of satisfaction of the user demand. 5.1.3 Hyperparameters. We implement Cross DQN with TensorFlow and apply a gird search for the hyperparameters. The hidden layer sizes of the IRM are(128,64,32,8,2)and the hidden layer sizes of the SDM are(16,8,1). The learning rate is 10, the optimizer is Adam [8] and the batch size is 8, 192. In this section, we train Cross DQN with oî€Ÿine data and evaluate the performance using an oî€Ÿine estimator. Through extended engineering, the oî€Ÿine estimator models the user preference and aligns well with the online service. We conduct experiments to answer the following two questions: i) How does Cross DQN perform compared with other baselines? ii) How do diî€erent designs (e.g., SACU, MCAU) and hyperparameter settings (e.g.,ğ›¼,ğ›½) aî€ect the performance of Cross DQN? 5.2.1 Baselines. We compare Cross DQN with the following î€›ve representative methods: â€¢ Fixed. This method displays ads in î€›xed slots, such as the slot indexed by 3, 6, 9, Â· Â· Â· . â€¢ GEA[21]. GEA is a non RL-based dynamic ads slots strategy. It takes the impact of ads intervals into consideration and ranks the ads and organic items jointly with a rank score RS = (CTR Ã— charge + GMV Ã— takerate) exp(ğ›½ğ‘‘), where charge is the fee paid by advertisers, takerate is the take rate (i.e., the fee charged on each transaction by the platform), and ğ‘‘ is the interval between two ads. â€¢ CTLRL[17]. Constrained Two-Level Reinforcement Learning (CTLRL) uses a two-level RL structure to allocate ads. The upper level RL model decomposes the platform-level constraint into hour-level constraints, and the lower level RL model sets the hour-level constraint as the request-level constraint. â€¢ HRL-Rec[20]. HRL-Rec is an RL-based dynamic ads slots strategy. It divides the integrated recommendation into two levels of tasks and solves using hierarchical reinforcement learning. Speciî€›cally, the model î€›rst decides the channel Table 2: The result of Revenue Indicators and Experience Indicators. Each experiment are presented in the form of mean Â± standard deviation. The improvement means the improvements of Cross DQN across the best baselines. (i.e., select an organic item or an ad) and then determine the speciî€›c item for each slot. â€¢ DEAR[24]. DEAR is also an RL-based dynamic ads slots strategy. It designs a deep Q-network architecture to determine three related tasks jointly, i.e., i) whether to insert an ad to the recommendation list, and if yes, ii) the optimal ad and iii) the optimal location to insert. 5.2.2 Performance Comparison. We present the experiment results under the same PAE level in Table 2 and have the following observations: i) Compared with all these baselines, Cross DQN achieves strongly competitive performance on both the revenuerelated metrics and experience-related metrics. Speciî€›cally, Cross DQN improves over the best baseline w.r.t.ğ‘…,ğ‘…,ğ‘…andğ‘… by 3.09%, 2.05%, 0.83% and 1.58% separately. ii) Cross DQN outperforms the î€›xed slots strategy. A reasonable explanation is that the ads positions calculated by Cross DQN are more personalized, which leads to an increase in revenue as well as an improvement of user experience. iii) Cross DQN outperforms GEA, which indicates that an RL-based method may perform better than a rule-based method. iv) Cross DQN also performs better than CTLRL possible due to the fact that the PAE of diî€erent requests of Cross DQN within the same hour are more personalized. v) Compared with the state-of-the-art RL-based methods, i.e., HRL-Rec and DEAR, the superior performance of Cross DQN justiî€›es the explicit modeling of the arrangement signal. 5.2.3 Ablation Study. To verify the impact of diî€erent designs (SACU, MCAU, batch-level constraint), we study three ablated variants of Cross DQN which have diî€erent components in SDM. DQN. Notice that without the help of the auxiliary loss, we can adjust the coeî€œcients in the reward function to realize the same PAE as the PAE of other baselines. â€¢Cross DQN (-aux-mcau) additionally blocks the MCAU and uses one self-attention unit instead on top of the previous ablated version. â€¢Cross DQN (-aux-mcau-sacu) concatenate the embeddings of the action and the state directly without SACU. The results shown in Table 2 reveal the following î€›ndings: i) The performance gap between Cross DQN (-aux-mcau-sacu) and Cross DQN (-aux-mcau) indicates the eî€ectiveness of SACU. By explicitly crossing the embeddings of the states and the actions, SACU can eî€ectively generate cross matrix representation for subsequent extraction of arrangement signal, therefore improving the overall metrics. ii) The MCAU is an additional process after the crossover to strengthen the mutual interaction. The performance of Cross DQN (-aux) is superior to Cross DQN (-aux-mcau), which veriî€›es the eî€ectiveness of extracting arrangement signal of different channel combinations. iii) Cross DQN outperforms Cross DQN (-aux), resulting from the fact that the batch-level constraint brings a certain revenue increase and makes the PAE in a period more stable. 5.2.4 Hyp erparameter Analysis. We analyze the sensitivity of these three hyperparameters:ğœ‚,ğ›¼andğ›½.ğœ‚is the weight for the user experience in the reward function (cf. Eq. (2)).ğ›½is the temperature parameter that controls the degree of the approximation in Eq. (19). ğ›¼is the hyperparameter which balances the main loss and auxiliary loss (cf. Eq (20)). Hyperparameter ğœ‚.The experimental results of diî€erent values ofğœ‚are presented in Figure 6a. Asğœ‚increases,ğ‘…increases butğ‘… decreases. A reasonable explanation is that the system of dynamic ads allocation tends to insert fewer ads whenğœ‚becomes larger, which has a beneî€›cial impact on user experience and fee. Hyperparameter ğ›¼.As shown in Figure 6b, we î€›nd that the auxiliary loss for batch-level constraint has greater inî€uence on return. Whenğ›¼increases, the standard deviation of reward decreases. This phenomenon shows that the PAE and revenue are more stable under batch-level constraint. It is worth noticing that the mean of reward increases whenğ›¼changes from 0 to 1. One possible explanation is that the ads allocation under a certain batch-level constraint of PAE will be more reasonable, which ensures the quality of display results and improves the revenue and user experience. However, if ğ›¼is too large, it will deviate from the learning goal, resulting in a decline in reward. Hyperparameter ğ›½.The right curve in Figure 6 reveals that the mean of reward increases and the standard deviation of reward decreases asğ›½increases within a certain range. This phenomenon demonstrates that accurate calculation of PAE results in stable and high reward. On the contrary, the reward may decrease whenğ›½ exceeds a certain threshold, suggesting the necessity to carefully tune this parameter in practice. We compare Cross DQN with î€›xed ads positions and both strategies are deployed on the Meituan platform through online A/B test. We keep total PAE the same for all methods for a fair comparison. As a result, we î€›nd thatğ‘…,ğ‘…andğ‘…increase by 12.9%, 10.2% and 9.1%, which demonstrates that our Cross DQN not only signiî€›cantly increases the platform revenue, but also improves user experience. It is worth noting that this increase values are 11.5%, 10.7% and 10.0% in oî€Ÿine experiments. One possible reason for this diî€erence in absolute value is the diî€erences in data distribution. In this paper, we propose Cross DQN to optimize ads allocation in feed. In Cross DQN, we design State and Action Cross Unit and Multi-Channel Attention Unit to explicitly extract the arrangement signal that is the inî€uence of the arrangement of items in mixed list on user behaviors. In addition, we introduce an auxiliary loss for batch-level constraint to achieve the personalization for diî€erent requests as well as the platform-level constraint. Practically, both oî€Ÿine experiments and online A/B test have demonstrated the superior performance and eî€œciency of our solution. In our scenario, user experience is also an important objective for the long-term growth of the platform since the improvement of user experience directly increases the retention rate and enhances the reputation of the platform. In the future, it is beneî€›cial to optimize for more user experience metrics and pay more attention to the modeling of long-term beneî€›ts. In addition, it is worth noting that our method follows the oî€Ÿine reinforcement learning paradigm. Compared with online reinforcement learning, oî€Ÿine reinforcement learning faces additional challenges (such as the distribution shift problem). The impact of these challenges to the ads allocation problem is also a potential research direction in the future.