Temporal set prediction is becoming increasingly important as many companies employ recommender systems in their online businesses, e.g., personalized purchase prediction of shopping baskets. While most previous techniques have focused on leveraging a userâ€™s history, the study of combining it with othersâ€™ histories remains untapped potential. This paper proposes Global-Local Item Embedding (GLOIE) that learns to utilize the temporal properties of sets across whole users as well as within a user by coining the names as global and local information to distinguish the two temporal patterns. GLOIE uses Variational Autoencoder (VAE) and dynamic graph-based model to capture global and local information and then applies attention to integrate resulting item embeddings. Additionally, we propose to use Tweedie output for the decoder of VAE as it can easily model zero-inî€ated and long-tailed distribution, which is more suitable for several real-world data distributions than Gaussian or multinomial counterparts. When evaluated on three public benchmarks, our algorithm consistently outperforms previous state-of-the-art methods in most ranking metrics. CCS Concepts: â€¢ Information systems â†’ Information systems applications; Recommender systems. Additional Key Words and Phrases: Temporal Sets, Set Prediction, Tweedie Distribution, Variational Autoencoder ACM Reference Format: Seungjae Jung, Young-Jin Park, Jisu Jeong, Kyung-Min Kim, Hiun Kim, Minkyu Kim, and Hanock Kwak. 2021. Global-Local Item Embedding for Temporal Set Prediction. In Fifteenth ACM Conference on Recommender Systems (RecSys â€™21), September 27-October 1, 2021, Amsterdam, Netherlands. ACM, New York, NY, USA, 8 pages. https://doi.org/10.1145/3460231.3478844 Many recommendation tasks can be viewed as a problem of predicting the next set given the sequence of sets, e.g., predicting the next basket in online markets and the next playlist in streaming services. Previous works mainly focused on the local information (a given userâ€™s history), applying RNNs [ tendency of a sequence of sets within a user. From the recommender system point of view, temporal set prediction can have a sparsity problem as many users interact with only a small number of items. Throughout the recommender system literature, the sparsity problem has been dealt with low-rank approximation or collaborative î€›ltering [ However, such attempts have been less explored in temporal set prediction literature. In this paper, we propose Global-Local Item Embedding (GLOIE) that integrates global and local information for temporal set prediction. To capture the global information, we utilize the Variational Autoencoders (VAEs) [ which are eî€ective on noisy and sparse data. GLOIE then integrates the local embeddings, that are learned through dynamic graphs [ 21], with global embeddings by using an attention method. In addition, we enhance the performance Fig. 1. Overview of GLOIE. We first make sum of time decayed vectorx(leî€œ part of the figure). VAE maximizes the ELBO of eachx. The weighted sum ofË†xÂ· wandzbecomes final embedding for interacted items. The weightğ›¼is determined by the aî€ention mechanism. For the items that the user never interacted, we just useË†x. of GLOIE by using Tweedie distribution for the likelihood of VAE instead of Gaussian or multinomial distributions. We show that purchase logs follow zero-inî€ated and long-tail distributions similar to the Tweedie distribution. We conduct experiments on three public benchmarks, i.e. Dunnhumby Carbo, TaoBao, and Tags-Math-Sx. Empirical results demonstrate that the proposed method outperforms state-of-the-art methods on most metrics. The main contributions of the paper are summarized as follows: â€¢ We propose GLOIE which diî€erentiate global and local information and integrates them. â€¢We claim that using Tweedie output for VAE decoder is beneî€›cial as it naturally models two properties of data distributions from temporal set prediction problems: zero-inî€ated and long-tailed. â€¢ We achieve state-of-the-art performance on three public benchmarks. 2 PRELIMINARIES 2.1 Problem Definition LetU = {ğ‘¢, ğ‘¢, . . . , ğ‘¢}be set of users andE = {ğ‘’, ğ‘’, . . . , ğ‘’}be set of items. Given a userğ‘¢â€™s sequence of sets S= {ğ‘†|1â‰¤ ğ‘˜ â‰¤ ğ‘‡}, our goal is to predict next setğ‘†. Each setğ‘†can also be represented as a binary vector form v, where each elementv= 1(ğ‘’âˆˆ ğ‘†).1(ğ‘¥)is a indicator function which returns 1 if x is true and 0 otherwise. We will use the set notation ğ‘†and vinterchangeably to denote user ğ‘¢â€™s ğ‘˜-th set. 2.2 Variational Autoencoders Variational Autoencoders (VAEs) [10,13] are a class of deep generative models. VAEs provide latent structures that can nicely explain the observed data (e.g., customerâ€™s purchase history). Formally speaking, VAEs î€›nd the latent variables (z) that maximize the evidence lower bound (ELBO), a surrogate objective function for the maximum likelihood estimation of the given observation x: where ğœƒ and ğœ™ are model parameters of a decoder and an encoder, respectively. Encoders are commonly structured by multilayer perceptrons (MLPs) that produce Gaussian distributions over the latent variablez. On the other hand, decoders are designed to have diî€erent probability distributions of output layer depending on the characteristics of datasets. For example, Gaussian distribution and multinomial distribution are often used to represent the real-valued continuous and binary data, respectively. 3 METHOD 3.1 Learning Global-Local Information by VAE We are interested in modeling other usersâ€™ history as well as the given userâ€™s history. We proceed with this under the VAE framework. However, a diî€œculty arises: every user has a diî€erent length of the sequence of sets. To resolve this, we sum time decayed sequence of sets. wherev sequence as explained in Section 2.1. Note that We then update our model to maximize the ELBO of each ğ‘¥ whereD (1) over D As most people interact with a few items, generated from the following process: is a dense vector and contains the information of expected preference of user ğ‘¢never interacted with item andxare close. In turn, the distance between two reconstructed vectors never interacted with item ğ‘’ Note that Hu et al. [5]â€™s Personalized Item Frequency (PIF) is similar to our sum of time decayed vectors. However, since their work is based on K-Nearest Neighbor, two shortcomings arise: 1) the inference time grows cubic with the number of users and 2) they cannot use features unlike traditional deep learning approaches. 3.2 Tweedie Output on Decoder When training VAE, using gaussian output on data generated from temporal set prediction problems are zero-inî€ated and long-tailed as shown in Figure 2. Tweedie distribution is a special case of exponential dispersion model (EDM) with a power parameter variance function distributions [ ğ‘– = 1, . . . , ğ‘ . Now we deî€›ne a random variable ğ‘ as follows: It is straightforward that the distribution deî€›ned by Equation asğ‘is addition of prediction is beneî€›cial as Tweedie distribution easily captures the properties of distributions that are shown in Figure 2. is a vector representation of userğ‘¢â€™sğ‘˜th setğ‘†,ğœ âˆˆ (0,1)is a decay factor andğ‘‡is the length of the userğ‘¢â€™s is a set of sum of time decayed sequence of setsx. Equation(3)is merely an expectation of negative of Equation . Equation (3) can be understood as the low-rank approximation with the constraint ğ·(ğ‘(z | x)||ğ‘ (z)). ğ‘‰ (ğœ‡) = ğœ‡[6]. Tweedie distribution with 1< ğ‘ <2 corresponds to a class of compound Poisson 7]. Consider two step sampling processğ‘ âˆ¼ ğ‘ƒğ‘œğ‘–ğ‘ ğ‘ ğ‘œğ‘›(ğœ†)andğ‘‹âˆ¼ ğºğ‘ğ‘šğ‘šğ‘(ğ›¼, ğ›½)forğœ†, ğ›¼, ğ›½ >0 and ğ‘‹âˆ¼ ğºğ‘ğ‘šğ‘šğ‘(ğ›¼, ğ›½)forğ‘ >0. Hence using Tweedie output on VAEâ€™s decoder for temporal set Fig. 2. Upper row: Histogram plot of samples from Tweedie distribution. Lower row: Histogram plot of elements of sum of decayed vector x. Across all benchmarks, the distributions are zero-inflated and long-tailed. Learning the mean parameterğœ‡and the power parameterğ‘of Tweedie distribution via maximum likelihood is easy. Minimizing maximizes log-likelihood. Here ğ‘§ is the target. See Yang et al. [19] for details. A line of works chose distributions other than Gaussian or Bernoulli on matrix factorization and VAE [3,12,18]. Poisson distribution or multinomial distribution are usual choices. This paper is the î€›rst attempt to apply Tweedie distribution for VAEâ€™s decoder output to the best of our knowledge. 3.3 Integrating Global-Local Information Though VAE with Tweedie output is already competent, it tends to underestimate a userâ€™s preference for the frequently interacted items. This tendency owes to the learning objective of VAE as the model has to maximize the likelihood of 0 for never interacted items. This sometimes sacriî€›ces the ability to maximize the likelihood of values of interacted items. Hence, we integrate item embeddings of frequently interacted items which are learned by state-of-the-art models to our VAE. We use DNNTSP [21] as it is the state-of-the-art method. DNNTSP makes user-dependent embeddings of items that are interacted at least once via dynamic graph neural networks. We denotezas the embedding of userğ‘¢ for item ğ‘’. When it comes to combining VAE with embeddingsz, a problem arises: the reconstructed valueË†xof Equation(4) is a scalar while learned embeddingzis a vector. Hence, to match the size betweenË†xandz, we simply multiply a vectorw, which is of same size asz, toË†x. We defer the discussion on the selection ofwto latter part of this section. Now we combine updated embedding withË†xÂ· ws and zs. The updated embeddingËœzis deî€›ned as Lastly, we calculate the aî€œnity of a user ğ‘¢ where ğœ (ğ‘¥) = We now going back to the selection of as a learnable parameter. If we set parameter w 4 EXPERIMENTS 4.1 Benchmarks We evaluate our method on three public benchmarks: Dunnhumby Carbo (DC), TaoBao, and Tags-Math-Sx (TMS). We partitioned each dataset into train, validation and test into 70%, 10% and 20% respectively following Yu et al. [21]. See Table 1 for statistics of benchmarks. 4.2 Compared Methods We compare four methods: Toppop, PersonalToppop, Sets2Sets and DNNTSP. Toppop simply serves the items that are interacted the most across all users. PersonalToppop serves the items that the given user interacted with the most. Sets2Sets uses encoder-decoder framework to predict the next set [ embeddings are made by pooling operation and set-based attention is used to model temporal correlation relation. This method also models repeated elements. DNNTSP is composed of three components: Element Relationship Learning (ERL), Temporal Dependency Learning (TDL) and Gated Information Fusing. ERL is simply a dynamic weighted graph neural networks. TDL captures temporal dependency. By Gated Information Fusing each user shares the embeddings of =âˆ’ 0.5 which normalizes all values ofË†xto [0.5, âˆ’0.5] and ğ´ğ‘¡ğ‘¡ (q, k) is deî€›ned as follows: , hence it preserves the order of aî€œnity that is learned by VAE. We also run experiments with learnable but the performance was on par with w= w. uninteracted items. Hence DNNTSP can be seen as an ensemble of model which learns local information and Toppop model. 4.3 Results & Analyses The results of our evaluation on three public benchmarks are shown in Table 2. We consider three metrics: Recall, Normal-Ãî€î€Œî€Œî€‘ ized Discounted Cumulative Gain (NDCG) and Personal Hit Ratio (PHR).ğ‘ƒğ»ğ‘…@ğ¾is calculated as1î€ŒË†ğ‘†âˆ© ğ‘†î€Œ> 0/ğ‘ where ğ‘is the number of test users,Ë†ğ‘†is the predicted top-K elements, and ğ‘†is the ground truth set. We trained VAE for 30 epochs and then trained DNNTSP for 30 epochs. We used only one layer for the VAEs across all benchmarks. For the decay factor in Equation(2), we setğœ =0.6. As illustrated in Figure 3,ğœ =0.6 shows the best performance on TaoBao dataset. We empirically observe thatğœ =0.6 could provide decent results across all metrics on the other datasets as well. The dimension of latent space is 128 for DC and TaoBao, and 512 for TMS. We used Adam optimizer [9] with learning rate 0.001. Across all benchmarks and metrics, GLOIE with Tweedie output outperforms or is on par with all compared methods. Especially, GLOIE with Tweedie output outperforms the other methods on all metrics on DC and TaoBao datasets. We can see that GLOIE with Tweedie outperforms DNNTSP on every metric whenğ¾ =10 which means that the embeddings learned by VAE are richly used as well as the embeddings learned by DNNTSP. One thing to remark is that VAE with Tweedie output shows comparable performance to all compared methods on most metrics. Given that the number of items a user interacted with is 5.44, 4.96, and 18.05 respectively in DC, TaoBao, and TMS, this shows that VAE with Tweedie output captures the preference of users to non-interacted items. To investigate the eî€ectiveness of the attention-based integration method illustrated in Equation(8), we compared GLOIE with attention to the one with itemwise learnable weight similar to the method proposed in Yu et al. [21]. For overall datasets and metrics, we could observe performance gains: 3.09%, 0.45%, 0.92% improvement of NDCG@10 on DC, Taobao, and TMS, respectively. Lastly, we note that the selection of output distribution of decoder on VAE largely aî€ects the performance. We empirically show that both Gaussian and multinomial outputs do not î€›t temporal set prediction problems even though Gaussian is a popular choice for VAE and multinomial is a common choice in recommender system literature after the advent of VAECF [12]. 5 CONCLUSION This paper proposes Global-Local Item Embedding (GLOIE) that learns to utilize the temporal properties of sets across whole users as well as within a user. The proposed model learns global-local information by maximizing ELBO of the sum of time decayed vectors under the VAE framework and integrates local embeddings learned by dynamic graph Table 2. Comparison between various state-of-the-art methods and ours on three public benchmarks. All highest scores are in and all second best scores are underlined. TMS neural networks. As users with similar histories are reconstructed to close vectors, we could model the preference of the given user for a non-interacted item if other users with similar histories frequently interacted with the item. Data analysis and empirical results show that using Tweedie output for VAEâ€™s decoder is eî€ective for modeling temporal set prediction. The proposed method achieves state-of-the-art results by considering global information which is less explored in temporal set prediction literature. Though the proposed VAE is powerful in itself, there is some room for improvement. Instead of using the sum of time decayed vector, we can model change of sets in continuous time by Neural ODE [ form of prior that captures long-tailed distribution for the temporal set prediction can also be a future direction. Graph modality can also be used [ and embeddings learned by other algorithms which focus on local information is also an important future work. GLOIE - Multinomial 0.3265 0.2465 0.4143 0.3870 0.2633 0.4798 0.4615 0.2803 0.5602 GLOIE - Multinomial 0.2980 0.1791 0.3040 0.3783 0.1995 0.3846 0.4750 0.2195 0.4819 GLOIE - Multinomial 0.1479 0.1029 0.2797 0.2192 0.1252 0.3872 0.3259 0.1524 0.5362