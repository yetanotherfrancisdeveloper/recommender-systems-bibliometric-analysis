Modern Web systems such as social media and e-commerce contain rich contents expressed in images and text. Leveraging information from multi-modalities can improve the performance of machine learning tasks such as classiî€›cation and recommendation. In this paper, we propose the Cross-Modality Attention Contrastive Language-Image Pre-training (CMA-CLIP), a new framework which uniî€›es two types of cross-modality attentions, sequence-wise attention and modality-wise attention, to eî€ectively fuse information from image and text pairs. The sequence-wise attention enables the framework to capture the î€›ne-grained relationship between image patches and text tokens, while the modality-wise attention weighs each modality by its relevance to the downstream tasks. In addition, by adding task speciî€›c modality-wise attentions and multilayer perceptrons, our proposed framework is capable of performing multi-task classiî€›cation with multi-modalities. We conduct experiments on a Major Retail Website Product Attribute (MRWPA) dataset and two public datasets, Food101 and Fashion-Gen. The results show that CMA-CLIP outperforms the pre-trained and î€›ne-tuned CLIP by an average of 11.9% in recall at the same level of precision on the MRWPA dataset for multitask classiî€›cation. It also surpasses the state-of-the-art method on Fashion-Gen Dataset by 5.5% in accuracy and achieves competitive performance on Food101 Dataset. Through detailed ablation studies, we further demonstrate the eî€ectiveness of both cross-modality attention modules and our methodâ€™s robustness against noise in image and text inputs, which is a common challenge in practice. Inspired by the recent rise of the pre-trained NLP models such as BERT [11], learning to classify image-text pairs for vision-language (ğ‘‰ ğ¿) tasks using Transformer [37] based encoders has received much attention as both modalities can be informative and beneî€›cial to each other. Current methods can be classiî€›ed into two main categories: one-stream methods and two-stream methods. One-stream methods capture the cross-modality attention across brywan@amazon.com yisun@amazon.com Seattle, WA, USA image and text by concatenating them at early stage and input the concatenated feature into one uniî€›ed transformer encoder. Twostream methods î€›rst extract the image and text features using two separate encoders and then learn their cross-modal relationship through various methods such as contrastive learning [3], etc. Among those one-stream and two-stream methods, the Contrastive Language-Image Pre-Training (CLIP) [30] has achieved great success recently. CLIP is trained on the WebImageText (WIT) Dataset which consists of 400 million image-text pairs collected from a variety of publicly available sources on the Web and achieves many state-of-the-art results in zero-shot learning tasks, pre-training tasks, and supervised classiî€›cation tasks when a linear probe is added on top of it. Despite of CLIPâ€™s [30] strength, it is mainly designed for zeroshot image classiî€›cation, resulting in the limitation of its ability to leverage both image and text input when available. Since the training of CLIP only involves global image and text features, thus the î€›ne-grained relationship between image patches and text tokens are not modeled. Such relationship is useful in î€›ne-trained classiî€›cation tasks, especially in the situations where only a small proportion of image patches or text tokens are related to the classiî€›cation tasks. Moreover, since it chooses the user-deî€›ned textual description called "prompts" with class value that matches the most with the image as the classiî€›cation result, signiî€›cant eî€orts to engineer the prompts for optimizing downstream tasks are required. Last but not least, in practice, it is quite common for the image-text pairs to contain noise. For instance, on E-commerce websites, some images or text could be irrelevant to the product due to catalog errors. In social media apps, users might enter irrelevant textual comments or upload unrelated images. Treating input from both modalities equally in such situation may lead to poor classiî€›cation performances as one of the modalities could be pure noise. To address the aforementioned issues, in this paper we propose the Cross-Modality Attention CLIP (CMA-CLIP). Our contributions include: â€¢We combine CLIP with a sequence-wise attention module, which reî€›nes the CLIP-generated image and text embeddings v-neck sleeveless Text ğ‘‡ cocktail EncoderT dressT by modeling the relationship among the embedding of the sequence of image patches and text tokens. This transformerbased module makes the embedding more context-aware. ğ¸.ğ‘”., the embedding of the black image patches are more correlated with the â€˜ğ‘ğ‘™ğ‘ğ‘ğ‘˜â€™ token in text. We experimentally prove that such reî€›nement can improve the performance of classiî€›cation tasks. â€¢We adopt a modality-wise attention module to assign learnable weights to each modality that measures its relevance to the classiî€›cation task. The impact of the irrelevant modality will be dampened, and therefore our network is robust against noisy image or text inputs, which is a common challenge in practice. â€¢We add task speciî€›c modality-wise attentions and MLP heads on top of the sequence-wise attention module, so that the same network can be leveraged for multi-task classiî€›cation. Moreover, compared with CLIP, this architecture enables the network to leverage both image and text inputs in both training and inference stage. â€¢On the MRWPA, CMA-CLIP outperforms the raw CLIP and the î€›ne-tuned CLIP (î€›ne-tuned using image-text pairs from a major retail website) on the classiî€›cation of three product attributes by 10.9% and 12.9% in recall at the same level of precision. It also improves the state-of-the-art performance [44] on Fashion-Gen Dataset from 88.1% to 93.6% in accuracy and achieves competitive performance on Food101 Dataset against [21]. Current multi-modality learning methods are mainly one-stream and two-steam where one-stream methods use a single Transformer ğ¼[Task2][Task2] encoder to process the concatenated image and text embedding, while two-stream methods use both image encoder and text encoder to extract image and text embeddings at early stage and then learn their cross-modal relationship. The one-stream methods, such as ViLBERT [28], VisualBERT [26], VL-BERT [35], Unicoder-VL [25], ImageBERT [29] and Uniî€›ed VLP [43], concatenate the imageâ€™s Region-Of-Interest (ROI) patches and text tokens as the input tokens for BERT [11]. These models are typically pre-trained using tasks including Masked Language Modeling (MLM), Masked Region Modeling (MRM), Multi-Model Alignment Prediction (MMAP). The UNITER [8] and OSCAR [27] incorporate additional pre-training tasks. The UNITER uses the Optimal Transport (OT) [38] to model the relationship between the image patches and the text tokens. The OSCAR uses the object categories detected by the Faster-RCNN [31] and encodes the category text as additional input tokens to BERT. Instead of using the Faster-RCNN to detect ROIs, methods such as ICMLM [33], Pixel-BERT [18], and SOHO [17] use a CNN to extract the feature maps of an image and use the depth vectors in feature maps as image tokens. Such conî€›guration is able to capture the semantic connection between image pixels and text tokens, which is overlooked by region based image features extracted by Faster-RCNN. Similar to ICMLM, the VirTex [10] also uses the depth vectors in feature maps as image tokens and input the image and text tokens into a forward transformer decoder and a backward transformer decoder. Instead of feeding the whole image or ROI into a CNN, methods like FashionBERT [13], KaleidoBERT [44], and ViLT [22] cut an image into patches and treat each patch as an "image token". For FashionBERT, it uses a pre-trained image model such as InceptionV3 [36] or ResNeXt-101 [40] to extract image features. Diî€erent from FashionBERT, KaleidoBERT adopts the SAT [41] network to generate description of salient image patches aiming to î€›nd an approximate correlation between image patches and text tokens to serve their pre-training tasks,ğ‘–.ğ‘’., Aligned Masked Language Modeling (AMLM), Image and Text Matching (ITM), and Aligned Kaleido Patch Modeling (AKPM). ViLT diî€ers from all above-mentioned methods by simply applying linear projection on î€attened image patches which greatly reduce the model size, thus leading to signiî€›cant runtime and parameter eî€œciency. The two-stream methods are mainly motivated by self-supervised learning methods [2â€“7,14,15]. In self-supervised learning, two views (ğ‘’.ğ‘”., two augmentations), of a single image are forwarded into one network respectively. Their outputs are compared using the contrastive loss [3], so that the two views of the same image are much similar than the two views from two diî€erent images. The ConVIRT [42] adopts this idea on the self-supervised learning of image-text pairs. Two networks are used to extract the image and text features respectively. The image and text features from the paired image-text input is trained to be much similar than the unpaired ones. CLIP [30] is a simpliî€›ed version of ConVIRT, where the text in each image-text pair is a single sentence instead of a pool of sentences as in ConVIRT. Similar as CLIP, BriVL [19] uses MoCo [15] which is a more advanced cross-modal contrastive learning algorithm to help train the network with limited GPU memory by leveraging more negative samples. The ALIGN [20] collects 1.8 billion image-text pairs, and adopts a similar network architecture as CLIP. The performance of ALIGN is comparable to CLIP on the ImageNet dataset for the classiî€›cation task, Flickr30K and MSCOCO datasets for image-text retrieval task. In order to perform image-text classiî€›cation tasks, a classiî€›cation layer needs to be added on top of the image-text embeddings of the pre-trained models such as VL-BERT [35], UNITER [8], et al. The MMBT [21] is speciî€›cally designed for image-text classiî€›cation tasks. Unlike VL-BERT or UNITER, MMBT directly loads weights from BERT which does not require pre-training. In MMBT, ResNet [16] is used to extract image features. The image features are projected into the same space as the text tokens, used as image tokens and feed into a BERT together with text tokens. A linear layer is added on the classiî€›cation embedding to perform supervised tasks. However, both one-stream and two-stream methods have their own inadequacies. One-stream methods heavily rely on pre-trained Faster-RCNN [31] or ResNet [16] to extract image feature which does not support end-to-end training of the whole network, and therefore, the extracted image and text features are not optimized to model the image-text relationship. Two-stream methods only focus on learning global image and text features, and cannot capture the î€›ne-grained relationship between image patches and text tokens. In order to overcome the aforementioned disadvantages, our proposed method fuses both one-stream and two-stream architectures which complements each otherâ€™s inadequacies. It leverages the pre-trained CLIP, a two-stream architecture, to capture the overall alignment between image and text. Subsequently, we add a sequence-wise attention module, which is a transformer based cross-modality attention module used in most one-stream architectures, to capture the î€›ne-grained relationship between image patches and text tokens. SemVLP [24] applies similar fusion logic. The diî€erence between our method and SemVLP is that, SemVLP leverages the same cross-modality attention module to capture both high-level and î€›ne-grained relationship between image and text. It was pre-trained on tasks such as MLM and MRM, whereas CLIP was directly pre-trained to maximize the overall image-text alignment through contrastive learning. More importantly, SemVLP does not consider the situation where one of the modalities is irrelevant to the downstream classiî€›cation tasks due to input noises, which is a common challenge in practice. To handle such situation, we add a modality-wise attention module, which learns the importance of both modalities so that the irrelevant modality can be dampened for the classiî€›cation tasks. At last, by adding task speciî€›c modalitywise attentions and MLPs, our model is able to perform multi-task classiî€›cations. The rest of the paper is arranged as follows: In Section 3, we î€›rst give a brief review of CLIP, and then we introduce our proposed CMACLIP with detailed explanation of each component. In Section 4, we introduce the datasets that we use, the corresponding experimental results, the visualization of the sequence-wise attention module, and the ablation study to prove the eî€ectiveness of modality- and sequence-wise attention modules. In Section 5, we conclude this paper and elaborate our future work. The Contrastive Language-Image Pre-Training (CLIP) consists of an image encoder and a text encoder. For each image-text pair, the image and text encoders project the pair into an image and text embedding in the same multi-modal space. Givenğ‘imagetext pairs, the training objective of CLIP is to maximize the cosine similarity of the paired image and text embedding while minimize the cosine similarity of the unpaired ones. During inference, for a classiî€›cation task withğ¾classes, it î€›rst uses theğ¾class values to constructğ¾prompts such as â€˜A photo of{class value}â€™. Theseğ¾prompts are then projected toğ¾text embeddings by the text encoder. For any given image, it is projected to an image embedding by the image encoder, then CLIP computes the cosine similarities between the image embedding and those ğ¾ text embeddings. The class value with the largest similarity is then considered as the class prediction. CLIP is trained using WIT Dataset which contains 400 million image-text pairs collected from the Web. According to the results reported in [30], its zero-shot classiî€›cation performance surpasses the supervised linear classiî€›er î€›tted on ResNet50 [16] features on datasets such as StanfordCars [23], Country211 [30], Food101 [1], and UCF101 [34] etc. CLIP focuses on the learning of the global image and text features. In CMA-CLIP, we build a sequence-wise attention module to capture the î€›ne-grained relationship between the image patches and the text tokens such as the black image patches and the â€˜blackâ€™ tokens in text. This module leverages the transformer architecture [37]. It takes the sequence of embeddings corresponding to all the image patches and text tokens generated by CLIP as input. The Fashion-Gen DatasetJeans module outputs two embeddings incorporating the aggregated image and text information. Instead of directly leveraging these two embeddings for classiî€›cation, we add a modality-wise attention module to handle the situation where a certain modality (image or text) is irrelevant to the classiî€›cation task. This is because, in practice, it is common for the image-text pairs to contain noise. ğ¸.ğ‘”., a retailer might upload wrong product images to E-commerce website, or a user might enter random textual comments on social media apps. To handle such situations, we leverage the similar architecture as in [39] to learn the importance of each modality to the classiî€›cation tasks. The sum of the two embeddings weighted by their importances is followed by a MLP head for the classiî€›cation. To leverage the network for multiple classiî€›cation tasks, we conî€›gure task speciî€›c modality-wise attention modules and MLP heads. The complete architecture of CMA-CLIP is shown in Fig. 1. 3.2.1 Sequence-wise Aî€ention. In our implementation, the sequencewise attention module is a transformer encoder [37]. Letğ‘‹ âˆˆ R be the matrix of the sequence of embedding of all the image patches and text tokens generated by CLIP, whereğ‘ is the length of the sequence andğ‘‘is the dimension of the embedding. Letğ‘Šâˆˆ R, ğ‘Šâˆˆ Randğ‘Šâˆˆ Rbe the projection matrices which project each embedding inğ‘‹to key space, query space and value space respectively: The self-attention block learns a similarity matrixğ‘„ğ¾between each pair of embeddings inğ‘‹. Each embedding in the sequence is then updated as the average of the projected embedding across all the embeddings in the value space weighted by their similarities. This sequence-wise attention module captures the î€›ne-grained relationship between each image patch and text token. 3.2.2 Modality-wise Aî€ention. After the image feature and the text feature are generated, they need to be aggregated to form the î€›nal feature for the classiî€›cation tasks. In order to dampen the irrelevant modality, we leverage the keyless attention module proposed in [39]. Given an image-text pair, the sequence-wise attention module will outputğ¼andğ‘‡as the global image and text embedding, respectively. The aggregated embedding is the weighted average of The weight ğœ† is computed by: whereğ‘¤is a learnable parameter vector that is of the same dimension as ğ¼and ğ‘‡. Algorithm 1 Training Process of CMA-CLIP. Line 4-16: Warm-up Stage, Line 18-21: End-to-End Training Stage, Line 23-26: Tuning Stage Input: Image-text pairs and their labels of K tasks (X, Y, L, L, ..., L denotes the modality-wise attention model. ğ‘€ğ¿ğ‘ƒ denotes the multi-layer perception head for classiî€›cation. Freeze the image encoder, the text encoder, the sequence-wise attention transformer. Fine-tune on the best check-points from the previous stage. Output: ğ¶ğ¿ğ¼ğ‘ƒ model, ğ¶ğ‘€ğ´model, ğ¶ğ‘€ğ´model, and ğ‘€ğ¿ğ‘ƒ model. Table 2: Recall (%) at 90% precision on the MRWPA dataset. Table 3: Accuracies (%) on the Food101 dataset. 3.2.3 MLP Heads for Classification. For any classiî€›cation task, an Multi-Layer Perception (MLP) head is added on top of the î€›nal feature outputted by the modality-wise attention. The cross entropy Table 4: Accuracies (%) on the Fashion-Gen dataset. is used to compute the loss for this classiî€›cation task. For multitask classiî€›cation, we add task-speciî€›c modality-wise attention and MLP for each task separately. This is because the relevance of modality is dependent on the task, hence we need task-speciî€›c modality-wise attentions and multiple MLPs as the classiî€›cation heads. Table 5: Examples where CMA-CLIP is able to give the correct attribute classiî€›cation while CMA-CLIP w/o the modality-wise attention cannot. Noting that, text tokens which are shown in red are the attribute label keywords that do not exist in the original titles. We add them in and re-do the prediction with b oth methods to further validate that, without the modality-wise attention, the model is not able to manage noise properly. Table 6: Ablation study of CMA-CLIP. Recall (%) at 90% precision on the MRWPA with Color, Pattern and Style attributes. ğ‘€ğ´ denotes modality-wise attention and ğ‘†ğ´ denotes to sequence-wise attention. We perform experiments on three datasets, the MRWPA Dataset, the Food101 Dataset [1] and the Fashion-Gen [32] Dataset. All three datasets consist of image-text pairs. Data samples from the three datasets are shown in Table 1. 4.1.1 The MRWPA Dataset. This dataset includes the product image and title pairs of dress products from a major retail website. The goal is to classify three dress related product attributes, color, pattern, and style. Color attribute has 17 classes such as black and white, pattern has 12 classes such as graphic and plain, and style has 21 classes such as pencil and a-line. The training data consists of 5.8 Million product image-title pairs. We also prepare 310 and 132 image-title pairs as the validation and test set, which are used for hyper-parameter tuning and performance evaluation respectively. Womenâ€™s Sexy V Neck Crisscross Backless Cocktail Party Bodycon Peplum [Plain] Dress, White, L 4.1.2 The Food101 Dataset. This dataset contains 101 food categories. The goal is to classify each image-text pair to a food category. We download the preprocessed images and texts from the Kaggle competition. In the processed data, 67971 images are in the training set, and 22715 images are in the testing set. During training, we randomly split 80% of the data in the training set for training and the rest 20% data for validation. 4.1.3 The Fashion-Gen Dataset. This dataset contains 293,008 fashion images. Each image is paired with a text describing the image. This dataset contains 48 main categories, such as "DRESSES", "JEANS", "SKIRTS", "SHIRTS", etc., and 121 sub-categories, such as "SHORT DRESSES", "LEATHER JACKETS", "MID LENGTH SKIRTS", "T-SHIRTS" and so on. In our experiments, we perform 121 subcategory classiî€›cation. We use the same data as used in [44] for training and testing. The number of training data is 260480, and the number of testing data is 32528. 4.2.1 Experiment Seî€ings. Same as CLIP, the image encoder of CMA-CLIP is a 12-layer 768-width ViT-B/32 [12] with 12 attention heads, and the text encoder of CMA-CLIP is a 12-layer 512-width Transformer with 8 heads used in [37]. The sequence-wise attention transformer is also a 12-layer 512-width model with 8 attention heads. In all the experiments, the batch size is set to 1024, weight decay of Adam is set to1ğ‘’ âˆ’ 4, and the learning rate is set to1ğ‘’ âˆ’ 5. Figure 2: Visualization examples. Each image is highlighted using the attention map between the image embedding and the embedding of the most relevant text token. 4.2.2 Training Strategy. We use the pre-trained weights of CLIP as the initial weights of the image encoder and text encoder in CMACLIP. We randomly initialize the weights in the sequence-wise attention module, modality-wise attention module and MLP. As CMA-CLIP contains a mixture of pre-trained weights and randomly initialized weights, instead of training the model end-to-end which may cause under- or over-î€›tting of certain modules, we adopt a multi-stage training strategy to train CMA-CLIP. The training stages are listed below: â€¢ Warm-up stage.In this stage, the weights of the image encoder and the text encoder are frozen. We train the sequencewise attention, the modality-wise attention and the MLP modules. â€¢ End-to-end training stage.In this stage, we unfreeze the weights of the image encoder and the text encoder, and train all the components together. â€¢ Tuning stage.This stage is for multi-task training. The weights of the image encoder, the text encoder and the sequence-wise attention are frozen. We train the modalitywise attentions and MLPs for all the tasks. 4.2.3 Implementation. Detailed training process of CMA-CLIP is summarized in Algorithm 1. For the MRWPA Dataset, all three stages are trained for 20 epochs. For Food101 and Fashion-Gen Datasets, Warm-up stage is trained for 100 epochs and End-to-end training stage is trained for 300 epochs. Since for the two public datasets, they are both single-task classiî€›cation so the Tuning stage is not needed. During the Warm-up stage, due to the freeze of the CLIP module, only the check-points of the sequence-wise attention, the modality-wise attention and the MLP modules are updated. During the End-to-end training stage, check-points of all three modules are updated. And during the Tuning stage, only the checkpoints of the modality-wise attention the MLP modules are updated. At the end of each training stage, the best check-points with the lowest validation accuracy are used for either next stageâ€™s î€›netuning or inference. 4.3.1 The MRWPA Dataset. We compare CMA-CLIP with the zeroshot performance of raw CLIP and î€›ne-tuned CLIP (î€›ne-tuned using image-title pairs in MRWPA dataset) in terms of the recall at 90% precision for the color, pattern, and style attributes. The results are included in Table 2. We observe that CMA-CLIP consistently outperforms both raw CLIP and î€›ne-tuned CLIP by a large margin across all three attributes. 4.3.2 The Food101 Dataset. On the Food101 dateset, we compare CMA-CLIP with two single-modality baseline methods including BERT [11] and ViT [12], and two multi-modality baseline methods including raw CLIP [30] with same ViT-B/32 and MMBT [21]. Results are included in Table 3. CMA-CLIP achieves the best accuracy of 93.1%, which improves 1% over the a current strong baseline method MMBT. Using only image features achieves 81.8% by ViT, and using only text features achieves 87.2% by BERT. 4.3.3 The Fashion-Gen Dataset. On the Fashion-Gen dataset, we compare CMA-CLIP with multiple SOTA methods including FashionBERT [13], ImageBERT [29], OSCAR [27] and KaleidoBERT [44]. CMA-CLIP achieves the highest accuracy of 93.6%, which improves over KaleidoBERT [44], the previous SOTA method, by 5.5%. We conduct systematic ablation study to validate the eî€ectiveness of modality-wise attention module and sequence-wise attention module by removing them sequentially and comparing the performance with CMA-CLIP. Detailed results are shown in Table On MRWPA, the average recall at 90% precision across the 3 attributes drops from 53.4% to 47.5% after removing the modalitywise attention module. This is because the proportions of titles that contain tokens related to color, pattern, and style are 67%, 25% and 15% respectively. When a title does not contain any tokens related to an attribute, it becomes irrelevant for the classiî€›cation of that attribute. The performance drop indicates that the modality-wise attention module signiî€›cantly improves CMA-CLIPâ€™s robustness against noisy inputs. To illustrate our modelâ€™s robustness to input noise, in Table 5 we randomly pick some product image-title examples that CMA-CLIP is able to give correct classiî€›cation whereas CMA-CLIP without modality-wise attention module cannot. We can clearly observe that in those examples, the product titles do not contain any tokens related to the attribute labels. Furthermore, for these examples, we complete the titles by adding the label related keywords and re-test them. This time, both methods can provide correct classiî€›cation results which further proves that the modality-wise attention has the ability of î€›ltering out irrelevant information (text without label information is considered as noise). We are not able to select similar examples in the Food101 and Fashion-Gen datasets, because in these two public datasets, there are no images or text that are irrelevant to the classiî€›cation task. The average recall drops from 47.5% to 45.8% after further removing the sequence-wise attention module. The sequence-wise attention module enhances the context-awareness of the image and text embedding by capturing the î€›ne-grained correlation among image patches and text tokens, and the resulting embedding is expected to yield better results for classiî€›cations. The performance drop supports this conclusion. We also visualize the result of sequence-wise attention for MRWPA, Food101 and Fashion-Gen datasets in Figure 2. For each text input, we locate the token that is related to the classiî€›cation task, and visualize the image patches that are most correlated to it by checking the inner product between the query embedding of the text token and the key embeddings of the image patches. In Figure 2, red regions are where the correlation is high. We observe that the sequence-wise attention is able to identify the highly correlated image patches and text tokens across all three datasets. In this paper, we propose the CMA-CLIP, which uniî€›es two types of cross-modality attentions: sequence-wise attention, a transformer based attention module that captures the î€›ne-grained relationship between image patches and text tokens, and modality-wise attention, which learns the importance of image and text modalities in order to î€›lter out the irrelevant modality for the classiî€›cation task. We also design task speciî€›c modality-wise attentions and MLPs so that we can leverage a uniî€›ed network for multi-task classiî€›cations. We evaluate our method on the MRWPA Dataset, the Food101 dataset and the Fashion-Gen dataset. CMA-CLIP outperforms the pre-trained and î€›ne-tuned CLIP by an average of 11.9% in recall at the same level of precision on the MRWPA Dataset for the classiî€›cations for color, pattern, and style attributes. It also surpasses the state-of-the-art method on the Fashion-Gen Dataset by 5.5% in accuracy and achieves competitive performance on the Food101 Dataset. For the future work, we are interested in training CMACLIP with other datasets to enable the contrastive loss, improving CMA-CLIPâ€™s robustness against noisy labels, and also, exploring semi-supervised learning methods so that unlabeled image-text pairs can be leveraged in the training process to improve model generalizability.