In order to support a variety of missions and deal with diî€erent î€ight environments, drone control programs typically provide conî€›gurable control parameters. However, such a î€exibility introduces vulnerabilities. One such vulnerability, referred to as range speciî€›cation bugs, has been recently identiî€›ed. The vulnerability originates from the fact that even though each individual parameter receives a value in the recommended value range, certain combinations of parameter values may aî€ect the drone physical stability. In this paper we develop a novel learning-guided search system to î€›nd such combinations, that we refer to as incorrect conî€›gurations. Our system applies metaheuristic search algorithms mutating conî€›gurations to detect the conî€›guration parameters that have values driving the drone to unstable physical states. To guide the mutations, our system leverages a machine learning predictor as the î€›tness evaluator. Finally, by utilizing multi-objective optimization, our system returns the feasible ranges based on the mutation search results. Because in our system the mutations are guided by a predictor, evaluating the parameter conî€›gurations does not require realistic/simulation executions. Therefore, our system supports a comprehensive and yet eî€œcient detection of incorrect conî€›gurations. We have carried out an experimental evaluation of our system. The evaluation results show that the system successfully reports potentially incorrect conî€›gurations, of which over 85% lead to actual unstable physical states. Drone security, conî€›guration test, range speciî€›cation bug, deep learning approximation ACM Reference Format: Ruidong Han, Chao Yang, Siqi Ma, JiangFeng Ma, Cong Sun, Juanru Li, and Elisa Bertino. 2021. Control Parameters Considered Harmful: Detecting Range Speciî€›cation Bugs in Drone Conî€›guration Modules via LearningGuided Search. In Proceedings of The 44th International Conference on Software Engineering (ICSE 2022). ACM, New York, NY, USA, 12 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn Drones â€“ î€ying mini robots, are rapidly growing in popularity. They have become essential in supporting the central functions of various business sectors (e.g., motion picture î€›lmmaking) and governmental organizations (e.g., surveillance). Because drones are pilotless and have small physical shape and fast speed, they are often used in missions, such as delivery and surveillance, targeting locations diî€œcult or expensive to reach with other means. However, as drones will be increasingly used also for critical missions, it is important that they be reliable and adaptable to ensure that missions successfully complete. To achieve reliability and adaptability, enhanced î€ight control systems have been developed. Such systems provide large numbers of control parameters that can be conî€›gured to modify the î€ight states of drones, such as linear and angular positions. Through parameter adjustment, diî€erent conî€›gurations can be set and sent to the î€ight control program. Based on such a conî€›guration, the î€ight control program controls the î€ight states of the drone to complete the î€ight mission. However, the possibility of adjusting parameters introduces certain vulnerabilities (referred to as range speciî€›cation bugs) arising from the lack of adequate checking of the control parameter values. Speciî€›cally, when setting some particular conî€›gurations by selecting parameter values within the ranges provided by the manufacturer, unstable î€ight states might be triggered, such as trajectory deviation or even drone crash. Existing vulnerability detection techniques cannot detect such range speciî€›cation bugs. If the source of the control program is available, static program analysis can be used to detect data/control dependencies [22,23] and î€›nd such bugs. However, such an approach works well only for small code snippets. If used on large and complex programs, static analysis would have scalability issues. To address scalability, taint analysis can be used by tracking input data î€ow [10,13,30], which depends more on input construction. However, when dealing with very large numbers of control parameters, each with a wide range of values, analyzing conî€›gurations by tainting all the parameter values is time consuming. A recently proposed tool, rvfuzzer [20], tries to address such an issue by generating conî€›guration inputs through fuzzing. Even though rvfuzzer is able to reduce the total amount of conî€›gurations to be tested, it is still ineî€œcient and unable to provide high-coverage analyses of conî€›gurations. As a result, it misses incorrect conî€›gurations. Major challenges for detecting range speciî€›cation bugs include how to validate conî€›gurations eî€ectively and how to eî€œciently search for correct parameter ranges. In this work, we address these challenges by developing a learning-guided fuzzing approach speciî€›cally designed to detect range speciî€›cation bugs. At a high level, our detection tool, LGDFuzzer, relies on a genetic algorithm (GA) [37] and a î€ight state predictor to detect the conî€›gurations that are potentially incorrect. Speciî€›cally, LGDFuzzer is equipped with three core components, State Change Predictor, Learning-Guided Mutation Searcher, and Range Guideline Estimator. First, we manually collected a list of î€ight logs, each of which contains a î€ight state, sensor data, conî€›gurations, and a timestamp. By using such logs, LGDFuzzer trains a state predictor, which is utilized to estimate the state, referred to as reference state, that will be reached by the drone at the next timestamp. Simultaneously, LGDFuzzer runs the learning-guided mutation searcher to generate conî€›gurations by leveraging the GA. Unlike the traditional conî€›guration validation schemes that test the conî€›guration on either a î€ight simulator or a realistic drone, LGDFuzzer leverages the state predictor to estimate the reference state and infers whether a conî€›guration is correct based on the deviation of the reference state with respect to the expected state. Finally, LGDFuzzer validates the conî€›gurations that are predicted as â€œincorrectâ€ and further generates a valid range for each parameter. We used LGDFuzzer to analyze the most popular î€ight control program, ArduPilot [35]. In total, LGDFuzzer validated 46,500 conî€›gurations and labeled 2,319 as â€œincorrectâ€, out of which 2,036 incorrect conî€›gurations were conî€›rmed. Apart from the identiî€›ed range speciî€›cation bugs, we also found 564 zero-day vulnerabilities, referred to as Incorrect Conî€›guration Tackling bugs. These refer to conî€›gurations that are detected as incorrect before the drone takes oî€ and, as a result, the î€ight is aborted. However, the control program will accept these incorrect conî€›gurations if they are sent to the control program after the drone has taken oî€. Our analysis also shows that a signiî€›cant number of incorrect conî€›gurations are set in order to enhance adaptability. To assist developers and users in building secure î€ight control programs, LGDFuzzer optimizes the parameter ranges based on a manually set adaptability level. If developers and users prefer higher adaptability, larger parameter ranges will be provided by LGDFuzzer; however the possibility of causing unstable î€ight would be higher. Otherwise, a smaller range will be set and the î€ight states of the drone will be more stable. Contributions. (1) We have designed and implemented LGDFuzzer to detect incorrect conî€›gurations eî€ectively and eî€œciently. Our system uses a GA to select the â€œhighly possibleâ€ incorrect conî€›gurations and validates conî€›guration correctness by using a deep learning based state predictor. (2) According to the requirements of reliability and adaptability by developers and users, we designed and implemented a range optimization component in LGDFuzzer to provide the most appropriate parameter value ranges to minimize the possibility of introducing incorrect conî€›gurations. (3) We applied LGDFuzzer to a real-world î€ight control program and identiî€›ed 2,036 incorrect conî€›gurations causing unstable î€ight states. We also veriî€›ed 106 incorrect conî€›gurations on real-world drones and conî€›rmed that these incorrect conî€›gurations cause trajectory deviations or drone crashes. (4) We found a new type of bugs, Incorrect Conî€›guration Tackling bugs, and veriî€›ed that these bugs also cause unstable î€ight states. (5) We have open sourced our LGDFuzzer at https://github.com/ BlackJocker1995/uavga/tree/main; the site makes available the tool, dataset, and video recordings of our tests of incorrect conî€›gurations. In this section, we î€›rst introduce some information about the î€ight control programs utilized by drones and the range speciî€›cation bugs. We then discuss the challenges in the design of an approach to validate parameter conî€›gurations, followed by our solutions to address these challenges. During a î€ight, the ground control station (GCS) communicates with the drone by sending a series of commands to the î€ight control program. Before the drone takes oî€, users can conî€›gure the î€ight control program by adjusting the parameters to manipulate the î€ight states of the drone (e.g., linear position, angular position, angular speed, velocity, and acceleration). To ensure that a drone completes its î€ight mission successfully, the î€ight control program periodically observes the current positioned î€ight state and sensor data (e.g., from GPS, gyroscopes, and accelerometers) to estimate a reference state indicating the next state of the drone. Then the control program generates actuator signals (e.g., motor commands) to move the drone to the reference state. The positioned state and the reference state need to be close enough, i.e., within a standard deviation. If this is not the case, the drone î€ight may become unstable, leading to trajectory deviations and crashes. Although the value ranges for control parameters are typically hardcoded in the control programs and one would expect that all possible combinations of these values be correct, some of the combinations are actually incorrect. Any conî€›gurations triggering unstable î€ight states are regarded as incorrect. The corresponding control parameter ranges are range speciî€›cation bugs [20]. Since hundreds of control parameters can be speciî€›ed by the î€ight control program, identifying range speciî€›cation bugs by validating all parameter values is time consuming because some parameters may not aî€ect î€ight stability. Therefore, we focus on the parameters that might aî€ect angular position and angular speed state, because they directly impact the î€ight attitude and their incorrect values are more likely to cause unstable states. By analyzing the physical impact on drones, we deî€›ne î€›ve unstable î€ight states: Flight Freeze.A drone is required to keep moving unless the î€ight control program generates a signal to keep the drone stationary or to instruct the drone to land. However, incorrect conî€›gurations may lead the drone to accidentally freeze at a waypoint, when moving forward/backward, or to wander around a position within a minimum range. To determine whether a î€ight is frozen, we calculate the movement distance between the previous and the current positions within a time interval. If the distance is less than a threshold, the î€ight state is regarded a frozen, i.e.,â€œî€ight freezeâ€. Deviation.According to the actuator signals generated by the î€ight control program, the drone will be driven to achieve the reference state as close as possible. In practice, however, the positioned state may not always match the reference state, that is, the drone is deviating from the expected trajectory. When the deviation is small, the control system can generate an actuator signal based on the diî€erences between the current positioned state and the reference state. However, an incorrect conî€›guration may trigger a signiî€›cant deviation. Such a deviation may lead to an erroneous trajectory from which the drone cannot return back to the correct trajectory. If the deviation exceeds a given threshold, we consider the î€ight state as unstable, i.e., a â€œdeviationâ€ state. Drone Crash.For general deviations, the drone can still land safely even though not at the expected location. A worse case is a deviation leading the drone against an object and eventually crash. Potential Thrust Loss.By driving the drone motor, the î€ight control program uses motor power to adjust the drone to the reference state. Nonetheless, the adjustment that can be done by the drone motor is limited. If an incorrect conî€›guration is set, the drone may not be able to move close to the reference state even when saturating the motor up to 100% throttle. Remaining in such an incorrect state can cause a decrease in the drone î€ight altitude and attitude, or even a crash. Incorrect Conî€›guration Tackling.Before taking oî€, the î€ight control program validates the conî€›guration and determines whether it will trigger unstable î€ight states. (i.e., the conî€›guration is incorrect). When one or more incorrect parameters of the conî€›gurations are identiî€›ed, the control program displays a warning message and aborts the taking-oî€ operation. However, if these conî€›gurations, î€agged as incorrect, are set after taking oî€, they can still be accepted by the î€ight control program. As a consequence, the drone may end up in some unstable states. In order to validate all conî€›gurations and detect range speciî€›cation bugs, the following challenges must be addressed: Challenge I: How to validate conî€›gurations eî€ectively?Approaches proposed for analyzing î€ight control programs are generally based on static program analysis techniques that explore control and data dependencies [11,19,26]. However, such techniques are not suitable for validating conî€›guration because large numbers of speciî€›ed parameter values need to be analyzed. Unlike conventional bug detection techniques that statically analyze only small code snippets, the entire î€ight control program needs to be analyzed in order to achieve high code coverage. The reason is that diî€erent parameter values often result in execution î€ows involving very diî€erent portions of the control program. Unfortunately, î€ight control programs have huge sizes (e.g., over 700K lines of code) and complex control and data dependencies, Therefore, it is critical that the approach designed for conî€›guration validation be eî€ective, that is, able to provide high coverage of all possible parameter values. Challenge II: How to conduct an eî€œcient conî€›guration validation?Referring to the unstable î€ight states deî€›ned in Section 2, we need to validate each conî€›guration through either a realistic or simulated î€ight execution. Because of the large number of control parameters, each with its value range, changing the parameter values to generate conî€›gurations and validating all these conî€›gurations is ineî€œcient Completing the entire validation procedure may then end up requiring hours. Therefore, existing approaches [17,32], which analyze all possible conî€›gurations, are not suitable. An alternative approach is to use fuzzing [20] combined with a binary search [21] to reduce the search space of the combinations to be analyzed. However, although the search scope is narrowed, the execution time increases because each fuzzing iteration needs to wait until the validation feedback of the previous conî€›guration is obtained. Challenge III: How to balance the requirements of drone adaptability and î€ight stability?The î€ight control program supports diî€erent conî€›gurations to adapt to diî€erent î€ight missions. When high adaptability is required, each control parameter must have a large value range to adapt to diî€erent scenarios. As a result, the number of incorrect conî€›gurations will be higher, which will then aî€ect î€ight stability. On the other hand, when parameters have small value ranges, the possible conî€›gurations are limited, which reduces adaptability. Hence, complex î€ight missions cannot be carried out. Identifying proper value ranges is thus challenging. We now introduce our approaches to the above challenges. Solution I: Grey-box based fuzzing.Since dependencies of the î€ight control program are complex, static program analysis techniques are ineî€ective. Our approach is instead to conduct grey-box based fuzzing by setting various conî€›gurations and validating the corresponding î€ight states. In particular, we apply a GA to carry out fuzzing. The algorithm î€›rst selects some parameter values and validate the resulting parameter conî€›guration by analyzing the impact of the conî€›guration on the î€ight states. After that, referencing the validation result, the algorithm conducts a mutation to select more incorrect parameter values and set new conî€›gurations to validate, We repeat this process to search more incorrect conî€›gurations. This approach address challenges I and II. Solution II: Flight state prediction.Although the GA and the fuzzing reduce the search range of parameter values, the number of combinations is still huge. Therefore, validating all the corresponding conî€›gurations through realistic/simulation execution is highly ineî€œcient. In response, we designed a state generation approach that leverages a machine learning algorithm to train a state predictor. Instead of validating conî€›gurations through a realistic/simulation execution, the state predictor takes each conî€›guration as input and predicts the potential î€ight state to guide the mutation. Such an approach requires much less time than a realistic/simulation execution. It is important to note that data labeling and predictor generation are one-time costs. Hence it is still more eî€œcient to conduct a prediction rather than a realistic/simulation execution. The reason is that conî€›guration validation has to be executed iteratively when each new conî€›guration is generated by the mutation algorithm. This solution addresses Challenge II. Solution III: Multi-objective optimization.To balance adaptability and stability, we utilize a multi-objective optimization approach. Our approach estimates multiple feasible range guidelines according to the detected incorrect conî€›gurations. The optimization target is to eliminate the incorrect conî€›gurations (improve stability) while providing a wider range for parameters (improve adaptability). Each solution of the multi-objective optimization (i.e., range guideline) is the best solution in speciî€›c conditions, i.e., the optimal balance of adaptability and stability. Our approach allows users to choose an appropriate range from multiple range guidelines based on their requirements. This approach addresses Challenge III. In this section, we present the design of LGDFuzzer, our system for î€›nding range speciî€›cation bugs drone control programs through learning-guided mutation search. We î€›rst present an overview of its architecture and then the detailed design of its three components. LGDFuzzer (see Fig. 1) contains three components: state change predictor, a module to predict the î€ight state and assess whether a conî€›guration will lead to unstable states; learning-guided mutation searcher, a GA search module to detect incorrect conî€›gurations; range guideline estimator, a module to provide secure parameter range guidelines. LGDFuzzer relies on log î€›les that we generate by repeatedly executing drone î€ight missions (1). The log data is split into two parts, one is used by the state change predictor to generate features (2), and the other by the learning-guided mutation searcher to carry out cluster sampling (5). After that, the features are used to generate a predictor (34). The searcher clusters data and selects representatives samples from each cluster (6). The searcher then uses each representative sample to î€›nd out the corresponding incorrect conî€›gurations (7). It iteratively mutates the conî€›guration and uses the predictor to evaluate which conî€›guration is more likely to cause unstable states (8). When the iteration stop condition is satisî€›ed, the searcher merges the search result of each sample and generates a potentially incorrect conî€›guration set (9). Then, these potentially incorrect conî€›gurations are validated by a simulation (1011 12). Finally, by using the validation results, the estimator uses a multi-objective optimization to generate multiple feasible range guidelines that balance availability and stability under speciî€›c conditions (1314). Since there is no standard data set for testing drones, in order to extract features for the predictor and generate data for the searcher, we manually î€y a drone through a simulator to generate a number of î€ight logs. A î€ight log contains multiple entries and each entry consists of state information, sensor data, conî€›guration, and a timestamp index. Each î€ight is set up with the same î€ight test mission, AVC2013 [34], which is often used to test the drone mission execution capabilities. Such a î€ight mission is repeatedly executed with diî€erent conî€›gurations. We record all î€ight logs but discard those causing unstable states. Because unstable state data is uncontrollable and complex (compared to stable state data), dropping them prevents them from aî€ecting the training of the predictor. In our experiments, we recorded 308, 533 system log entries. As mentioned in Sec. 2, the control algorithm estimates the next reference state according to the current positioned state and sensor data. Based on this input-output control process, we leverage a machine learning (ML) predictor to emulate this input-output relationship and assess the impact of conî€›gurations. There are several reasons for using such a ML predictor. First, the diversity and î€exibility of ML predictors make them appropriate to emulate the non-linear input-output control process. Second, a ML predictor can estimate the next reference state while accurately reî€ecting the deviation caused by incorrect conî€›gurations. Speciî€›cally, for î€ight data, the learning algorithm trains a predictor that takes the positioned state, sensor data, and conî€›guration as input to predicts (estimates) the next reference state. Like the control algorithm, a large deviation between the predicted and actual reference states indicates that the î€ight state could become unstable. Two steps are executed to train the predictor: feature extraction and predictor generation. Feature Extraction.To train a predictor by using our dataset, the state, sensor data, and conî€›guration must be extracted from the original logs and represented according to a î€›xed feature vector format. Since we focus on the unstable states that inî€uence the angle (i.e., position and speed), our predictor considers: (i) angular position and angle speed of the state; (ii) data obtained from gyroscopes and accelerometers. A feature consists of a state unitğ‘, a sensor data unitğ‘ , and a conî€›guration unitğ‘¥. For ease of presentation, we refer to the combination of a speciî€›c state unit and sensor data unit as contextsğ‘ = {ğ‘, ğ‘ }. Finally, a feature consists of those three units with a timestamp and is normalized to a vectorğ‘£{ğ‘, ğ‘¥ }. Predictor Generation.According to previous research [6,7,28] the Long Short-Term Memory (LSTM) [16] technique can eî€œciently î€›t complex input/output of non-linear models. Therefore, we use LSTM as predictor for the next reference state. Speciî€›cally, for a pre-processed feature vector ğ‘£= {ğ‘, ğ‘¥} with timestamp ğ‘¡, LSTM takes a numberâ„of consecutive vectors with timestamp before ğ‘¡as input (i.e.,ğ‘‰ {ğ‘£, ..., ğ‘£, ğ‘£}), and returns the maximum conditional probability prediction of the next reference state units ğ‘. We show in Sec. 4 how we determine the best values forâ„. In the training stage, the weight of the predictor is iteratively updated to ensure that the predicted reference stateğ‘is closer to the ground truth stateğ‘. We use the Mean Squared Error (MSE) [36] loss for training. To search the incorrect conî€›gurations, we use a GA, that is, a metaheuristic search relying on biologically inspired operators such as mutation, crossover, and selection. Initially, as incorrect conî€›gurations may only produce unstable states in speciî€›c contexts (i.e., state and sensor data), our system conducts searches for multiple speciî€›c contexts to î€›nd incorrect conî€›gurations. We split the data from logs into multiple segmentsğ¶= {ğ‘, ğ‘, ..., ğ‘}, ğ‘– ğ‘šğ‘œğ‘‘ (â„+1) =0, whereğ‘–is the number of data. But considering that there are similarities among segments, searching for each one will generate a huge number of duplicate results. To address such issue, we cluster the segments and sample from these clusters in order to reduce redundancy while maintaining diversity. We leverage the meanshift [12], a probability density-based non-parametric adaptive clustering algorithm, to cluster segments and randomly sampleğ‘šrepresentative examples from each cluster for the subsequent search. Then, for each sample segment, the searcher carries out a GA search to explore incorrect conî€›gurations by iterative mutation, crossover, and selection. When the searcher has collected all the incorrect conî€›gurations for each sample, it merges and de-duplicates them as a unique set, referred to as set of potentially incorrect conî€›gurations. In what follows we provide details on the Fitness Evaluation Function, used to evaluate the î€›tness of a conî€›guration, and the Searching Process. Fitness Evaluation Function.The GA search applies î€›tness to quantify, by using the predictor, how much deviation a given conî€›guration may cause. Intuitively, the î€›tness evaluation function returns the probability of deviation for a conî€›guration. Speciî€›cally, assume that the current search is carried out for the context segment ğ¶{ğ‘, ...ğ‘, ğ‘}; the function selectsğ¶{ğ‘, ğ‘, ...ğ‘}to be used as input to the prediction model andğ‘ofğ‘to be used as ground truth for calculating the deviation. When evaluating a conî€›gurationğ‘¥{ğ‘¥, ..., ğ‘¥}(ğ·is the number of parameters), the function merges it with the segment to create featuresğ‘‰ {{ğ‘, ğ‘¥}, ..., {ğ‘, ğ‘¥}}. Then, such features are given as input to the predictor, which returns a predicted reference stateğ‘. The î€›tness is the L1-distance âˆ¥ğ‘âˆ’ ğ‘âˆ¥between the predicted state and the ground truth state. The goal of the search is then to î€›nd incorrect conî€›gurations, which are predicted to maximize the î€›tness, that is, the deviation (and thus maximize the probability of causing unstable states). Searching Process.For each context segment sample, the searcher î€›rst initializes a population whose individuals are conî€›gurations, and the parameter values of each conî€›guration are set to their default values. We assume that the population size isğ‘ ğ‘ƒand the maximum number of iterations isğº. The search process iteratively mutates and updates the population as follow: In theğ‘”-th generation iteration (ğ‘” âˆˆ [1, ğº]), the searcher î€›rst mutates the current populationğ‘ğ‘œğ‘{ğ‘¥..., ğ‘¥}and generates a variant populationğ‘ğ‘œğ‘{ğ‘¦, ..., ğ‘¦}. Each conî€›gurations of the variant population is obtained as follows: ğ‘¦= ğ‘¥+ ğ¹ âˆ— (ğ‘¥âˆ’ ğ‘¥) + ğ¹ âˆ— (ğ‘¥âˆ’ ğ‘¥ whereğ‘¥are random conî€›gurations,ğ‘¥is the best î€›tness conî€›guration, and ğ¹ is the scaling factor. Then, theğ‘ğ‘œğ‘is crossed over with theğ‘ğ‘œğ‘to produce a new experimental populationğ‘ğ‘œğ‘{ğ‘’, ..., ğ‘’}, whoseğ‘–-th conî€›guration isğ‘’{ğ‘’, ğ‘’, ..., ğ‘’}, whereğ‘— âˆˆ [1, ğ·]. The parameter values ğ‘’of each conî€›guration are calculated as follows: ğ‘’=ğ‘¦, ğ‘– ğ‘“ ğ‘Ÿğ‘ğ‘›ğ‘‘ (0, 1) < ğ¶ğ‘… ğ‘œğ‘Ÿ ğ‘— = ğ‘—ğ‘¥, ğ‘œğ‘¡â„ğ‘’ğ‘Ÿğ‘¤ğ‘–ğ‘ ğ‘’(2) whereğ‘—is a random integer in[1, ğ·],ğ¶ğ‘…is the crossover rate, ğ‘¥is the ğ‘—-th parameter value of ğ‘–-th conî€›guration in ğ‘ğ‘œğ‘, and ğ‘¦is the ğ‘—-th parameter value of ğ‘–-th conî€›guration in ğ‘ğ‘œğ‘. Then, the searcher evaluates the î€›tness of each conî€›guration both inğ‘ğ‘œğ‘andğ‘ğ‘œğ‘. According to their î€›tness, the searcher selects some conî€›gurations from the population to obtain the next generation of populationğ‘ğ‘œğ‘{ğ‘¥, ..., ğ‘¥}by using the following selection function: where ğ‘“ is the î€›tness evaluation function. Finally, if the î€›tness of each conî€›guration in the population does not longer increase or the maximum numberğºis reached, the search stops the mutation. We select the top 10 highest-î€›tness conî€›gurations from the î€›nal generation population as incorrect conî€›gurations. The predictor classiî€›es a conî€›guration as incorrect if the conî€›guration has a high probability of causing unstable states. However the set of all such conî€›gurations need to be validated to conî€›rm that they actually cause unstable states. We thus simulate the î€ight and use a monitor to observe which potential incorrect conî€›gurations actually lead to unstable states during the simulated execution. Speciî€›cally, the drone is set with potentially incorrect conî€›gurations to perform the AVC2013 î€ight mission. For î€ight freeze, if drone moves at distances always less than 0.5 meters in 15 seconds, the monitor identiî€›es this as a î€ight freeze. For deviations, if the î€ight deviation continues to be 15 times higher than 1.5 meters, the î€ight is considered a deviation. The range guidelines provided to the users should consider stability and adaptability, while at the same time eliminate discrete incorrect conî€›gurations and reserving relatively complete space for each parameter. Meanwhile, as we cannot ensure the stability of unveriî€›ed conî€›gurations, the estimation of range guidelines should reference the obtained validation results and refrains from considering incorrect conî€›gurations. Therefore, we leverage a multivariate optimization to determine the suitable range guideline. If all parameter values in a conî€›guration are within the range speciî€›ed by the guideline, we consider the conî€›guration to be covered. Our system attempts to estimate the range guideline (ğ‘…ğ‘ğ‘›ğ‘”ğ‘’) based on the following optimization problem: The optimization problem has two objectives: (a) to maximize the number of validated conî€›gurationsğ‘…covered by the guideline; (b) to minimize the coverage of incorrect conî€›gurations ğ‘…. If we shrink the allowed ranges to avoid incorrect conî€›gurations, the range of each parameter value would also shrink and vice versa. Therefore, instead of deî€›ning strict ranges for each parameter, the system solves this optimization problem with multiple constraints to obtain a diverse group of Pareto solutions. These Pareto solutions form a boundary consisting of the best feasible solutions, allowing users to select the best conî€›guration according to their requirements. The system provides this range guideline mainly based on the following considerations: (1) As the number of parameter increases, it would be diî€œcult to completely rule out insecure parameter values as the secure parameter ranges may not be continuous. (2) While stability is paramount, users may be willing to incur some minor risk for better controlling î€exibility or availability. We assessed LGDFuzzer by considering its eî€ectiveness and eî€œciency in validating conî€›gurations. The following research questions (RQs) were answered: â€¢ RQ1: Eî€ectiveness.How many incorrect conî€›gurations are detected by LGDFuzzer? â€¢ RQ2: Adaptability.Can LGDFuzzer provide the most suitable value ranges for parameters with minimum incorrect conî€›gurations? â€¢ RQ3: Enhancement.How do the mutation and the predictor help improve conî€›guration validation? We applied LGDFuzzer to an open source î€ight control program, ArduPilot(4.0.3) [35], which is widely used by drone manufacturers such as Parrot, Mamba, and Mateksys [33]. To validate conî€›gurations speciî€›ed in ArduPilot, we utilized three experimental vehicles (shown in Fig. 2) for testing, including two drones with Pixhawk [24] (i.e., CUAV ZD550 and AMOVLab Z410) and a drone simulator (i.e., Airsim [29]). Figure 2: Real and virtual drone vehicles used for experiments. According to the control parameter descriptions provided by the manufacturer, we selected 20 parameters that may aî€ect î€ight angular position and angular speed. We provide details about these parameters in Appendix A, including parameter name, speciî€›ed value range, default value, and parameter description. The predictor and the GA searcher are implemented in Python. Concerning the GA, its evolutionary stagnation judgement threshold is set to 0.1, the number of representativesğ‘šis set to 3. and the maximum number of evolutionary generations is set to 200. We further set the size of initial population to 1,000 (i.e.,ğ‘ ğ‘ƒ =1,000) and the scaling factor ğ¹ to 0.4. To evaluate whether LGDFuzzer identiî€›es incorrect conî€›gurations accurately, we î€›rst assess the prediction accuracy of the predictor in the î€ight state predictor phase. Then we validate whether the predicted incorrect conî€›gurations can impact î€ight stability. State Prediction.We labeled 1,500 conî€›gurations manually by sending the conî€›gurations to Airsim and recording the resulting î€ight states. Then we use previously collected log data about stable states to train and evaluate predictors with diî€erent input lengthsâ„. Speciî€›cally, our experiment î€›rst extracts the context (i.e., state and sensor data) segments from log data. Then, each labeled conî€›guration is randomly merged with a segment. We use the same method in Fitness Evaluation Function to calculate the deviation for each conî€›guration. If the deviation is greater than a threshold, the conî€›guration is considered unstable; it is considered stable, otherwise. In particular, we use the maximum deviation in the predictor training process, that is, the maximum deviation from the stable î€ight data, as the threshold. This experiment tests the accuracy, precision, and recall of predictors. The results are reported in Table 1. In general, the results show that our predictor is robust for different input lengths. The deviation values, observed from the experiments, of incorrect conî€›gurations are larger than the values of the correct conî€›gurations. That is the reason why we can use the Table 1: Predictor accuracy, precision and recall for diî€erent input lengths. predictor in the GA search to drive conî€›gurations to a higher deviation. In addition, the experiment results show that the recall of the prediction does not linearly increase whenâ„exceeds 4. Therefore, we choose the predictor with the best recall i.e.,â„ =4 as the input length to carry out the subsequent experiments. To assess the prediction accuracy for state changes, we send the conî€›gurations to Airsim and record the corresponding î€ight data. Then, we randomly select 150 consecutive features from the data and give them as input to the predictor to predict the state change and then compare it with the ground truth state. The results in Fig. 3 show that the predictions closely match the real states (an example of angle roll). In the î€›gure, the dotted curve denotes the actual states, the solid curve denotes the predicted states, and the histogram at the bottom (the blue bar) indicates the prediction errors as diî€erences between the two curves. The small error shows that the trained predictor is able to accurately predict î€ight state changes. Figure 3: Match between real behavior and prediction. Conî€›guration Validation. Given the predicted incorrect conî€›gurations, we validate them through realistic/simulation execution. Since the incorrect conî€›gurations may cause drone crash, we examine all of them on Airsim. For the 465 samples obtained by clustering and sampling, LGDFuzzer searches out 2,319 unique incorrect conî€›gurations. Finally, after a validation, 2,036 conî€›guration of 2,319 are marked as truly incorrect resulting in 500 Deviations, 2 Flight Freeze, 2 Crashes, 564 Incorrect Conî€›guration Tackling, and 968 Potential Thrust Losses. In this experiment, we use the previous validation results about incorrect conî€›gurations to estimate the range guidelines, and discuss how they balance the stability and adaptability. With reference to the validation results, the estimator î€›nds out 143 Pareto solution results (see Fig. 4). In the graph, the horizontal axis represents the number of validated conî€›gurations covered by the range guideline, and the vertical axis represents the ratio of incorrect conî€›guration in the range guideline. Each Pareto solution represents a feasible solution (i.e., range guideline) satisfying speciî€›c constraints (i.e., adaptability and stability constraints). For instance, we select some range guideline examples in Table 2 and further analyze their stability and adaptability. To illustrate the modiî€›ed guidelines generated by LGDFuzzer, we select several parameters out of 20 ones to show their details. The table shows three examples in which stability decreases and the conî€›gurable space (i.e., adaptability) increases. Guideline i avoids the majority of incorrect conî€›gurations. It covers 28 validated conî€›gurations, only one of which causes an unstable state; this means high stability. However, compared with the original parameter ranges, this guideline reduces too much the available space, which results in low adaptability. In contrast, Guideline iii reserves relatively complete ranges for the parameters. It covers 236 validated conî€›gurations but more than half of them (133) are incorrect, which results in low stability. Guideline ii is an intermediate choice, covering 91 validated conî€›gurations where only 29 are incorrect. If users have more stringent stability requirements, they can use the lower error ratio range guideline, at the cost of limiting the conî€›guration space and thus being unable to satisfy other î€ight requirements. On the contrary, if users have to satisfy special mission requirements (e.g., the mission is a time-limited task, or it needs a large î€ight angle to reach the target speed), they may consider sacriî€›cing a bit the stability in order to improve adaptability. In fact, they can choose an appropriate range guideline from the Pareto solutions according to their stability and adaptability requirements. To show the advantages of our system, we experimentally compare it with rvfuzzer [20], a recent approach for searching range speciî€›cation bug. rvfuzzer relies on the One-dimensional Mutation and Multi-dimensional Mutation search to detect incorrect conî€›gurations. By using One-dimensional Mutation, centered on the default parameter values, rvfuzzer separately conducts binary searches to narrow the upper and lower bounds until a midpoint value is obtained that does not any longer cause unstable states. In the Multi-dimensional Mutation multiple parameters are considered, each of which determines a novel binary mutation search conî€›gured with diî€erent extreme values (i.e., maximum and minimum) of the other parameters. All these experiments are based on the six parameters utilized in rvfuzzer (see Appendix A). In the experiments we consider the unstable states listed in Section 2.1. Comparison with Respect to Missed Incorrect Conî€›gurations. In the One-dimensional Mutation, the optimal solution may not be consistent with the right optimum. For example, using such a mutation strategy, the search indicates that the correct range for INS_POS1_Zshould remain within[âˆ’4.7,0.0]when it searches for the lower bound. However, there are still incorrect conî€›gurations, speciî€›cally betweenâˆ’1.0 and 0.0, inside this range. The reason is that, because of the binary search, as the î€›rst midpoint (i.e.,âˆ’2.5) does not cause an unstable state, the search directly skips values greater thanâˆ’2.5. It thus does not cover the[âˆ’1.0,0.0]space, which results in missing potentially incorrect conî€›gurations. In the case of multiple parameters mutations, we î€›rst apply the Multi-dimensional Mutation to determine the correct range. After that, based on this correct range, we leverage LGDFuzzer to start another search to evaluate whether there are incorrect conî€›gurations. LGDFuzzer still detects 727 potentially incorrect conî€›gurations, of which only 185 are conî€›rmed. Such result indicates that the ranges provided by the Multi-dimensional Mutation are not correct. In our opinion, the Multi-dimensional Mutation is still a one-dimension mutation, because it uses binary search to mutate parameters separately but only imports the extremes of the value ranges of other parameters. It can be regarded as multiple one-dimensional mutations with a limited correlation between control parameters; as such, it does not consider the inî€uence of values diî€erent from the extremes of the ranges. Comparison with Respect to Correct Range Guidelines.The six parameters utilized in this experiment are listed in Appendix A. We leverage LGDFuzzer to search incorrect conî€›gurations for these six parameters. The search detects 1,199 unique potentially incorrect conî€›gurations. Then the validation determines that 714 out of those conî€›gurations actually lead to unstable states. After that, we use them to generate the feasible range guidelines and choose the lowest incorrect ratio result. Table 3 lists the ranges obtained by three methods, where 1 is One-dimensional mutation, M is Multi-dimensional mutation, and GA (our Genetic Algorithm mutation). rvfuzzer method 1 obtains little reduction for each parameter range; thus it is not able to rule out incorrect conî€›gurations. M avoids some of the incorrect conî€›gurations but still misses others. Because our GA greatly reduces the ranges, it provides high stability. A special case is related toINS_POS3_Z; the range given by GA is larger than the range given by M. But this does not mean that our range forINS_POS3_Zis incorrect. The reason is that, as other parameters have a smaller range,INS_POS3_Zcan have a larger range, since the validity of conî€›gurations is decided based on multiple parameters instead of a single one. As for other parameters, ANGLE_MAX inî€uences the î€ight inclination angle. Under the inî€uence of other parameters, a too large value forANGLE_MAXis more likely to cause î€ight problems. A too small value for the waypoint speedWPNAV_SPEEDmakes the drone more likely to freeze; both M and GA reduce the lower part of the range for this parameter. ForINS_POS*_Z, the ranges returned by GA are smaller than the ones returned by M, and closer to default values. In fact, changingINS_POS*_Zinî€uences the position judgment of the inertial measurement unit. Therefore, this parameter should be changed carefully and should not deviate too much from the default value. PSC_VELXY_Paî€ects the output gain of the system for acceleration; a too large gain can easily cause drone deviations or thrust losses. Comparison with Respect to Time Requirements.We analyze the time taken by LGDFuzzer and Multi-dimensional Mutation. To determine the range guideline for the six parameters, we started multiple simulations and took 696 seconds to collect log data. The predictor takes 700 seconds to train until reaching convergence. The GA search takes another 156 seconds to iterate and update its population (1,000 conî€›gurations). Finally, from generating data to searching out 1,000 incorrect conî€›gurations, our system totally takes 1,552 seconds. Also, over 85% conî€›gurations are validated and detected to actually cause unstable states. On the contrary, depending on the conî€›guration values, rvfuzzer usually takes between 20 and 120 seconds per-round to complete a AVC2013. Even we allocate 1,552 seconds to Multi-dimensional Mutation, it can only carry out 78 rounds of mission test. In fact, the Multi-dimensional Mutation search takes more than 2 hours to estimate the guidelines for the six parameters. In addition, for 20 parameters, the time taken by toolMulti-dimensional Mutation increases exponentially. Instead, the time consumption of LGDFuzzer is still closer to the consumption of six parameters because the time required by predictor is almost unchanged. After obtaining the fuzzing results, we select several representative examples of unstable states to analyze their characteristics. Deviation.There are two main deviation situations in our experiment: overshoot and î€y away. When î€ying from one waypoint to another, the drone î€›rst accelerates to reach the target velocity and decelerates while approaching the next waypoint. If the conî€›guration is set improperly, the drone is unable to reduce its speed when close to the waypoint, which causes an overshoot deviation (see the video of a simulation in which the drone is not able to brake at [4]). In comparison, î€y away is much more dangerous, in that the drone keeps moving away from its mission-planned path. We leverage a real drone experiment to demonstrate the damage resulting from those unstable states. The experiment sets up a simple surround î€ight mission and a drone conî€›guration is set that, in the simulation test, results in a î€y away deviation. As shown in the video [3], the drone deviates from the mission-planned path. Unfortunately, even though we used the RC controller (remote manual controller) to manually switch its mode to land in order to stabilize the drone and make sure it would land slowly, the drone was still unable to stabilize and kept deviating. In fact, after checking the oî€Ÿine î€ight log, when we manually switched to the land mode, the system reported a switching error indicating that it could not stabilize and land. In other words, if the incorrect conî€›guration is not dealt with in time, the î€ight stability cannot be even corrected manually. Flight Freeze.There are two main situations of î€ight freeze. On the one hand, an incorrect control gain parameter makes the drone fail to reach the desired position in time, or prolong the time to reach a particular waypoint or the desired position. On the other hand, an invalid conî€›guration aî€ects the position and causes the drone to around-î€ight within a small area. We tested the around-î€ight situation with a real drone experience. The video at [5] shows that the drone keeps circling and is unable to complete the given mission. We analyzed the oî€Ÿine î€ight log, and from the log, we could see that the drone kept î€ight switching between althold (hovering î€y) and land but could not land successfully. Drone Crash.Drone crash may be caused by a rollover during takeoî€ or by hitting the ground due to improper descent speed. The video at [2] shows an actual example of a drone crashing when taking oî€. Potential Thrust Loss.This situation is mainly caused by an excessive change in angle or speed; the î€ight controller attempts to recover its position to normal, but the power of the motor cannot satisfy the requirement. Failure in recovering from the thrust loss or stabilizing the altitude would lead to a drone crash. In the postmortem analysis, we found that changing the control parameters related to the inertial measurement unit position (e.g.,INS_POS*_Z and the PID angle controller (e.g.,ATC_ANG_*_P/I/D) can force the drone into a gradually ampliî€›ed oscillation. Incorrect Conî€›guration Tackling.Incorrect conî€›guration tackling is the new type of error we deî€›ned. A control program usually contains a checking mechanism to prevent obvious errors in conî€›gurations. If the parameters are related to the position controller (e.g., ATC_*_*_P/I/D), the incorrect conî€›guration will raise a warning when the GCS tries to arm the drone. But in fact, if we update the conî€›guration after the drone take oî€, the problem is that the the control system still accepts it, which ultimately drives the drone to become unstable and out of control. The video at [1] shows an example in which uploading incorrect conî€›guration during the mission causes the drone to crash. A large number of drone fault detection methods/systems have been proposed to prevent errors during î€ights. Among them, Choi et al. [14] use the control invariant to identify physical attacks against robotic vehicles. But their approach is unable to prevent attacks that exploit the range speciî€›cation bug. To detect faulty components in a drone system, G. Heredia et al. [18] construct an observer model to estimate the output in fault-free conditions from the history inputs and outputs. Such a system utilizes the diî€erence between actual outputs and the predicted values to detect faulty sensor and actuator components, but it still does not consider the instability caused by range speciî€›cation bug. Ihab et al. [27] leverage the analytical redundancy between î€ight parameters to detect sensor faults. Like the other approaches, it only considers external factors. In comparison, LGDFuzzer implements a search system to avoid incorrect conî€›gurations that are instability factors within the system. From the perspective of learning-based testing, there are three relevant systems or methods. NEUZZ [31] is a gradient-guided search strategy. It uses a feed-forward neural network to approximate program branching behaviors and predicts the control î€ow edges to cover more test space, but the predictive model is not used for guiding testing. ExploitMeter [38] uses dynamic fuzzing tests with machine learning-based prediction to updates the belief in software exploitability. Yuqi et al. [9] use an LSTM network to model the input-output relation of the target system and a metaheuristic method to search for speciî€›c actuator commands that could drive the system into unsafe states. Konstantin et al. [8] apply a deep Q-learning [25] algorithm to generate an optimized policy for the fuzzing-based testing of PDF processing programs. DeepSmith [15] trains a generative model with a large corpus of open source code and uses this model to produce testing inputs to examine the OpenCL compiler automatically. These approaches make use of prior knowledge to drive the mutation input. Similarly, we introduced a machine learning model to guide the search test process. However, our LGDFuzzer combines the model with the genetic algorithm to carry out a large-scale multi-parameter search. Incorrect conî€›gurations of drone control parameters, set by legitimate users or worst sent by attackers, can result in î€ight instabilities disrupting drone missions. In this paper, we propose a fuzzing-based system that eî€œciently and eî€ectively detects the incorrect parameter conî€›gurations. LGDFuzzer uses a machine learning-guided fuzzing approach that uses a predictor, a genetic search algorithm, and multi-objective optimization to detect incorrect conî€›gurations and provide correct feasible ranges. We have experimentally compared LGDFuzzer with a state-of-the art tool. The experimental results show that LGDFuzzer is superior to such a tool in all respects. Even though our methodology has been designed for aerial drones, we believe that it can be used for other mobile devices with complex controls, such as underwater drones.