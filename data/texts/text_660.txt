Matrix factorization (MF) can extract the low-rank features and integrate the information of the data manifold distribution from high-dimensional data, which can consider the nonlinear neighbourhood information. Thus, MF has drawn wide attention for low-rank analysis of sparse big data, e.g., Collaborative Filtering (CF) Recommender Systems, Social Networks, and Quality of Service. However, the following two problems exist: 1) huge computational overhead for the construction of the Graph Similarity Matrix (GSM), and 2) huge memory overhead for the intermediate GSM. Therefore, GSM-based MF, e.g., kernel MF, graph regularized MF, etc., cannot be directly applied to the low-rank analysis of sparse big data on cloud and edge platforms. To solve this intractable problem for sparse big data analysis, we propose Locality Sensitive Hashing (LSH) aggregated MF (LSH-MF), which can solve the following problems: 1) The proposed probabilistic projection strategy of LSH-MF can avoid the construction of the GSM. Furthermore, LSH-MF can satisfy the requirement for the accurate projection of sparse big data. 2) To run LSH-MF for î€›ne-grained parallelization and online learning on GPUs, we also propose CULSH-MF, which works on CUDA parallelization. Experimental results show that CULSH-MF can not only reduce the computational time and memory overhead but also obtain higher accuracy. Compared with deep learning models, CULSH-MF can not only save training time but also achieve the same accuracy performance. CCS Concepts: bility. Additional Key Words and Phrases: CUDA Parallelization On GPU And Multiple GPUs, Graph Similarity Matrix (GSM), Locality Sensitive Hash (LSH), Matrix Factorization (MF), Online Learning For Sparse Big Data, Top-ğ¾ Nearest Neighbours. ACM Reference Format: Zixuan Li, Hao Li, Kenli Li*, Fan Wu, Lydia Chen, and Keqin Li. 2020. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data Analysis. 1, 1 (November 2020), 27 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Department of Electric Engineering, Mathematics and Computer Science, Distributed Systems, Delft â€¢ Computer systems organization â†’ Embedded systems; Redundancy; Robotics;â€¢ Networks â†’Network relia- In the era of big data, the data explosion problem has arisen. Thus, a real-time and accurate solution to alleviate information overload on industrial platforms is nontrivial [2]. Big data come from human daily needs, i.e., social relationships, medical data and recommendation data from e-commerce companies [31]. Moreover, due to the large scale and mutability of spatiotemporal data, sparsity widely exists in big data applications [52]. For accurate big-data processing, representation learning can eliminate redundant information and extract the inherent features of big data, which makes big-data analysis and processing more accurate and eî€œcient [3]. Furthermore, for sparse data from social networks and recommendation systems, low-rank representation learning can extract features as latent variables to represent the node and user properties from the high-dimension space, which can alleviate the information loss owing to missing data [62]. MF is the state-of-the-art unsupervised representation learning model with the same role as Principal Component Analysis (PCA) and an autoencoder that can project the high-dimensional space into the low-rank space [9]. Due to its powerful extraction capability for big data, linear and nonlinear dimensionality reduction is widely used as an emerging low-rank representation learning model [4]. As one of the most popular dimensionality reduction models, MF can factorize high-dimensional data into two low-rank factor matrices via the constraints of prior knowledge, i.e., distance metrics and regularization items [19]. Then, MF uses the product of the two low-rank matrices to represent the original high-dimension data, which endows the MF with a strong generalization ability [40]. However, due to the variety of big data, e.g., multiple attributes of images [55], context-aware text information [28], etc., linear MF is not applicable to an environment with hierarchical information; thus, it should consider the inherent information of big data [1]. Nonlinear MF, e.g., neural MF [60] and the graph for manifold data [46] [34], which relies on the construction of the GSM, can mine deep explicit and implicit information. However, the Deep Learning (DL) model for neural MF needs multilayer parameters to extract inherent variables, which can limit the training speed and create huge spatial overhead for constructing a GSM; thus, DL cannot be adopted by industrial big data platforms. Thus, modern industrial platforms are anxious to save parameters in nonlinear MF models [67]. Neighbourhood information for nonlinear MF is an emerging topic [68] [23]. The neighbourhood model can strengthen the feature representation by capturing the strong relationship points within the data; and this model is popular in Recommendation Systems, Social Networks, and Quality of Service (QoS) [29] [67]. Handling neighbourhood information is based on several important neighbourhood points that should construct a GSM [51], [67]. However, the use of the GSM should consider the following two problems: 1) the selection and deî€›nition of the similarity function should be accurate, and 2) the huge time and spatial overhead caused by the construction of the GSM. The î€›rst problem can be solved by using DL to select the best similarity [10]. However, the huge computational costs make DL unsuitable for cloud-side platforms. The construction of the GSM takes a huge amount of time and spatial overhead, and its parallelization is diî€œcult. Due to the quadratically increased spatial costs, the second problem is fatal to real applications using high-dimensional data. In this case, the approximated strategy is considered to replace the calculation of the GSM. LSH is a statistical estimation technique that is widely used in high-dimensional data for the Approximate Nearest Neighbourhood (ANN), and it maps the high-dimensional data to low-dimensional latent space using random projection, which can simplify the approximated search problem into a matching lookup problem [41]. Due to low time complexity, LSH has a fast processing capability for high-dimensional data [66]. Furthermore, LSH has the following drawbacks: 1) the LSH scheme has a slight loss of accuracy; 2) the use of DL can lead to high-precision hashes, but DL is not applicable to cloud-side platforms; 3) online tracking of the hash value for incremental big data; 4) due to information missing, Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data the similarity between sparse data is not very accurate and should be handled by a speciî€›c LSH function. Thus, it is nontrivial to achieve a reasonable accuracy in less time with î€›ne-grained parallelization for LSH. Furthermore, with the rapid development of GPU-based cloud-edge computing, increasingly more vendors will tend to use GPU acceleration [37]. There are three challenges to aggregate LSH with nonlinear MF eî€œciently to extract the deep features of sparse and high-dimensional data: 1) How can a suitable LSH function be deî€›ned to reduce the computation time while ensuring reasonable accuracy? 2) How can LSH be accommodated with the nonlinear neighbourhood MF to achieve low spatial overhead in an online way? 3) How can a GPU and multiple GPUs be used to achieve a faster calculation speed? This work is proposed to solve the above problems, and the main contributions are presented as follows: (1)A novel Stochastic Gradient Descent (SGD) algorithm for MF on a GPU (CUSGD++) is proposed. This method can utilize the GPU registers more and disentangle the involved parameters. The experimental results show that it achieves the fastest speed compared to the state-of-the-art algorithms. (2)simLSH is proposed to replace the GSM and accomplish sparse data encoding. simLSH can greatly reduce the time and memory overheads and improve the overall approximation accuracy. Furthermore, an online method for simLSH is proposed for incremental data. (3)The proposed CULSH-MF can combine the access optimization on GPU memory of CUSGD++ and the neighbourhood information of simLSH for nonlinear MF. Thus, CULSH-MF can complete the training very fast and attain an 8000 compared to CUSGD++. Compared with deep learning models, CULSH-MF can achieve the same eî€ect, and CULSH-MF only needs to spend 0.01% of the training time. In this work, related works and preliminary î€›ndings are presented in Sections 2 and 3, respectively. The proposed model for LSH aggregated MF is presented in Section 4. Experiment results are shown in Section 5. Owing to the powerful low-rank generalization ability, MF is widely used in various î€›elds of big data processing, i.e., Source Localization [ mender Systems [ LSH is a powerful hashing tool that can also strengthen the performance of nonlinear dimension reduction, including PCA and MF, for recommendation [ in optimization and machine learning communities, e.g., Maximum Margin Matrix Factorization (MMMF) [ Nonnegative Matrix Factorization (NMF) [ Matrix Factorization (WMF)[ classic nonconvex problem [ SGD [59] [64], or Cyclic Coordinate Descent (CCD) [47], is adopted to solve this nonconvex problem. An eî€œcient big data processing method requires highly eî€œcient hardware and algorithms. The rapid development and good performance of GPUs also tend to accelerate basic optimization algorithms that consider the global memory access, threads and thread block synchronization on a GPU. Thus, the parallelization processes of related methods on GPUs have unique specialties. on a GPU. ğ‘‹ğ‘–ğ‘’ ğ‘’ğ‘¡ ğ‘ğ‘™. [59] proposed cuSGD based on data parallelization. cuSGD [59] achieves the goal of acceleration by adopting data parallelization on a GPU, and it has no load imbalance problem. algorithm and proposed the GPU-based CCD++ algorithm. 25], [24], Hyperspectral Image Classiî€›cation [65] and Biological Data Analysis [11]. Furthermore, tuple multiplication and summation and CUMSGD based on the elimination of row and column dependencies. These basic algorithms have good performance on a GPU. However, scalability is not considered, which results in signiî€›cant limitations of model compatibility. Nonlinear MF comprises two components, i.e., a DL model for neural MF [61] and a neighbourhood model with GSM for graph MF [29] [14].ğ»ğ‘’ ğ‘’ğ‘¡ ğ‘ğ‘™.[18] proposed Neural Collaborative Filtering (NCF) using the DL model, and this model involves a multilayer neural network that can extract the low-rank feature of MF [61]. The neighbourhood model is often integrated into the algorithm and brings better results [29] [14]. The construction of a GSM requires calculating the similarity between high-dimensional points, the choice of similarity functions play a key role in speciî€›c environments, and the selection of the Top-ğ¾nearest neighbours from the GSM is time consuming [26]. However, designing an eî€ective similarity function is a diî€œcult task. Research on training similarities through DL is emerging [15]. However, high-dimensional data cause the computational complexity of DL to dramatically increase. In order to further optimize the calculation and save space, pruning strategies and approximation algorithms have been proposed [12]. LSH is such an approximate algorithm based on probability projection [44]. Furthermore, the inverse use of LSH can also achieve the farthest neighbour search [64]. However, most LSH algorithms do not work well in sparse data environments. minLSH is able to calculate the similarity between sets, but does not consider the weights of the elements in the set. Although a considerable amount of work has sought to improve minLSH, this work increases the complexity [56]. simHash [44] showed good performance in similar text detection. LSH can project the feature vectors of similar items to equal hash values with a high probability [20], and this makes LSH widely used for nearest neighbour searches, fast high-dimensional information searches, and similarity connections [36,63]. Due to the inherent sparsity of big data, using LSH to construct a GSM to aggregate sparse MF on a big data platform is nontrivial work. Furthermore, the accuracy of the low-rank tracking of online learning for incremental big data is a key problem [27]. ğ¶â„ğ‘’ğ‘› ğ‘’ğ‘¡ ğ‘ğ‘™. proposed an online hash for incremental data [7]. However, there is a lack of an online LSH strategy for sparse and online data on parallel and distributed platforms. In this section, LSH for neighbouring points with closer projective hash values is presented in Section 3.1. The basic MF model and nonlinear MF with notations are introduced in Section 3.2, and the related symbols are listed in Table 1. Deî€›nition 3.1 (Graph Similarity Matrix (GSM)). We assume 2 sets asğ¼ = {ğ¼, Â· Â· Â· , ğ¼, Â· Â· Â· , ğ¼}andğ½ = {ğ½, Â· Â· Â· , ğ½, Â· Â· Â· , ğ½}. Given two variables{ğ½, ğ½} âˆˆ ğ½and a similarity functionS(ğ‘—||ğ‘—), the goal is to construct a weighted fully directed graphG, where each vertex represents a variable inğ½, and the weight of each edge represents the similarity of the output vertex to the input vertex calculated byS(ğ‘—||ğ‘—). The construction of GSMGshould consider the relationî€ˆî€‰ between ğ½ and ğ¼. The value of Grelies on{ğ‘Ÿ|ğ‘– âˆˆbÎ©}, {ğ‘Ÿ|ğ‘– âˆˆbÎ©}. The neighbourhood similarity query for variable setğ½relies on the GSMGâˆˆ R[29] [67] [20]. The most important problem in the neighbourhood model is to î€›nd a set of Top-ğ¾similar variables. For this problem, the Top-ğ¾ nearest neighbours query is emerging. Deî€›nition 3.2 (Top-ğ¾Nearest Neighbours). Given a set of variablesS, each variable as a vertex constitutes a fully directed graphG. The goal is to î€›nd a subgraphSwhere each vertex hasKand onlyKout edges point to the vertices of its Top-K similar variables. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data By querying the GSM, the Topof the GSM is huge. If variable set overhead for the Topnearest neighbours for the variable set overall overhead is GSM using high-dimensional sparse big data is not advisable. In the context of high-dimensional sparse big data, the calculation costs of a GSM are squared. In this case, we need to reduce unnecessary calculations or î€›nd an alternative method. LSH is a probabilistic projection method that projects two similar variables with a high probability to the same hash value while two dissimilar variables are projected to diî€erent hash values with a high probability. We need to judge the similarity between the two variables and î€›nd the Top-ğ¾ nearest neighbours for each variable. ğ‘Ÿ(ğ‘–, ğ‘—)th element in matrix R; Î©The set ğ‘— of non-zero value in matrix R for variable ğ¼; bÎ©The set ğ‘– of non-zero value in matrix R for variable ğ½; U/ğ‘¢Left low-rank feature matrix âˆˆ R/ ğ‘–th row; V/ğ‘£Right low-rank feature matrix âˆˆ R/ ğ‘— th row; ğ‘The deviation between variable ğ¼âˆˆ ğ¼ and ğœ‡; bğ‘The deviation between variable ğ½âˆˆ ğ½ and ğœ‡; ğ‘Overall baseline rating = ğœ‡ + ğ‘+bğ‘; ğ‘›The number of entries in variable set ğ¼ which have relations withVariables {ğ‘—, ğ‘—} in variable set ğ½ ; ğœŒPearson similarity of two variables { ğ‘—, ğ‘—} âˆˆ ğ½ ; ğ‘†âˆˆ ğ½ ; WExplicit inî€uence matrix âˆˆ Rto represent the degree of explicitInî€uence for variable set ğ½ ; CImplicit inî€uence matrix âˆˆ Rto represent the degree of implicitInî€uence for variable set ğ½ ; ğ‘¤/ğ‘¤ğ‘—th Explicit inî€uence vector âˆˆ Rof W / the ğ‘˜th element of ğ‘¤; ğ‘/ğ‘ğ‘—th Implicit inî€uence âˆˆ Rof C / the ğ‘˜th element of ğ‘; ğ‘‚ğ‘(2ğ¾ +1) + ğ‘ (ğ¾ âˆ’ ğ¾âˆ’1), and the spatial overhead isğ‘‚ (ğ‘ ğ¾). Thus, the construction of a Deî€›nition 3.3 (Locality Sensitive Hash (LSH)). The LSH function is a hash function that satisî€›es the following two points: â€¢For any pointsğ‘¥andğ‘¦inRthat are close to each other, there is a high probabilityğ‘ƒthat they are mapped to the same hash value ğ‘ƒ[â„(ğ‘¥) = â„(ğ‘¦)] â©¾ ğ‘ƒfor||ğ‘¥ âˆ’ ğ‘¦||â©½ ğ‘…; and â€¢For any pointsğ‘¥andğ‘¦inRthat are far apart, there is a low probabilityğ‘ƒ< ğ‘ƒthat they are mapped to the same hash value ğ‘ƒ[â„(ğ‘¥) = â„(ğ‘¦)] â©½ ğ‘ƒfor||ğ‘¥ âˆ’ ğ‘¦||â©¾ ğ‘ğ‘…= ğ‘…. The use of LSH has allowed us to reduce the complexity from ğ‘‚ (ğ‘) to ğ‘‚(ğ‘ ). As Fig. 1 shows, the construction of a GSM requiresğ‘‚ (ğ‘)similarity calculations and consumesğ‘‚ (ğ‘)space while the calculation and spatial consumption of LSH is ğ‘‚(ğ‘ ). LSH can alleviate the problem of huge computational overhead. However, there are several problems when the LSH is applied to a system with a neighbourhood model: 1) How can a system with a neighbourhood model using LSH obtain the same overall accuracy as the original method? 2) How can the computational model for LSH be incorporated in a big data processing system? 3) How can the system with the LSH model accommodate online learning for incremental data? In big data analysis communities, representation learning can disentangle the explicit and implicit information behind the data, and the low-rank representation problem is presented as follows. Deî€›nition 3.4 (Representation Learning for Sparse Matrix [3]). Assume a sparse matrixR âˆˆ Rpresents the relationship of 2 variable sets{ğ¼, ğ½ }. The valueğ‘Ÿrepresents the relation degree of the variables{ğ¼}inğ¼and{ğ½}in ğ½. Due to missing information, the representation learning task for variable{ğ¼}trains the feature vectorğ‘¢relying on nonzero values {ğ‘Ÿ| ğ‘— âˆˆ Î©}, and the representation learning task for variable {ğ½} is to train the feature vector ğ‘£ relying on nonzero values {ğ‘Ÿ|ğ‘– âˆˆbÎ©}. Deî€›nition 3.5 (Sparse Matrix Low-rank Approximation). Assume a sparse matrixR âˆˆ Rand a divergence functionî€€î€ DRâˆ¥bRthat evaluates the distance between two matrices. The purpose of the low-rank approximation is to î€›nd an optimal low-rank matrixbR and then minimize the divergence. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data MF only involves low-rank feature matrices, and the feature vectors are used for cluster and social community detection [ is applied to this problem because it factorizes the sparse matrix into two low-rank feature matrices. In addition, MF model has two limitations: 1) this model is too shallow to capture more aî€Ÿuent features, and 2) this model cannot capture dynamic features. There are 4 parts in Equation (1), and those parameters can combine the explicit and implicit information of the neighbourhood for nonlinear MF, which are introduced as follows[29] [67] [20]: 1â—‹ {ğœ‡, ğ‘ ğ½in set ğ½ . Considering that diî€erent variables ğ¼ diî€erent variables supposeğœ‡ variableğ½Ã ğœ‡ = average relation of the known elements in relation of the known elements in ğ½ is the number of variables variables{ğ½ regularization parameter that adjusts the importance. By searching for the GSM, the Topofğ½with explicit relation with variable variable{ğ½ the above explicit relations. Feature vectors ğ‘†( ğ‘—)of variable ğ½. The closer the basic predicted value by a scaling factor 3â—‹ {ğ‘ (ğ‘–), ğ‘ relation with the variable the variable vectorsğ‘ 9]. A sparse matrix has only a few elements that are valuable, and all other elements are zero. Sparse MF |{z}|{z}(1) ,bğ‘, ğ‘}: The baseline score is represented asğ‘= ğœ‡ + ğ‘+bğ‘for the relation of variableğ¼âˆˆ ğ¼and variable ğ½âˆˆ ğ½have their own diî€erent preferences for the entire variable setğ¼. To simplify the description, is the overall relation between variable setğ¼and variable setğ½;ğ‘represents the deviation between variable ğœ‡, which indicates the preference of variableğ¼to variable setğ½; andbğ‘represents the deviation between âˆˆ ğ½andğœ‡, which indicates the preference of variableğ½to variable setğ¼. A simple case is presented as:Ã ğ‘Ÿ/|Î©|(the average relation of the known elements),ğ‘=ğ‘Ÿ/|Î©| âˆ’ ğœ‡(the diî€erence between the , ğ‘†( ğ‘—), ğ‘…(ğ‘–), ğ‘…(ğ‘–;ğ‘—), ğ‘¤}: Suppose thatğ½andğ½are any two variables inğ½, andğ‘›= |bÎ©bÎ©| , ğ½} âˆˆ ğ½as a baseline. The( ğ‘—, ğ‘—)th element of GSM is deî€›ned asğ‘†=ğœŒ, whereğœ†is the of the variableğ½âˆˆ ğ½can be obtained. To retain the generalizability,ğ‘…(ğ‘–)is denoted as the variable subset } âˆˆ ğ‘…(ğ‘–;ğ‘—) = ğ‘…(ğ‘–)ğ‘†( ğ‘—), variableğ¼âˆˆ ğ¼has more explicit relations with variableğ½. We parameterize ğ½.ğ‘¤is used to represent the information gain that variableğ½âˆˆ ğ‘…(ğ‘–;ğ‘—)explicitly brings toğ½âˆˆ )is used as the coeî€œcient ofğ‘¤. Combining all(ğ‘Ÿâˆ’ ğ‘)ğ‘¤,ğ½âˆˆ ğ‘…(ğ‘–;ğ‘—)and multiplying the resultî€Œî€Œî€Œî€Œ (ğ‘–;ğ‘—), ğ‘}: To retain the generalizability,ğ‘ (ğ‘–)is denoted as the variable subset ofğ½with an implicitÃ‘ ğ¼âˆˆ ğ¼has more implicit relations with variableğ½. We parameterize the above implicit relations. Feature âˆˆ Rare used as the implicit factors for the Top-ğ¾nearest neighboursğ‘†( ğ‘—)of a variableğ½.ğ‘is used to represent the information gain that variableğ½âˆˆ ğ‘(ğ‘–;ğ‘—)implicitly brings to variableğ½âˆˆ ğ½. Combining allğ‘,î€Œî€Œî€Œî€Œ ğ½âˆˆ ğ‘(ğ‘–; ğ‘— ) and multiplying the result by a scaling factorî€Œğ‘(ğ‘–; ğ‘— )î€Œ, we obtainî€Œğ‘(ğ‘–; ğ‘— )î€ŒÃğ‘. 4â—‹ {ğ‘¢, ğ‘£}: Original MF model.ğ‘¢is the low-rank feature vector for variableğ¼âˆˆ ğ¼, andğ‘£is the low-rank feature vector for variable ğ½âˆˆ ğ½ . With the neighbourhood consideration andğ¿norm constraints for the parameters{U, V, ğœ‡, ğ‘,bğ‘, ğ‘¤, ğ‘}, the optimization objective is presented as: where {ğœ†, ğœ†, ğœ†, ğœ†, ğœ†, ğ‘ğ‘›ğ‘‘ğœ†} are the corresponding regularization parameters. There are two improvements: 1) the neighbourhood inî€uences are inherent in some big data applications [1] [68] [22], and 2) the Top-ğ¾nearest neighbourhood with explicit and implicit information can replace all queries of neighbourhood points [29] [67] [20]. Fig. 2 illustrates the structure of this work. First, we consider the interaction value of variableğ¼in variable setğ¼ and variableğ½in variable setğ½and generate the interaction matrixRfrom this. Second, the original method, which is based on the GSM, can calculate the similarity of every two variablesğ½andğ½in variable setğ½to generate a similarity graphG; and queryingGto obtain the subgraphScan hold the Top-ğ¾nearest neighbours of each variableğ½âˆˆ ğ½. The diî€erence is that the simLSH method we proposed constructs a hash table throughğ‘coarse-grained hashings and ğ‘î€›ne-grained hashings. Then, we obtain the subgraphSthrough the hash table. Finally, we train the feature vectors using the updating rule (5). As Fig. 2 shows, this work should consider the following three parts: 1) Interaction matrixRof two variable sets {ğ¼, ğ½ }, which should consider the incremental data and add the coupling ability of the overall system. 2) The construction of a neighbourhood relationship should reduce the overall space and computational overhead and maintain the overall accuracy. 3) Training the representation feature vectors in a low computational and high accuracy way. The above objectives guide this section. In this section, LSH for sparse big data and CUDA parallelization are presented in Section 4.1; and then stochastic optimization strategy, CUDA parallelization and multiple GPUs for sparse big data are presented in Section 4.2, Finally, the online learning solution is presented in Section 4.3. The Top-ğ¾nearest neighbours, which relies on the construction of the GSM, is a key step in the nonlinear neighbourhood model. However, the GSM requires a huge amount of calculations, and the time complexity isğ‘‚ (ğ‘)based on the Pearson similarity. A variety of LSH functions are not friendly to sparse data, Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data because the accuracy of most distance measures will be greatly reduced. This is caused by there being very few positions where the nonzero elements of each vector are the same. The Jaccard similarity is suitable for sparse data, and its representative algorithm is minHash[ and neglects the real value. In order to solve this problem, simLSH, which is inspired by simHash applied to text data, is proposed for sparse dig data projection [ value of the elements and maintains low computational complexity. simLSH can eî€ectively combine the number of interactions of variable sets the computational complexity. simLSH is comprised of the following two parts: 1) Coding for Sparse Big Data: simLSH randomly generates value. The hash value hash value diî€erentğ‘Ÿ can be expressed as: ğ»should also be ağº-bits{0,1}string. After the hash valueğ»for variableğ½âˆˆ ğ½is calculated, we obtain by accumulatingÎ¦(ğ») Â· Î¨(ğ‘Ÿ),ğ‘– âˆˆ Î©.Î¨(ğ‘Ÿ)is a function such that there is a suitable interval between s, andÎ¦(ğ»)is a function that mapsğ»from{0,1}to{âˆ’1, +1}. Finally,Î¥()maps the nonnegative value {1}and the negative value to{0}. Then, theğº-bit{0,1}stringğ»is obtained. The entire process of simLSH As Fig. 3 shows, variableğ½has three relation valuesğ‘Ÿ{3,4,5}with{ğ‘–, ğ‘–, ğ‘–} âˆˆ Î©. Whenğº =3,{ğ», ğ», ğ»}areî€ˆî€‰ randomly assigned to{001,010,100}, respectively. It takesÎ¨(ğ‘Ÿ) = ğ‘Ÿby calculating(âˆ’3âˆ’4+5), (âˆ’3+4âˆ’5), (3âˆ’4âˆ’5); and then, theğºpositions{âˆ’2, âˆ’4, âˆ’6}ofğ»are obtained, respectively. Finally, we obtain theğº-bit{0,1}stringğ» {0, 0, 0} by mapping operations. 2)Coarse-grained and Fine-grained Hashing: LSH is an approximation method to estimate the GSM, but it will achieve accuracy losses when applied to sparse big data. In this case, simLSH is proposed to speed up the calculations and improve the accuracy. Since the maximum probability of two extremely dissimilar variables {ğ½, ğ½} with the same hash value is ğ‘ƒ, the mapping of a hash function does not guarantee that the variables{ğ½, ğ½}with the same hash value are similar. In order to alleviate this situation, the multiple random mapping strategy is considered as follows. (1) Coarse-grained Hashing: Similar variables with the same hash values of all mappings are considered. Ifğ‘random mappings are conducted, whereğ‘ â‰ª ğ‘, the probability of two dissimilar variables projected as similar pairs is reduced to at mostğ‘ƒ. Furthermore, the probability of two similar variables projected as similar pairs is also reduced to at leastğ‘ƒ. Under this condition, many similar variable pairs will be missed. (2) Fine-grained Hashing: In this strategy, as long as at least one of the two variables{ğ½, ğ½}projected as similar pairs is subjected to coarse-grained hashing, the similar variable pairs{ğ½, ğ½}are selected. Suppose thatğ‘coarse-grained hashings are conducted. The probability of two similar variables{ğ½, ğ½}projected as similar pairs is increase to at least 1âˆ’ (1âˆ’ ğ‘ƒ). By increasing the values of Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data ğ‘andğ‘, the probability that two similar pairs of variables method can improve the probability, and its calculation amount is sizes ofğ‘ variable pairs, which can reduce the computational complexity to probability, the computational complexity is only ğ‘ Ã— ğ‘ Ã— ğ‘ , and ğ‘ Ã— ğ‘ Ã— ğ‘ is much smaller than ğ‘ Our goal is to î€›nd the Topnearest neighbours for We use the coarse-grained and î€›ne-grained hashing of simLSH and select the ğ½in the hash table of variable each thread block for simLSH (CULSH) manages a variable 1âˆ’9: The calculation of simLSH with coarse-grained hashing and î€›ne-grained hashing. In lines 3-5, calculate the hash valueğ»for variable Search the Top-ğ¾ nearest neighbours {ğ½ Input: Sparse matrix R of variable sets {ğ¼, ğ½ }, Random Hash values ğ» 4.2 Stochastic Optimization Strategy And CUDA Parallelization On GPUs And Multiple GPUs The basic optimization objective (2) involves 6 tangled parameters strategy of SGD in [ the optimization objective (2) is nonconvex, and alternative minimization is adopted [ andğ‘. Before the model training, we only need to perform multiple simLSHs onğ‘variables to î€›nd similar : The Top-ğ¾Nearest Neighbors MatrixJâˆˆ R. Each row represents the Top-ğ¾Nearest Neighbors of âˆˆ ğ½ . disentangle the involved parameters as follows: SGD is a powerful optimization strategy for large-scale optimization problems [17] [54]. Using SGD to solve the optimization problem (4) is presented as: where the parameters{ğ›¾,ğ›¾,ğ›¾,ğ›¾,ğ›¾,ğ›¾}are the corresponding learning rates andğ‘’= ğ‘Ÿâˆ’bğ‘Ÿ. The update rule (5) has parallel inherence. Then, the proposed CULSH-MF is comprised of the following three steps: 1)Basic Optimization Structure(CUSGD++): CUSGD++ only considers the basic two parameters{U, V}. Compared with cuSGD, CUSGD++ has the following two advantages: (1) Due to the higher usage of GPU registers in Stream Multiprocessors (SMs),ğ‘¢orğ‘£can be updated in the registers, avoiding the time overhead caused by a large number of memory accesses. The memory access model is illustrated in Fig. 4. SM{1,2}update{ğ‘¢, ğ‘¢}in the registers,î€ˆ respectively; and{ğ‘£, ğ‘£, ğ‘£, ğ‘£, ğ‘£, ğ‘£, ğ‘£}, {ğ‘£, ğ‘£, ğ‘£, ğ‘£, ğ‘£, ğ‘£,î€‰ ğ‘£}are returned to global memory after each update step. (2) Due to the disentanglement of the parameters in the update rule (5), the data access conî€ict is reduced, which ensures a high access speed. From the update rule (5), the update processes of{U, V}are symmetric. Algorithm 2 only describes the update process of{U}in the reg-î€ˆî€‰ isters as follows:(1)Lines 2âˆ’3: Givenğ‘‡ ğµthread blocks, feature vectorsğ‘¢|ğ‘– âˆˆ {1, Â· Â· Â· , ğ‘€}are evenly assignedî€ˆî€‰ to thread blocksğ‘‡ ğµ|ğ‘¡ğ‘_ğ‘–ğ‘‘ğ‘¥ âˆˆ {1, Â· Â· Â· ,ğ‘‡ ğµ}. Each thread blockğ‘‡ ğµreads its own feature vectorğ‘¢from the global memory into the registers.(2)Line 4: The feature vectorğ‘¢with all nonzero values{ğ‘Ÿ| ğ‘— âˆˆ Î©}in the thread blockğ‘‡ ğµis updated.(3)Lines 5âˆ’7: Use the warp shuî€Ÿe instructions [34] to accelerate the dot product Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data ğ‘¢ğ‘£of two vectors ters that are faster than shared memory and does not involve thread synchronization. Furthermore, this technology aligns and merges memory to reduce the access time. The number of threads in a form is 32, and elementsî€ˆ ğ‘‡|ğ‘¡_ğ‘–ğ‘‘ğ‘¥ âˆˆ { elements corresponding products the dot product rereading from global memory for the next update, and feature vectors Line 11: After all nonzero values will no longer be used. 2)Aggregated Model imbalance does not aî€ect the serial model. However, it obviously aî€ects the running speed of the parallel model. The most signiî€›cant impacts are the following two points: (1) discontinuous memory access, and (2) imbalanced load on each thread this section. In CULSH-MF, the adjustment takes the setÃ each variable expression, we use align and merge memory to reduce the overhead for GPU memory access, are used. 1, Â· Â· Â· ,32}. A threadğ‘‡in each thread blockğ‘‡ ğµsequentially reads the correspondingî€ˆî€‰ ğ‘¢, ğ‘£|ğ‘“%32= ğ‘¡_ğ‘–ğ‘‘ğ‘¥, ğ‘“ âˆˆ {1, Â· Â· Â· , ğ¹ }in feature vectors{ğ‘¢, ğ‘£}, and the threadğ‘‡calculates theî€ˆî€‰ ğ‘¢ğ‘£=ğ‘¢ğ‘£.(4)Lines 8âˆ’10: Feature vectorsğ‘¢are updated in the registers to avoid ğ‘‡. In order to solve the above problems, an adjustment for the parameters{W, C}is proposed in ğ‘(ğ‘–;ğ‘—),ğ‘…(ğ‘–;ğ‘—)ğ‘(ğ‘–;ğ‘—) = âˆ…. Thus, the number of the involved elements for{W, C}are equal andî€ˆî€‰ ğ½involves 2ğ¾parameters{ğ‘¤|ğ‘˜ âˆˆ {1, Â· Â· Â· , ğ¾}}, {ğ‘|ğ‘˜ âˆˆ {1, Â· Â· Â· , ğ¾}}. For the convenience of the ğ‘˜andğ‘˜to represent the indexes ofğ‘—andğ‘—in theseğ¾parameters, respectively, which means thatÃ are represented asğ‘¤andğ‘, respectively. The computational process of(ğ‘Ÿâˆ’ğ‘)ğ‘¤ ğ‘involves the dot product and summation operations. Thus, thewarp shuî€Ÿe instructions, which can Algorithm 2: CUSGD++ Input: Initialization of low-rank feature matrices {U, V}, interaction matrix R, learning rate {ğ›¾,ğ›¾}, regularization parameter {ğœ†, ğœ†}, and training epoches ğ‘’ğ‘ğ‘œ. Output: U. CULSH-MF also takes advantage of the register to reduce the memory access overhead and then increase the overall speed. Due to the limited space, we only introduce the update rule of{V,bğ‘, W, C}in the registers. In Al-Ãî€ gorithm 3, the update process is presented in detail as follows:(1)Line 1: Average valueğœ‡ =ğ‘Ÿ|Î©|as the basis value.(2)Lines 3âˆ’7: Given TB thread blocks, parameters{ğ‘£,bğ‘, ğ‘¤, ğ‘| ğ‘— âˆˆ {1, Â· Â· Â· , ğ‘ }}are evenly as-î€ˆî€‰ signed to thread blocksğ‘‡ ğµ|ğ‘¡ğ‘_ğ‘–ğ‘‘ğ‘¥ âˆˆ {1, Â· Â· Â· ,ğ‘‡ ğµ}. Each thread blockğ‘‡ ğµreads its own parameters {ğ‘£,bğ‘, ğ‘¤, ğ‘}from the global memory into the registers. In addition, the reading of memory is also aligned and merged.(3)Lines 8: The parameters{ğ‘£,bğ‘, ğ‘¤, ğ‘}with all nonzero values{ğ‘Ÿ|ğ‘– âˆˆbÎ©}in thread blockğ‘‡ ğµare updated.(3)Lines 9âˆ’11: Use the warp shuî€Ÿe instructions [34] to accelerate the dot productğ‘¢ğ‘£and summationÃÃî€ˆî€‰ {(ğ‘Ÿâˆ’ ğ‘)ğ‘¤,ğ‘}. Elementsğ‘¢, ğ‘£, ğ‘¤, ğ‘|ğ‘“ âˆˆ {1, Â· Â· Â· , ğ¹ }, ğ‘˜, ğ‘˜âˆˆ {1, Â· Â· Â· , ğ¾}in parameters{ğ‘¢, ğ‘£, ğ‘¤, ğ‘| ğ‘— âˆˆ {1, Â· Â· Â· , ğ‘ }}are evenly assigned to thread blocksî€ˆğ‘‡|ğ‘¡_ğ‘–ğ‘‘ğ‘¥ âˆˆ {1, Â· Â· Â· ,32}î€‰. A threadî€ˆ ğ‘‡in each thread blockğ‘‡ ğµsequentially reads the corresponding elementsğ‘¢, ğ‘£, ğ‘¤, ğ‘|ğ‘“%32=î€‰ ğ‘˜%32= ğ‘˜%32= ğ‘¡ _ğ‘–ğ‘‘ğ‘¥, ğ‘“ âˆˆ {1, Â· Â· Â· , ğ¹ }, ğ‘˜, ğ‘˜âˆˆ {1, Â· Â· Â· , ğ¾}in parameters{ğ‘¢, ğ‘£, ğ‘¤, ğ‘}, and the threadğ‘‡î€ˆ calculates the corresponding calculationsğ‘¢ğ‘£, (ğ‘Ÿâˆ’ ğ‘)ğ‘¤, ğ‘|ğ‘“%32= ğ‘˜%32= ğ‘˜%32= ğ‘¡_ğ‘–ğ‘‘ğ‘¥, ğ‘“ âˆˆî€‰ÃÃ‘ {1, Â· Â· Â· , ğ¹ }, ğ‘˜, ğ‘˜âˆˆ {1, Â· Â· Â· , ğ¾}. Please note that sinceğ‘†( ğ‘—) = ğ‘…(ğ‘–;ğ‘—)ğ‘(ğ‘–;ğ‘—)andğ‘…(ğ‘–;ğ‘—)ğ‘(ğ‘–;ğ‘—) = âˆ…, the thread only calculates one of(ğ‘Ÿâˆ’ ğ‘)ğ‘¤andğ‘. This makes the load of each threadğ‘‡relatively balancedÃî€€Ã during the update process. Then, the warp shuî€Ÿe in threadğ‘‡to obtain thebğ‘Ÿ= ğœ‡+ğ‘+bğ‘+ğ‘¢ğ‘£+ Ã(ğ‘Ÿâˆ’ ğ‘)ğ‘¤+Ãğ‘î€.(4)Lines 12âˆ’18: Parameters{ğ‘£,bğ‘, ğ‘¤, ğ‘}are updated in the registers to avoid rereading from global memory for the next update, and parameters{ğ‘¢, ğ‘}are updated directly in global memory.(5)Lines 19âˆ’22: After all nonzero values{ğ‘Ÿ|ğ‘– âˆˆbÎ©}have been updated, the latest{ğ‘£,bğ‘, ğ‘¤, ğ‘}are written to global memory because they will no longer be used. These operations are similar to CUSGD++. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data The algorithm has the following advantages: access to global memory and decreasing the time consumption; and each thread with CUSGD++, CULSH-MF can assemble more tangled parameters of the nonlinear MF model. The parameters optimize the memory access by allowing the computational overhead to be further reduced. The spatial overhead is ğ‘‚ (|Î©| + ğ‘€ğ¹ + ğ‘ ğ¹ + and the Top-ğ¾ GSM matrix J R{ğ‘ğ‘ğ‘Ÿğ‘ğ‘šğ‘’ğ‘¡ğ‘’ğ‘Ÿ }: parameter in register memory Input: Initialization for {U, V, ğœ‡, ğ‘ regularization parameters {ğœ† 3)Multi-GPU Model be extended to multiple GPUs (MCUSGD++/MCULSH-MF). We use data parallelism to allow multiple GPUs to run our algorithms at the same time. To avoid data conî€icts, each GPU-updated block cannot be on the same ğ½. After the update is completed, the updated parameters are not sent back to the CPU because another GPU needs these data directly. Transferring data directly in the GPUs avoids the extra time overhead of uploading to the CPU and ğ‘‡is balanced, which can avoid idle threads and can improve the active rate of threads. Compared , ğ‘}are taken as a whole, and the memory is merged and aligned. Then, the use ofwarp shuî€Ÿecan further } â† G{ğ‘£} } â† G{ğ‘¤} } â† G{ğ‘} } â† R{ğ‘£} } â† R{ğ‘¤} } â† R{ğ‘} then allocates them to other GPUs. Each GPU is assigned some speciî€›c parameters, which are not needed by other GPUs. After all updates are completed, each GPU passes the parameters that are stored at that time back to the CPU.î€ˆî€‰ Assume that we haveğ·GPUs. The sparse matrixRis divided intoğ· Ã— ğ·partsR|ğ‘‘, ğ‘‘âˆˆ {1, Â· Â· Â· , ğ·}. Low-î€ˆî€‰ rank feature matrices{U, V}are divided into{U|ğ‘‘âˆˆ {1, Â· Â· Â· , ğ·}}, {V|ğ‘‘âˆˆ {1, Â· Â· Â· , ğ·}}, respectively. Inî€uenceî€ˆî€‰ matrices{W, C}are divided into{W|ğ‘‘âˆˆ {1, Â· Â· Â· , ğ·}}, {C|ğ‘‘âˆˆ {1, Â· Â· Â· , ğ·}}, respectively. The parametersî€ˆî€‰ R, V, W, C|ğ‘‘âˆˆ {1, Â· Â· Â· , ğ·}are allocated to theğ‘‘th GPU and do not require transmission. Parameter Vis allocated to theğ‘‘th GPU at initialization and then transferred to another GPU after each update step. Fig. 5 depicts MCUSGD++ on three GPUs. MCULSH-MF is similar and is given in parentheses below. The sparse matrixR is divided into 3Ã—3 blocks. The training process of all the parameters is divided into three parts: (1): GPUs{1,2,3}î€ˆî€‰î€ˆî€‰ update{U, V, (W, C)}, {U, V, (W, C)}, {U, V, (W, C)}and then transmitU, U, Uto GPUs{3,1,2},î€ˆ respectively; (2): GPUs {1, 2, 3} update{U, V, (W, C)}, {U, V, (W, C)}, {U, V, (W,î€‰î€ˆî€‰î€ˆ C)}and transmitU, U, Uto GPUs {3, 1, 2}, respectively; and (3): GPUs {1, 2, 3} update{U, V,î€‰î€ˆî€‰ (W, C)}, {U, V, (W, C)}, {U, V, (W, C)}and transmitU, U, Uto GPUs {3, 1, 2}, respectively. Big data analysis should consider the incremental data, and the corresponding model can be compatible with the incremental data. The amount of incremental data is much smaller than the amount of original data. Thus, the time overhead for retraining the overall data is not worthwhile. It is nontrivial to design an online model for incremental data. The variable sets{ğ¼, ğ¼,bğ¼ }and{ğ½, ğ½,bğ½ }are denoted as the original variable set, new variable set, and overall variable set, respectively. In this work, we consider that the new variable setsğ¼andğ½enter the system and interact with variable sets ğ½ and ğ¼, respectively. Please note that this allows variable set ğ¼ to interact with variable set ğ½ . For the original variableğ½âˆˆ ğ½, the Top-ğ¾nearest neighbours{ğ½, Â· Â· Â· , ğ½} âˆˆ ğ½are kept. For the new variable ğ½âˆˆ ğ½, we search its Top-ğ¾nearest neighbours{bğ½, Â· Â· Â· ,bğ½} âˆˆbğ½. The hash value of variable setğ½depends on ğ¼, and the hash value of variable setğ½depends onbğ¼. In order to keep them consistent, we update the hash valueÃ of variableğ½âˆˆ ğ½; then, we save the intermediate variablesÎ¨(ğ‘Ÿ)(2Â· ğ»âˆ’1)of simLSH and updateğ»= Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data learning solution is described in Algorithm 4 as follows: (1) Lines 1 ğ½. Saving the intermediate variables makes the process only require a small amount of calculation. (2) Lines 4 Calculate hash value depend on neighbours in the overall variable set reduce memory access. (5) Lines 13 CULSH-MF is comprised of two parts: 1) Basic parallel optimization model depends on CUSGD++, which can utilize the GPU registers more and disentangle the involved parameters. CUSGD++ achieves the fastest speed compared to the state-of-the-art algorithms. 2) The Topreduce the time and memory overheads. Furthermore, it can improve the overall approximation accuracy. In order to demonstrate the eî€ectiveness of the proposed model, we present the experimental settings in Section 5.1. The speedup performance of CUSGD++ compared with the state-of-the-art algorithms is shown in Section 5.2. The accuracy, robustness, online learning and multiple GPUs of CULSH-MF are presented in Section 5.3. CULSH-MF is a nonlinear neighbourhood model for low-rank representation learning, and we compare CULSH-MF with the DL model in Section 5.4 to demonstrate the eî€ectiveness of CULSH-MF. Î¨(ğ‘Ÿ)Î¦(ğ») +ÃÎ¨(ğ‘Ÿ)Î¦(ğ»). Furthermore, we obtainğ»= Î¥î€€ÃÎ¨(ğ‘Ÿ) Î¦(ğ»)î€. The online bğ¼. (3) Lines 7âˆ’9: Search the Top-ğ¾nearest neighbours{bğ½, Â· Â· Â· ,bğ½}of variableğ½âˆˆ ğ½. The Top-ğ¾nearest |ğ¼âˆˆ ğ¼, ğ½âˆˆ ğ½ }is used and{bğ‘, ğ‘£, ğ‘¤, ğ‘}remains unchanged, but they can still be stored in registers to , ğ‘} remains unchanged. , ğ‘¢,bğ‘, ğ‘£, ğ‘¤, ğ‘}, new variable sets ğ¼ and ğ½ , random Hash values ğ». The experiments were run on an NVIDIA Tesla P100 GPU with CUDA version 10.0. The same software and hardware conditions can better reî€ect the superiority of the proposed algorithm. The experiments are conducted on 3 public datasets: Netî€ix, MovieLensand Yahoo! Music. For MovieLens and Yahoo! Music, data cleaning is conducted, and 0 values are changed from 0 to 0.5. This will make cuALS work properly, which is one of the shortcomings of cuALS. The speciî€›c situations of the datasets are shown in Table 2. The ratings in the Yahoo! Music dataset are relatively large, which aî€ects the training process. In the actual training process, we divided all the ratings in the Yahoo! Music dataset by 20, and then we multiply by 20 when verifying the results. In this way, the ratings of the three datasets are in the same interval, which facilitates the parameter selection. The accuracy is measured by the ğ‘…ğ‘€ğ‘†ğ¸ as: where Î“ denotes the test sets. The number of threads in athread warpunder the CUDA system is 32. Therefore, we set the number of threads in the thread block to a multiple of 32. This is done to maximize the utilization of the warp. Then, in order to align access, we set the parameters {ğ¹, ğ¾ } as multiples of 32. 5.2 CUSGD++ CUSGD++ is used to compare cuALS [54] and cuSGD [59] on the three datasets. The parameters of cuALS and cuSGD were set as described in their papers and optimized according to the hardware environment, and CUSGD++ uses the dynamic learning rate in [64] as where the parameters{ğ›¼, ğ›½, ğ‘¡,ğ›¾}represent the initial learning rate, adjusting parameter of the learning rate, the number of current iterations, and the learning rate atğ‘¡iterations, respectively. The learning rate and other parameters in CUSGD++ are listed in Table 3. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data The GPU experiments are conducted on three datasets. In order to ensure running fairness, we ensure that the GPU executes these algorithms independently, and there is no other work. Fig. 6 shows the relationship between the ğ‘…ğ‘€ğ‘†ğ¸and training time. In Table 4, the times it takes to achieve an acceptable MovieLens and Yahoo! Music, respectively) are presented. cuALS has an extremely fast descent speed, but the time of each iteration is very long because the matrix inversion calculation is performed twice for each iteration. Furthermore, because the number of same, the thread load imbalance further increases the time overhead. cuSGD has a slower descent speed but less time overhead per iteration due to using data parallelism without load balancing issues. cuSGD has an obvious î€aw in that it does not take full advantage of the hardware resources of the GPU. cuSGD stores data in global memory, which makes it take too much time to read and write data. Our proposed CUSGD++ is signiî€›cantly faster than the state-of-the-art algorithms on the GPU. CUSGD++ and cuSGD have the same number of iterations to obtain an acceptable the same gradient descent algorithm, the proposed CUSGD++ and cuSGD algorithms are basically the same in terms of descent speed. CUSGD++ makes full use of the GPU hardware. Therefore, the time overhead of each iteration is only approximately 1 our further work is to solve this problem. Simultaneously, we simply sort the index of the row or column for according to the number of This approach can reduce the time overhead on a single iteration and achieve speedups of Netî€ix, MovieLens and Yahoo! Music datasets, respectively. Fig. 6. RMSE vs time: the experimental results demonstrate that CUSGD++ converges faster than other approaches. Before introducing the experiment, we will introduce the selection of the relevant parameters. CULSH-MF still uses the dynamic learning rate in Equation (7). The initial learning rate and regularization parameters are shown in Table 5, andğ›½for all three datasets is 0 into the following 5 parts: 1) The overall performance comparison, 2) The performance comparison for the various methods of Top-ğ¾nearest neighbourhood query, 3) The performance comparison of neighbourhood nonlinear MF with naive MF methods, 4) The performance comparison on a GPU and multiple GPUs, and 5) The robustness of CULSH-MF. We î€›rst compare the serial algorithms, i.e., LSH-MF and GSM-based Top-ğ¾nearest neighbourhood MF [29]. To ensure the fairness of the comparison, the parameters used are the same[29]. The serial algorithms are conducted on an Intel Xeon E5-2620 CPU, and the CUDA parallelization algorithms are conducted on an NVIDIA Tesla P100 GPU. Parameters{ğ¹, ğ¾ }are set as{32,32}, respectively. Table 6 presents the time overhead of the three algorithms on the MovieLens dataset (baseline RMSE 0.80). The experimental results show that the LSH-MF can achieve a 44.3ğ‘‹speedup compared to the GSM-based Top-ğ¾nearest neighbourhood MF. CULSH-MF can achieve a 196.22ğ‘‹speedup compared to the LSH-MF serial algorithm. These results demonstrate that the proposed algorithms are eî€œcient. The comparison baselines of the GSM and simLSH are set under the same experimental conditions. To make the experiment more rigorous, a randomized control group was added, and it randomly selectsğ¾variables for each variable rather than the Top-ğ¾ nearest neighbours query. Furthermore, we compared two other LSH algorithms, random projection (RP_cos) based on cosine distance and minHash based on Jaccard similarity. On sparse data, compared to the Euclidean distance, the LSH algorithms based on the cosine distance have less accuracy loss. In addition, minHash can approximately calculate the Jaccard similarity between sets or vectors. The above two LSH functions are simple and have low computational complexity, Furthermore, the more complex LSH functions are not suitable for high-dimensional sparse data. The baselineğ‘…ğ‘€ğ‘†ğ¸s are{0.92,0.80,22.0}for Netî€ix, MovieLens and Yahoo! Music, respectively. For the MovieLens and Netî€ix datasets,Î¨(ğ‘Ÿ) = ğ‘Ÿis set to expand the gap between interaction values, and the Yahoo! Music dataset has more dense interaction values. Thus, Î¨(ğ‘Ÿ) = ğ‘Ÿ. We use a byte as a hash value (ğº = 8) and set ğœ†as the commonly used 100. Fig. 7 shows that the random selection method performs worse than the GSM-based method, simLSH and other LSH algorithms on the three datasets. When the parameters{ğ‘, ğ‘}are set as{ğ‘ =3, ğ‘ =100}, simLSH is almost the same as that of the GSM. Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data When the parameters and minHash are far from that of simLSH. The reason is that the datasets are very sparse, and the descent speed brought by minHash is not very impressive. Table 7 shows the optimal can achieve a better only in descent speed but also in accuracy. Table 7 (middle) shows the time overhead of GSM, simLSH and other LSH algorithms on the three datasets, and simLSH takes much less time than the GSM. The calculation time required for RP_cos is slightly larger than that of simLSH, and minHash requires considerable calculation time due to the high dimensionality of the datasets. Table 7 (bottom) shows the spatial overhead of GSM, simLSH and other LSH algorithms on the three datasets, and simLSH takes much less space than the GSM. Furthermore, simLSH can surpass the GSM since it can adjust the parameters to achieve a balance between accuracy and time and it can set appropriate parameters according to actual needs. Fig. 8 shows the inî€uence of various values of ğ‘will reduce the probability of two dissimilar variables projecting to the same hash value to higher accuracy. Fig. 7. ğ‘…ğ‘€ğ‘†ğ¸ vs time: The comparison between GSM, simLSH (various ğ‘ and ğ‘ values) and other LSH algorithms. We should select the best parameters and ensure which parameters play a greater role. In order to ensure that the threads are fully utilized, the parameters on CULSH-MF. As the Fig. 9 shows, under the same accuracy than CUSGD++ without the neighbourhood model in terms of the with CUSGD++ to demonstrate to what degree the neighbourhood model can improve the accuracy. Fig. 10 shows that CULSH-MF with the parameters )of two similar variables projected to the same hash value will decrease. Choosing a suitableğ‘will achieve Fig. 9. ğ‘…ğ‘€ğ‘†ğ¸ vs influence of various value of {ğ¹, ğ¾ }. Compared with ğ¹ , increasing ğ¾ can reduce RMSE more. Table 7. The optimalğ‘…ğ‘€ğ‘†ğ¸of various Top-ğ¾methods (Up), the time overhead of various Top-ğ¾methods (Seconds) (Middle) and the The neighbourhood model with a lowğ¾can greatly improve the descent speed, and it can reach the targetğ‘…ğ‘€ğ‘†ğ¸with only a few iterations. CUSGD++ has a shorter training time per iteration, but it requires more training periods. Thus, CULSH-MF can outperform CUSGD++ owing to the overall training time with the optimalğ‘…ğ‘€ğ‘†ğ¸. Another noteworthy Locality Sensitive Hash Aggregated Nonlinear Neighbourhood Matrix Factorization for Online Sparse Big Data results is that CULSH-MF runs faster than CUSGD++ as the value of can achieve {2.67ğ‘‹, 2.97ğ‘‹, 1.36ğ‘‹ } speedups compared to CUSGD++ when ğ¹ = {32, 64, 128}, respectively. Finally, we present the experimental results of the robustness of CULSH-MF and CUSGD++, the online learning and multiple GPU solutions of CULSH-MF. First, data inevitably have noise, and a robust model should suppress noise interference. The experiment is conducted on all datasets with noise rates of experimental results in Table 8 show that CULSH-MF has more robustness than CUSGD++, which means that the neighbourhood nonlinear model performs more robustly than the naive model. Second, we divide the training datasets of Netî€ix, MovieLens and Yahoo! Music into original set of the dataset are shown in Table 9. In the online experiments, the MovieLens, and Yahoo! Music datasets only increased by online CULSH-MF avoids the retraining process. Third, multiple GPUs can accommodate a larger data, and CULSH-MF is extended to MCULSH-MF. Due to the communication overhead between each GPU, MCULSH-MF cannot reach the linear speeds, and properly distributing communications can shorten the computation time. CULSH-MF can obtain {1.6ğ‘‹, 2.4ğ‘‹, 3.2ğ‘‹ } speedups on {2, 3, 4} GPUs, respectively, compared to CULSH-MF on a GPU. Our model also applies to recommendations for implicit feedback and has a very obvious time advantage. NCF works well but takes too much time, and CULSH-MF can achieve similar results with a lower time overhead. We change the loss function of CULSH-MF to the cross entropy loss function, and the update formula will also follow the corresponding change. This derivation is too simple and will not be repeated here. Because the time overheads to Table 10. Time comparison (Seconds) to obtain basic HR of various nonlinear MF methods train the deep learning models on large-scale datasets are unacceptable, three deep learning models, e.g., Generalized Matrix Factorization (GMF), the Multilayer Perceptron (MLP) and Neural Matrix Factorization (NeuMF), of [18] are just tested on two small datasets, e.g., MovieLens1m and Pinterest. 1) GMF is a deep learning model based on matrix factorization that extends classic matrix factorization. It î€›rst performs one-hot encoding on the indexes in the sets{ğ¼, ğ½ } of the input layer, and the obtained embedding vectors are used as the latent factor vectors. Then, through the neural matrix decomposition layer, it calculated the matrix Hadamard product of factor vectorğ¼and factor vectorğ½. Finally, a weight vector and the obtained vector are projected to the output layer by the dot product. 2) The MLP is used to learn the interaction between latent factor vectorğ¼and latent factor vectorğ½, which can give the model greater î€exibility and nonlinearity. With the same conditions as GMF, the MLP uses the embedded vector of the one-hot encoding of indicesğ¼andğ½as the latent factor vector ofğ¼andğ½. The diî€erence is that MLP concatenates latent factor vectorğ¼ with latent factor vectorğ½. The model uses the standard MLP; and each layer contains a weight matrix, a deviation vector, and an activation function. 3) GMF uses linear kernels to model the interaction of potential factors while MLP uses nonlinear kernels to learn the interaction functions from data. To consider the above two factors at the same time, NeuMF integrates GMF and the MLP, embeds GMF and the MLP separately, and combines these two models by connecting their last hidden layers in series. This allows the fusion model to have greater î€exibility. The Hit Ratio (HR) is used to measure the accuracy of the nonlinear models. We use the same datasets and the same metrics. For the same baseline HR, we compare the time overheads of CULSH-MF and the three nonlinear models, i.e., GMF, the MLP and NeuMF. The experimental results are shown in Table 10. Table 10 shows that the time overhead of the CULSH-MF is only 0.01% that of the three nonlinear models, i.e., GMF, the MLP and NeuMF. Furthermore, the parameters of the CULSH-MF are much smaller than those of the three nonlinear models, i.e., GMF, the MLP and NeuMF. The research was partially funded by the National Key R&D Program of China (Grant No. 2020YFB2104000) and the Programs of National Natural Science Foundation of China (Grant Nos. 61860206011, 62172157). This work has been partly funded by the Swiss National Science Foundation NRP75 project (Grant No. 407540_167266) and the China Scholarship Council (CSC) (Grant No. CSC201906130109).