University of Science and TechnologyWestlake University alexandros.karatzoglou@gmail.commathshenli@gmail.com Deep neural networks (DNN) have achieved great success in the recommender systems (RS) domain. However, to achieve remarkable performance, DNN-based recommender models often require numerous parameters, which inevitably bring redundant neurons and weights, a phenomenon referred to as over-parameterization. In this paper, we plan to exploit such redundancy phenomena to improve the performance of RS. Speciî€›cally, we propose PCRec, a top-N itemrecommendation framework that leverages collaborative training of two DNN-based recommender models with the same network structure, termedpeercollaboration. PCRec can reactivate and strengthen the unimportant (redundant) weights during training, which achieves higher prediction accuracy but maintains its original inference eî€œciency. To realize this, we î€›rst introduce two criterions to identify the importance of weights of a given recommender model. Then, we rejuvenate the unimportant weights by transplanting outside information (i.e., weights) from its peer network. After such an operation and retraining, the original recommender model is endowed with more representation capacity by possessing more functional model parameters. To show its generality, we instantiate PCRec by using three well-known recommender models. We conduct extensive experiments on three real-world datasets, and show that PCRec yields signiî€›cantly better recommendations than its counterpart with the same model (parameter) size. Recommender Systems (RS) have become an essential tool for large social media and e-commerce platforms. A large number of useritem interaction behaviors (i.e., feedback) are produced explicitly or implicitly every day on such systems [39]. In particular, implicit feedback, such as clicks, purchases, watched videos and played songs, are easy to be collected and often at a very large scale. For example, users on Tiktok may easily watch thousands of short videos per day, given that the playing time of each video takes usually less than 20 seconds. As such, recent studies on top-N item recommendations mainly pay attention to the implicit feedback problem [1]. The essence of item recommendation from implicit feedback is to predict a list of top-N items that a user would like to interact with by learning from his/her previous feedback. Figure 1: Performance change by pruning on SASRec. We perform the standard pruning based on the weight magnitude following [29]. The experimental settings are given in Section 4. Embedding and deep neural networks (DNN) based recommender models have achieved superior performance and practically dominated the RS domain. Among these models, BPR [27], DSSM [18] and YouTube DNN [4] have become some of the most representative work for the general item recommendation task, while GRU4Rec [14], NextItNet [41] and SASRec [19] are more representative for the sequential recommendation settings. The success of these models often comes with a large embedding size or deep network structure [35,38]. However, large and deep models are very prone to be over-parameterized, resulting in redundant neurons and weights. As illustrated in Figure 1, simply pruning 10% parameters in the SASRec model yields very minor performance degradation. Whatâ€™s more, pruning 30% of unimportant parameters with a î€›ne-tuning strategy performs even a bit better than the original SASRec. On the other hand, SASRec with a smaller embedding dimension (i.e., ğ‘‘ =128), around 50% parameters of itself withğ‘‘ =256, performs noticeably worse on ML-20M. These observations evidence that (1) the over-parameterization phenomenon widely exists in large recommender models; (2) training a smaller-size recommender model from scratch yields considerably worse performance. Pruning redundant parameters from a large neural network model could bring higher parameter eî€œciency.These experiments have been extensively performed in the computer vision (CV) [6,10, 11] and natural language processing (NLP) [8,21] î€›elds. However, in recommender systems, simply reducing a portion of parameters (e.g., 30% in Figure 1) for large recommender models may not beneî€›t as much as in CV and NLP since large-scale RS models are often deployed in a cloud platform rather than an edge/mobile device (like many CV and NLP models) with very limited hardware resources. Thereby, inspired by these work, but diî€erent from them, we hope to explore whether such redundant parameters can be used more eî€ectively instead of abandoning them so as to increase the model expressivity and alleviate the data sparsity issue in the recommender system domain. To approach the above problem, we present apeercollaboration framework for top-N itemrecommendation tasks, called PCRec. Speciî€›cally, we propose rejuvenating invalid (i.e., unimportant) weights of a recommender model by transplanting important weights from a peer model with an identicalnetwork. To do so, we î€›rst propose two criteria, including L1-norm based and entropy based, to identify which weights are important and which are redundant. To eî€ectively strengthen invalid weights, we create two rules regarding how to complement information between two identical networks and how much information each one needs to be complemented from its peer. To validate the eî€œcacy of PCRec, we instantiate it using three popular models, including both general item recommender models and sequential recommender models. We summarize our main contributions as four-fold: â€¢We propose PCRec to promote collaboration of two recommender models with a selfsame network architecture. PCRec is a novel learning paradigm for recommender models, which can reactivate invalid weights by explicitly transplanting effective weights from its outside peer network. â€¢We introduce two criteria to measure the importance of weights in a recommender model. Besides, we propose an adaptive coeî€œcient to determine how much the external information is required from its peer. â€¢We instantiate PCRec using three well-known recommender models, namely, BPR, YouTube DNN, and SASRec. PCRec is conceptually simple, easy to implement, and applicable to a broad class of recommender models. â€¢Through thorough experiments and ablation studies, we show that PCRec obtains noticeably improved performance on three real-world RS datasets. We brieî€y review related work regarding the DNN-based RS and multiple model ensemble learning. Deep neural networks (DNNs) have made great progress for item recommendations thanks to their high model capacity and expressivity. In general, deep RS can be broadly classiî€›ed into general (i.e., non-sequential) item recommendations and sequential item recommendations according to whether sequential patterns are modeled. In terms of general item recommendations, neural network models such as Deep Crossing [30], DeepFM [9], NeuralFM [13], Wide & Deep [3], and YouTube DNN [4] have become the most representative works. Compared to the shallow embedding models, the main advantages of these models highly depend on their neural network structures and non-linearities, who are believed to be able to approximate any continuous function [16, 17]. On the other hand, sequential recommender systems (SRS) have also attracted much attention recently. By capturing userâ€™s dynamic interests, SRS, in general, is more powerful in generating the next recommendation. Moreover, SRS can be trained in a self-supervised manner [40,43], and thus do not need handcrafted labels and features. According to existing literature, GRU4Rec [14], Caser [36], NextItNet [41], SASRec [19] and BERT4Rec [34] are especially popular. Among them, GRU4Rec and Caser based on shallow network structure fail to model very long-term sequential patterns and usually oî€er sub-optimal performance. By contrast, NextItNet, SASRec and BERT4Rec are able to obtain state-of-the-art performance by eî€ectively capturing long-term and complex sequential dependencies. In this paper, we design PCRec by instantiating it with three popular recommender models including BPR with shallow embeddings, YouTube DNN and SASRec with deep neural network. It is worth noting that the framework of PCRec is model-agnostic and potentially applicable to various embedding and deep models. PCRec relates to the ensemble learning (EL) [12,20] and knowledge distillation (KD) [15] in a similar spirit that more than one model is used during training. Here, we brieî€y review related works and clarify their key diî€erences against PCRec. Ensemble Learning (EL) refers to the process that multiple learning models are strategically combined to achieve better predictive performance than any of its individual model trained alone [25]. Bagging [2], boosting [7] and stacking [32] are thought of as three representative EL algorithms. The main principle behind them is that a set of weak learners are combined together to form a strong learner. While EL is generic for diî€erent types of models, we notice that there are relatively few works that explore deep learning (DL) based ensemble methods. We suspect that DL-based methods are not conceptually weak learners and combining a large number of DL models could be computationally and memory expensive during the model prediction phase, and thus could be in-eî€œcient and in-practical. By contrast, PCRec merely needs one well-trained single model at the inference stage. Apart from that, PCRec is also relevant to KD-based methods [15, 37] which are designed to enhance a small-capacity model by one (or multiple) large teacher model(s). However, unlike KD-based methods, PCRec does not include the mutual learning [23,42] process which optimizes multiple losses together. Moreover, PCRec explicitly combines the advantage of two identical models by enhancing the invalid weights, which is very explainable. By contrast, the knowledge transferred by KD-based methods is usually called dark knowledge [15], and the working mechanism of it is not as explainable as PCRec. In addition, PCRec focuses on performance improvement which is diî€erent from the motivation of the KDbased methods â€” injecting knowledge from a large teacher model into a smaller student one to obtain the eî€ect of model compression. Figure 2: PCRec with PW cooperation, where dark colors represent important weight. As mentioned in the introduction part, over-parameterization or redundancy commonly exist in large and deep recommender models. Inspired by this, in this paper we set our goal to reactivate these redundant weights (rather than abandoning them) so as to enhance the model capacity and expressivity. To be speciî€›c, we present the PCRec learning framework, which enhances an individual recommender model by transplanting important information from a selfsame network of this recommender, referred to as a peer. In the following, we î€›rst introduce criteria to measure the importance of weights in a recommender model. Then, we propose a parameter-wise approach to reactivate the redundant weights of the two peer models. At last, we develop the î€›nal version based on the layer-wise cooperation, which addresses the limitations of the parameter-wise approach. 3.1.1 L1-norm. The idea of the L1-norm criterion is borrowed from the pruning [10,22,29] literature. Denoteğ‘Šâˆˆ Ras the weight matrices of theğ‘–-th layer in a model. We can identify the importance of weights from two perspectives: single weight perspective and entire layer perspective. The importance of a single weight is directly determined by its absolute value (L1-norm) â€” the higher absolute value it has, the more important it is. We can use a threshold to distinguish the important and unimportant weights. From the entire layer perspective, we could identify the importance of all weights by using a neural network layer (including the embedding, middle layers, and î€›nal prediction layers) as the measure unit. Intuitively, measuring the importance of a layer could maintain the layer consistency as much as possible, which will beneî€›t the information transplanting process as mentioned later. Formally, its L1-norm can be given below by using the entire layer as the measure unit: Denoteğ‘Šandğ‘Šas L1-norm ofğ‘–-th layer of two collaborated recommender models. We deî€›ne theğ» (ğ‘Š)as the relative information of the layer: where % is the modulo operation,ğ‘˜is the model ID. While L1norms has been widely applied in pruning, it only cares about the magnitude of the weights, and ignores the variation of the weights. For example, given a weight matrixğ‘Šâˆˆ ğ‘…ofğ‘–-th layer, where each element inğ‘Šis assigned to the same valueğ‘§, whose absolute value is big. If we use such a weight matrix to transform the(ğ‘– âˆ’1)-th layer, then each part of it contributes equally to the ğ‘–-th layer evenğ‘§is very big. This suggests that L1-norm might not be the best criterion to discriminate the importance of layer (all weights) information. 3.1.2 Entropy. To address the limitation mentioned above, we introduce an entropy-based criterion to measure the variation of weights in each layer. Entropy is often used to evaluate the degree of chaos (information) in a system [5,26]. Inspired by [24,31,33], we transform the weight matrix into a vector and discretize the vector intoğ‘šbins. Then we can calculate the probabilities of each bin. To be speciî€›c, we î€›rst sort the weights in the vector based on their actual values and divide the vector intoğ‘šbins with equal numeric intervals (wheremaxandminrepresent the maximum and minimum values of the weight matrix, respectively). The probability of the ğ‘—-th bin is: Input: ğ‘€, ğ‘€with weights ğ‘Š,ğ‘Š, . . . ,ğ‘Š, ğ‘˜ = 1, 2; whereğ‘andğ‘›are the parameter sizes of the weight vector and the ğ‘—-th bin, respectively. Then, we calculate the entropy (information) of the weight matrix ğ‘Šas follows: A smaller score of H(W)means the layer in this model has less variation (information). We illustrate the proposed PCRec framework in Figure 3. Assume that both models haveğ‘layers. We denoteğ‘Šandğ‘Šas the weight matrices of theğ‘–-th layer of the two models. Our core idea is to use the corresponding weight information of the two networks, and generate more expressive weightsbğ‘Šas The weightğ‘Šandğ‘Šare signiî€›cantly diî€erent since they are optimized with diî€erent hyper-parameters (mentioned later) and initialization. That is, the unimportant weights of a layer may correspond to the important weights of the same layer in his peer, and vice versa. Before describing the layer-wise (LW) cooperation mechanism, we î€›rst show a more intuitive parameter-wise (PW) method by exploiting redundancy pruning. 3.2.1 PW Cooperation. The process is shown in Figure 2, we î€›rst deî€›ne a positive thresholdğ›¾and then identify unimportant parameters if their absolute values are smaller thanğ›¾. To realize information transfer from its peer model, we simply replace these unimportant parameters with parameters in its peer model of the same layer and index position. To realize this, we deî€›ne a binary mask matrix ğ¼âˆˆ Rwhich has the same shape withğ‘Što indicate the Figure 3: PCRec framework with LW cooperation. ğ‘“ represents linear combination based on layer information, i.e., ğ» (ğ‘Š) and ğ» (ğ‘Š). indices of these invalid weights inğ‘ŠThis process is symmetrical for the two peer models. Correspondingly, we can formulate the PW process as follows. where % is the modulo operation and each element of ğ¼is: ğ¼=0 if ğ‘Šâ‰¥ ğ›¾1 if ğ‘Š< ğ›¾0 â‰¤ ğ‘š < ğ‘‘and 0 â‰¤ ğ‘› < ğ‘‘(7) The learning process of PCRec with the PW cooperation is illustrated in Algorithm 1. While this PW cooperation is intuitively simple, it has some shortcomings as mentioned below. 3.2.2 LW Cooperation. Using individual weight as the measure unit only focuses on the importance of the weight itself, which unfortunately ignores the layer consistency and may thus hurt the model expressivity and performance. We argue that using the entire layer as the measure unit can enable all weights at the same layer to contribute synergistically to the transformation of the layer. Thus, we propose a layer-wise transplanting method by deî€›ningğ‘“as a linear combination function: where 0â‰¤ ğœ‡â‰¤1 is the coeî€œcient. Particularly, we treat this coeî€œcientğœ‡as an adaptive parameter so as to promote cooperation and optimization automatically. Below, we give two instructions on designing a suitable adaptive parameter ğœ‡: (1)We expect that layers with less information could get additional information from its peer model. Hence, we use the diî€erenceğ» (ğ‘Š) âˆ’ ğ» (ğ‘Š)to measure the relative importance of information in the two layers. When the diî€erence is zero,ğœ‡should be set to 0.5, otherwisebğ‘Šshould assign a largeğœ‡(i.e.,ğœ‡>0.5) to the layer that has more information. Note that evenğœ‡=0.5 could be also helpful since the same information does not mean all weights are identical according to Eq.(4). Consider an extreme situation where the distributions (e.g., normal distribution) of weight Table 1: Statistic of the evaluated datasets. "M" and "K" is short for million and kilo, "t" is the length of interaction sequences. matrices are identical, but the magnitude of each weight (with the same position) is the opposite. In such a case, the information of each layer is the same, but the entropy ofbğ‘Š is enlarged by Eq. (8). (2)Even the diî€erenceğ» (ğ‘Š) âˆ’ ğ» (ğ‘Š)is large, we expect thatbğ‘Šcontains part information of itself and is able to adaptively control the impact of ğ» (ğ‘Š) âˆ’ ğ» (ğ‘Š). To meet the above requirements, we design an adaptiveğœ‡which is wrapped by the sigmoid function: whereğ›¼is a hyper-parameter to control the degree of the information from the outside layer. It is worth noting that the calculation criterion of information for a layer as the measure unit can be L1-norm (i.e., Eq.(2)) or entropy (Eq.(4)), which is diî€erent from the individual weight as the measure unit with L1-norm criterion. During training, we just need to perform this combination operation at each epoch. The new weight matrices (bğ‘Šandbğ‘Š) should be the same for the two individual models due to the dual linear combination. In practice, we need to guarantee that each model has diverse and suî€œcient information so as to complement each other. In this paper, we adopt two simple strategies for the two models to make each of them capture unique information, i.e., using diî€erent learning rates and sampling of the training data. PCRec can be optimized in two modes, namely, parallel and serial training. In terms of the parallel mode, the two individual networks of PCRec are essentially trained independently, but each batch of them is trained concurrently. The information of each identical recommender model can be transferred by using the saved checkpoint. As a result, parallel optimization requires more memory and computations, but saves substantial training time. For clarity, if we assume the time and space complexity of each model are the scalarsğ‘‡ğ¶ andğ‘†ğ¶, the time and space complexity of PCRec in parallel training mode areğ‘‡ğ¶and 2ğ‘†ğ¶. On the other hand, we can perform serial optimization for each individual network by sequentially training them per batch. As such, compared with the parallel mode, the serial optimization inevitably sacriî€›ces training time but consumes no extra memory and computation. The time and space complexity of PCRec in the serial training model are roughly 2ğ‘‡ğ¶andğ‘†ğ¶. Algorithm 2 illustrates the peer cooperation process. In summary, we maintain two networks with an identical structure but diî€erent learning rates and sampling orders. When a training epoch is î€›nished, we calculate the informationğ» (ğ‘Š)of each layer of the two models and perform cooperation. Note that the parameters of the bias and normalization terms of the same layer share the same ğœ‡calculated based onğ‘Š. After training, PCRec needs only one Model ğ‘ ğ‘‘ ğœ‚ ğ¿ğ‘ ğ‘ ğ‘‘ ğœ‚ ğ¿ğ‘ ğ‘ ğ‘‘ ğœ‚ ğ¿ğ‘ SASRec 128 64 1e-3 - 0.3 128 256 1e-3 - 0 128 256 1e-3 - 0.5 DNN 128 64 1e-4 1e-5 - 128 256 1e-4 1e-6 - 128 256 1e-4 1e-5 peer model for inference, and thus, is as eî€œcient as the original individual recommender model. This property is distinct from the traditional ensemble methods that have to rely on the decisions of multiple â€˜weakâ€™ learners during inference. We describe the experimental setup in this section, including datasets, baselines, implementation details and evaluation metrics. â€¢ ML-20M: This is a well-known benchmark dataset widely used for both traditional and sequential recommendation tasks [19,34,35]. It contains around 20 million user-item interactions with 27,000 movies and 138,000 users. Following the common practice in [19,39,41], we assume that an observed feedback is available if an explicit rate is assigned to this item. We perform basic pre-processing to î€›lter out the interactions with less than 5 users and users with less than 5 items to alleviate the eî€ect of cold users and items. Then, we use timestamps to determine the order of interactions. Following [19,34], we adopt the leave one out evaluation scheme. For each user, we hold out the last item of the interaction sequence as the test data, treat the item just before the last as the validation set, and utilize the remaining items for training. For the sequential recommendation task, we construct userâ€™s interaction sequences by using his recentğ‘¡interactions by the chronological order. For sequences shorter than t, we simple pad them with zero at the beginning of the sequence following [41], while for sequences longer than t, we split them into several sub-sequences with lengthğ‘¡in the training set. In this paper, we setğ‘¡to 100 on this dataset. â€¢ QQBrowser: It is an industrial dataset which was collected from the QQBrowser platform of Tencent. The items in QQBrowser include news, videos and ads. It consists of more than 70,000 items and almost 1 million users. We perform a similar pre-processing as above and setğ‘¡to 50. We will open source this dataset later for reproducibility. â€¢ Retailrocket: It is a public dataset collected from a realworld ecommerce website, consisting user shopping behaviors in 4.5 months. It contains 235,061 items and 1.4 million users. Similarly, we setğ‘¡to 10 to investigate recommendation performance for short-range interaction sequences. Table 1 summarizes the statistics of evaluated datasets after the basic pre-processing. Table 3: Overall performance of all models. PCRec with two SASRec, DNN and BPR is referred to PC-SAS, PC-DNN and PC-BPR, respectively. Here, we present the results of PCRec with LW-cooperation and entropy-based information criterion because of its best performance. We set ğ›¼ of PC-SAS to 30, 30, 30, ğ›¼ of PC-DNN to 40, 40, 10, and ğ›¼ of PC-BPR to 20, 20, 20, on Retailrcoket, ML-20M, QQbrowser, respectively. Improvements over baselines are statistically signiî€›cant with p < 0.01. Model MRR@5 MRR@20 HIT@5 HIT@20 MRR@5 MRR@20 HIT@5 HIT@20 MRR@5 MRR@20 HIT@5 HIT@20 We evaluate the PCRec framework by using three popular recommender models, namely, SASRec [19], YouTube DNN [4] (DNN for short) and BPR [27]. For SASRec, we use its oî€œcial code online, while for BPR and YouTubeDNN, we implement it by strictly following the original paper. It is worth noting that compared with SASRec, DNN and BPR are unable to capture user sequential patterns. This is because DNN model userâ€™s previous interactions as common features, while BPR with matrix factorization as the scoring function is a typical collaborative î€›ltering baseline. We want to emphasize that the purpose of our study is not to propose a state-ofthe-art model beating existing baselines. The purpose is rather to introduce a new learning paradigm that could eî€ectively leverage the parameter redundancy issues in large and deep recommender models so as to achieve some additional improvement in accuracy. We train all models using the Adam optimizer on GPU. For common hyper-parameters, we consider the hidden dimension size (denoted byğ‘‘) from {16, 32, 64, 128, 256} and the learning rate (denoted byğœ‚) from {0.0001, 0.00025, 0.0005, 0.00075, 0.001, 0.005}, theğ¿ regularization coeî€œcients from {0.01, 0.001, 0.0005, 0.0001, 0.00005 0.00001}, and dropout rate (denoted byğ‘) from {0, 0.1, 0.2, .. . , 0.9} by grid search in the performance of the validation set. Speciî€›cally, we set theğ‘‘256 for SASRec (except on Retailrocket), DNN (except on Retailrocket) and BPR. On Retailrocket,ğ‘‘of SASRec and DNN is set to 64 to prevent overî€›tting. We useğœ‚1e-3 for SASRec and BPR, and 1e-4 for DNN on all datasets. In addition, we set batch size (denoted byğ‘) to 128 for SASRec and DNN, and 2048 for BPR because of its enormous triple samples. As for model-speciî€›c hyper-parameters, we use two self-attention blocks (denoted byğ‘™) with one head for SASRec according to the original paper. Regarding DNN, we use one hidden layer on all datasets since using more layers does not lead to any improved results. Our PCRec uses exactly the same hyper-parameters (exceptğœ‚) as these individual base models. Forğœ‚, one peer in PCRec uses exactly the same one with its base model, while the other peer uses a sub-optimalğœ‚. The model-speciî€›c hyperparameter of PCRecğ›¼is studied in the ablation study part. Without special mention, we report our results with the optimalğ›¼. Detailed hyper-parameters are reported in Table 2. We follow previous works [19,35,39,41] by comparing the topN metrics, namely, MRR@N(Mean Reciprocal Rank), HR@N(Hit Ratio) and NDCG@N(Normalized Discounted Cumulative Gain). To save space, we omit the formulas of these metrics. N is set to 5 and 20 in this paper. In this section, we would answer the following research questions: â€¢ RQ1: Does PCRec improve the performance of these typical neural networks, such as SASRec, YouTubeDNN and BPR? â€¢ RQ2: What is the performance of these variants of PCRec, which include PCRec with PW and LW cooperation, PCRec with L1-norm and entropy criteria. â€¢ RQ3: What is the impact of the collaboration of diî€erent components in PCRec, such as, the embedding layer, softmax layer and hidden layers? â€¢ RQ4: What is the impact ofğ›¼for PCRec? Are the diî€erent learning rates and training data orders necessary? â€¢ RQ5: Does PCRec really enhance these unimportant weights of the original model? We present the overall results in Table 3. First, we observe that SASRec performs better than DNN and BPR with notable improvements. To our surprise, on Retailrocket and ML-20M, SASRec achieves several times improvements on all these top-N metrics. By examining the real dataset, we î€›nd that there indeed exist some short sequence fragments (formed with 2âˆ¼4 videos) on the two datasets, which can be observed from the actions of many users. Unfortunately, DNN and BPR are unable to model such sequential patterns, and thus yield much worse results than the state-of-the-art sequential recommendation model SASRec. Second, as expected, PCRec, including PC-SAS, PC-DNN and PCBPR, outperforms their individual base models (i.e., SASRec, DNN and BPR), demonstrating the eî€ectiveness of peer collaboration. For example, compared with SASRec, PC-SAS achieves around 5% improvement in terms of MRR@5 on ML-20M; Compared with BPR, PC-BPR obtains up to 8% improvement regarding MRR@5 on Retailrocket and ML-20M. In particular, PC-BPR outperforms BPR with around 11% improvement regarding HIT@5 on QQBrowser. Notable improvements can also be observed by comparing PC-DNN Table 4: The Comparison of PCRec variants. The standard ensemble learning method [28] by averaging the prediction scores of two individual models is denoted by Ensemble-M2. to DNN on all datasets and all metrics. In what follows, we would conduct ablation studies to verify the eî€ectiveness of PCRec. To save space, we could only show partial results if the performance trends of them keep consistent. In Section 3.1, we have proposed using a single weight and a layer as the measure unit in PCRec, We denoted them as PCRec-W and PCRec-L, respectively. Further, in PCRec-L, we can adopt two criteria, L1-norm and entropy, to identify which layer of the two individual networks has less information, denoted as PCRec-LN and PCRec-LE, respectively. In addition, we also evaluate a very simple method by reactivating the invalid weights using gaussian noise to increase the L1-norm, denoted as PCRec-N. We experimentally examine these methods and report results in Table 4. First, we î€›nd that PCRec-N yields worse accuracy than the base model, which potentially indicates PCRec should use a useful information source, rather than random noise, for information transplanting. By contrast, PCRec-LE, PCRec-LN always perform better than SASRec, DNN on almost all datasets. This clearly veriî€›es our main claim Table 5: The impact of learning rates and sampling orders of training data for PCRec. PC-SAS with diî€erent learning rates and sampling orders, the same learning rate and diî€erent sampling orders, diî€erent learning rates and the same sampling order, is denoted by PC-SAS-DD, PC-SAS-SD, PCSAS-DS respectively. Similar expressions apply to PC-DNN. regarding the beneî€›t of peer collaboration. Meanwhile, PCRecLN outperforms PCRec-W on most settings, demonstrating the eî€ectiveness of layer-wise cooperation; PCRec-LE outperforms PCRec-LN, demonstrating the eî€ectiveness of entropy-based criterion, since it can more precisely identify how much information is required when performing information transplanting. On the other hand, we also compare the results that are produced by standard ensemble learning. It can be seen that the basic ensemble learning method (Ensemble-M2) is very eî€ective and obviously surpasses these individual models. It even performs slightly better than PCRec-LN on the Retailrocket dataset when using DNN as the base model. However, our PCRec-LE in general can beat it, or at least they are competitive. Hence, we do not claim our PCRec is better than the standard ensemble learning method in this paper. But we emphasize that PCRec provides an alternative learning paradigm for getting information from an outside model, and more importantly, it is much more eî€œcient than the standard ensemble learning during the inference phase, since it only requires one single model for prediction, rather than relying on predictions of two or more models. We further î€›nd that increasing the individual models for the ensemble learning, e.g., Ensemble-M3, does not yield better results. Figure 5: Convergence behaviors of PCRec by peer collaboration of diî€erent components. PC-SAS that applies peer collaboration only on the embedding layer, middle layers, and softmax layer as PC-SAS-E, PC-SAS-M, PC-SAS-S, respectively. 5.3.1 Impact of learning rates and train data orders. Table 5 presents the impact of diî€erent learning rates and sampling orders of training data. As shown, PC-SAS-DD always yields the best recommendation accuracy compared with their counterparts, i.e.,PC-SAS-SD and PC-SAS-DS. On the other hand, we observe that PC-SAS-SD and PC-SAS-DS consistently outperform the original SASRec. Similar observations can be made for PC-DNN. The results conî€›rm that PCRec that applies diî€erent learning rates and sampling orders is necessary. This is likely because training individual networks with diî€erent learning rates and sampling could increase diversity of network weights, so as to increase the layer information when linearly combining them. The results hold well for PCRec with BPR and are thus simply omitted. 5.3.2 Impact ofğ›¼. In this subsection, we study the impact ofğ›¼ which controls the amount of information to be transplanted. Figure 4 shows the model performance of PC-SAS and PC-DNN with diî€erentğ›¼on Retailrocket and ML-20M. First, PC-SAS is sensitive toğ›¼, and the optimal results are obtained whenğ›¼equals to 30 on Retailrocket and ML-20M. Similarly, PC-DNN obtains the best performance whenğ›¼is set to 40 on Retailrocket and ML-20M. It can be seen that PCRec with a properğ›¼could achieve 1âˆ¼4% improvement than a randomğ›¼. It is also worth noting that PCRec outperforms its individual base model evenğ›¼is not set to its optimal value. In practice, we suggest running PCRec by tuningğ›¼from 30 to 40. By doing this, we observe that the coeî€œcientğœ‡ranges from 0.7 to 1.0 in most cases. 5.3.3 Impact of the peer collaboration with diî€›erent components. We conduct an ablation study in Figure 5 by applying peer collaboration for some components of the model. First, it can be observed that PC-SAS-E, PC-SAS-M and PC-SAS-S outperform SASRec, demonstrating that the information transplanting on every component of usually performs better than its original model SASRec. Second, PC-SAS-E improves SASRec by a larger margin, compared with PC-SAS-M. In particular, PC-SAS-E even surpasses PC-SAS on ML20M. This is likely because the embedding layer usually contains much more parameters than the middle layers in recommender models. Besides, the embedding layer contains the most important information for item recommendations â€” i.e., personalization. As such, performing information transplanting on the embedding layer Figure 6: Ratios of Invalid layers in PC-SAS and SASRe c, where invalid layer ratio denotes the number of valid layers (with information ğ» (ğ‘Š) under a speciî€›ed threshold value) over the number of all layers. makes more sense than only doing it for these middle layers. This also suggests that it might be suî€œcient to perform peer collaboration on only necessary components of the base model, rather than all components. Similar conclusions hold for PC-DNN in general. In this part, we simply analyze the information transplanting mechanism in PCRec. To validate whether the peer collaboration really work not, we calculate the number of invalid layers (including both fully-connected layers and self-attention layers) whose entropy is under a speciî€›ed threshold after training. Experimental results are reported in Figure 6. It can be seen that with the threshold of 0.5, there are about 20% layers that are invalid for SASRec on ML-20M, whereas PCRec with peer collaboration training only has less than 5% invalid layers. With the increase of threshold, the ratios of invalid layers in both SASRec and PCRec rise. However, the ratio of PCRec is always smaller than SASRec. These observations verify our key assumption that peer collaboration does help model to strengthen the information (i.e., ğ» (ğ‘Š)) of network layers. In this work, we have discussed the network redundancy phenomenon in deep recommender models. Taken inspiration from this, we have proposed PCRec, a î€exible and generic peer collaboration learning paradigm that is able to rejuvenate invalid parameters (instead of abandoning them) in a recommender model by transplanting information from its outside peer network. To identify which parameters are invalid, we have introduced L1-norm and entropy based criteria. Then, we propose two collaboration strategies regarding how to transplant information between two peer models. Through extensive experiments on three real-world recommendation datasets, we have demonstrated that PCRec generated consistently better recommendations than its original base model. We expect PCRec to be valuable for existing recommender systems based on the embedding or deep neural network models.