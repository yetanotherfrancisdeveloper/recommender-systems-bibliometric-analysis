We study the problem of recommending items to occasional groups (a.k.a. cold-start groups), where the occasional groups are formed ad-hoc and have few or no historical interacted items. Due to the extreme sparsity issue of the occasional groupsâ€™ interactions with items, it is diî€œcult to learn high-quality embeddings for these occasional groups. Despite the recent advances on Graph Neural Networks (GNNs) incorporate high-order collaborative signals to alleviate the problem, the high-order cold-start neighbors are not explicitly considered during the graph convolution in GNNs. This paper proposes a self-supervised graph learning paradigm, which jointly trains the backbone GNN model to reconstruct the group/user/item embeddings under the meta-learning setting, such that it can directly improve the embedding quality and can be easily adapted to the new occasional groups. To further reduce the impact from the cold-start neighbors, we incorporate a selfattention-based meta aggregator to enhance the aggregation ability of each graph convolution step. Besides, we add a contrastive learning (CL) adapter to explicitly consider the correlations between the group and non-group members. Experimental results on three public recommendation datasets show the superiority of our proposed model against the state-of-the-art group recommendation methods. â€¢ Information systems â†’ Recommendation. Occasional group recommendation, self-supervised learning, graph neural network ACM Reference Format: Bowen Hao, Hongzhi Yin, Jing Zhang, Cuiping Li, and Hong Chen. 2018. Self-supervised Graph Learning for Occasional Group Recommendation. In Woodstock â€™18: ACM Symposium on Neural Gaze Detection, June 03â€“05, Figure 1: A toy example for group recommendation. 2018, Woodstock, NY. ACM, New York, NY, USA, 11 pages. https://doi.org/ 10.1145/1122445.1122456 Recommender systems have played a crucial role in social media platforms, due to their promising ability in mitigating the information overload issue. With the recent advances in social platform services like Meetup and Facebook Event [23,30], it is increasingly convenient for people with similar backgrounds (e.g., hobbies, locations) to form social groups to participate in activities such as group tours, class reunion, family dinners [9,40]. The proliferation of groups in the social media platforms demands an eî€ective way to perform group recommendation. This paper addresses the problem of recommending items to occasional groups (a.k.a. cold-start groups), where the occasional groups are formed ad-hoc and have few or no historical interacted items. Due to the extreme sparsity issue of the occasional groupsâ€™ interactions with items, it is diî€œcult to learn high-quality embeddings for these occasional groups. To solve this problem, some early studies adopt heuristic predeî€›ned aggregation strategy such as average [2], least misery [1] and maximum satisfaction [3] to aggregate the user preference to obtain the group preference. However, due to the î€›xed aggregation strategies, these methods are insuî€œcient to capture the complicated and dynamic process of group decision making, which results in the unstable recommendation performance [25]. Further, Cao et al. [4] propose to assign each user an attention weight, which denotes the inî€uence of group member in deciding the groupâ€™s choice on the target item. However, when some users in the occasional group only interact with few items (a.k.a. cold-start users), the attention weight assigned for each user is diluted by these cold-start users, and thus results in biased group proî€›le. Recently, inspired by the development of graph neural networks (GNNs), a few GNN-based recommender models are proposed [12, 13,29,34]. The basic idea is to incorporate high-order neighbors to enhance the representations of the cold-start users, and then obtain reî€›ned group representation. As shown in Figure 1, the GNN model î€›rst conducts graph convolution multiple steps on the user-user and user-item interaction graphs to learn the preference of group members, and then performs average [12], summation and pooling [43] or attention mechanism [12] to aggregate the preferences of group members to obtain the group representation. Finally, based on the aggregated embeddings of groups and users, the likelihood of a group/user adopt an item is estimated, and the BPR loss [28] or cross entropy loss [15] is usually adopted to compare the likelihood and the true observations. Moreover, Zhang et al. [44] propose hypergraph convolution network (HHGR) with self-supervised node dropout strategy, which can model complex high-order interactions between groups and users. Through incorporating self-supervised signals, HHGR can alleviate the data sparsity issue in some extent. However, the above methods still suî€er from the following challenges. First, the group representation not only depends on the group membersâ€™ preferences, but also relies on the group-level preferences towards items and collaborative group signals (the groups that share common users/items). Although some GNNs consider either group-level preferences [19] or collaborative group signals [12,44] to form the group representation, they do not consider all these signals together. Second, the GNNs can not explicitly deal with the high-order cold-start neighbors when performing graph convolution. For example in Figure 1, for the target group ğ‘”, its group memberğ‘¢and high-order neighborğ‘–only have few interactions. The embeddings ofğ‘¢andğ‘–are inaccurate, which will aî€ect the embedding ofğ‘”when performing graph convolution. Third, existing GNNs only consider the correlations between the group and its members, but ignore the correlations between the group and non-group members. Thus, this inspires the following research problem: how can we learn more accurate embeddings by GNNs for occasional group recommendation? To this end, motivated by the self-supervised learning (SSL) technique [16,21,26], which aims to spontaneously î€›nd the supervised signals from the input data itself and can further beneî€›t the downstream tasks, we propose a newSelf-supervisedGraph learning paradigm forGroup recommendation (SGG), which trains the backbone GNN model to jointly reconstruct the group/user/item embeddings from multiple interaction graphs under the meta-learning setting [33], such that it can directly improve the embedding quality. Speciî€›cally, we î€›rst pick groups/users/items with suî€œcient interactions as the target groups/users/items and then learn their ground truth embeddings from the observed abundant interactions. To simulate the cold-start scenarios, in each training episode, we randomly sampleğ¾neighbors for each target group/user/item in their counterpart interaction graphs, based on which we perform graph convolution multiple steps in each interaction graph, and fuse the corresponding reî€›ned embeddings to predict the target embedding. Finally, we jointly optimize the reconstruction losses between the predicted embeddings and the ground truth embeddings of groups/users/items, making the GNN model easily and rapidly being adapted to new cold-start groups/users/items. Through the above process, the reconstructed group representation can contain all the signals as presented in the î€›rst challenge. Nevertheless, the above proposed pretext task still can not explicitly deal with the high-order cold-start neighbors. Besides, the correlations between the groups and non-group members are still not explicitly considered. To further deal with the high-order cold-start neighbors, we incorporate a meta aggregator to enhance the aggregation ability of each graph convolution step. Speciî€›cally, the meta aggregator learns cold-start nodeâ€™s embedding on its î€›rst-order neighbors in the counterpart interaction graphs by self-attention mechanism under the same meta-learning setting, which is then incorporated into each graph convolution step to enhance the aggregation ability. To explicitly consider the correlations between groups and non-group members, motivated by Contrastive Learning (CL) [7], which pulls the similar instances together while pulling away dissimilar instances, we propose a CL adapter, which contrasts the group embedding with its membersâ€™ and non-membersâ€™ embeddings to regularize the group and user embeddings under the same meta-learning setting. The contributions are as follows: â€¢We design a new self-supervised graph learning paradigm for group recommendation, which jointly trains the backbone GNN model to reconstruct the group/user/item embedding under the meta-learning setting. To deal with the cold-start neighbors, we further introduce a meta aggregator to enhance the aggregation ability of each graph convolution step. â€¢To explicitly consider the correlations between the group and non-group members, we further propose a CL adapter to regularize the group and user embeddings. â€¢Experimental results on the group recommendation task demonstrate the superiority of our proposed model against the stateof-the-art group recommendation models. In this section, we î€›rst deî€›ne the problem and then present a base GNN framework that can be used to solve the problem. There are three sets of entities in the group recommendation scenario: a user setğ‘ˆ={ğ‘¢, Â· Â· Â· , ğ‘¢}, an item setğ¼={ğ‘–, Â· Â· Â· , ğ‘–}and a group setğº={ğ‘”, Â· Â· Â· , ğ‘”}. There are three kinds of observed interaction graphs amongğ‘ˆ,ğ¼andğº: group-item subgraphG, user-item subgraphGand group-user subgraphG. Since the social connections of user-user and group-group are also important to depict the user and group proî€›les, we build two kinds of implicit interaction graphs based onGandG, namely useruser subgraphGand group-group subgraphG. InG, the two users are connected if they share with more thanğ‘items. Similarly, inG, the two groups are connected if they share with more thanğ‘items. Formally, we use notationG={V, E}to denote the set of observed and implicit interaction graphs, i.e.,G= Gâˆª Gâˆª Gâˆª Gâˆª G, whereVis the set of nodes {ğ‘ˆ , ğ¼,ğº }, and E is the set of edges. Deî€›nition 1.GNN for Group Recommendation. Given the interaction graphG, we aim to train a GNN-based encoderğ‘“that can recommend top-ğ‘˜ items for the target group ğ‘”. Although existing GNN-based group recommendation methods show their diversity in modelling group interactions with users and items [12,13,19,34], we notice that they essentially share a general model structure. Based on this î€›nding, we present a base GNN model, which consists of a representation learning module and a jointly training module. The representation learning module learns the representations of groups and users upon their counterpart interaction graphs, while the jointly training module optimizes the user/group preferences over items to compare the likelihood and the true user-item/group-item observations. 2.2.1 Representation Learning Module. This module î€›rst learns the user representation upon the user-item and user-user subgraphs, and then learns the group representation upon the group-group, group-item and group-user subgraphs. Speciî€›cally, for each user ğ‘¢, we î€›rst sample his î€›rst-order neighbors onGandG, and then perform graph convolution:h= CONV(h, h), h= CONV(h, h), whereCONVcan be instantiated into any GNN models, such as LightGCN [17] or GCN [22].hand hdenote the user embeddings calculated fromGandG at theğ‘™-th graph convolution step,handhare randomly initialized embeddings.handhmean the averaged neighbor embeddings, where the neighbors are sampled fromG andG, respectively. After performingğ¿-th convolution steps, we can obtain the reî€›ned user embeddingshandhfrom the counterpart subgraphs. Finally we use attention mechanism [18] to aggregate these embeddings to form the î€›nal user embedding {W|ğ‘ âˆˆ {ğ‘ˆ ğ¼, ğ‘ˆğ‘ˆ }}are trainable parameters,{ğ‘|ğ‘ âˆˆ {ğ‘ˆ ğ¼, ğ‘ˆğ‘ˆ } are the learned attention weight for each subgraph. Then for each group ğ‘”, we î€›rst sample its î€›rst-order neighbors from the G, Gand G, and perform graph convolution: whereh,handhdenote the group embeddings calculated fromG,GandGat theğ‘™-th graph convolution step,h,handhare randomly initialized embeddings. h,handhmean the averaged neighbor embeddings, where the neighbors are sampled fromG,Gand G, respectively. After performingğ¿-th convolution steps, we can obtain the reî€›ned group embeddingsh,handh from these three subgraphs. Same as existing works [12,19,19,44], we further average the î€›rst-order neighbors inGto obtain the aggregated group embedding h, wherehis obtained by performing graph convolutionğ¿steps in G,N (ğ‘”)denotes the î€›rst-order user set sampled fromG, ğ‘“is the aggregate function such as average [12], summation and pooling [43], or attention mechanism [32]. In our experiments, we î€›nd attention mechanism leads to the best performance. Finally, we use attention mechanism to aggregate the above embeddings to form the î€›nal group embedding h: where{W|ğ‘ âˆˆ {ğºğ¼, ğºğ‘ˆ , ğºğ‘ˆ, ğºğº }}are trainable parameters,{ğ‘|ğ‘ âˆˆ {ğºğ¼, ğºğ‘ˆ ,ğºğ‘ˆ, ğºğº } are the learned attention weights. 2.2.2 Jointly Training Module. This module jointly optimizes the user preferences over items with the user-item lossLand the group preferences over items with the group-item lossL, i.e., L= L+ ğœ†L, whereLis the î€›nal recommendation loss with a balancing hyper-parameterğœ†. Here, we use BPR loss [28] to calculate Land L:îƒ• L=âˆ’ ln ğœ (ğ‘¦(ğ‘”, ğ‘–) âˆ’ ğ‘¦(ğ‘”, ğ‘—)), whereğ‘¦(ğ‘¢, ğ‘–) = hh,ğ‘¦(ğ‘”, ğ‘–) = hh,ğœis an activation function, Eand Erepresent the edges in Gand G. Although the above presented GNNs can address the occasional groups through incorporating high-order collaborative signals, they still can not deal with the groups/users/items with few interactions, and thus can not learn high-quality embeddings for them. We present the proposed self-supervised graph learning paradigm for group recommendation (SGG). We î€›rst describe the process of embedding reconstruction with GNN, and then explain a meta aggregator and a CL adapter that are incorporated in the GNN model to further improve the embedding quality. Finally, we explain howSGGis trained and analyze its time complexity. The overall framework of SGG is shown in Figure 2. We propose embedding reconstruction with GNN, which jointly reconstructs the groups/users/items embeddings from multiple subgraphs under the meta-learning setting. Here we take the group embedding reconstruction as an example. User and item embedding reconstruction can be explained in the same way. To achieve this Figure 2: The overall framework of SGG for group recommendation. SGG contains a meta aggregator which has incorporated a self-attention-based meta learner at each step of the original GNN aggregation, and a CL adapter, which explicitly captures the correlations between the group and non-group members. goal, we need abundant occasional groups as the training instances. Since we also need ground truth embeddings of the occasional groups to learnğ‘“, we simulate those groups from the target groups with abundant interactions with items. The ground truth embeddings for each groupğ‘”, i.e.,h, is learned upon the observed abundant interactions by any group recommendation model such as AGREE [4] or LightGCN [17]. To mimic the occasional groups, in each training episode, for each target group, we randomly sample ğ¾items,ğ¾users andğ¾groups from the corresponding groupitem subgraphG, group-user subgraphGand group-group subgraphG. Then for each subgraph, we repeat the sampling processğ¿steps from the target group to theğ¿-order neighbors, which results in at mostğ¾(1â‰¤ ğ‘™ â‰¤ ğ¿) ğ‘™-order neighbors for each target group. Next we perform graph convolutionğ¿steps from scratch using Eq.(1)to obtain the reî€›ned group embeddingsh, handh, use Eq.(2)to obtain the aggregated group embeddingh, and use Eq.(3)to obtain the fused group embedding h. Finally, same with [16,20], we use cosine similarity to measure the similarity between the predicted embeddinghand the ground-truth embeddingh, as the cosine similarity is a popularity indicator for the semantic similarity between embeddings: whereÎ˜is the set of the parameters inğ‘“. Similarly, we can reconstruct the user embedding based onGandGwith loss L, and reconstruct the item embedding based on Gwith loss L. In practice, we jointly optimize group/user/item embedding reconstruction tasks with loss L: Training GNNs in the meta-learning setting can explicitly reconstruct the group embeddings, making GNNs easily and rapidly being adapted to new occasional groups. After the model is trained, for a new arriving occasional group, based on its î€›rst- and high-order neighbors, we can predict an accurate embedding for it. However, the basic embedding reconstruction task does not specially address the cold-start neighbors. During the original graph convolution process, the inaccurate embeddings of the cold-start neighbors and the embeddings of other neighbors are equally treated and aggregated to represent the target group. Although some GNN models such as GrageSAGE [14] or FastGCN [6] î€›lter neighbors before aggregating them, they usually follow the random or importance sampling strategies, which ignores the cold-start characteristics of the neighbors. Out of this consideration, we incorporate a meta aggregator into the above embedding reconstruction model. We propose the meta aggregator to deal with the high-order coldstart neighbors. Speciî€›cally, before training the GNNğ‘“, we train another functionğ‘“under the same meta-learning setting as proposed in Section 3.1. The meta aggregatorğ‘“learns an additional embedding for each node only based on its î€›rst-order neighbors sampled from the counterpart graphs, thus it can quickly adapt to new cold-start nodes and produce more accurate embeddings for them. The embedding produced byğ‘“is combined with the original embedding at each convolution inğ‘“. Although both the GNN model and the meta aggregator are trained under the same meta-learning setting, the GNN model is to tackle the cold-start target nodes, but the meta aggregator is to enhance the cold-start neighborsâ€™ embeddings. Here we take group embedding as an example. User and item embedding can be explained in a similar way. Speciî€›cally, we instantiateğ‘“as a self-attention encoder [32]. For each group ğ‘”,ğ‘“accepts the randomly initialized î€›rst-order embeddings {h, Â· Â· Â· , h},{h, Â· Â· Â· , h}and{h, Â· Â· Â· , h} from the corresponding subgraphsG,GandGas input, calculates the attention scores of all the neighbors to each neighbor of ğ‘”, aggregates all the neighborsâ€™ embeddings according to the attention scores to produce the smoothed embeddings{h, Â· Â· Â· , h}, {h, Â· Â· Â· , h} and {h, Â· Â· Â· , h}. The process is: {h, Â· Â· Â· , h} â† SELF_ATTENTION({h, Â· Â· Â· , h}), where the embeddingË†handË†hcan be obtained in the same way. Furthermore, the aggregated group embeddingh in Eq.(2)is also considered to reconstruct the group embedding. Finally,ğ‘“fuses these embeddings using Eq.(3)to obtain the predicted embeddingË†h, named as the meta embedding of groupğ‘”. The self-attention technique, which pushes the dissimilar neighbors further apart and pulls the similar neighbors closer together, can capture the major preference of the nodes from its neighbors. The same cosine similarity described in Eq.(5)is used as the loss function to measure the similarity between the predicted meta embeddingË†hand the ground truth embedingh. Onceğ‘“is learned, we add the meta embeddingË†h,Ë†handË†hinto each graph convolution step of the GNN ğ‘“ in Eq. (1): For a target groupğ‘”, Eq.(8)is repeatedğ¿steps to obtain the embeddingsh,h,h, Eq.(2)is used to obtain the aggregated group embeddingh, and Eq.(3)is applied to get the î€›nal embeddingh. Finally, the same cosine similarity in Eq.(5)is used to optimize the model parameters, which include the parameters Î˜of the GNN model andÎ˜of the meta aggregator. Similarly, ğ‘“can obtain the meta user embedding onGandG, and the meta item embedding on G. To further consider the correlations between the group and nongroup members, motivated by the contrastive learning technique [7], which pulls the similar instances together while pulling away dissimilar instances, we propose a CL adapter, which contrasts the group embedding with its membersâ€™/non-membersâ€™ embeddings to regularize the group and user embeddings under the same metalearning setting. The group and its members form the positive pair, while the group and its non-group members form the negative pair. The CL loss is deî€›ned as: îƒ•îƒ•exp(cos(h, h)/ğœ) whereN (ğ‘”)denotes the group member set of groupğ‘”,ğ‘¢denotes the group member,ğ‘¢denotes non-group member, where for each ğ‘¢, we sample at mostğ¾instances forğ‘¢.\deî€›nes set subtraction operation,h,h,hare the reconstructed embeddings, andğœis a temperature parameter. To improve recommendation with the SSL task, we leverage a multitask training strategy [36] to jointly optimize the classic recommendation task (cf. Eq.(4)) and the self-supervised learning task (cf. Eq. (6) and Eq. (9)): whereÎ˜={Î˜, Î˜}is the model parameters,ğœ†andğœ†are hyperparameters to control the strengths ofSGGand L2 regularization, respectively. We also consider the alternative optimization â€” pre-training onL+ Land î€›ne-tuning onL. We detail its recommendation performance in Section 4.4.3. Here we present the time and space complexity ofSGG. Same as LightGCN [17], we implementSGGas the matrix form. Suppose the number of edges in the interaction graphGis|E |. The number of edges in the masked interaction graphË†Gis|Ë†E|. Since we mask a large proportion neighbors for each node inGto simulate the cold-start scenario, the size of masked edge set is far less than the original edge set, i.e.,|Ë†E| â‰ª |E|. Letğ‘ denote the number of epochs,ğ‘‘denote the embedding size andğ¿denote the number of Table 1: The comparison of analytical time complexity b etween GNN and SGG. GCN layers. SinceSGGintroduces meta embedding to enhance the aggregation ability, the space complexity ofSGGis twice than that of the vanilla GNN model. The time complexity comes from four parts, namely normalization of adjacency matrix, graph convolution, recommendation loss and self-supervised loss. As there is no change on the model structure and inference process, the time complexity ofSGGin the graph convolution and recommendation loss is the same as the vanilla GNN model. We present the main diî€erences between the vanilla GNN and SGG models as follows: â€¢Normalization of adjacency matrix. Since we generate î€›ve independent subgraphs per epoch, given the fact that the number of non-zero elements in the adjacency matrices of full training graph and the î€›ve subgraphs are 2|E |, 2|Ë†E|, 2|Ë†E|, 2|Ë†E|, 2|Ë†E|and 2|Ë†E|, respectively, its total complexity isğ‘‚ ((2|Ë†E|+ â€¢Self-supervised loss. We evaluate the self-supervised tasks upon the masked subgraphs. For the user or item embedding reconstruction task, the time complexity isğ‘‚ (2ğ‘‘ âˆ— (2|Ë†E| +2|Ë†E|) âˆ— ğ‘  âˆ— ğ¿) â‰ˆ8|Ë†E|ğ¿ğ‘‘ğ‘ . For the group embedding reconstruction task, 12|Ë†E|ğ¿ğ‘‘ğ‘ , where 2ğ‘‘represents the concatenated embedding size, as we incorporate the meta embedding to the graph convolution process. For the mutual information maximization task, the time complexity isğ‘‚ (2ğ‘‘ âˆ—2|Ë†E| âˆ— ğ‘  âˆ— ğ¿) â‰ˆ4|Ë†E|ğ¿ğ‘‘ğ‘ . Thus the total time complexity of self-supervised loss is 24|Ë†E|ğ¿ğ‘‘ğ‘ . We summarize the time complexity between the vanilla GNNs and SGGin Table 1, from which we observe that the time complexity of SGGis in the same magnitude with the vanilla GNNs, which is totally acceptable, since the increased time complexity ofSGGis only from the self-supervised loss. The details are shown in Section 4.4.1. To verify the superiority and eî€ectiveness ofSGG, we conduct extensive experiments and answer the following research questions: â€¢ RQ1:How doesSGGperform occasional group recommendation compared with the state-of-the-art GNN models? â€¢ RQ2:What are the beneî€›ts of performing pretext tasks in occasional group recommendation? â€¢ RQ3:How do diî€erent settings inî€uence the eî€ectiveness of the proposed SGG model? 4.1.1 Datasets. We evaluate on three public datasets including Weeplaces [29], CAMRa2011 [4] and Douban [39]. Table 1 illustrates the statistics of these datasets. 4.1.2 Baselines. We select three types of baselines including the state-of-the-art attention based model, the general GNN models and the hypergraph GNN models for group recommendation: â€¢ MoSAN [31]: adopts sub-attention mechanism to model the group-item interactions. â€¢ AGREE [4]: adopts attention mechanism for jointly modelling user-item and group-item interactions. â€¢ SIGR [39]: further incorporates social relationships of groups and users to model the attentive group and user representations. â€¢ GroupIM [29]: further regularizes group and user representations by maximizing the mutual information between the group and its members. â€¢ GAME [19]: performs graph convolution only based on the î€›rstorder neighbors from the group-group, group-user and groupitem graphs for group recommendation. â€¢ GCMC [37]employs the standard GCN [22] model to learn the node embeddings. â€¢ NGCF [35]adds second-order interactions upon the neural passing based GNN model [10]. â€¢ LightGCN [17]: devises the light graph convolution upon NGCF. â€¢ HHGR [44]: designs coarse- and î€›ne-grained node dropout strategies upon the hypergraph for group recommendation. We discard potential baselines like Popularity [8], COM [42], and CrowdRec [27], since previous works [4,19,29,31,39] have validated the superiority over the compared ones. For the GNN model GCMC, NGCF and LightGCN, we extend it to address group recommendation as proposed in Section 2.2; Besides, we use notation GNN* to denote the corresponding proposed modelSGG. We further evaluate three variants ofSGG, named Basic-GNN, Meta-GNN and CL-GNN, which are equipped with basic embedding reconstruction with GNNs (Section 3.1), meta aggregator (Section 3.2) and only CL adapter (Section 3.3), respectively. 4.1.3 Training Seî€ings. We present the details of dataset segmentation, model training process and hyper-parameter settings. Dataset Segmentation.We î€›rst select the groups/users with sufî€›cient interactions as the target groups/users in the meta-training setğ·/ğ·, and leave the rest groups/users in the meta-test set ğ·/ğ·, as we need the true embeddings of groups/users inferred from the suî€œcient interactions. In order to avoid information leakage, we further select items with suî€œcient interactions fromğ·/ğ·, and obtain the meta-training setğ·. For simplicity, we useğ·and ğ·to denote these meta-training and meta-test sets. For each group/user inğ·, we further select topğ‘% of its/his interacted items in chronological order into the training setğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›, and leave the rest items into the test set ğ‘‡ ğ‘’ğ‘ ğ‘¡. For addressing the occasional groups, we divide the groups with the number of direct interacted items more thanğ‘›intoğ·and leave the rest groups intoğ·. We selectğ‘›as 10 for Weeplaces and Douban. Similarly, we divide the users (or items) with the number of direct interacted items (users) more thanğ‘›(ğ‘›) intoğ·and leave the rest users (items) intoğ·, where bothğ‘›andğ‘›is set as 10 for Weeplaces and Douban. In CAMRa2011, since the groups, users and items have abundant interactions, we randomly select 70% groups, users and items inğ·and leave the rest inğ·. For each group and user inğ·, we only keep top 10 interacted items in chronological order to simulate the real cold-start scenario. Similarly, for each item in ğ·, we only keep its î€›rst 5 interacted groups/users. Model Training Process.We train each of the baseline methods to obtain the ground-truth embeddings onğ·, as these methods are good enough to learn high-quality embeddings from the abundant interactions. For MoSAN, AGREE, SIGR, GroupIM and GAME, we directly fetch the trained embeddings as ground-truth embeddings; For HHGR, GCMC, NGCF and LightGCN, we combine the embeddings obtained at each layer to form the ground-truth embeddings, take the group embedding as an example,h=h+Â· Â· Â·+h. User and item embeddings can be explained in the similar way. The SSL tasks is trained onğ·, while the recommendation task is trained onğ·andğ‘‡ğ‘Ÿğ‘ğ‘–ğ‘›. Both the SSL and the recommendation tasks are evaluated inğ‘‡ğ‘’ğ‘ ğ‘¡. We adopt Recall@Kand NDCG@K as the metrics to evaluate the items ranked by the relevance scores. Hyper-parameter Settings.For fair comparison, all models are trained from scratch which are initialized with the Xavier method [11]. The learning rate is 0.001 and mini-batch size is 256. We tuneğ¾,ğ¿, ğ‘% within the ranges of {3, 4, 5, 6, 7, 8, 9, 10, 11, 12}, {1,2,3,4} and {0.1, 0.2, 0.3}, respectively. We tuneğœ†with the ranges of {0.01, 0.1, 0.5, 1.0, 1.2}, and empirically setğœ†andğœ†as 1 and 1e-6, respectively. We tuneğ‘andğ‘with the ranges of {10, 20, 30}. By default, we set ğ¿ as 3, ğ¾ as 5, ğ‘% as 0.1, ğœ as 0.2, ğ‘and ğ‘as 20, and K as 20. 4.2.1 Overall Performance Comparison. We report the overall recommendation performance in Table 3. The results show that compared with other baselines, our proposedSGG(denoted asGNN) signiî€›cantly improves the recommendation performance, which indicates the proposed SSL tasks are useful to learn high-quality embeddings, and can further beneî€›t the recommendation task. Besides,SGGis better than the most competitive baseline method HHGR, which indicates the superiority of the proposed SSL tasks in dealing with high-order cold-start neighbors. 4.2.2 Interacted Number and Sparse Rate Analysis. It is still unclear how doesSGGhandle the cold-start groups and users with diî€erent interacted items (ğ‘›andğ‘›) and diî€erent sparse rateğ‘%. To this end, Table 3: Overall recommendation performance with sparse rate ğ‘%=0.1, layer size ğ¿=3 and neighbor size ğ¾=5. Figure 3: Recommendation performance under diî€erent interacted numbers ğ‘›and ğ‘›(shown in Fig 3(a)), and under diî€erent sparse rate ğ‘% (shown in Fig 3(b)).Results on CAMRa2011 and Douban show the same trend which are omitted for space. we changeğ‘›andğ‘›in the range of{5,10,15}while keepingğ‘% as 0.1,ğ¿as 3 andğ¾as 5; and changeğ‘% in the range of{0.1,0.2,0.3} while keepingğ‘›andğ‘›as 5,ğ¿as 3 andğ¾as 5. We compare our proposed modelSGG(denoted as LightGCN*, in which we select LightGCN as the backbone GNN model) with competitive baseline methods AGREE, GroupIM, HHGR and LightGCN, and report the recommendation performance in Figure 3. The smallerğ‘›,ğ‘›and ğ‘% are, the groups and users inğ·have fewer interactions. Based on the results, we î€›nd that: (1)LightGCNis consistently superior to all the other baselines, which justiî€›es the superiority ofSGGin handling cold-start recommendation with diî€erentğ‘›,ğ‘›andğ‘%. (2) Whenğ‘›andğ‘›decrease from 15 to 5, and whenğ‘% decreases from 0.3 to 0.1,SGGall has a larger improvement compared with other baselines, which veriî€›es its capability to solve the cold-start groups/users with extremely sparse interactions. It is still not clear which part of the pretext tasks is responsible for the good performance inSGG. To answer this question, we conduct an ablation study to investigate the recommendation performance ofSGGand its variant models in Table 4. We î€›nd that: (1) BasicGNN, Meta-GNN and CL-GNN is consistently superior than the vanilla GNNs, which indicates the eî€ectiveness of the proposed Table 4: The comparison of diî€erent SGG variants with sparse rate ğ‘%= 0.1, layer size ğ¿=3 and neighbor size ğ¾=5. Table 5: Recommendation performance, training time per epoch and convergent epochs w/wo meta-learning setting. SSL tasks. (2) Among all the variant models, Meta-GNN performs the best, which indicates enhancing the cold-start neighborsâ€™ embedding quality is much more important. (3) GNN* performs the best, which veriî€›es the superiority of combining these SSL tasks. 4.4.1 Eî€›ectiveness of Meta-Learning Seî€ing. As mentioned in Section 3.1, we trainSGGunder the meta-learning setting. To explore whether the meta-learning setting can beneî€›t the recommendation performance and have satisfactory time complexity, we compare SGGand the vanilla GNN model with a variant modelSGG-M, which removes the meta-learning setting. More Concretely, inSGGM, for each group/user/item, we do not sampleğ¾neighbors, but instead directly using their î€›rst-order and high-order neighbors to perform graph convolution. We report the average recommendation performance, the average training time in each epoch and the average convergent epoch in Table 5. Based on the results, we î€›nd thatSGGis consistently superior thanSGG-M, and has much more smaller training time in each epoch, much faster converges speed. This indicates trainingSGGin the meta-learning setting can not only improve the model performance, but also improve the training eî€œciency and make the model easily and rapidly being adapted to new occasional groups. 4.4.2 Sensitive of Ground-truth Embedding. As mentioned in Section 3.1, when performing embedding reconstruction with GNNs, we select any group recommendation models to learn the groundtruth embeddings. One may consider whether the recommendation performance is sensitive to the ground-truth embeddings obtained Figure 4: Sensitive analysis of ground-truth emb eddings. Table 6: Recommendation performance, training time for each epoch and convergent epochs under the multi-task learning or pre-training paradigms. by diî€erent models. To this end, we use competitive baselines to learn the ground-truth embeddings as proposed in Section 4.1.3, and report the performance of NGCF* and LightGCN* in Figure 4. Notation NGCF*-AGREE denotesSGGis equipped with the groundtruth embeddings, which are obtained by AGREE. Other notations are deî€›ned in a similar way. The results show that all the models that equipped with diî€erent ground-truth embeddings achieve almost the same performance, which indicatesSGGis not sensitive to the ground-truth embeddings, as the baselines are good enough to learn high-quality embeddings from the abundant interactions. 4.4.3 Multi-task Learning Vs Pre-training. Here we would like to answer the question in Section 3.4: Can the recommendation performance beneî€›t from the pre-training or the multi-task learning paradigm? Towards this goal, we î€›rst pre-train the SSL tasks onğ· to obtain the model parameters, use them to initializeSGG, and then î€›ne-tuneSGGonğ·via optimizing the main task. We term this variant asSGG-P. We report the recommendation performance, the average training time in each epoch and the average convergent epoch in Table 6. Based on the results, we î€›nd that: (1)SGG-P performs worse thanSGG, but still better than other baselines (cf. Table 3). As jointly training inSGGadmits that the representations in the main and SSL tasks are mutually enhanced with each other, which is superior than only oî€er a better initialization for the GNNs inSGG-P. This î€›nding is consistent with the observation in previous studies [36]. (2) Compared withSGG-P,SGGhas faster convergence speed. AlthoughSGG-P has smaller training time per epoch, it still has larger total training time thanSGG. This veriî€›es the multi-task learning paradigm can speed up the model convergence. 4.4.4 Hyper-parameter analysis. We move on to study diî€erent designs of the layer depthğ¿and the neighbor sizeğ¾. We select LightGCN*, NGCF* and GCMC*, and report their performances Figure 5: Recommendation performance under diî€erent layer ğ¿ (Fig. 5(a) and Fig. 5(b)), and under diî€erent neighbor size ğ¾ (Fig. 5(c) and Fig. 5(d)). Results on CAMRa2011 and Douban show the same trend with Weeplaces which are omitted for space. under diî€erent layer depthğ¿in Figure 5(a) and Figure 5(b), under diî€erent neighbor sizeğ¾in Figure 5(c) and Figure 5(d). The results show that: (1) The performance î€›rst increases and then drops when increasingğ¿from 1 to 4. The peak point is 3 at most cases. This indicates GNN can only capture short-range dependencies, which is consistent with LightGCNâ€™s [17] î€›nding. (2) The performance î€›rst increases and then drops when increasingğ¾from 3 to 12. The peak point is 8 at most cases. This indicates incorporating proper size of neighbors can beneî€›t the recommendation task. Existing works on group recommendation can be generally divided into two categories: score aggregation and proî€›le aggregation. Score Aggregation.This strategy pre-deî€›nes a scoring function to obtain the preference score of all members in a group on the target item. The scoring functions include average [2], least misery [1] and maximum satisfaction [3]. However, due to the static recommendation process of the predeî€›ned functions, these methods easily fall into local optimal solutions. Proî€›le Aggregation.This strategy aggregates the proî€›les of group members and feeds the fused group proî€›le into individual recommendation models. Essentially, probabilistic generative models and deep learning based models are proposed to aggregate the group proî€›le. The generative model î€›rst selects group members for a target group, and then generates items based on the selected members and their associated hidden topics [24,38,42]. The deep learning based model conducts attention mechanism to assign each user an attention weight, which denotes the inî€uence of group member in deciding the groupâ€™s choice on the target item [4,5,39]. However, both of these two methods suî€er from the data sparsity issue. Recently, researches propose GNN-based recommendation models, which incorporate high-order collaborative signals in the built graph [13,19,34] or hypergraph [12,41]. Moreover, Zhang et al. [44] propose hypergraph convolution network (HHGR) with selfsupervised node dropout strategy to alleviate the data sparsity issue. However, the GNNs still can not deal with the cold-start neighbors when performing graph convolution, and do not explicitly capture the correlations between the group and non-group members. Motivated by the SSL technique, we propose to jointly reconstruct the group/user/item embeddings under the meta-learning setting, and further incorporate a meta aggregator and a CL adapter to improve the embedding quality. We design a new self-supervised graph learning paradigm for group recommendation, which trains the GNN model to reconstruct the group embedding under the meta-learning setting. To deal with the cold-start neighbors during the graph convolution process, we further introduce a meta aggregator to enhance the aggregation ability of each graph convolution step. To explicitly consider the correlations between the group and non-group members, we further propose a CL adapter to regularize the group and user embeddings. Experimental results demonstrate the superiority of our proposed model against the state-of-the-art group recommendation models.