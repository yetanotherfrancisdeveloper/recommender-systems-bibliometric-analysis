With the prevalence of social media, there has recently been a proliferation of recommenders that shift their focus from individual modeling to group recommendation. Since the group preference is a mixture of various predilections from group members, the fundamental challenge of group recommendation is to model the correlations among members. Existing methods mostly adopt heuristic or attention-based preference aggregation strategies to synthesize group preferences. However, these models mainly focus on the pairwise connections of users and ignore the complex high-order interactions within and beyond groups. Besides, group recommendation suî€ers seriously from the problem of data sparsity due to severely sparse group-item interactions. In this paper, we propose a self-supervised hypergraph learning framework for group recommendation to achieve two goals: (1) capturing the intraand inter-group interactions among users; (2) alleviating the data sparsity issue with the raw data itself. Technically, for (1), a hierarchical hypergraph convolutional network based on the userand group-level hypergraphs is developed to model the complex tuplewise correlations among users within and beyond groups. For (2), we design a double-scale node dropout strategy to create selfsupervision signals that can regularize user representations with diî€erent granularities against the sparsity issue. The experimental analysis on multiple benchmark datasets demonstrates the superiority of the proposed model and also elucidates the rationality of the hypergraph modeling and the double-scale self-supervision. â€¢ Information systems â†’ Recommender systems. Group Recommendation, Graph Neural Networks, Hypergraph Learning, Self-Supervised Learning ACM Reference Format: Junwei Zhang, Min Gao, Junliang Yu, Lei Guo, Jundong Li, and Hongzhi Yin. 2021. Double-Scale Self-Supervised Hypergraph Learning for Group Recommendation. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 11 pages. https: //doi.org/10.1145/3459637.3482426 Owing to the rapid development of social media in recent years, users with similar interests now have opportunities to form various online groups [34,43]. Traditional recommender systems designed for individuals can no longer serve the interest of groups. There is an urgent need to develop practical group recommender systems [3,6,50]. The group recommendation aims to reach a consensus among group members, generating suggestions that can cater to most group members [21,29]. However, the inî€uences of users vary from group to group, leading to hard decision-making. An illustrative example is shown in Figure 1: Alice and her colleagues (Bob and Tony) form a group that are fond of slow food. However, when Alice and her boss Allen (who likes fast food) go for lunch, fast food would be the î€›rst choice due to Allenâ€™s higher position. Meanwhile, since Allen has an appetite for pizza, pizza is also recommended to Alice and Bob. It is natural that collective decisions tend to be dynamic, i.e., a groupâ€™s preference may vary due to the inî€uence of other user members and other groupsâ€™ preferences. As such, a critical issue is to model the complex interactions of users and groups in the research of group recommendations. Figure 1: An illustrative example of group recommendation. However, existing group recommendation models fail to model the interactions of users and groups well. Many of them adopt heuristic rules, such as average [3,4], least misery [1], and maximum satisfaction [5], which ignore the impact of user interactions. With the success of deep learning, several group recommendation models employ the attention mechanism over user-item graphs to exploit user interactions and synthesize group preferences [20,35,44]. Nevertheless, these models keep attention on the pairwise connections of users and ignore the complex highorder interactions within and beyond groups. Besides, since many groups are formed temporally, the group-item interaction records are very sparse, making it more diî€œcult to learn an accurate group preference directly from the interaction data [20,44,45]. To tackle the aforementioned two limitations, in this paper, we propose a self-supervised hypergraph learning framework for group recommendation to achieve two goals: (1) capturing the intra- and intergroup interactions among users; (2) alleviating the data sparsity issue with the raw data itself. To handle the interaction issue, we explicitly model the complex tuplewise relationships as a hierarchical hypergraph, which aggregates the correlated users within and beyond groups. Distinct from the edge in simple graphs, the hyperedge can connect multiple nodes, which is a natural way to capture tuplewise relations. The proposed hierarchical hypergraph consists of two levels: user- and group-level, where each level of hypergraphs contains the corresponding type of nodes. In the user-level hypergraph, each hyperedge connects the set of users belonging to the same group. To capture the intra-group interactions among diî€erent users, we adopt the hypergraph convolutional network to aggregate the related users and generate the dynamic user embeddings. Meanwhile, for exploiting the informative inter-group interactions among users, we adopt triangular motifs [55] which allow each group to select the most relevant groups to acquire information to form the hyperedges in the group-level hypergraph. Despite the beneî€›ts of hypergraph modeling, the group recommendation performance is still compromised by the data sparsity problem, which not only exists in group interaction but also in user interaction. To alleviate this issue, we integrate self-supervised learning (SSL), which can discover self-supervision signals from the raw data, into the training of the hierarchical hypergraph convolutional network. Many existing SSL-based models commonly conduct node/edge dropout in the user-item graph to augment supervised signals [16,31,47]. However, this may drop some essential information, especially when the raw group is very sparse, leading to unserviceable supervision signals. Therefore, we propose a double-scale node dropout strategy to create supervisory labels with diî€erent and î€›ner granularities. The intuition behind is that not all user-user interactions signiî€›cantly aî€ect the î€›nal group decision, and leveraging partial information can help learn invariant representations. Meanwhile, two diî€erent-grained views may to some degree prevent the loss of essential information (e.g. drop important nodes). Concretely, at a coarse granularity, we randomly remove some nodes from the user-level hypergraph structure. As a result, when a removed node belongs to multiple groups, it will disappear in all related hyperedges. As for the î€›ne-grained node dropout, we remove some member nodes only from a speciî€›c group, which does not aî€ect this nodeâ€™s existence in other groups. We then maximize the mutual information between the node representations learned from two granularities to regularize the user and group representation against the sparsity issue. Finally, we unify the recommendation task and the self-supervised learning task to optimize model parameters by a two-stage training approach. In summary, the main contributions of this paper are as follows: â€¢We devise a hierarchical hypergraph learning framework to capture the intra- and inter-group interactions among users in the group recommendation. â€¢We propose a SSL strategy with diî€erent granularities to enhance user and group representations and alleviate the data sparsity problem, which is seamlessly coupled with the hierarchical design of the hypergraph convolutional network. â€¢We conduct extensive experiments on three group recommendation datasets to exhibit the superiority of the proposed model over some recent baselines and elucidate why the hypergraph learning and the double-scale SSL strategy can improve group recommendation. The code implementation of our model is released at https://github.com/0411tony/HHGR. The remainder of this paper is organized as follows. We î€›rst provide preliminaries of the hypergraph and the deî€›nition of the group recommendation in Section 2. Then, we present our proposed model in detail in Section 3. Section 4 describes experimental research. Next, we further discuss the related work in Section 5. Finally, we summarize our work and look forward to future work in Section 6. To facilitate understanding, we present the deî€›nition of the hypergraph and the task of the group recommendation in this section. The hypergraph is deî€›ned asğº = (V, E), whereVis the vertex set containingğ‘€unique vertices, andEis the hyperedge set containingğ‘hyperedges. Each hyperedgeğœ– âˆˆ Econtains multiple vertices. The hypergraph can be represented by an incidence matrix ğ‘¯ âˆˆ R, whereğ’‰= 1if the hyperedgeğœ–contains the vertex ğ‘£ âˆˆ ğ‘‰, otherwiseğ’‰= 0. For each hypergraph, we use the diagonalÃ matrixğ‘«to denote the degrees of vertex, whereğ’…=ğ’˜ğ’‰. ğ‘¾ âˆˆ Ris the diagonal matrix of the weight of the hyperedge. Let diagonal matrixğ‘©denote the degree of hyperedge, whereÃ ğ’ƒ=ğ’‰represents the number of vertices connected by the hyperedge ğœ–. The task of the group recommendation is to predict the item ranking list for the given group that has multiple members. We considerî€ˆî€‰î€ˆî€‰ U =ğ‘¢, ğ‘¢, ..., ğ‘¢andI =ğ‘–, ğ‘–, ..., ğ‘–to denote the userî€ˆî€‰ set and item set, respectively. LetG =ğ‘”, ğ‘”, ..., ğ‘”be the group set, whereğ‘”âˆˆ Gis theğ‘š-th group. There are three observable interaction behaviors: user-item interaction, group-item interaction, and user-group interaction. Letğ‘¹ âˆˆ Rdenote the useritem interaction matrix, whereğ‘Ÿ= 1if the userğ‘¢consumed itemğ‘–, otherwiseğ‘Ÿ= 0.ğ‘º âˆˆ Rdenotes the group-item interaction matrix, whereğ‘ = 1if the groupğ‘”consumed the item ğ‘–, otherwiseğ‘ = 0. We embed each userğ‘¢into the space vector ğ’‘âˆˆ ğ‘·of the dimensionğ‘‘. Letğ’’âˆˆ ğ‘¸represent the vector representation of itemğ‘–, which is randomly initialized with a d-dimensional vector.ğ’›âˆˆ ğ’denotes the representation of groupğ‘”. We use bold capital letters (e.g.,ğ‘¿) and bold lowercase letters (e.g., ğ’™) to represent matrices and vectors, respectively. In this section, we î€›rst introduce the details of the proposed model HHGR(short for "HierarchicalHypergraph Learning Framework forGroupRecommendation"). Then, we present itsself-supervised variant,S-HHGR, with the double-scale dropout strategy. Finally, we conduct the complexity analysis onğ‘†-HHGR to demonstrate its scalability. The proposed framework is shown in Figure 2, which includes three components: 1) Hierarchical hypergraph, which captures the user interactions within and beyond groups by propagating information from user level to group level; 2) Double-scale self-supervised learning, which contains coarse- and î€›ne-grained node dropout strategies to reî€›ne the user and group representations and alleviate the data sparsity problem; and 3) Model optimization, which uniî€›es the objectives of group recommendation and selfsupervised learning to enhance both tasks. By organizing users and groups in a hierarchical hypergraph, we can leverage their connectives to help exhibit high-order tuplewise user interactions within and beyond groups, which is of crucial importance to group recommendation. Inspired by [10,23], we devise a new hierarchical hypergraph convolution to perform the embedding propagation mechanism over our hierarchical hypergraph on two levels, user-level and group-level. In what follows, we elaborate on these two ingredients. 3.1.1 User-level hypergraph.Hypergraph construction.Since users are correlated within and beyond groups in group recommendation scenario, it is vital to deî€›ne appropriate connections among them and exploit the usersâ€™ mutual interactions. The conventional graph structure can only support pairwise relations between users, which is not î€›t for this case. Thus, we propose to model such user interactions with a hypergraph, which is shown in the bottom-left corner of Figure 2. In the user-level hypergraphğº, multiple users can be connected through one hyperedge (or group). User representation learning.We aim to exploit the tuplewise user interactions for learning their inî€uences, in which the correlated users should be aî€ected by the intra-group users. To achieve that, we introduce the hypergraph convolution operation to capture user interactions among their neighbors and learn each userâ€™s dynamic representation. For the speciî€›c user nodeğ‘£, the hypergraph convolutional network î€›rst learns the representations of all its connected hyperedgesğ‘’, which is from the gathered node features. Then the nodeğ‘£â€™s representation would integrate the related hyperedge feature information. Simple graphs use the adjacency matrix to represent relationships between two connected nodes, whereas hypergraphs introduce the incidence matrixğ‘¯to describe the relationship between nodes and hyperedges, which is deî€›ned in Section 2.1. Referring to the spectral hypergraph convolution proposed in HGNN [10,38], the hypergraph convolution is deî€›ned as follows: whereğ‘«denotes the vertex degree matrix of the user-level hypergraph, andğ‘©denotes the hyperedge degree matrix.ğ‘¾is regarded as the weight matrix of the hyperedges. In this paper, we initialize the weight matrixğ‘¾with the identity matrix yielding equal weights for all hyperedges.ğ‘«andğ‘©play a role of the normalization in hypergraph convolutional operator.ğš¯is the parameter matrix between two convolutional layers.ğ‘·is the usersâ€™ embeddings in theğ‘™-th hypergraph convolutional network, where ğ‘·=Ëœğ‘·.Ëœğ‘·is the initial vectors of all usersğ‘¢withğ‘‘-dimension. Since the nonlinear activation is found to be redundant in recent research [38], we remove this part in Eq. (1), which is shown as follows: Group representation learning.Our target is to obtain the group embeddings to estimate their preferences on items. Since group members have diî€erent importance, we perform a weighted sum on the representations of user members to generate the attentive group representationËœğ’. The weights can reî€ect the usersâ€™ contribution to the groupâ€™s decision-making. whereğ›¼is the weight of a userğ‘¢in the group decision,ğ’‘denotes other members vectors in the same groupğ‘”, andğ‘¾âˆˆ Rand ğ’™ âˆˆ Rare parameters used to compute the weights. 3.1.2 Group-level hypergraph.Hypergraph construction.A user often belongs to multiple groups, which means she is connected with other users of diî€erent groups. However, not all user interactions aî€ect user preferences. To select informative user interactions and optimize inter-group interactions among users, we adopt the triadic motif relation as the motif-induced hyperedge to construct the group-level hypergraph. Since the triadic motif can increase homophily cohesion in the social community [11,26], its interaction is a stronger bond, which is highly beneî€›cial to modeling user interactions. Speciî€›cally, we î€›rst convert the user-level hypergraph to a projected graph, where group hyperedges act as nodes. In the projected graph, if two group hyperedges have common user members, they will be connected, ensuring the relevance of preferences between groups. Then, we adopt the triadic motif to select the most relevant groups in the group-level hypergraph. If any three groups conform to the deî€›nition of the motif, we will deî€›ne these three groups to be divided into one motif-induced hyperedge. The group-level hypergraph is represented as ğº. Group representation learning.Intuitively, the motif can be regarded as a closed path connecting three diî€erent nodes. Referring to the motif-based PageRank model proposed in [52], when we do not consider the self-connection, motif-based adjacency matrix can be calculated as Eq. (5), which can be represented as ğ‘¯ğ‘¯. whereğ‘¯denotes the motif incidence matrix of the group-level hypergraph.ğ‘ª âˆˆ Ris the symmetric adjacency matrix of the projected graph, whereğ‘= 1if there is a connection between groupğ‘–and groupğ‘—, otherwiseğ‘= 0.ğ‘ªğ‘ªdenotes the paths connecting three vertices, andâŠ™ ğ‘ªtransforms the paths into closed triangles. Similar to the hypergraph convolution proposed in the userlevel hypergraph, the transformed hypergraph convolutions in the group-level hypergraph can be deî€›ned as Eq. (6). Following the LightGCN and MHCN [18, 49], since the self-connection has little eî€ect on performance, the Eq. (6) is equivalent to Eq. (2). whereğ‘«âˆˆ Ris the degree matrix of the hypergraph motif ğ‘¯ğ‘¯.ğ‘¯ğ‘¯can be regarded as the motif-induced adjacency matrix.ğš¿is the parameter matrix in theğ‘™-th layer.ğ’is the group representation in theğ‘™-th layer of the hypergraph convolutional network, whereğ’=Ëœğ’. TheËœğ’learns from the attention-based group preference aggregator. 3.1.3 Loss function. Here, we adopt the pairwise learning task loss function to optimize user and item representations in the user-level hypergraph, which is designed as follows: whereËœğ’‘represents the embedding vector for userğ‘¢.Ëœğ’’represents the embedding vector for itemğ‘–.ğ‘‚represents the training set, in which each one includes the userğ‘¢, interacted itemğ‘–, and unobserved itemsğ‘—. Our target is to make the margin between the positive samples(ğ‘¢, ğ‘–)and negative samples(ğ‘¢, ğ‘—)is as close to 1 as possible. For the group-level hypergraph, its loss function can be achieved as follows:îƒ•î€€î€ whereğ’›is the group representation learned from the attentionbased preference aggregation strategy. Similar to the user-level loss,ğ‘‚represents the training set, which includes the groupğ‘”, interacted itemğ‘–, and unobserved itemsğ‘—. To enhance the learning process of these two recommendation methods, we apply a joint training strategy, whose loss function consists of two parts: user preference loss and group preference loss. It is given by Despite the remarkable capability of hypergraph modeling, as groups may occasionally form, only minimal group-item interactions can be observed (i.e., the data sparsity issue), which may lead to sub-optimal recommendation performance [14,20,44]. Several models utilize the user member preferences to synthesize group preferences and alleviate the sparsity issue. However, they overlook the fact that the user-item interaction is also sparse. To address this problem and reî€›ne the representations of users and groups, we propose a double-scale node dropout strategy on the hierarchical hypergraph (including coarse and î€›ne granularity) to augment the raw data and create two types of self-supervision signals for contrastive learning to boost the performance of HHGR. A selfsupervised variant of HHGR,ğ‘†-HHGR, is presented as follows. It should be noted that, forğ‘†-HHGR, we upgrade the design in Section 3.1.1 and employ two new hypergraph convolutional networks to learn user representations, which can be seamlessly coupled with the double-scale self-supervised strategy. Coarse-grained node dropping strategy.We drop a certain portion of users at a coarse-grained granularity, where the magnitude of missing nodes is a hyperparameter. Unlike node dropping in a simple graph accompanied by deleting connected edges, the hyperedge may not be deleted when discarding some nodes in the hypergraph. Besides, when a deleted node belongs to multiple group hyperedges, the node would be deleted in all certain hyperedges. We deî€›ne the coarse-grained node dropping functionğ‘“, as follows: whereğ’‰represents the column vector of the coarse-grained hypergraph incidence matrixğ‘¯, andğ’‚âˆˆ {0, 1}is a mask vector with its entries being 0 at a given probability, controlling the dropout magnitude of the nodes inğ‘¯.âŠ™represents the elementwise product, which means the mask vector would multiply with each column in the user-level hypergraph incidence matrixğ‘¯. After the coarse-grained node dropping, we can obtain a perturbed user-level hypergraph. We encode it through a new hypergraph convolutional networkğ‘”(Â·)to get the coarse-grained user representationğ‘·, which can be as one data augmentation of the raw user representations. whereğ‘«andğ‘©are the vertex degree and hyperedge degree diagonal matrices of the coarse-grained hypergraph.ğšªdenotes the parameter matrix in the coarse-grained hypergraph convolutional network.ğ‘·is the user representation in theğ‘™-th layer of the coarse-grained hypergraph neural network, whereğ‘·=Ëœğ‘·.Ëœğ‘·is the randomly initialized d-dimensional representation matrix. Fine-grained node dropping strategy.We perturb the hypergraph construction by dropping a certain number of users in a particular group hyperedge. Analogously, we deî€›ne a î€›ne-grained node dropping function ğ‘“, which is represented as follows: whereğ’‰represents the column vector of the î€›ne-grained hypergraph incidence matrixğ‘¯, andğ’‚âˆˆ {0, 1}is the î€›ne-grained mask vector, which will drop nodes with a î€›xed probability in only one hyperedge. Diî€erent from the coarse-grained strategy, each timeğ’‚is multiplied byğ’‰, it is reassigned. Next, we encodeğ‘· through ğ‘”(Â·) to obtain a new user representation: whereğ‘«andğ‘©are the vertex and hyperedge degree diagonal matrices of the î€›ne-grained hypergraph convolutional network. ğ‘¯is the incidence matrix.ğš½represents the parameter matrix. ğ‘·is the user representation in theğ‘™-th layer of the î€›ne-grained hypergraph neural network, whereğ‘·=Ëœğ‘·. To avoid the loss of some essential information, we add two representations at the two granulates to get the î€›nal user representation: ğ‘· = ğ‘·+ ğ‘·. Contrastive learning.Having established diî€erent granularity augmented views of nodes, we treat the granularity of the same node as the positive pairs and any diî€erent granularity of nodes as the negative pairs. We hope that the distribution of the user representation vectors from two pretext tasks can be as close as possible. Hence, we design a discriminator functionf(Â·)to learn a score between two input vectors and assign higher scores to positive pairs compared with negative samples. We adopt the crossentropy loss as contrastive loss function to enforce maximizing the agreement between positive pairs, which is deî€›ned as follows: whereğ’‘is the representation of userğ‘—in the coarse-grained hypergraph;ğ‘›is the number of negative samples randomly selected from the same batch. To optimize theğ‘†-HHGRâ€™s parameters, we unify the loss functions of the task of group recommendation and self-supervised learning. We î€›rst update the loss function of self-supervised learning,L, as the pre-training strategy to optimize the user representations. Then we optimize the supervised loss function ofLandL to obtain the representations of users and items. Meanwhile, the model will be î€›ne-tuned and updated during the process of selfsupervised learning. The learned user embedding would be used to generate group representations, which are updated in the objective functionL. Speciî€›cally, we train our model using the Adam algorithm. The overall objective is deî€›ned as follows: whereğ›½is the hyper-parameter to balance the task of self-supervised learning and supervised learning task for group recommendation. In this section, we discuss the complexity ofğ‘†-HHGR from model size and time complexity. Model size.The parameters of our model consist of three parts: 1) hierarchical hypergraph convolutional network, 2) attentionbased group preference aggregator, and 3) discriminator network. The coarse-grained and î€›ne-grained user-level hypergraph convolutional network have parameters of size2[ğ‘™Ã—ğ‘‘Ã—ğ‘‘]. The parameters size of the group-level hypergraph convolutional network isğ‘™ Ã—ğ‘‘ Ã—ğ‘‘. The parameter size of the trainable weighted matrix in the preference aggregator is3 Ã— ğ‘‘ Ã— ğ‘‘. The discriminator measures the similarity between user representations of diî€erent granularities, the weight matrix inf(Â·)has a trainable weighted matrixğ‘¾ with the shape ğ‘‘ Ã— ğ‘‘. Time complexity.The computational cost mainly comes from two parts: the hierarchical hypergraph convolution and the doublescale self-supervised learning strategy. The time complexity of hierarchical hypergraph is mainly from the information propagation consumption, throughğ‘™, is less thanğ‘‚ (ğ‘™ Ã— |ğ‘¯ | Ã— ğ‘‘), where|ğ‘¯ | denotes the number of nonzero elements in the incidence matrix ğ‘¯. Since our model contains a user-level hypergraph convolutional network and a group-level hypergraph convolutional network, the time complexity of the hierarchical hypergraph convolutional neural network is aboutğ‘‚ (ğ‘™Ã— |ğ‘¯ |Ã— ğ‘‘). As for the self-supervised learning strategy, the cost derives from the hypergraph generation of diî€erent perspectives and contrastive learning. The time cost of hypergraph generation is less thanğ‘‚ (2 Ã— ğ‘€ Ã— ğ‘€). The time cost of contrastive learning is less thanğ‘‚ (|U | Ã— ğ‘›), whereğ‘›is the number of negative samples. In this section, we conduct extensive experiments to justify our modelâ€™s superiority and reveal the reasons for its eî€ectiveness. Speciî€›cally, we will answer the following research questions to unfold the experiments. RQ1:Compared with the state-of-the-art group recommendation models, how does our model perform? RQ2:What are the beneî€›ts of each component (i.e., the hierarchical hypergraph and the self-supervised learning) in our model? RQ3:How do the hyper-parameters inî€uence the eî€ectiveness of the ğ‘†-HHGR? 4.1.1 Datasets. We conduct experiments on three public datasets: Weeplaces [32], CAMRa2011 [28], and Douban [44]. Weeplaces dataset includes the usersâ€™ check-in history in a location-based social network. We follow GroupIM [32] to construct group interactions by using the user check-in records and their social network. As for the CAMRa2011, it is a movie rating dataset containing individual users and households records. We follow the idea of AGREE [6] and convert the explicit rating to implicit preference, where the rating records are regarded as 1. Douban dataset is from the Douban platform, which consists of a variety of group social activities. As the Douban dataset does not contain explicit group information, we follow the idea of SIGR [44] to extract implicit group data. Speciî€›cally, we regard usersâ€™ friends who have participated in the same activity as the group members. The statistical information of the three datasets is shown in Table 1. We randomly split the set of all groups into training(70%), validation(10%), and test sets(20%). 4.1.2 Baselines. To answer theRQ1, we compare the proposed model with the following models: â€¢ Popularity. This method ranks items according to their popularity. â€¢ NeuMF[19]. NeuMF is a neural network-based collaborative î€›ltering model to benchmark the recommendation performance. We treat all groups as virtual users and utilize group-item interactions to generate the group preferences. â€¢ AGREE[6]. This method utilizes attentional preference aggregation to compute group member weights and adopts neural collaborative î€›ltering to learn the group-item interaction. â€¢ MoSAN[35]. MoSAN is a neural group recommender that employs a collection of sub-attentional networks to learn each userâ€™s preference and model member interactions. â€¢ SIGR[44]. This is a state-of-the-art group recommendation model, which introduces a latent variable and the attention mechanism to learn usersâ€™ local and global social inî€uence. It also utilizes the bipartite graph embedding model to alleviate the data sparsity problem. â€¢ GroupIM[32]. This model aggregates the usersâ€™ preferences as the group preferences via the attention mechanism. It maximizes the mutual information between the user representations and its belonged group representations to alleviate the data sparsity problem. HHGRis the vanilla version of our proposed model, andSâˆ’ HHGR represents the self-supervised version. 4.1.3 Evaluation metrics. To measure the performance of all methods, we employ the widely adopted metricsğ‘ ğ·ğ¶ğº@ğ¾andğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ¾ withğ‘˜ ={20, 50}.ğ‘ ğ·ğ¶ğº@ğ¾evaluates the ranking of true items in the recommendation list.ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™@ğ¾is the fraction of relevant items that have been retrieved in the Top-K relevant items. 4.1.4 Parameter seî€ings. For the general settings, the embedding size is 64, the batch size for the mini-batch is 512, and the number of negative samples is 10. During the training process of the doublescale self-supervised learning, the initial learning rate is5ğ‘’ âˆ’ 4. For the group-level hypergraph training, the learning rate is1ğ‘’ âˆ’ 4. The user-level and group-level hypergraph neural network structure is two-layer and one-layer, respectively. For the baseline models, we refer to their best parameter setups reported in the original papers and directly report their results if we use the same datasets and evaluation settings. 4.2.1 Overall performance comparison. In this part, we validate the superiority of HHGR andğ‘†-HHGR on three datasets. Table 2 shows the experimental results of the proposed modelsâ€™ performance compared with the baselines. We highlight the best results of all models in boldface. According to the results, we note the following key observations: 1) Among these methods, the attention-based group models outperform baseline recommenders (i.e., Popular and NeuMF) on most datasets due to their ability to dynamically model the user interactions within groups and learn various weights of diî€erent members in the group. 2) In the models based on attention mechanism, HHGR is better than most models (including AGREE, MoSAN, and SIGR). We believe that the performance improvement of HHGR veriî€›es the eî€ectiveness of hypergraph and hypergraph neural network modules to exploit high-order user interactions. 3) SIGR performs better than AGREE and MoSAN due to considering the form of bipartite graph to represent user-item interactions, especially on the CAMRa2011 dataset. Besides, MoSAN also achieves better results because of its expressive power of preference aggregators to capture diî€erent personal weights in group-item interactions. 4) On the other hand, although HHGR is slightly inferior to GroupIM, the enhanced modelğ‘†-HHGR beats the most advanced group recommendation models on three datasets, which veriî€›es the eî€ectiveness of self-supervised learning strategies. Figure 3: Performance comparison of attention-based group recommendation models on sparsity datasets. 4.2.2 Performance on sparsity datasets. We adopt the self-supervised learning strategy to alleviate the data sparsity problem. To validate the eî€ectiveness of the proposed double-scale node dropout strategy, we study experiments on sparse datasets. We split the dataset into four groups based on the number of group-item interactions. We î€›nd that the performance of all models varies with the number of group interactions. Due to the limited space, we only show the performance of attention-based models on the Weeplaces dataset. The results are shown in Figure 3. As shown in Figure 3, NDCG and Recall generally increase as the number of interacting groups of an item increases, since more interactions will provide more information for learning the group preference representation. Without the auxiliary information, models that adopt self-supervised learning (GroupIM andğ‘†-HHGR) are competitive to the models without self-supervised learning. Besides, we can observe thatğ‘†-HHGR shows the best performance compared with other models. We argue that the eî€ectiveness ofğ‘†HHGR can be attributed to the double-scale node dropout strategy in hypergraph neural networks, which can enhance the representations of group preferences. 4.3.1 Investigation of the hierarchical hypergraph. To investigate the eî€ectiveness of the hierarchical hypergraph, we conduct experiments on Weeplaces and CAMRa2011 for two HHGR variants, each of which has one of the level removed.HHGR-wgandHHGRwuto denote the ablated model without group level or user level. HHGR-wg only considers the user-level hypergraph, which means the group preference representations generated from the preference aggregator are the î€›nal group preference. HHGR-wu adopts the group-item interaction information to initialize the group preference and only considers the group level hypergraph to update the group preference. The experimental results are demonstrated in Table 3. From the results, we can observe that the performance of HHGR-wu falls to the performance of HHGR-wg and HHGR on two datasets. Compared with the Weeplace dataset, the performance of the HHGR and its variants on the CAMRa2011 is better. The possible reason might be that the CAMRa2011 dataset is denser, which is beneî€›cial to selecting informative inter- and intra-group user interactions. Table 3: Comparison between HHGR and its variants. 4.3.2 Investigation of self-supervised learning.Diî€erent types of self-supervised learning.To investigate the feasibility and eî€œciency of self-supervised learning, we conduct an ablation study to investigate each componentâ€™s contributions. We propose two variants ofğ‘†-HHGR:HHGR-FandHHGR-C.HHGR-Fmeans that only î€›ne-grained node dropping is considered.HHGR-Cmeans only coarse-grained node dropping is used. For the above two variant models, we maximize the mutual information between the initial node representations and the dropping ones. Considering the limited space of the paper, we only compare these models with ğ‘†-HHGR on Weeplaces and CAMRa2011. Figure 4 shows thatğ‘†-HHGR achieves the best performance over independent granularity models, where self-supervised learning strategies contribute to the recommendation performance. When we only use î€›ne-grained node dropping, the HHGR-F performs slightly worse thanğ‘†-HHGR but better than HHGR-C, which implies the î€›ne-grained node dropping contributes more to the node representation learning. We argue that the î€›ne-grained task would generate more hard samples, strengthening model training. When we only use the coarse-grained node dropout strategy, the group recommendation performance may be slightly better than HHGR. Besides, without the self-supervised learning strategy would result in a performance decline in most cases on two datasets. The performance of HHGR is worse than HHGR-F and HHGR-C in most cases, indicating that the self-supervised strategy is eî€ective. Overall, the experimental results show that self-supervised learning is useful. Figure 4: The inî€uence of diî€erent self-supervised learning strategies. Diî€erent extent of self-supervised learning.We note that the use of diî€erent granularities of node dropping beneî€›ts the node representation. We further analyze the extent of coarse and î€›ne granularity node dropout. Speciî€›cally, we set diî€erent dropping rates from 0.1 to 0.9 in two granularity tasks. When we change the rate of one granularity task, we will î€›x another granularityâ€™s rate. As presented in Figure 5, with the increase of dropping rate, there is a signiî€›cant increase in performance. When the rate reaches the peak, approximately 0.3 and 0.2 on î€›ne and coarse granularity dropping, respectively, it decreases. It suggests that node dropping can improve the HHGR performance with increasing dropping strength, but too many deletions will worsen the data sparsity problem. Figure 5: The performance of diî€erent extent of two pretext tasks. This part focuses on pointing out which hyper-parameters aî€ect our model. The analyzed hyper-parameters include the learning rate, the depth of hypergraph convolutional layers, the number of batch sizes, and the number of negative samples. Due to limited space, we only show the experimental results on the Weeplaces dataset. Speciî€›cally, we search the proper value in a small interval and set the learning rate as{1ğ‘’ âˆ’ 5, 5ğ‘’ âˆ’ 5, 1ğ‘’ âˆ’ 4, 5ğ‘’ âˆ’ 4, 1ğ‘’ âˆ’ 3}, and report the performance ofğ‘†-HHGR with î€›ve values{1, 2, 3, 4, 5}of the hypergraph convolutional layer. Besides, the batch size changes from 16 to 512 at intervals of increasing powers of 2, and the number of negative samples is set from 5 to 30 at intervals of 5. Figure 6: The inî€uence of the model parameters. As shown in Figure 6 (a), we observe5ğ‘’ âˆ’ 4is suî€œcient for the learning rate. With the increase of the rate, the performance shows a trend of rising î€›rst and then falling, reaching the peak at 5ğ‘’ âˆ’ 4. According to Figure 6 (b), when the depth of hypergraph convolutional layers is 2, the model reaches the performance peak. As the depth of convolutional layers increases, the performance ofğ‘†-HHGR steadily declines. A possible reason is that a multilayer hypergraph convolutional network would lead to the oversmoothing problem. HHGR model also encounters this problem, but it tends to over-smoothing at lower layers. We believe that a self-supervised learning strategy can alleviate the data sparsity problem because it would delete some redundant information to enhance the model performance. Besides, the parameter of batch size is also important. Figure 6 (c) depicts the change of model performance with respect to diî€erent batch sizes from 16 to 512. The optimal size is 512. Therefore, we set the batch size to 512 when we train the model. As shown in Figure 6 (d), when the number of negative samples is 10, our model achieves the best performance in the Weeplaces dataset. We can conclude that a small number of negative samples can promote the recommendation task, while a bigger one would mislead it. Early work on group recommendation is usually based on the preference integration of group members or the score aggregation of an item across users. The three most common aggregations include the average [5], the least misery [1], and the maximum satisfaction strategy [3,4]. The average method takes the mean score of all members in the group. The least misery is committed to î€›ltering all membersâ€™ smallest score to please everyone. The maximum satisfaction strategy aims to reach the maximum satisfaction of the member in the group. These three aggregating methods have a signiî€›cant drawback: they are oversimpliî€›ed and ignore user interactions in groups. Recently, with the successful development of deep learning, models based on the attention mechanism are becoming more and more popular for modeling intra-group user interactions and learning diî€erent usersâ€™ inî€uences [6,35]. For example, GAME [20] utilizes the heterogeneous information network and attention mechanism to learn the nodesâ€™ multi-view embeddings and membersâ€™ weights. Yin et al. [44] propose to take the attention mechanism to exploit each userâ€™s intra-group user interactions and adopt the bipartite graph embedding to mitigate the data sparsity problem. Although the above methods based on the attention mechanism have made remarkable achievements in group recommendation, they hardly consider the high-order user interactions. Although the graph neural network approaches have achieved successful results on capturing high-order relations in various tasks [7,25,56], these approaches are only appropriate in pairwise connections, which has limitation in expressing complex structures of data. Hypergraph has shown promising potential in modeling complex high-order relations [36,46,51,53]. Recently, some studies try to combine the hypergraph with the recommender systems to improve their performances. For example, DHCF constructs two hypergraphs for users and items to model their high-order correlations and enhance recommendation models based on collaborative î€›ltering [22]. Xia et al. [39] model session-based data as a hypergraph and use the hypergraph neural network to enhance session-based recommendation. Wang et al. [37] incorporate the hypergraph into next-item recommendation systems to represent the short-term item correlations. Yu et al. [49] propose a multi-channel hypergraph convolutional network and design multiple motif-induced hypergraphs to exploit high-order user relations patterns. Despite the tremendous eî€orts devoted to these models, they just adopt the paradigm of hypergraph representation learning to model high-order relations. Our models diî€er signiî€›cantly from previous approaches in that we use the motif to select informative group interactions. Self-supervised learning oî€ers a new angle to generate additional supervised signals via transferring the unlabeled data [2,9,15,24, 48]. Generally, there are two types of self-supervised learning approaches: generative models and contrastive models. Generative models learn to reconstruct the input data and make it similar to raw data [12,27]. Contrastive models learn discriminative representations by contrasting positive and negative samples [8,13,17,30]. Recently, some studies attempt to explore the self-supervised learning framework for the recommendation [41,42,54]. For example, Zhou et al. [54] propose theğ‘†-Rec model and design some selfsupervised training strategies to pre-train the model. Bert4Rec [33] adopts the Cloze objective to the sequential recommendation by predicting the random masked items in the sequence. Xin et al. [41] adopt the cross-entropy as self-supervised Q-learning to î€›netune the next-item recommendation model. Xie et al. [40] propose three data augmentation operations: item crop, item mask, and item reorder, as self-supervision signals to enhance sequential recommendation. The closest work to ours is GroupIM [32], which maximizes the mutual information between users and groups to optimize the representation of both. Unlike the above approaches, our work is the î€›rst to consider the double-scale node dropout strategies in hypergraph as the supervised signals in group recommendation. We maximize the mutual information between the same hypergraphâ€™s diî€erent views to enhance data representations and improve recommendation performance. In this paper, we proposed a novel neural group recommendation with the hierarchical hypergraph convolutional network and selfsupervised learning strategy to capture inter- and intar-group user interactions and alleviate the data sparsity problem. With the triadic motifs, the proposed model can obtain more reliable user interactions beyond groups for more accurate group representations. We innovatively designed a double-scale node dropout strategy as the supervision signals against the data sparsity problem. We conducted extensive experiments on three public datasets to evaluate our modelsâ€™ performance. The experimental results veriî€›ed the superiority of HHGR and its enhanced version based on self-supervised learning (ğ‘†-HHGR) compared with other state-of-the-art models. In the future work, we tend to deepen the application of self-supervised learning in group recommendation models and design more general auxiliary tasks for the recommendation to improve the recommendation performance. This work was partially supported by National Natural Science Foundation of China (6217022345), Natural Science Foundation of Chongqing, China (cstc2020jcyj-msxmX0690), Fundamental Research Funds for the Central Universities of Chongqing University (2020CDJ-LHZZ-039), ARC Discovery Project (DP190101985), and ARC Future Fellowship (FT210100624).