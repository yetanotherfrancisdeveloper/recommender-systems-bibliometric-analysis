<title>A Survey of Deep Reinforcement Learning in Recommender Systems: A Systematic Review and Future Directions</title> <title>arXiv:2109.03540v2  [cs.IR]  9 Sep 2021</title> deep learning and reinforcement learning. Since an agent in DRL can actively learn from usersâ€™ real-time feedback to infer dynamic user preferences, DRL is especially suitable for learning from interactions, such as human-robot collaboration; it has also driven signiî€›cant advances in a range of interactive applications ranging from video games, Alpha Go to autonomous driving [ ]. In light of the signiî€›cance and recent progresses in DRL for recommender sytsems, we aim to timely summaize and comment on DRL-based recommendation systems in this survey. A recent survey on reinforcement learning based recommender systems [ ] provides a general review about reinforcement learning in recommender systems without a sophsiticated investigation of the growing area of deep reinforcement learning. Our survey distinguishes itself in providing a systematic and comprehensive overview of existing methods in DRL-based recommender systems, along with a discussion of emerging topics, open issues, and future directions. This survey introduces researchers, practitioners and educators into this topic and fostering an understanding of the key techniques in the area. The main contributions of this survey include the following: We provide an up-to-date comprehensive review of deep reinforcement learning in recommender systems, with state of the art techniques and pointers to core references. To the best of our knowledge, this the î€›rst comprehensive survey in deep reinforcement learning based recommender systems. We present a taxonomy of the literature of deep reinforcement learning in recommender systems. Along with the outlined taxonomy and literature overview, we discuss the beneî€›ts, drawbacks and give suggestions for future research directions. We shed light on emerging topics and open issues for DRL-based recommender systems. We also point out future directions that could be crucial for advancing DRL-based recommender systems. The remainder of this survey is organized as follows: Section 2 provides an overview of recommender systems, DRL and their integration. Section 3 provides a literature review with a taxonomy and classiî€›cation mechanism. Section 4 reviews emerging topics, and Section 5 points out open questions. Finally, Section 6 provides a few promising future directions for further advances in this domain. In this section,we introduce key concepts related to dynamic recommender systems (RS) and deep reinforcement learning (DRL), and motivate the introduction of DRL to dynamic recommender systems. 2.1 Why Deep Reinforcement Learning for Recommendation? Recommender systems require coping with dynamic environments by estimating rapidly changing usersâ€™ preferences and proactively recommending items to users. Let be a set of users of cardinality |U| and be a set of items of cardinality |I| . For each user ğ‘¢ âˆˆ U , we observe a sequence of user actions = [ğ‘¥ , ğ‘¥ , Â· Â· Â· , ğ‘¥ with item âˆˆ I , i.e., each event in a user sequence comes from the item set. We refer to a user making a decision as an interaction with an item. Suppose the feedback (e.g., ratings or clicking behavior) provided by users is , then a dynamic recommender system maintains the corresponding recommendation policy , which will be updated systematically based on the feedback ğ‘“ âˆˆ F received during the interaction for item ğ‘– âˆˆ I at the timestamp ğ‘¡. The marriage of deep learning and reinforcement learning has fueled breakthroughs in recommender systems. DRL-based RS consists of a pipeline with three building blocks: environment construction, state representation and recommendation policy learning. Environment construction aims to build an environment based on a set of usersâ€™ historical behaviors. State representation is provided by the environment containing certain user information including historical behavior, demographic data (etc.). Recommendation policy learning is the key component to understand and predict usersâ€™ future behavior. DL-based RS receives user feedback (e.g., ratings or clicks) to reî€ect usersâ€™ interests and update the recommender, while DRL-based RS receives the reward provided by the environment to update the policy. The reward provided by the environment is a pre-deî€›ned function containing several factors. The detailed process of DL based RS and DRL-based RS mapping can be found in Figure 3. The typical deî€›ning feature of DRL is to use the deep learning to approximate reinforcement learningâ€™s value function and solve high-dimensional Markov Decision Processes (MDPs) [ ]. Formally, a MDP can be represented as a tuple ( S, A, P, R, ğ›¾ ). The agent chooses an action âˆˆ A according to the policy (ğ‘  at state âˆˆ S . The environment receives the action and produces a reward âˆˆ R and transfers the reward into the next state according to the transition probability ğ‘ƒ (ğ‘  |ğ‘  , ğ‘ ) âˆˆ P . The transition probability is unknown beforehand in DRL. Such a process continues until the agent reaches the terminal state or exceeds a pre-deî€›ned maximum time step. The overall objective is to maximize the expected discounted cumulative reward, where ğ›¾ âˆˆ [ is the discount factor that balances the future reward and the immediate reward. Deep reinforcement learning can be divided into two categories: model-based and model-free methods (a detailed taxonomy can be found in Figure 2). The major diî€erence between the two is whether the agent can learn a model of the environment. Model-based methods aim to estimate the transition function and reward function, while model-free methods aim to estimate the value function or policy from experience. In model-based methods, the agent accesses the environment and plans ahead while model-free methods gain sample eî€œciency from using models which are more extensively developed and tested than model-based methods in recent literature [3]. Deep reinforcement learning approaches are divided into three streams: value-based, policy-based and hybrid methods. In value-based methods, the agent updates the value function to learn a policy; policy-based methods learn the policy directly; and hybrid methods combine value-based and policy-based methods called actor-critic methods. Actor-critic contains two diî€erent networks where an actor network uses a policy-based method and the critic uses a value-based method to evaluate the policy learned by the agent. Deep reinforcement learning can be divided into on-policy and oî€-policy methods. In oî€-policy methods, the behavior policy is used for exploration while the target policy is used for decision-making. For on-policy methods, the behavior policy is the same as the target policy. Q-learning 101 ] is an oî€-policy value-based learning scheme for î€›nding a greedy target policy: Deep Q learning (DQN) [ 67 ] uses deep learning to approximate a non-liner Q function parameterized by (ğ‘ , ğ‘) . DQN designs a network that is asynchronously updated by minimizing the MSE: (Â·) and ğ‘„ (Â·) have the following relationship: The value function is updated using the following rule with the Temporal Diî€erence (TD) method, (ğ‘  ) â† ğ‘‰ (ğ‘  ) + ğ›¼ [ğ‘Ÿ (ğ‘  , ğ‘ ) + ğ›¾ğ‘‰ (ğ‘  ) âˆ’ ğ‘‰ (ğ‘  ] (7) |                             {z                             } where ğ›¼ is a constant. Policy gradient 102 ] is an on-policy policy-based method which can handle high-dimensional or continuous actions which cannot be easily handled by Q-learning. Policy gradient aims to î€›nd the parameter of to maximize the accumulated reward. To this end, it maximizes the expected return from the start state: where (ğœ) is the probability of the occurrence of . Policy gradient learns the parameter by the gradient âˆ‡ ğ½ (ğœ‹ ) as deî€›ned below: The above derivations contain the following substitution, where ğ‘ (Â·) are independent from the policy parameter , which is omitted during the derivation. Monte-Carlo sampling has been used by previous policy gradient algorithm (e.g,. REINFORCE) for ğœ âˆ¼ ğ‘‘ Actor-critic networks combine the advantages from Q-learning and policy gradient. They can be either on-policy [ 49 ] or oî€-policy [ 21 ]. An actor-critic network consists of two components: i) an actor, which optimizes the policy under the guidance of ğ½ (ğœ‹ ; and ii) a critic, which evaluates the learned policy ğœ‹ by using ğ‘„ (ğ‘ , ğ‘). The overall gradient is represented as follows: When dealing with oî€-policy learning, the value function for (ğ‘|ğ‘ ) can be further determined by deterministic policy gradient (DPG) as shown below: While traditional policy gradient calculates the integral for both the state space and the action space , DPG only requires computing the integral to the state space . Given a state ğ‘  âˆˆ S , there will be only one corresponding action ğ‘ âˆˆ A (ğ‘ ) = ğ‘ using DPG. Speciî€›cally, deep Deterministic Policy Gradients (DDPG) is an algorithm that combines techniques from DQN and DPG. DDPG contains four diî€erent neural networks: Q Network , policy network, target Q network , and target policy network. It uses the target network for both the Q Network and policy network to ensure stability during training. Assume , ğœƒ , ğœƒ and are parameters of the above networks; then DDPG soft-updates the parameters for the target network [56]: DRL is normally formulated as a Markov Decision Process (MDP). Given a set of users U = {ğ‘¢, ğ‘¢ , ğ‘¢ , ğ‘¢ , ...} , a set of items I = {ğ‘–, ğ‘– , ğ‘– , ğ‘– , ...} , the system î€›rst recommends item to user and then gets feedback . The system aims to incorporate the feedback to improve future recommendations and needs to determine an optimal policy regarding which item to recommend to the user to achieve positive feedback. The MDP modelling of the problem treats the user as the environment and the system as the agent. The key components of the MDP in DRL-based RS include the following: State : A state âˆˆ S is determined by both usersâ€™ information and the recent items in which the user was interested before time ğ‘¡ . Action : An action âˆˆ A represents usersâ€™ dynamic preference at time as predicted by the agent. A represents the whole set of (potentially millions of) candidate items. Transition Probability : The transition probability ğ‘ (ğ‘  |ğ‘  , ğ‘ is deî€›ned as the probability of state transition from to when action is executed by the recommendation agent. In a recommender system, the transition probability refers to usersâ€™ behavior probability. is only used in model-based methods. Reward : Once the agent chooses a suitable action based on the current state at time , the user will receive the item recommended by the agent. Usersâ€™ feedback on the recommended item accounts for the reward ğ‘Ÿ (ğ‘† , ğ‘ . The feedback is used to improve the policy ğœ‹ learned by the recommendation agent. Discount Factor : The discount factor ğ›¾ âˆˆ [ is used to balance between future and immediate rewardsâ€”the agent focuses only on the immediate reward when ğ›¾ = 0 and takes into account all the (immediate and future) rewards otherwise. The DRL-based recommendation problem can be deî€›ned by using MDP as follows. Given the historical MDP, i.e., (S, A, P, R, ğ›¾) , the goal is to î€›nd a set of recommendation polices ( {ğœ‹ } S â†’ A that maximizes the cumulative reward during interaction with users. Problem Formulation. Given an environment that contains all items , when user ğ‘¢ âˆˆ U interacts with the system, an initial state is sampled from the environment which contains a list of candidate items and usersâ€™ historical data. The DRL agent needs to work out a recommendation policy based on the state and produces the corresponding recommended item list . The user will provide feedback on the list which is normally represented as click or not click. The DRL agent will then utilize the feedback to improve the recommendation policy and move to the next interaction episode. DRL-based RS has some unique challenges such as state construction, reward estimation and environment simulation. We categorize the existing work of DRL-based recommendation into model-based and model-free methods (the taxonomy is shown in Figure 2). Model-based methods assume an expected reward or action available for the next step to help the agent update the policy. Method Work Value-based [18, 96, 119, 135] Policy-based [4, 38] Hybrid [125] where the MC (ğ‘ ) represents the sampled sequences from the interaction between and the agent using the Monte-Carlo tree search algorithm, is the discriminator, is the length of represents the oî€Ÿine data, and data represents the generated data. Hong et al . [38] propose NRSS for personalized music recommendation. NRSS uses wireless sensing data to learn usersâ€™ current preferences. NRSS considers three diî€erent types of feedback: score, option, and wireless sensing data. Because multiple factors are considered as the reward, NRSS designs a reward model which consists of usersâ€™ preference reward and a novel transition reward which are parameterized by and . The goal for NRSS is to î€›nd the optimal parameters and by using the Monte-Carlo tree search thus improving recommendation performance. However, wireless sensing feedback lacks generalization ability as it is only available for certain tasks or scenarios, making it hard to determine dynamic user interest. Value-based methods. Prior to Q-learning, value iteration is a more traditional value-based reinforcement learning algorithm that focuses on the iteration of the value function. Gradient Value Iteration (GVI) [ 119 ] is proposed to improve the traditional value iteration algorithm by utilizing the transition probability and a multi-agent setting to predict chronological author collaborations. It introduces a new parameter named â€˜statusâ€™ to reî€ect the amount of knowledge that the agent needs to learn from this state. The policy is updated only when the distance between the new status and the old status is lower than a pre-deî€›ned threshold. However, value iteration requires the transition probability, which is hard to obtain in most cases. Hence, Q-learning and its variants are widely used in DRL-based RS. Cascading DQN (CDQN) with a generative user model [18] is proposed to deal with the environment with unknown reward and environment dynamics. The generative user model adopts GANs to generate a user model based on an oî€Ÿine dataset. Diî€erent from previous work, it will generate the reward function for each user to explain the usersâ€™ behavior. The user model can be written as, arg max [ğ‘Ÿ (ğ‘  , ğ‘ )] âˆ’ ğ‘…(ğœ™)/ğœ‚ (15) where is the probability simplex, ğ‘…(ğœ™) is the regularization term for exploration and is a constant. Pseudo Dyna-Q (PDQ) [135] points out that Monte-Carlo tree search may lead to an extremely large action space and an unbounded importance weight of training samples. Hence, a world model is proposed to reduce the instability of convergence and high computation cost for interacting with users by imitating the oî€Ÿine dataset. With the world model, the agent will interact with the learned world model instead of the environment to improve the sample eî€œciency and convergence stability. The world model learning process introduced in PDQ can be described as î€›nding the parameter ğœƒ ğœ‹ (ğ‘  , ğ‘ arg min (ğœƒ )] (16) (ğ‘  , ğ‘ where is generated by the logged policy is the ratio used for importance sampling and is the diî€erence between the reward in the world model and real reward. Furthermore, with a customized liner function ğ‘ˆ (Â·). Hybrid methods. Hybrid method can be recognized as a midpoint between value-based and policy gradient-based methods. DeepChain [ 125 ] uses the multi-agent setting to relieve the sub-optimality problem. The sub-optimality problem is caused by the one for all setting that optimizes one policy for all users. Hence, DeepChain designs a multi-agent setting that adopts several agents to learn consecutive scenarios and jointly optimizes multiple recommendation policies. The main training algorithm used is DDPG. To this end, usersâ€™ actions can be formulated in a model-based form as follows: where represents the number of actor networks, ğ‘, ğ‘™, ğ‘  represent the three diî€erent scenarios, 1 is used to control the activation of two actors and (ğ‘š, ğ‘‘) âˆˆ {(1, 2), (2, 1)}. Discussion. Model-based methods aim to learn a model or representation to represent the whole environment so that the agent can plan ahead and receive better sample eî€œciency. The drawback of such a method is that the ground-truth representation of the environment is unavailable in recommendation scenarios as it dynamically changes, leading to a biased representation. Moreover, model-based methods use the transition probability function to estimate the optimal policy. As mentioned, the transition probability function is normally equivalent to usersâ€™ behavior probability which is hard to determine in a recommender system. Hence, existing works [ 18 96 119 125 135 approximate using a neural network or embedding it into the world model. Zhao et al . [125] design a probability network to estimate while [ 18 ] uses a GAN to generate user behavior where is embedded in the latent space. Diî€erent from them, [ 96 135 ] relies on the world model to predict usersâ€™ next behavior and feed it into the policy learning process. The challenges of model-based DRL are not widely used in RS and can be summarized into the following facets: â€¢ P is hard to determine in real-world recommender systems. If approximation is used to estimate , the overall model complexity will substantially increase as it requires approximating two diî€erent functions and the recommendation policy ğœ‹ by using a large amount of user behavior data. World model-based methods require periodic re-training to ensure the model can reî€ect user interests in time which increases the computation cost. Compared with model-based methods, model-free methods are relatively well-studied. Diî€erent from model-based methods, is unknown and not required in model-free methods. Model-based methods enable the agent to learn from previous experiences. In this subsection, we categorize model-free based DRL in RS into three parts: value-based, policy-based and hybrid methods. Value based methods. As mentioned, Deep Q-learning and its variants are typical value-based DRL methods widely used in DRL-based RS. DRN [ 130 ] is the î€›rst work utilizing Deep Q-Networks (DQN) in RS. It adopts Double DQN (DDQN) [ 91 ] to build a user proî€›le and designs an activeness score to reî€ect how frequently a user returns after one recommendation plus usersâ€™ action (click or not) as the reward. DRN provides a new approach to integrating DRL into RS when dealing with a Tasks Note Work Vanilla DQN and its extensions [54, 124, 130] DQN with state/action space optimization [42, 52, 104, 134] Value-based DQN with graph/image input [28, 33, 53, 71, 88, 127, 131] DQN for joint learning [73, 123, 129] Vanilla REINFORCE [13, 14, 45, 64, 68, 72, 99, 107, 110] Policy-based REINFORCE uses graph structure/input [11, 98, 100, 103] Non-REINFORCE based [39, 114] Vanilla DDPG [9, 59, 97, 124, 128] Hybrid with Knowledge Graph [17, 24, 35, 36, 36, 105, 117, 120, 121] dynamic environment. The key objective function can be found as follows, where is the action that gives the maximum future reward according to and are diî€erent parameters for two diî€erent DQNs. Zhao et al . [124] points out that negative feedback will also aî€ect recommendation performance which DRN does not consider. Moreover, positive feedback is sparse due to the large number of candidate items in RS. Only using positive feedback would lead to convergence problems. Hence, DEERS is proposed to consider both positive and negative feedback simultaneously by using DQN. Gated Recurrent Units (GRU) are employed to capture usersâ€™ preferences for both a positive state and negative state and the î€›nal objective function can be computed as: where is the learned user latent representation. Zou et al . [134] also points out that most studies do not consider usersâ€™ long-term engagement in the state representation as they focus on the immediate reward. FeedRec is proposed that combines both instant feedback and delayed feedback into the model to represent the long-term reward and optimize the long-term engagement by using DQN. To be speciî€›c, time-LSTM is employed to track usersâ€™ hierarchical behavior over time to represent the delayed feedback which contains three diî€erent operations: , â„ , â„ . The state space is the concatenation of those operations and usersâ€™ latent representation. Diî€erently, Xiao et al . [104] focuses on the user privacy issue in recommender systems. Deep user proî€›le perturbation (DUPP) is proposed to add perturbation into the user proî€›le by using DQN during the recommendation process. Speciî€›cally, DUPP adds a perturbation vector into usersâ€™ clicked items as well as the state space, which contains usersâ€™ previous behavior. Distinct from previous studies which focus on optimizing user proî€›les or state spaces, some studies aim to optimize the action space formed by interactions with items. In the situation of basket recommendation, the user is suggested multiple items as a bundle, which is called a recommendation slate. It leads to combinatorially large action spaces making it intractable for DQN based recommendation models. SlateQ [ 42 ] is proposed to decompose slate Q-value to estimate a long-term value for individual items, and it is represented as, where (ğ‘ , ğ‘–) is the decomposed Q-value for item . The decomposed Q-value will be updated by the following rule which is similar to traditional DQN, Diî€erent from other mode-free methods, Slate-Q assumes that the transition probability ğ‘ (ğ‘–|ğ‘  , ğ‘ is known. Vanilla DQN methods may not have suî€œcient knowledge to handle complex data such as images and graphs. Tang and Wang [89] î€›rstly models usersâ€™ click behavior as an embedding matrix in the latent space to include the skip behaviors of sequence patterns for sequential recommendation. Based on that, Gao et al . [28] propose DRCGR, which adopts CNN and GAN into DQN to help the agent to better understand high-dimensional data, e.g., a matrix. Two diî€erent convolution kernels are used to capture usersâ€™ positive feedback. In the meantime, DRCGR uses GANs to learn a negative feedback representation to improve robustness. Another typical data format is the graph, which is widely used in RS, including knowledge graphs. Lei et al . [53] propose GCQN which adopts Graph Convolutional Networks (GCN) [ 48 ] into the DQN which constructs the state and action space as a graph-aware representation. Diî€erently, GCQN introduces the attention aggregator: which demonstrates better performance than the mean-aggregator and pooling-aggregator. For item ğ‘–, the graph-aware representation can be represented as, where , ğ‘ are the parameters for the fully-connected layer, is the embedding for user and N (ğ‘–) is the set of one-hot neighbours of item in graph ğº (ğ‘–) . Zhou et al . [131] propose KGQR uses a similar strategy to transform the information into a knowledge graph which is fed into the GCN to generate the state representation. Notably, KGQR presents a diî€erent state representation generation method. For given node , the neighbourhood representation with a -hop neighborhood aggregator can be represented as, = ğœ + ğµ (26) |N (ğ‘–)| where N (ğ‘–) is the set of neighboring nodes, , ğµ are the parameter of the aggregator. Those neighbourhood representations will be fed into a GRU and the state representation will be generated. Another application domain for using graph data is job recommendation which requires considering multiple factors jointly such as salary, job description, job location etc. SRDQN [ 87 ] constructs a probability graph to represent a candidateâ€™s skill set and employs a multiple-task DQN structure to process these diî€erent factors concurrently. There are some studies targeting recommendation and advertising simultaneously in e-commerce environments [ 73 123 129 ]. Pei et al . [73] mentions when deploying RS into real-world platforms such as e-commerce scenarios, the expectation is to improve the proî€›t of the system. A new metric, Gross Merchandise Volume (GMV), is proposed to measure the proî€›tability of the RS to provide a new view about evaluating RS in advertising. Diî€erent from GMV, Zhao et al . [129] separates recommendation and advertising as two diî€erent tasks and proposes the Rec/Ads Mixed display (RAM) framework. RAM designs two agents: a recommendation agent and an advertising agent, where each agent employs a CDQN to conduct the corresponding task. Zhao et al . [123] î€›nd that advertising and recommendation may harm each other and formulate a rec/ads trade-oî€. Their proposed solution, DEARS, contains two RNNs. Two RNNs are employed to capture user preferences toward recommendations and ads separately. Based on that, DQN is employed to take those two outputs as the input to construct the state and output the advertising. where is the behavior policy trained by state-action pairs without the long-term reward and is trained based on the long-term reward only. It is worth mentioning that the vanilla REINFORCE algorithm is on-policy, and importance sampling will make REINFORCE behave like an oî€-policy method with the following gradient format, (ğ‘  , ğ‘ ğ‘Ÿ (ğ‘  , ğ‘ log ğœ‹ (ğ‘  , ğ‘ (29) (ğ‘  , ğ‘ where is the sample policy parameter. Xu et al . [107] also î€›nds that the REINFORCE method suî€ers from a high variance gradient problem and Pairwise Policy Gradient (PPG) is proposed. Diî€erent from policy correction, PPG uses Monte Carlo sampling to sample two diî€erent actions ğ‘, ğ‘ and compare the gradient to update ğœƒ, where is the hidden representation from the GRU for sequential behavior and is the knowledge graph. Diî€erent from KERL, Xian et al . [103] propose Policy-Guided Path Reasoning (PGPR), which formulates the whole environment as a knowledge graph. The agent is trained to î€›nd the policy to î€›nd good items conditioned on the starting user in the KG by using REINFORCE. PGPR uses the tuple (ğ‘¢, ğ‘’ , â„ to represent the state instead of the graph embedding where is the entity the agent has reached at for user and is the previous action before . The action in PGPR is deî€›ned as the prediction of all outgoing edges for ğ‘’ based on â„ Wang et al . [100] propose a knowledge graph policy network (KGPolicy) which puts the KG into the policy network and adopts REINFORCE to optimize it. In addition, KGPolicy uses negative sampling instead of stochastic sampling to overcome the false negative issueâ€”sampled items behave diî€erently during training and inference. Similar to GCQN, attention is also employed to establish the representation for its neighbors. Due to the on-policy nature of REINFORCE, it is diî€œcult to apply it to large-scale RS as the convergence speed will be a key issue. To relieve this, Chen et al . [11] propose TPGR, which designs a tree-structured policy gradient method to handle the large discrete action space hierarchically. TPGR uses balanced hierarchical clustering to construct a clustering tree. Speciî€›cally, it splits a large-scale data into several levels and maintains multiple policy networks for each level to conduct the recommendation. The results are integrated at the î€›nal stage. As mentioned, policy gradient can be further extended to deterministic policy gradient (DPG) 86 ]. Hu et al . [39] propose Deterministic Policy Gradient with Full Backup Estimation (DPG-FBE) to complete a sub-task of recommendation. DPG-FBE considers a search session MDP (SSMDP) that contains a limited number of samples, where the stochastic policy gradient method like REINFORCE cannot work well. Hybrid methods. The most common model-free hybrid method used would be the actor-critic algorithm where the critic network uses the DQN and the actor uses the policy gradient. The common algorithm used to train actor-critic is DDPG with the following objective function, where , ğœƒ is the parameter for Q-learning at time ğ‘¡, ğ‘¡ + 1 while is the parameter for deterministic policy gradient at time ğ‘¡ + 1. Zhao et al . [128] propose LIRD, which uses the vanilla actor-critic framework to conduct list-wise recommendations. In order to demonstrate the eî€ectiveness of LIRD, a pre-trained user simulator is used to evaluate the eî€ectiveness of LIRD where the transition probability is approximated using the cosine similarity for a given state-action pair , ğ‘ . Zhao et al . [124] further extend LIRD into page-wise recommendation and proposed DeepPage. Similar to other previous work, GRU is employed to process the sequential pattern. Moreover, similar to DRCGR, DeepPage formulates the state as a page, then CNNs are employed to capture features and fed to the critic network. The î€›nal state representation is the concatenation of the sequential pattern and the page features. Additionally, there are a few studies focusing on diî€erent scenarios such as top-aware recommendation [ 59 ], treatment recommendation [ 97 ], allocating impressions [ etc. Liu et al . [59] introduces a supervised learning module (SLC) as the indicator to identify the diî€erence between the current policy and historical preferences. SLC will conduct the ranking process to ensure the recommendation policy will not be aî€ected by the positional bias â€“ the item appearing on top receives more clicks. Similarly, Wang et al . [97] also integrates the supervised learning paradigm into DRL but in a diî€erent way. An expert action is provided when the critic evaluates the policy and the update rule is slightly diî€erent than normal DQN, However, such a method is not universal as the acquisition of expert action is diî€œcult and depends on the application domain. Similar to policy gradient and DQN, Knowledge Graphs (KG) are also used in actor-critic-based methods. Chen et al . [17] propose KGRL to incorporate the substantial information of knowledge graphs to help the critic to better evaluate the generated policy. A knowledge graph is embedded into the critic network. Diî€erent from previous studies which use the KG as the environment or state representation, KGRL uses KG as a component in the critic, which can guide the actor to î€›nd a better recommendation policy by measuring the proximity from the optimal path. Speciî€›cally, a graph convolutional network is used to weight the graph and Dijkstraâ€™s algorithm is employed to î€›nd the optimal path for î€›nally identifying the corresponding Q-value. Zhao et al . [121] claim that humanâ€™s demonstration could improve path searching and propose ADAC. ADAC also searches for the optimal path in the KG but further adopts adversarial imitation learning and uses expert paths to facilitate the search process. Feng et al . [24] propose MA-RDPG, which extends the standard actor-critic algorithm to deal with multiple scenarios by utilizing a multi-actor reinforcement learning setting. Speciî€›cally, two diî€erent actor-networks are initialized while only one critic network will make the î€›nal decision. Those two actor networks can communicate with each other to share information and approximate the global state. Zhang et al . [117] î€›nd that there are multiple factors can aî€ect the selection of electric charging station. Hence, it uses a similar idea to recommend the electric vehicle charging station by considering current supply, future supply, and future demand. He et al . [36] î€›gure out that the communication mechanism in MA-RDPG will harm actors as they are dealing with independent modules, and there is no intersection. Hence, He et al . [36] extend MA-RDPG into multi-agent settings which contain multiple pairs of actors and critics and remove the communication mechanism to ensure independence. Diî€erent from [ 24 ], He et al . [36] use â€˜softâ€™ actor-critic (SAC) [ 35 ], which introduces a maximum entropy term H (ğœ‹ (ğ‘  , ğœ™ )) to actor-critic to improve exploration and stability with the stochastic policy ğœ‹ (ğ‘  , ğœ™ . Similar to the multi-agent idea, Zhao et al . [120] use a hierarchical setting to help the agent learn multiple goals by setting multiple actors and critics. In comparison, hierarchical RL uses multiple actor-critic networks for the same task. It splits a recommendation task into two sub-tasks: discovering long-term behavior and capturing short-term behavior. The î€›nal recommendation policy is the combination of the optimal policies for the two sub-tasks. Similarly, Xie et al . [105] use the hierarchical setting for integrated recommendation by using diî€erent sourced data. The objective is to work out the sub-polices for each source hierarchically and form the î€›nal recommendation policy afterward. Discussion. In RS, model-free methods are generally more î€exible than model-based methods as they do not require knowing the transition probability. We summarize the advantages and disadvantages of the three kinds of methods described under the model-free category. DQN is the î€›rst DRL method used in RS, which is suitable for small discrete action spaces. The problems with DQN in RS are: â€¢ RS normally contains large and high-dimensional action spaces. The reward function is hard to determine which will lead to inaccurate value function approximation. Speciî€›cally, the high dimensional action space in context of recommender systems is recognized as a major drawback of DQN [ 67 90 ]. The reason lies in the large number of the candidate items. Hence, DQN, as one of the most popular schemes, is not the best choice for RS in many situations. Moreover, some unique factors need to be considered when designing the reward function for RS such as social inference. It introduces extra parameters to the Q-network and hinders the convergence. Policy gradient does not require the reward function to estimate the value function. Instead, it estimates the policy directly. However, policy gradient is designed for continuous action spaces. More importantly, it will introduce high variance in the gradient. Actor-critic algorithms combine the advantages of DQN and policy gradient. Nonetheless, actor-critic will map the large discrete action space into a small continuous action space to ensure it is diî€erentiable, which may cause potential information loss. Actor-critic uses DDPG and thus inherits disadvantages from DQN and DPG, including diî€œculty in determining the reward function and poor exploration ability. There are a few studies that use DRL in RS for goals other than improving recommendation performance or proposing new application domains. We split the literature based on the following components: environment, state representation, and reward function. Exisitng studies usually focus on optimizing one single component in the DRL setting (as illustrated in Figure 1). Component Work Environment [40, 41, 75, 76, 82â€“84, 126] State [58, 60, 61] Reward [15] 3.3.1 Environment Simulation and Reconstruction. Many environments are available for evaluating deep reinforcement learning. Two popular ones are OpenAI gym-based environment [ ] and MuJoCo . Unfortunately, there is no standardized simulation platform or benchmark speciî€›c to reinforcement learning based recommender systems. Existing work on DRL in RS is usually evaluated through oî€Ÿine datasets or via deployment in real applications. The drawback for evaluating oî€Ÿine datasets include: Diî€erent studies use diî€erent environment construction methods which leads to unfair comparison. For instance, some studies use the KG as the environment while some studies assume the environment is gym-like or design a simulator for speciî€›c tasks. With oî€Ÿine datasets, usersâ€™ dynamic interests, and environment dynamics are hard to maintain. Deploying the method into a real application is diî€œcult for academic research as it takes time and costs money. Hence, a standardized simulation environment is a desirable solution. where ğ¿(Â·) is the loss function, ğ· (Â·) is a discriminator and ğ» (ğœ‹ ) is introduced in GAIL. 3.3.2 State Representation. State representation is another component in DRL-based RS which exists in both model-based and model-free methods. Liu et al . [60] î€›nd that the state representation in model-free methods would aî€ect recommendation performance. Existing studies usually directly use the embedding as the state representation. Liu et al . [58 61] propose a supervised learning method to generate a better state representation by utilizing an attention mechanism and a pooling operation as shown in Figure 7. Such a representation method requires training a representation network when training the main policy network, which increases the model complexity. 3.3.3 Robustness of Reward Functions. The reward function is crucial for methods involving DQN. A robust reward function can signiî€›cantly improve training eî€œciency and performance. Kostrikov et al . [50] î€›nd that the DQN may not receive the correct reward value when entering the absorbing state. That is, when the absorbing state is reached, the agent will receive zero reward and harm policy learning. The reason behind this is that when designing the environment zero reward is implicitly assigned to the absorbing state as it is hard to determine the reward value in such a state. Chen et al . [15] propose a robust DQN method, which can stabilize the reward value when facing the absorbing state. The new reward formula can improve the robustness, which is deî€›ned as follows: if ğ‘  is an absorbing state ğ‘Ÿ = (36) + ğ›¾ğ‘„ (ğ‘  , ğ‘ ) otherwise. The major diî€erence is that ğ‘Ÿ is assigned to the absorbing state to ensure the agent can continue learning. One remaining problem in current DRL-based RS is the reward sparsity, i.e,. the large state and action spaces make the reward sparsity problem more serious. One of the possible solution would be a better designed reward by using the reward shaping [69]. While existing studies have established a solid foundation for DRL-based RS research, this section outlines several promising emerging research directions. Recommender systems are monolithic systems containing tasks such as searching, ranking, recommendation, advertising, personalization, and diverse stakeholders such as users and items. Most existing methods are based on single agent. Multi-Agent Reinforcement Learning (MARL) is a subî€›eld of reinforcement learning and it is capable of learning multiple policies and strategies. While a single-agent reinforcement learning framework can only handle a single task, many studies consider the multi-task situation in RS and employ multi-agent DRL (MADRL) or hierarchical DRL (HDRL). HDRL [ 51 ] is proposed to handle complex tasks by splitting such tasks into several small components and asks the agent to determine sub-policies. HDRL belongs to a single-agent reinforcement learning framework such that the agent contains a meta-controller and several controllers. The meta-controller splits the task, and the controllers learn the value and reward functions for designated tasks to get a series of sub-policies. There are a few studies already utilizing HDRL in RS. Xie et al . [105] target integrated recommendation to capture user preferences on both heterogeneous items and recommendation channels. Speciî€›cally, the meta-controller is used for item recommendation, and controllers aim to î€›nd the personalized channel according to user channel-level preferences. Zhang et al . [113] uses HDRL for course recommendation in MOOCs, which contains two diî€erent tasks: proî€›le reviser and recommendation. The meta-controller aims to make course recommendations by using the revised proî€›le pruned by the controllers. Diî€erent from HDRL, MADRL [ 23 ] introduces multiple agents to handle the sub-tasks. Gui et al [33] uses the MADRL for twitter mention recommendation where three agents are initialized. The three agents need to generate diî€erent representations for the following tasks: query text, historical text from authors and historical text from candidate users. Once the representations are î€›nalized, the model will conduct the recommendation based on the concatenation of representations. Feng et al [24] and He et al . [36] provide two diî€erent views of the communication mechanism in MADRL and demonstrate that agents could work collaboratively or individually. Zhao et al . [129] designs a MADRL framework for two tasks where two agents are designed to conduct advertising and recommendation respectively. Zhang et al . [119] uses MADRL for collaborative recommendation where each agent is responsible for a single user. MADRL is adopted to help the recommender consider both collaboration and potential competition between users. Zhang et al . [117] designs a charging recommender system for intelligent electric vehicles by using decentralized agents to handle sub-tasks and a centralized critic to make the î€›nal decision. Hierarchical multi-agent RL (HMARL) [ 66 ] proves that MARL and HRL can be combined. Recently, Yang et al . [109] introduces HMADRL into the continuous action space, which provides a direction for RS. Zhao et al . [120] uses HMARL for multi-goal recommendation where the meta-controller considers usersâ€™ long-term preferences and controllers focus on short-term click behavior. While the meta-controller and controllers in HDRL deal with sub-tasks that belong to a single task, HMARL focuses on multi-task or multi-goal learning where the meta-controller and controllers belong to diî€erent agents and deal with diî€erent tasks or goals. HMADRL would be a suitable solution for future research work in DRL-based RS where HDRL can be used to split a complex task into several sub-tasks such as usersâ€™ long-term interests and short-term click behavior, and MADRL can jointly consider multiple factors such as advertising Zhao et al. [120]. As mentioned, the reward function plays a critical role in DRL-based recommender systems. In many existing works, reward functions are manually designed. The common method uses usersâ€™ click behavior to represent the reward and to reî€ect usersâ€™ interests. However, such a setting can not represent usersâ€™ long-term goals [ 134 ] as clicking or not only depicts part of the feedback information from users. It requires signiî€›cant eî€ort to design a reward function, due to the large number of factors that can aî€ect usersâ€™ decision, such as social engagement or bad product reviews, which may adversely aî€ect recommendation performance. It is diî€œcult to include all potential factors into the reward function because not every factor can be represented properly. A few works [ 19 31 ] show that manually designed reward functions can be omitted by employing inverse reinforcement learning (IRL) [ 70 ] or generative adversarial imitation learning (GAIL) [ 37 ]. Such inverse DRL-based methods require using expert demonstration as the ground truth. However, expert demonstration is often hard to obtain for recommendation scenarios. Those two studies conduct experiments in an oî€Ÿine dataset-based simulation environment that can access expert demonstration. In contrast, Chen et al . [19] use IRL as the main algorithm to train the agent while Gong et al . [31] use both demonstration and reward to train the agent. Zhao et al . [121] also employ GAIL to improve recommendation performance. In this work, GAIL is used to learn the reasoning path inside the KG to provide side information to help the agent learn the policy. Although IRL achieves some progress in RS, the lack of demonstration is a key shortcoming that impedes adoption in RS. One possibility is to use the IRL method in casual reasoning to help improve interpretability [ ] thus boosting recommendation performance. Alternately, IRL may be suitable for learning usersâ€™ long-term and static behavior to support the reward function. Graph data and KG are widely used in RS. Graph modeling enables an RS to leverage interactions between users and the recommender for reasoning or improving interpretability. According to existing studies about deep learning-based RS [ 115 ], embedding is a technique used to get the representation for the input data. Graph embedding is a common solution to handle graph-like data. GCN is a type of graph embedding method which are broadly used in RS to process graph data. Wang et al . [94] propose a variant of GCN to learn the embedding for KG. Speciî€›cally, they propose knowledge graph convolutional networks (KGCN) to capture the high-order structural proximity among entities in a knowledge graph. In DRL-based RS, graph data are handled similarlyâ€”s underthe transformed into an embedding and fed to the agent. Wang et al . [98] uses a traditional graph embedding method TransE [ ] to generate the state representation for DRL-based RS. There are several studies that use GCN in DRL for recommendations under diî€erent settings. Jiang et al . [46] propose a graph convolutional RL (DGN) method which integrates the GCN into the Q-learning framework for general RL problems by replacing the state encoding layer with the GCN layer. Lei et al . [53] extend this method into the deep learning î€›eld and apply it to recommender systems. To be speciî€›c, multiple GCN layers are employed to process the sub-graphs for a given item . Chen et al . [17] employs KG inside the actor-critic algorithm to help the agent learn the policy. Speciî€›cally, the critic network contains a GCN layer to give weight to the graph and conduct searches in the graph to î€›nd an optimal path and hence guide the optimization of policy learning. However, such a method is relatively computationally expensive as it requires jointly training the GCN and the actor-critic network. Gong et al . [31] adopts a Graph Attention Network (GAT) [ 92 ] into the actor-critic network to conduct recommendation. In addition, the GAT is used as an encoder to obtain a state representation. A common way of using GCN or its variants in DRL-based RS is the state encoder. The related challenge is the diî€œculty for the environment to provide a graph-like input to the GCN. Self-supervised learning (SSL) is a technique in which the model is trained by itself without external label information. SSL-DRL is receiving growing interest in robotics [ 47 112 ]. Kahn et al . [47] shows that SSL can be used to learn the policy when doing navigation by providing real-world experience. Zeng et al . [112] demonstrates that SSL-DRL can be used to help the agent learn synergies between two similar policies, thus empowering the agent to conduct two diî€erent tasks. Recent advances in SSL RL show that SSL can also provide interpretability for RL, which is promising for interpretable RS research [ 85 ]. Shi et al . [85] shows that SSL based RL can highlight the task-relevant information to guide the agentâ€™s behavior. Moreover, Xin et al . [106] shows that SSL can be used to provide negative feedback for DRL-based RS to improve recommendation performance. To be speciî€›c, a self-supervised loss function is appended into the normal DRL loss function, log + ğ¿ (37) where is an indicator function to show users interact with the item or not. could vary, if the DQN is adopted, Equation (4) should be used. SSL demonstrates promising performance in visual representation in recent years, which would be a possible solution to generate the state representation as there are a few DRL-based RS studies that adopt CNNs to process image-like data and transform it into a state [ 28 59 ]. Furthermore, as an unsupervised learning approach, SSL would provide a new direction about deî€›ning the reward function by learning common patterns between diî€erent states as well as multi-task learning. In this section, we outline several open questions and challenges that exist in DRL-based RS research. We believe these issues could be critical for the future development of DRL-based RS. Sample ineî€œciency is a well-known challenge in model-free DRL methods. Model-free DRL requires a signiî€›cant number of samples as there is no guarantee that the received state is useful. Normally, after a substantial number of episodes, the agent may start learning as the agent î€›nally receives a useful state and reward signal. A common solution is the experience replay technique, which only works in oî€-policy methods. Experience replay still suî€ers the sample ineî€œciency problem [ 77 ] as not every past experience is worth replaying. Isele and Cosgun [43] propose selected experience replay (SER) that only stores valuable experience into the replay buî€er and thus improves sample eî€œciency. while traditional DRL environments only contain several candidate items, in DRL-based RS, the agent must deal with a signiî€›cantly larger action space as RS may contain lots of candidate items. Existing DRL-based RS studies on traditional experience replay methods often demonstrate slow converge speed. Chen et al . [14] design a user model to improve the sample eî€œciency through auxiliary learning. Speciî€›cally, they apply the auxiliary loss with the state representation, and the model distinguishes low-activity users and asks the agent to update the recommendation policy based on high-activity users more frequently. On the other hand, model-based methods are more sample eî€œcient. However, they introduce extra complexity as the agent is required to learn the environment model as well as the policy. Due to the extremely large action space and possibly large state space (depending on usersâ€™ contextual information) in RS, approximating the environment model and policy simultaneously becomes challenging. The exploration and exploitation dilemma is a fundamental and challenging problem in reinforcement learning research and receives lots of attention in DRL. This dilemma describes a trade-oî€ between obtaining new knowledge and the need to use that knowledge to improve performance. Many DQN-based methods focus on exploration before the replay buî€er is full and exploitation afterward. Consequently, it requires an extremely large replay buî€er to allow all possibilities in recommendation can be stored. DRN employs Dueling Bandit Gradient Descent (DBGD) [ 111 to encourage exploration while [ 18 36 ] introduces a regularization or entropy term into the objective function to do so. [ 42 ] uses the sheer size of the action space to ensure suî€œcient exploration. [ 98 100 103 ] uses a separate KG or elaborated graph exploration operation to conduct exploration. [ 13 ] employs Boltzmann exploration to get the beneî€›t of exploratory data without negatively impacting user experience. In addition, -greedy is the most common technique used to encourage exploration [ 52 54 59 96 105 134 135 ]. Remaining studies rely on a simulator to conduct exploration. However, it may suî€er from noise and over-î€›tting [ 105 ] because of the gap between simulation and real online application. For most DRL-based methods such as vanilla DQN, policy gradient, or actor-critic-based methods, -greedy would be a good choice for exploration. In addition, injecting noise into the action space would also be helpful for those actor-critic-based methods [ 56 ]. For methods involving KGs, -greedy may help, but the elaborated graph exploration methods may receive better performance. Existing work generally trains DRL algorithms in simulation environments or oî€Ÿine datasets. Deploying DRL algorithms into real applications is challenging due to the gap between simulation and real-world applications. Simulation environments do not contain domain knowledge or social impact. They can not cover the domain knowledge and task-speciî€›c engineering in the real-world recommendation. How to bridge the gap between simulation and real applications is a challenging topic. Sim2real [ 122 ] is a transfer learning approach that transfers DRL policies from simulation environments to reality. Sim2real uses domain adaption techniques to help agents transfer learned policy. Speciî€›cally, it adopts GANs to conduct adaption by generating diî€erent samples. RLCycleGAN [ 74 ] is a sim2real method for vision-based tasks. It uses CycleGAN [ 132 ] to conduct pixel-level domain adaption. Speciî€›cally, it maintains cycle consistency during GAN training and encourages the adapted image to retain certain attributes of the input image. In DRL-based RS, sim2real would be a possible solution for generalizing the learned policy from simulation environments to reality. However, sim2real is a new technique still under exploration. It shows an adequate capability in simple tasks and requires more eî€ort to handle the complex task such as recommendation. We believe it is a workable solution for generalizing from simulation to reality. Chen et al . [12] observe that user behavior data are not experimental but observational, which leads to problems of bias and unfairness. There are two reasons why bias is so common. First, the inherent characteristic of user behavior data is not experimental but observational. In other words, data that are fed into recommender systems are subject to selection bias [ 78 ]. For instance, users in a video recommendation system tend to watch, rate, and comment on those movies that they are interested in. Second, a distribution discrepancy exists, which means the distributions of users and items in the recommender system are not even. Recommender systems may suî€er from â€™popularity biasâ€™, where popular items are recommended far more frequently than the others. However, the ignored products in the â€œlong tailâ€ can be equally critical for businesses as they are the ones less likely to be discovered. Friedman and Nissenbaum [26] denote the unfairness as that the system systematically and unfairly discriminates against certain individuals or groups of individuals in favor of others. A large number of studies explore dynamic recommendation systems by utilizing the agent mechanism in reinforcement learning (RL), considering the information seeking and decisionmaking as sequential interactions. How to evaluate a policy eî€œciently is a big challenge for RL-based recommenders. Online A/B tests are not only expensive and time-consuming but also sometimes hurt the user experience. Oî€-policy evaluation is an alternative strategy that historical user behavior data are used to evaluate the policy. However, user behavior data are biased, as mentioned before, which causes a gap between the policy of RL-based RS and the optimal policy. To eliminate the eî€ects of bias and unfairness, Chen et al . [13] use the inverse of the probability of historical policy to weight the policy gradients. Huang et al . [40] introduce a debiasing step that corrects the biases presented in the logged data before it is used to simulate user behavior. Zou et al . [135] propose to build a customer simulator that is designed to simulate the environment and handle the selection bias of logged data. Although deep learning-based models can generally improve the performance of recommender systems, they are not easily interpretable. As a result, it becomes an important task to make recommender results explainable, along with providing high-quality recommendations. High explainability in recommender systems not only helps end-users understand the items recommended but also enables system designers to check the internal mechanisms of recommender systems. Zhang and Chen [118] review diî€erent information sources and various types of models that can facilitate explainable recommendation. Attention mechanisms and knowledge graph techniques currently play an important role in realizing explainability in RS. Attention models have great advantages in both enhancing predictive performance and having greater explainability [ 116 ]. Wang et al . [99] introduce a reinforcement learning framework incorporated with an attention model for explainable recommendation. Firstly, it achieves modelagnosticism by separating the recommendation model from the explanation generator. Secondly, the agents that are instantiated by attention-based neural networks can generate sentence-level explanations. Knowledge graphs contain rich information about users and items, which can help to generate intuitive and more tailored explanations for the recommendation system [ 118 ]. Recent work has achieved greater explainability by using reinforcement and knowledge graph reasoning. The algorithm from [ 103 ] learns to î€›nd a path that navigates from users to items of interest by interacting with the knowledge graph environment. Zhao et al . [121] extract imperfect path demonstrations with minimum labeling eî€ort and propose an adversarial actor-critic model for demonstrationguided path-î€›nding. Moreover, it achieves better recommendation accuracy and explainability by reinforcement learning and knowledge graph reasoning. Adversarial samples demonstrate that deep learning-based methods are vulnerable. Hence, robustness becomes an open question for both RS and DRL. Speciî€›cally, adversarial attack and defense in RS have received a lot of attention in recent years [ 22 ] as security is crucial in RS. Moreover, DRL policies are vulnerable to adversarial perturbations to agentâ€™s observations [ 57 ]. Gleave et al . [30] provide an adversarial attack method for perturbing the observations, thus aî€ecting the learned policy. Hence, improving the robustness is the common interest for DRL and RS, which would be a critical problem for DRL-based RS. Cao et al . [10] provide an adversarial attack detection method for DRL-based RS which uses the GRU to encode the action space into a low-dimensional space and design decoders to detect the potential attack. However, it only considers Fast Gradient Sign Method (FGSM)-based attacks and strategically-timed attacks [ 57 ]. Thus, it lacks the capability to detect other types of attack. Moreover, it only provides the detection method while the defence is still an opening question. We believe zero-shot learning techniques would be a good direction for training a universal adversarial attack detector. For defence, it is still an open question for DRL-based RS, though recent advances in adversarial defence in DRL may provide some insights [16, 63, 93]. In this section, we provide a few potential future directions of DRL-based RS. Beneî€›ting from recent advances in DRL research, we believe those topics can boost the progress of DRL-based RS research. Causality is a generic relationship between a cause and eî€ect. Moreover, inferring causal eî€ects is a fundamental problem in many applications like computational advertising, search engines, and recommender systems [7]. Recently, some researchers have connected reinforcement learning with learning causality to improve the eî€ects for solving sequential decision-making problems. Besides, Learning agents in reinforcement learning frameworks face a more complicated environment where a large number of heterogeneous data are integrated. From our point of view, causal relationships would be capable of improving the recommendation results by introducing the directionality of cause and the eî€ect. The usersâ€™ previous choices have impact on the subsequent actions. This can be cast as an interventional data generating the dynamics of recommender systems. By viewing a policy in RL as an intervention, we can detect unobserved confounders in RL and choose a policy on the expected reward to better estimate the causal eî€ect [ 82 ]. Some studies improve RL models with causal knowledge as side information. Another line of work uses causal inference methods to achieve unbiased reward prediction [34]. Yang et al . [108] propose a Causal Inference Q-network which introduces observational inference into DRL by applying extra noise and uncertain inventions to improve resilience. Speciî€›cally, in this work, noise and uncertainty are added into the state space during the training state, and the agent is required to learn a causal inference model by considering the perturbation. Dasgupta et al [20] give the î€›rst demonstration that model-free reinforcement learning can be used for causal reasoning. They explore meta-reinforcement learning to solve the problem of causal reasoning. The agents trained by a recurrent network able to make causal inferences from observational data and output counterfactual predictions. Forney et al . [25] bridge RL and causality by data-fusion for reinforcement learners. Speciî€›cally, online agents combine observations, experiments and counterfactual data to learn about the environment, even if unobserved confounders exist. Similarly, Gasse et al . [29] make the model-based RL agents work in a causal way to explore the environment under the Partially-Observable Markov Decision Process (POMDP) setting. They consider interventional data and observational data jointly and interprete model-based reinforcement learning as a causal inference problem. In this way, they bridge the gap between RL and causality by relating common concepts in RL and causality. Regarding explainability in RL, Madumal et al . [65] propose to explain the behavior of agents in reinforcement learning with the help of causal science. The authors encode causal relationships and learn a structural causal model in RL, which is used to generate explanations based on counterfactual analysis. With counterfactual exploration, this work is able to generate two contrastive explanations for â€˜whyâ€™ and â€˜why notâ€™ questions. It is so important to search for a Directed Acyclic Graph (DAG) in causal discovery. Considering traditional methods rely on local heuristics and predeî€›ned score functions, Zhu et al . [133] propose to use reinforcement learning to search DAG for causal discovery. They use observational data as an input, RL agents as a search strategy and output the causal graph generated from an encoder-decoder NN model. Recommender systems often need to deal with multiple scenarios such as joint recommendation and adverting, oî€Ÿine DRL and meta DRL provide a promising direction for achieving multiple scenarios at the same time. Oî€Ÿine DRL is a new paradigm of DRL that can be combined with existing methods such as self-supervised learning and transfer learning to move toward real-world settings. Oî€Ÿine DRL [ 55 (also known as batch DRL) is designed for tasks which contain huge amounts of data. Given a large dataset that contains past interactions, oî€Ÿine DRL uses the dataset for training across many epochs but does not interact with the environment. Oî€Ÿine DRL provides a solution that can be generalized to new scenarios as it was trained by a large sized dataset. Such generalization ability is critical to RSs, which may need to deal with multiple scenarios or multiple customers. While oî€Ÿine DRL could provide a new direction for DRL-based RS, it still faces a few problems regarding handling the distributional shifts between existing datasets and real-world interactions. Meta DRL [ 95 ] is deî€›ned as meta learning in the î€›led of DRL. Meta DRL is another approach to help agents to generalize to new tasks or environments. Diî€erent from oî€Ÿine DRL, meta DRL contains a memory unit which is formed by the recurrent neural network to memorize the common knowledge for diî€erent tasks. Diî€erent from oî€Ÿine DRL, meta DRL does not require a large amount of data to train. An actor-critic method uses the traditional policy gradient method, which suî€ers from the high variance problem due to the gap between behavior policy (i.e., the policy that is being used by an agent for action select) and target policy (i.e., the policy that an agent is trying to learn). A method commonly used to relieve the high variance problem is Advantage Actor-critic (A2C). Diî€erent from traditional actor-critic methods, A2C uses an advantage function to replace the Q-function inside the critic network. The advantage function ğ´(ğ‘  is deî€›ned as the expected value of the TD-error. The new objective function for policy gradient can be written as, (ğ‘„ (ğ‘  , ğ‘ ) âˆ’ ğ‘‰ (ğ‘  )) log ğœ‹ (ğ‘  , ğ‘ )]. (38) |                  {z                  } However, A2C still uses DDPG as the main training algorithm, which may suî€er function approximation errors when estimating the Q value. Twin-Delayed DDPG (TD3) [ 27 ] is designed to improve the function approximation problem in DDPG which uses clipped double Q-learning to update the critic. The gradient update can be expressed as, where ğœ– âˆ¼ clip(N (0, ğœ, âˆ’ğ‘, ğ‘)), ğœ is the standard deviation and ğ‘ is a constant for clipping. Another two ways to improve actor-critic methods are Trust Region Policy Optimization (TRPO) [ 79 ] and Proximal Policy Optimization (PPO) [ 80 ], which focus on modiî€›cation of the advantage function. TRPO aims to limit the step size for each gradient to ensure it will not change too much. The core idea is to add a constraint to the advantage function, where the KL divergence will be used to measure the distance between the current policy and the old policy is small enough. PPO has the same goal as TRPO which is to try to î€›nd the biggest possible improvement step on a policy using the current data. PPO is a simpliî€›ed version of TRPO which introduces the clip operation, Soft Actor-Critic (SAC) [ 35 ] is another promising variant of the actor-critic algorithm and is widely used in DRL research. SAC uses the entropy term to encourage the agent to explore, which could be a possible direction to solve the exploration and exploitation dilemma. Moreover, SAC assigns an equal probability to actions that are equally attractive to the agent to capture those near-optimal policies. An example of related work [ 36 ] uses SAC to improve the stability of the training process in RS. In this survey, we provide a comprehensive overview the use of deep reinforcement learning in recommender systems. We introduce a classiî€›cation scheme for existing studies and discuss them by category. We also provide an overview of such existing emerging topics and point out a few promising directions. We hope this survey can provide a systematic understanding of the key concepts in DRL-based RS and valuable insights for future research.