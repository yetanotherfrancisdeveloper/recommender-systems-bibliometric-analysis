Casting session-based or sequential recommendation as reinforcement learning (RL) through reward signals is a promising research direction towards recommender systems (RS) that maximize cumulative proî€›ts. However, the direct use of RL algorithms in the RS setting is impractical due to challenges like oî€-policy training, huge action spaces and lack of suî€œcient reward signals. Recent RL approaches for RS attempt to tackle these challenges by combining RL and (self-)supervised sequential learning, but still suî€er from certain limitations. For example, the estimation of Q-values tends to be biased toward positive values due to the lack of negative reward signals. Moreover, the Q-values also depend heavily on the speciî€›c timestamp of a sequence. To address the above problems, we propose negative sampling strategy for training the RL component and combine it with supervised sequential learning. We call this method Supervised Negative Q-learning (SNQN). Based on sampled (negative) actions (items), we can calculate the â€œadvantageâ€ of a positive action over the average case, which can be further utilized as a normalized weight for learning the supervised sequential part. This leads to another learning framework: Supervised Advantage Actor-Critic (SA2C). We instantiate SNQN and SA2C with four state-of-the-art sequential recommendation models and conduct experiments on two real-world datasets. Experimental results show that the proposed approaches achieve signiî€›cantly better performance than state-of-the-art supervised methods and existing self-supervised RL methods . Code will be open-sourced. â€¢ Information systems â†’ Recommender systems;Retrieval models and ranking; Novelty in information retrieval. Recommendation; Reinforcement Learning; Actor-Critic; Q-learning; Advantage Actor-Critic; Negative Sampling ACM Reference Format: , Xin Xin, Alexandros Karatzoglou, Ioannis Arapakis, and Joemon M. Jose. 2021. Supervised Advantage Actor-Critic for Recommender Systems. In Proceedings of ACM Conference (Conferenceâ€™17) . ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Over the last 20 years, users have been navigating online services such as, e-commerce [10], video platforms, and music apps [39] with the help of RS. Most of these use cases involve session-based/nextitem recommendation, in which recommendation are generated from the sequence of user interactions. Session-based recommendation models can be trained in a (self)supervised learning fashion, in which a sequential model (e.g., a transformer [14,35] or a RNN [7]) is trained to predict the next item in the sequence itself, rather than some â€œexternalâ€ labels [7,14,39]. This training approach is also widely adopted in language modeling tasks, to predict the next word given the previous word sequence [19]. Supervised learning can lead to sub-optimal recommendations, since the loss function used in supervised learning is purely deî€›ned on the discrepancy between model predictions and the actual interactions in the sequence. Recommmendations from a model trained on such a loss function may not match the desired properties of a RS from the perspective of both users and service providers. For example, service providers may want to promote recommendations that can lead to real purchases not just clicks. Other desirable properties like diversity and novelty of the recommended item lists, could be considered, which leads to a multi-objective optimization problem [18,25]. Recommendation models trained with simple supervised learning do not tackle the above expectations and objectives. Reinforcement learning (RL) has achieved success in game control [3,20,30,31], robotics [16] and related î€›elds. Unlike game control and robotics, directly utilizing RL for RS comes with sets of unique diî€œculties and challenges. Model-free RL algorithms train the agent through an â€œerror-and-correctionâ€ manner, in which the RL agent needs to interact with the environment and collect experience. The training procedure forces the agent to imitate good actions and avoid bad ones. Applying this in RS is problematic, since interactions with an under-trained policy would negatively aî€ect the user experience. A typical solution is to perform oî€-policy learning from the logged implicit feedback data [1,37]. This entails trying to infer a target policy from the data generated by a diî€erent behavior policy, which is still an open research problem due to its high variance [21]. Moreover, learning from implicit feedback also introduces the challenge of insuî€œcient negative signal [24,37]. Another alternative is to use model-based RL algorithms, in which a model is î€›rstly constructed to simulate the environment (users). Then the agent can learn from the interactions with the simulated environment [2,29]. However, these two-stage methods depend heavily on the accuracy of the constructed simulator. Self-supervised reinforcement learning [37] has been proposed for RS, achieving promising results on oî€-line evaluation metrics. Two learning frameworks namely Self-Supervised Q-learning (SQN) and Self-Supervised Actor-Critic (SAC) are proposed. The key insight of self-supervised RL is to utilize the RL component as a form of a regularizer to î€›ne-tune the recommendation model towards the deî€›ned rewards, for instance in the e-commerce domain provide recommendations that lead to more purchases rather than just clicks [37]. Although SQN and SAC achieve good performance, they still suî€er from some limitations. For example, the RL headin SQN and SAC is only deî€›ned on positive (interacted) actions (items), so the negative comparison signals only come from the cross-entropy loss of the supervised part. As a result, the RL head contributes to reward-based learning but cannot be used to generate recommendations, as it lacks negative feedback to remove the bias introduced by the existence of only positive reward signals. SAC uses the output Q-valuesas the critic to re-weight the actor (supervised part). Q-values depend heavily on the speciî€›c timestamp of a sequence, which introduces further bias to the learning procedure. To address the above issues, we î€›rst propose a negative sampling strategy for training RL in a RS setting and then combine it with supervised sequential learning. We call this Supervised Negative Q-learning (SNQN). Another interpretation of negative sampling in RL is imitation learning under sparse reward settings [23]. Diî€erent from SQN, which only performs RL on positive actions (clicks, views, etc.), the RL output head of SNQN is learned on both positive actions and a set of sampled negative actions. This design allows the RL part of the SNQN to not only act as a regularizer but also as a good ranking model, that can also be used to generate recommendations. Based on the sampled negative actions and the estimate of the Q-values, we can calculate the â€œadvantageâ€ of a positive action over the other actions. We propose the Supervised Advantage Actor-Critic (SA2C), that uses this advantage instead of the raw Q-values to re-weight the supervised output layer. The advantage values can be seen as normalized Q-values that help us alleviate the bias from sequence timestamp on the estimation of Q-values. This work makes the following contributions: 1) We propose SNQN introducing negative sampling for the RL training of the RS model and then combine it with supervised sequential learning. Both the supervised head and the RL head can be used to generate recommendations. We show that joint training of the two heads with a shared base model helps to achieve better performance than separate learning. 2) We propose SA2C to calculate the advantage of a positive action. This advantage can be seen as a normalized Q-value and is further utilized to re-weight the supervised component. 3) We integrate the proposed SNQN and SA2C with four state-of-the-art recommendation models and conduct experiments on two real-world e-commerce datasets. Experimental results demonstrate the proposed methods are eî€ective in improving the performance of RS compared to existing methods. Recurrent neural networks (RNN) and convolutional networks (CNN) have shown promising results in modeling recommendation sequences [7,33,39]. Transformer architectures have been proven to be highly successful [35] for language modeling tasks, and selfattention for recommendations has received a lot of attention [14]. RL has been previously applied in RS. Chen et al. [1]proposed to calculate a propensity score to perform oî€-policy correction for oî€-policy learning. Model-based RL approaches [2,27,41] attempt to eliminate the oî€-policy issue by building a model to simulate the environment. The policy can then be trained through interactions with the simulator. Two-stage approaches depend heavily on the accuracy of the simulator. Although related methods, such as generative adversarial networks (GANs) [5], achieve good performance when generating content like images and speeches, simulating usersâ€™ responses is a much more complex and diî€œcult task [2]. Recently, Xin et al. [37]proposed self-supervised reinforcement learning for RS. Two learning frameworks SQN and SAC are suggested. SQN augments the recommendation model with two heads. One is deî€›ned on the supervised mode and the other RL head is based on the Q-learning for positive reward actions. SQN co-trains the supervised loss and RL loss to conduct transfer learning between each other [37]. Long term rewards e.g. a purchase at the end of a session can be incorporated into the learning process, while the model is still trained eî€œciently on logged data. As the Q-values are an estimation of the goodness of the actions, SAC further utilizes these Q-values to re-weight the supervised part. SQN and SAC can be seen as attempts to utilize a Q-learning based RL estimator to â€œreinforceâ€ session-based supervised recommendation models [37] and achieve promising results on oî€-line evaluation metrics. Research on slate-based recommendation has also been conducted in [1,2,4,12], where actions are considered to be sets (slates) of items. This setting leads to an exponentially increased action space. Finally, bandit algorithms are also reward-driven and have long-term optimization perspective. However, bandit algorithms assume that taking actions does not aî€ect the state [17], while actually recommendations do have an eî€ect on user behavior [26]; hence RL is a more suitable choice for the RS task. Another related î€›eld is imitation learning, where the policy is learned from expert demonstrations [8, 9, 23, 34]. LetIdenote the item set, then a user-item interaction sequence can be represented asğ‘¥={ğ‘¥, ğ‘¥, ..., ğ‘¥, ğ‘¥}, whereğ‘¥âˆˆ I(0< ğ‘– â‰¤ ğ‘¡)denotes the interacted item at timestampğ‘–. The task of nextitem recommendation is to recommend the most relevant itemğ‘¥ to the user, given the sequence of ğ‘¥. A common solution is to build a recommendation model whose output is the classiî€›cation logitsy= [ğ‘¦, ğ‘¦, ...ğ‘¦] âˆˆ R, whereğ‘› is the number of candidate items. Each candidate item corresponds to a class. The recommendation list for timestampğ‘¡ +1 can be generated by choosing top-ğ‘˜items according toy. Typically one can use a generative sequential modelğº (Â·)to encode the input sequence into a hidden statesass= ğº (ğ‘¥). Generally speaking, plenty of deep-learning based models [7,14,33,39] can serve as the generative modelğº (Â·). After that, a decoder can be utilized to map the hidden state to the classiî€›cation logits asy= ğ‘“ (s). It is usually deî€›ned as a simple fully connected layer or the inner product with candidate item embeddings [7, 14, 33, 39]. From the perspective of RL, the next item recommendation task can be formulated as a Markov Decision Process (MDP) [28], in which the recommendation agent interacts with the environments E(users) by sequentially recommending items to maximize the discounted cumulative rewards. The MDP can be deî€›ned by tuples of (S, A, P, ğ‘…, ğœŒ,ğ›¾) [1, 2, 37] where â€¢ S: a continuous state space to describe the user state. The state of a user at timestampğ‘¡can be represented ass= ğº (ğ‘¥) âˆˆ S (ğ‘¡ > 0). â€¢ A: a discrete action space which contains candidate items. The actionğ‘of the agent is to recommend the selected item. In oî€-line training data, we can get the positive action at timestampğ‘¡from the input sequence (i.e.,ğ‘= ğ‘¥(ğ‘¡ â‰¥0)). â€¢ P:S Ã— A Ã— S â†’ Ris the state transition probability. When learning from oî€-line data, we can make an assumption that only positive actions can aî€ect the user state. In other words, taking a negative (unobserved) action doesnâ€™t update the user state [12, 40]. â€¢ ğ‘…:S Ã— A â†’ Ris the reward function, whereğ‘Ÿ (s, ğ‘)denotes the immediate reward by taking actionğ‘at states. The î€exible reward scheme allows the agent to optimize the recommendation models towards expectations that are not captured by simple supervised loss functions. â€¢ ğœŒis the initial state distribution with sâˆ¼ ğœŒ. â€¢ ğ›¾ is the discount factor for future rewards. The goal of RL is to seek a target policyğœ‹(ğ‘|s)so that sampling trajectories according toğœ‹(ğ‘|s), would lead to the maximum expected cumulative reward: whereğœƒ âˆˆ Rdenotes policy parameters. Note that the expectation is taken over trajectoriesğœ = (s, ğ‘, s, ...), which are obtained by performing actions according to the target policy. In on-line RL environments like game control, itâ€™s easy to sample the trajectoriesğœ âˆ¼ ğœ‹and the agent is trained through an â€œerror-and-correctionâ€ approach. However, under the RS setting, we cannot aî€ord to make â€œerrorsâ€ (i.e. letting the user interact with under-trained policies) due to the negative impact on the user experience. Even if we can split a small portion of traî€œc to make the RL agent interact with live users, the î€›nal recommended items may still be controlled by other recommenders with diî€erent policies, since many recommendation models are deployed in a real-live RS. As a result, the sampled trajectories will come from another behavior policyğœ âˆ¼ ğ›½and we will resort to oî€-policy RL [1,21] and in particular Q-learning [30]. Given an input user-item interaction sequenceğ‘¥and an existing recommendation modelğº (Â·), the supervised training loss is deî€›ned as the cross-entropy over the classiî€›cation distribution: ğ‘Œis an indicator function deî€›ned asğ‘Œ=1 if the user interacted with theğ‘–-th item in the next timestamp, elseğ‘Œ=0. The crossentropy loss pushes positive logits to high values. This loss provides negative learning signals by pushing down the output values of items that the user has not interacted with. This is particularly helpful in a RS setting where ranking items which are likely to be interacted by the user in the top-ğ‘˜ positions is the main goal. Sinceğº (Â·)already encodes the input sequence into a latent state s, we can directly reuse it as the state of the RL model. This sharing schema of the base model enables the transfer of knowledge between supervised learning and RL. On the shared base modelğº (Â·), we formulate another output layer to map the state into Q-values: whereğ›¿denotes the activation function,handğ‘are trainable parameters of the Q-learning output layer. When learning from logged implicit feedback data, typically there are no negative reward signals [11,24]. Q-learning solely based on positive reward signals (clicks, views, etc.), without negative interaction signals, leads to a model with a positive bias. Such Q-values based on only observed (positive) actions cannot be used for generating recommendation. To address this issue, we propose a negative reward sampling strategy for the RL training procedure. More precisely, the Q-learning loss function of SNQN is deî€›ned not only on positive action rewards but also on the sampled negative ones. We deî€›ne the one-step time diî€erence (TD) Q-loss of SNQN (Figure 1a) as: ğ¿= (ğ‘Ÿ (s, ğ‘) + ğ›¾ maxğ‘„ (s, ğ‘) âˆ’ ğ‘„ (s, ğ‘)) +(ğ‘Ÿ (s, ğ‘) + ğ›¾ maxğ‘„ (s, ğ‘) âˆ’ ğ‘„ (s, ğ‘)),(4) whereğ‘andğ‘are the positive action and negative action at timestampğ‘¡, respectively.ğ‘denotes the set of sampled unobserved (negative) actions. For the negative TD error, the maximum operation is performed inğ‘„ (s, ğ‘)other thanğ‘„ (s, ğ‘)since we assume that taking negative actions will not aî€ect the user state as discussed in section 3.1. We assign a constant reward valueğ‘Ÿ for negative actions (i.e.,ğ‘Ÿ (s, ğ‘) = ğ‘Ÿ), while the positive reward ğ‘Ÿ (s, ğ‘)we can deî€›ne it according to the speciî€›c demands of the recommendation domain, e.g. in e-commerce we can assign a higher reward to actions which lead to purchases rather than just clicks. We jointly train the supervised and RL loss on the replay buî€er generated from the logged implicit feedback data: We use double Q-learning for better stability [6], training two Algorithm 1 Training procedure of SNQN Input:user-item interaction sequence setX, recommendation modelğº (Â·), reinforcement headğ‘„ (Â·), supervised headğ‘“ (Â·), pre-deî€›ned reward function ğ‘Ÿ (s, ğ‘) Output: all parameters in the learning space Î˜ Createğº(Â·)andğ‘„(Â·)as copies ofğº (Â·)andğ‘„ (Â·), respectively copies of model parameters. Algorithm 1 describes the training procedure. Figure 1a shows the SNQN architecture. Actor-Critic (AC) methods have been successfully used in RL. The key idea of AC methods is the introduction of a critic that evaluates the goodness of an action taken and assigns higher weights to actions with high cumulative rewards. In the SNQN method, the supervised component can be seen as the actor which aims at imitating the logged user behavior. A simple solution for the critic is to use the output Q-values from the RL head, as these Q-values measure the cumulative rewards the system gains given the stateaction pair. These Q-values are sensitive to the speciî€›c timestamp of the sequence, a bad action in an early timestamp of a long sequence could also have a high Q-value since Q-values are based on the cumulative gains of all the following actions in this sequence. Instead of the absolute Q-value, what we actually would like to measure is how much â€œadvantageâ€ we obtain by applying an action, compared to the average case (i.e. average Q-values). This advantage can help us alleviate the bias introduced from the sequence timestamp. However, calculating the average Q-values along the whole action space would introduce additional computation cost, especially when the candidate item set is large. To this end, we have introduced negative samples in the SNQN method. A concise solution is to calculate the average among the sampled actions (including both positive and negative examples) as an approximation. Based on this motivation, the average Q-values can be deî€›ned as: The advantage of an observed (positive) action is formulated as: We use this advantage to re-weight the actor (i.e. the supervised head). If a positive action has higher advantage than the average, we increase its weight, and vice versa. To enhance stability, we stop the gradient î€ow and î€›x the Q-values when they are used to calculate the average and advantage. We then train the actor and critic jointly. The training loss of SA2C is formulated as: Figure 1b illustrates the architecture of SA2C. During the training procedure, the learning of Q-values can be unstable [22], particularly in the early stage. To mitigate these issues, we pre-train the model using SNQN in the î€›rstğ‘‡training steps (batches). When the Q-values become more stable, we start to use the advantage to reweight the actor and perform updates according to the architecture of Figure 1b. We use double Q-learning and the training procedure of SA2C is similar to Algorithm 1 except for the computation of advantage and the re-weighting of ğ¿. We conduct experimentson two real-world datasets to evaluate SNQN and SA2C in the e-commerce scenario. Both datasets contain click and purchase interactions. We use the supervised head to generate recommendations without special mention. We address the following research questions: RQ1:How do the proposed methods perform when integrated with diî€erent base models? RQ2What is the performance if we use the Q-leaning head to generate recommendation? RQ3:What is the performance if we introduce an additional oî€-policy correction term in the actor of SA2C? RQ4:How does the negative sampling strategy aî€ect the performance? 4.1.1 Datasets: RC15and RetailRocket. RC15.This is based on the dataset of RecSys Challange 2015. The dataset is session-based and each session contains a sequence of clicks and purchases. We remove sessions whose length is smaller than 3 and then sample a subset of 200k sessions. RetailRocket.This dataset is collected from a real-world ecommerce website. It contains session events of viewing and adding to cart. To keep in line with the RC15 dataset, we treat views as clicks and adding to cart as purchases. We remove the items which are interacted less than 3 times and the sequences whose length is smaller than 3. Table 1 summarizes the statistics of the two datasets. 4.1.2 Evaluation protocols. We adopt cross-validation to evaluate the performance of the proposed methods. The ratio of training, validation, and test set is 8:1:1. We randomly sample 80% of sequences as the training set. Each experiment is repeated î€›ve times, and the average performance is reported. The recommendation quality is measured with two metrics: Hit Ratio (HR) and Normalized Discounted Cumulative Gain (NDCG). HR@ğ‘˜is a recall-based metric, measuring whether the groundtruth item is in the top-ğ‘˜positions of the recommendation list. We can deî€›ne HR for clicks as: HR(purchase) is deî€›ned similarly with HR(click) by replacing the clicks with purchases. NDCG is a rank sensitive metric which assign higher scores to top positions in the recommendation list [13]. As we focus on the e-commerce scenario, we assign a higher reward to actions leading to purchases (i.e. conversions) compared to actions leading to only clicks. If a recommended item is not interacted with by the user, we give this action a zero reward. Hence the cumulative reward for evaluation is proportional to HR. 4.1.3 Baselines. We integrated the proposed SNQN and SA2C with four state-of-the-art sequential recommendation models: â€¢GRU [7]: This method utilizes a GRU to model the input sequences. The î€›nal hidden state of the GRU is treated as the latent representation for the input sequence. â€¢Caser [33]: This is a recently proposed CNN-based method, which captures sequential signals by applying convolution operations on the embedding matrix of previous items. â€¢NItNet [39]: NItNet uses dilated CNN for larger receptive î€›eld and residual connection to increase network depth. â€¢SASRec [14]: This baseline is based on self-attention and uses the Transformer [35] architecture. The output of the Transformer encoder is treated as the latent sequence state. We compare SNQN, SA2C with SQN, SAC[37], respectively. 4.1.4 Parameter seî€ings. For both datasets, the input sequences are composed of 10 interacted items. If the sequence length is less than 10, we complement the sequence with a padding item. We train all models with the Adam optimizer [15]. The mini-batch size is 256. For SNQN, the learning rate is 0.01 on RC15 and 0.005 on RetailRocket, which is the identical to SQN [37]. For SA2C, we use the same learning rate with SNQN at the early pre-training stage. After that, the learning rate is set as 0.001 on both datasets. We use the basic uniform distribution for negative sampling strategy to eliminate any inî€uence from the sampler. The item embedding size is set to 64 for all models. For GRU, the size of the hidden state is 64. For Caser, we use 1 vertical convolution î€›lter and 16 horizontal î€›lters whose heights are set from {2,3,4}. The drop-out ratio is set to 0.1. For NextItNet, we use the published implementation [39] with the predeî€›ned settings. For SASRec, the number of heads in self-attention is set as 1, according to the original paper [14]. When SNQN and SA2C are integrated with a base model, the hyperparameter setting of the base model remains exactly unchanged. For the training of SNQN and SA2C, the discount factorğ›¾is set as 0.5. The ratio between the click reward (ğ‘Ÿ) and the purchase reward (ğ‘Ÿ) is set asğ‘Ÿ/ğ‘Ÿ=5. These settings are the same as in [37] for a fair comparison. If without special mention, for one positive action we sample 10 negative actions in the training procedure. The reward for negative actions is set as ğ‘Ÿ= 0. Table 2: Top-ğ‘˜ recommendation performance comparison of diî€erent models (ğ‘˜ = mendations are generated from the supervised head. NG is short for NDCG. Boldface denotes the highest score. Figure 2: Model convergence comparison on RC15 Table 2 and Table 3 show the performance of top-ğ‘˜recommendations on RC15 and RetailRocket, respectively. (1) The introduced negative sampling strategy on the RL head does improve the learning performance also on the supervised component. This can be attributed to the shared recommendation model ğº (Â·)between the supervised part and the RL part. We also observe that SNQN achieves faster convergence than SQN. Figure 2 shows the comparison between model convergence under the same learning rate on the validation set of RC15, using GRU as the base model ğº (Â·). Results on RetailRocket and other base models lead to the same conclusion. This further demonstrates that negative sampling helps the model to learn faster and improves its performance. (2) SA2C achieves better performance than SAC in most cases. This indicates that the advantage estimate used in SA2C is a more eî€ective critic compared with the raw Q-values used in SAC. This HR@5NG@5 HR@10 NG@10 HR@20 NG@20 can be attributed to the fact that the advantage estimation helps to alleviate the sequence timestamp bias. (3) SA2C always achieves the highest NDCG. This is due to the fact that positive actions are weighted (advantaged) in a more eî€ective manner during the training procedure of SA2C. Table 4 shows the performance comparison when we use the Qlearning head to generate recommendations. We compare the performance of SNQN with a simple double Q-learning (DQN) algorithm with the same negative sampling strategy but without a supervised head upon the base model. The performance of SA2C is not signiî€›cantly diî€erent with SNQN as the two methods are essentially identical with regards to the Q-learning head. We use the same base model GRU and the same hyper-parameters for DQN and SNQN. Results on the other base models show identical trends. We observe that SNQN achieves better performance than DQN in all evaluation metrics on both purchase and click predictions. Combined with the results of Table 2 and Table 3, we observe that joint training of supervised learning and RL with shared base models helps to improve the performance of each component. Chen et al. [1]introduced an oî€-policy correction term (propensity score) for the policy-gradient method. The propensity score is deî€›ned asğœŒ =. In this subsection, we investigate the eî€ect of this propensity score when introduced into the actor component of Table 3: Top-ğ‘˜ recommendation performance comparison of diî€erent models (ğ‘˜ = mendations are generated from the supervised head. NG is short for NDCG. Boldface denotes the highest score. Table 4: Recommendation from the RL head. Boldface denotes the highest score. DQN denotes only a Qlearning head is use d without the supervised head. Table 5: Eî€ect of oî€-policy correction. w/o means without oî€-policy correction in the actor while w means the opposite. Boldface denotes the highest score. SA2C. In that case, the training loss of the actor becomes: HR@5NG@5 HR@10 NG@10 HR@20 NG@20 We also introduce another NDCG-based oî€-policy corrected evaluation metric [36] which is formulated as In this implementation, we use the item frequency to approximate the behavior policyğ›½, which is also adopted in [32]. Table 5 shows the result when generating top-10 recommendations with GRU as the base model. Results on the other base models lead to the same conclusion. We note the following observations: (1) Oî€-policy correction doesnâ€™t improve the (standard) NDCG score. NDCG is actually deî€›ned on non-corrected data, so the noncorrected actor performs better at this evaluation metric. (2) Onğ‘ğº, the oî€-policy correction helps the model to achieve better performance for click predictions but not for purchases. The reason for this is that clicks account for the biggest part of the dataset. Hence the oî€-policy correction term is actually better deî€›ned to correct the click data, leading to a better performance ofğ‘ğºfor clicks, while the high variance of the oî€-policy correction for the small portion of purchase data leads to less of an improvement. This observation indicates that perhaps we should design diî€erent corrections for diî€erent kinds of interactions. We found that computing the oî€-policy correction term involves a lot of normalization techniques (e.g., clipping and smoothing) [1]. The behavior policyğ›½can also be a long-tail distribution [32]. This introduces substantial noise and high variance into the training procedure. Designing more eî€ective and stable oî€-policy correction terms remains an open research problem. We conduct a series of experiments to demonstrate the eî€ect of negative sampling on the RL component. Figure 3 and Figure 4 show the recommendation accuracy with diî€erent sizes of negative examples (i.e.|ğ‘|) on RC15 and RetailRocket, respectively (the base model is GRU). On both click and purchase predictions, the recommendation performance initially increases and then decreases (except in Figure 3c). When more negative actions are introduced, the model has more data to learn from. By introducing negative actions, the model does not only learn that actions leading to purchases are better than actions leading to clicks, but also learns to draw a contrast between negative (uninteracted) and positive actions. Increasing the sample size means that the model can have access to more diverse negative signals and, thus, leads to better performance. In Figure 3c, we observe that the model achieves a good performance with small sample sizes. A small sample could introduce more noise into the estimation of the advantage and may help the model to î€›nd a better local optimal with higher performance with more update steps to converge. We have observed in the experiments that SA2C needs more iterations to converge when the sample size is small. Table 6 shows the eî€ect of diî€erent negative reward settings (i.e., ğ‘Ÿ) on RC15 dataset when using GRU as the base model. Results on RetailRocket lead to the same conclusion.ğ‘Ÿcan be seen as the strength of negative signals. We can see from Table 6 that diî€erentğ‘Ÿsettings make no signiî€›cant diî€erence regarding the recommendation performance. However, through the performance comparison between SNQN and SQN, we î€›nd that the presence of negative samples in the RL training procedure dramatically aî€ects the recommendation accuracy. This î€›ts with the î€›nding in [23,38]. Table 6: Eî€ect of negative rewards settings on RC15. In this paper, we propose two learning frameworks (SNQN and SA2C) to explore the usage of RL under recommendation settings. SNQN combines supervised learning and RL with the shared base model and introduces negative sampling into the RL training procedure. The explicitly introduced negative comparison signals help the RL output layer to perform good ranking. Based on the sampled actions, SA2C î€›rst computes the advantage of actions which can be seen as normalized Q-values and then use this advantage estimate as a critic to re-weight the actor. To verify the eî€ectiveness of our methods, we integrate them into four state-of-the-art recommendation models and conduct experiments on two real-world ecommerce datasets. Our experimental î€›ndings demonstrate that the proposed SNQN and SA2C are eî€ective in further improving the recommendation performance, compared to existing self-supervised RL methods.