Cross-domain recommendation (CDR) aims to provide better recommendation results in the target domain with the help of the source domain, which is widely used and explored in real-world systems. However, CDR in the matching (i.e., candidate generation) module struggles with the data sparsity and popularity bias issues in both representation learning and knowledge transfer. In this work, we propose a novel Contrastive Cross-Domain Recommendation (CCDR) framework for CDR in matching. Speciî€›cally, we build a huge diversiî€›ed preference network to capture multiple information reî€ecting user diverse interests, and design an intra-domain contrastive learning (intra-CL) and three inter-domain contrastive learning (inter-CL) tasks for better representation learning and knowledge transfer. The intra-CL enables more eî€ective and balanced training inside the target domain via a graph augmentation, while the inter-CL builds diî€erent types of cross-domain interactions from user, taxonomy, and neighbor aspects. In experiments, CCDR achieves signiî€›cant improvements on both oî€Ÿine and online evaluations in a real-world system. Currently, we have deployed CCDR on a well-known recommendation system, aî€ecting millions of users. The source code will be released in the future. â€¢ Information systems â†’ Recommender systems. recommendation, contrastive learning, cross-domain recommendation, matching Personalized recommendation aims to provide attractive items for users according to their proî€›les and historical behaviors, which has been widely implemented in various î€›elds of our lives. Real-world large-scale recommendation systems usually adopt the classical two-stage architecture containing ranking and matching (i.e., candidate generation) [4,42,54]. The matching module focuses more on the eî€œciency and diversity, which î€›rst retrieves a small subset of (usually hundreds of) item candidates from the million-level large corpora. Next, the ranking module gives the speciî€›c ranks of items for the î€›nal display. Figure 1: An example of CDR in matching. With the increase of recommendation scale and the expansion of recommendation scenarios, real-world recommendations usually need to bring in additional data sources (i.e., domains) as supplements to improve their content coverage and diversity. These cold-start items of new data sources only have very few user behaviors at their warm-up stage. Hence, it is diî€œcult to recommend these cold-start items appropriately. Cross-domain recommendation (CDR), which aims to make full use of the informative knowledge from the source domain to help the target domainâ€™s recommendation [5], is proposed to solve this issue. EMCDR [23] is a classical CDR method, which focuses on building user mapping functions via aligned user representations in the source and target domains. CoNet [14] proposes another approach that jointly models feature interactions in two domains via a cross connection unit. However, existing CDR methods often heavily rely on aligned users for crossdomain mapping (e.g., EMCDR), ignoring other rich information in recommendation such as taxonomy. It will harm the knowledge transfer between diî€erent domains, especially in cold-start scenarios. Moreover, lots of CDR methods are designed for ranking that consider complicated cross-domain user-item interactions (e.g., CoNet), which cannot be directly adopted in matching due to the online eî€œciency. CDR in the matching module should consider not only recommendation accuracy, but also diversity and eî€œciency. In this work, we aim to improve the matching moduleâ€™s performance on new (few-shot or strict cold-start) domains via the CDR manner. Fig. 1 shows an illustration of this task. Precisely, CDR in matching mainly has the following three challenges: (1) How to address the data sparsity and popularity bias issues of CDR in matching? Real-world recommendation usually suî€ers from serious data sparsity issues when modeling the interactions between million-level users and items. Moreover, these sparse interactions are even highly skewed to popular items with high exposure owing to the Matthew eî€ect [26], which makes hot items become hotter. These two issues inevitably harm the representation learning of cold-start and long-tail items, whose damages will even be multiplied in matching where all items should be considered. (2) How to conduct more eî€ective knowledge transfer for the (coldstart) target domain with few user behaviors? As stated above, conventional CDR methods strongly depend on aligned users and their behaviors. The performance of CDR in matching will be greatly reduced, if most users and items have few interactions and models cannot learn reliable representations in cold-start domains. Moreover, other heterogeneous information (e.g., taxonomy) should also be fully considered in CDR to bridge diî€erent domains. We should build more eî€ective and robust cross-domain knowledge transfer paths to well learn both popular and long-tail objects. (3) How to balance the practical demands of accuracy, diversity and eî€œciency of CDR in matching? Online eî€œciency requirements need to be strictly followed. Moreover, matching is more responsible for the diversity than ranking, for it determines the inputs of ranking. A good CDR matching model should comprehensively transfer user diverse preferences via multiple paths to the target domain. To address these issues, we propose a novelContrastive CrossDomain Recommendation (CCDR)to transfer user preferences in matching. Speciî€›cally, we build two global diversiî€›ed preference networks for two domains, containing six types of objects to enhance diversity and cross-domain connections. We conduct a GNN aggregator with a neighbor-similarity based loss on heterogeneous interactions to capture user diverse interests. To strengthen the cross-domain knowledge transfer, we design theintra-domain contrastive learning (intra-CL)andinter-domain contrastive learning (inter-CL)in CCDR. The intra-CL conducts an additional self-supervised learning with sub-graph based data augmentations to learn more reliable representations for matching in the target domain. In contrast, the inter-CL designs three contrastive learning tasks focusing on the cross-domain mapping between aligned users, taxonomies, and their heterogeneous neighbors. The mutual information maximization with diî€erent types of objects multiplies the eî€ectiveness of cross-domain knowledge transfer. Finally, all additional CL losses are combined with the original CDR losses under a multi-task learning (MTL) framework. We conduct a cross-domain multi-channel matching to further improve the diversity in online. CCDR has the following three advantages: (1) The intra-CL brings in self-supervised learning for long-tail users and items, which can alleviate the data sparsity and popularity bias issues in matching. (2) The inter-CL introduces new CL-based cross-domain interactions, which enables more eî€ective and robust knowledge transfer for CDR in matching. (3) The diversiî€›ed preference network, multiple CL tasks, and the cross-domain multi-channel matching cooperate well to capture user diverse preferences, which meets the requirements of diversity and eî€œciency in online system. In experiments, we compare CCDR with competitive baselines on real-world recommendation domains, and achieve signiî€›cant improvements on both oî€Ÿine and online evaluations. Moreover, we also conduct some ablation tests and parameter analyses to better understand our model. The contributions are concluded as: â€¢We propose a novel contrastive cross-domain recommendation for CDR in matching. To the best of our knowledge, we are the î€›rst to conduct contrastive learning to improve both representation learning and knowledge transfer in CDR. â€¢We propose the intra-CL task with a sub-graph based data augmentation to learn better node representations inside the single domain, which can alleviate the data sparsity issue in CDR of matching. â€¢We also creatively design three inter-CL tasks via aligned users, taxonomies, and their neighbors in our diversiî€›ed preference networks, which enable more eî€ective and diverse knowledge transfer paths across diî€erent domains. â€¢We achieve signiî€›cant improvements on both oî€Ÿine and online evaluations. CCDR is eî€ective, eî€œcient, and easy to deploy. Currently, it has been deployed on a real-world recommendation system, aî€ecting millions of users. Matching in Recommendation.The matching (i.e., candidate generation) module aims to retrieve a small subset of (usually hundreds of) items from large corpora eî€œciently [42]. Conventional recommendation matching algorithms often rely on information retrieval based models [30]. Recently, embedding-based retrieval [1,15] is also widely used in practical systems with the help of fast retrieval servers [16]. Due to the need for eî€œciency, embeddingbased methods usually adopt a two-tower architecture, which conducts two diî€erent towers for building user and item representations separately. Diî€erent feature interaction methods such as FM [29], Youtube candidate generation [4], DeepFM [8], AutoInt [32], ICAN [42], AFN [3], DFN [40], and AFT [10] could be used in these towers. In contrast, tree-based matching models [54,56] give another way to address the matching problem with structured item trees. Graph-based matching models [41] are also proposed to learn user/item representations. However, few works focus on the CDR in matching, which often exists in practical recommendations. Cross-domain Recommendation.Cross-domain recommendation attempts to learn useful knowledge from the source domain to help the target domainâ€™s recommendation [5]. EMCDR [23] is a classical embedding mapping approach, which builds the mapping function via aligned usersâ€™ representations. SSCDR [17] designs a semi-supervised manner to learn item mapping based on EMCDR. In contrast, CoNet [14] is another classical type of CDR method that uses the cross connection unit to model domain interactions. Zhao et al. [49]combines items of both source and target domains in one graph to learn representations. Neural attentive transfer [6], dual transfer [19,53], source-target mixed attention [25], dual generator [48], and meta-learning [55] are also proposed for CDR. Additional review information is also used to enhance the attentive knowledge transfer [50]. Some works explore non-overlapping CDR [46]. However, most of these CDR models are specially designed for the ranking module (which involve user-item interactions in cross-domain modeling). ICAN [42] is the most related work, which captures î€›eld-level feature interactions to improve matching in multiple domains. In CCDR, we introduce several novel CL tasks. To the best of our knowledge, we are the î€›rst to conduct CL to jointly improve representation learning and knowledge transfer in CDR. Contrastive Learning.Contrastive learning (CL) is a representative self-supervised learning (SSL) method, which aims to learn models by contrasting positive pairs against negative pairs [9]. Contrastive predictive coding (CPC) [24] designs the widely-used InfoNCE loss. MoCo [11] builds a large dynamic dictionary with a queue and a moving-averaged momentum encoder. SimCLR [2] designs a simple contrastive learning framework with a composition of data augmentations and projectors for CL. BYOL [7] relies on its online and target networks, which iteratively bootstraps the outputs of a network to serve as targets for learning. Qiu et al. [28] proposes a graph contrastive coding for GNN pre-training. CL in recommendation.Recently, SSL and CL are also veriî€›ed in recommendation [47]. S-Rec [52] builds contrastive learning tasks among items, attributes, sentences, and sub-sentences in sequential recommendation. UPRec focuses on user-aware SSL [39]. Xie et al. [44]designs an adversarial and contrastive VAE for sequence modeling. Moreover, CL has also been used in disentangled recommendation [22,51], session modeling [38], social recommendation [45], and cold-start recommendation [35]. For graph-based CL, Wu et al. [37]introduces embedding, node, edge dropouts to graphbased recommendation. Diî€ering from these works, we build three CL tasks to facilitate the user preference transfers between diî€erent domains in cold-start cross-domain recommendation. In this work, we propose CCDR to enhance the cross-domain recommendation in matching via contrastive learning. In this section, we î€›rst describe our task and the overall framework of CCDR (Sec. 3.1). Second, we introduce the diversiî€›ed preference networks and the single-domain GNN-based aggregator (Sec. 3.2 and Sec. 3.3). Next, we introduce the intra-domain and inter-domain contrastive learning (Sec. 3.4 and Sec. 3.5). Finally, we combine three losses with a multi-task optimization (Sec. 3.6). The online system and deployment of CCDR will be introduced in Sec. 4. More detailed discussions are given in Appendix. CDR in matching.We concentrate on the matching module of the classical two-stage recommendation systems [4]. Matching is the î€›rst step before ranking, which attempts to eî€œciently retrieve hundreds of items from million-level item candidates. It cares more about whether good items are retrieved (often measured by hit rate), not the speciî€›c top item ranks which should be considered by the following ranking module (often measured by NDCG or AUC) [21,42]. The CDR in matching task attempts to improve the target domainâ€™s matching module with the help of the source domain. Overall framework.CCDR is trained with three types of losses, including the original source/target single-domain losses, the intradomain CL loss, and the inter-domain CL loss. (1) We î€›rst build a huge global diversiî€›ed preference network separately for each domain as the sources of user preferences. This diversiî€›ed preference network contains various objects such as user, item, tag, category, media, and word with their interactions to bring in user diverse preferences from diî€erent aspects. (2) Next, we train the single-domain matching model via a GNN aggregator and the neighbor-similarity based loss. (3) Since the cold-start domain lacks suî€œcient user behaviors, we introduce the intra-domain CL inside the target domain to train more reliable node representations with a sub-graph based data augmentation. (4) To enhance the cross-domain knowledge transfer, we design three inter-domain CL tasks via aligned users, taxonomies, and their neighbors between two domains, which cooperate well with the diversiî€›ed preference network. All three losses are combined under a multi-task learning framework. Conventional matching [4] and CDR [17,23] models usually heavily rely on user-item interactions to learn CTR objectives and crossdomain mapping. However, it will decrease the diversity of matching due to the popularity bias issue. Moreover, it does not take full advantage of other connections (e.g., tags, words, medias) besides users between diî€erent domains, which is particularly informative in cross-domain knowledge transfer. Therefore, inspired by [20,41], we build a global diversiî€›ed preference network for each domain, considering 6 types of important objects in recommendation as nodes and their heterogeneous interactions as edges. Speciî€›cally, we use item, user, tag, category, media, and word as nodes. Tags and categories are item taxonomies that represent usersâ€™ î€›ne- and coarse- granularity interests. Media indicates the itemâ€™s producer. Words reî€ect the semantic information of items extracted from itemsâ€™ titles or contents. To alleviate data sparsity and accelerate our oî€Ÿine training, we also gather users into user groups according to their basic proî€›les (all users having the same gender-age-location attributes are clustered in the same user group). These user groups are viewed as user nodes in CCDR. As for edges, we consider the following six item-related interactions: (a) User-item edge (U-I). This edge is generated if an item is interacted by a user group at least 3 times. We jointly consider multiple user behaviors (i.e., click, like, share) to build this edge with diî€erent weights. (b) Item-item edge (I-I). The I-I edge introduces sequential information of user behaviors in sessions. It is built if two items appear in adjacent positions in a session. (c) Tag-item edge (T-I). The T-I edges connect items and their tags. It captures itemsâ€™ î€›ne-grained taxonomy information. (d) Category-item edge (C-I). It records itemsâ€™ coarse-grained taxonomy information. (e) Media-item edge (M-I). It links items with their producers/sources. (f) Word-item edge (W-I). It highlights the semantic information of items from their titles. Each edge is undirected but empirically weighted according to the edge type and strength (e.g., counts for UI edges). Comparing with conventional U-I graphs, our diversiî€›ed preference network tries its best to describe items from diî€erent aspects via these heterogeneous interactions. The advantages of this diversiî€›ed preference network are as follows: (1) it brings in additional information as supplements to user-item interactions, which jointly improve both accuracy and diversity (see Sec. 3.3.2). (2) It can build more potential bridges between diî€erent domains via users, tags, categories, and words, which cooperates well with the inter-CL tasks and the online multi-channel matching in CDR (see Sec. 3.5 and Sec. 4). 3.3.1 GNN-based Aggregator. Inspired by the great successes of GNN, we adopt GAT [34] as the aggregator on the diversiî€›ed preference network for simplicity and universality. Precisely, we randomly initializeğ’†for all heterogeneous nodes. For a nodeğ‘’and its neighborğ‘’âˆˆ ğ‘(ğ‘is the neighbor set ofğ‘’after a weighted sampling), we haveğ‘’â€™s node representationğ’†at theğ‘¥-th layer as: ğ‘¾is the weighting matrix,ğœis the sigmoid function.ğ›¼represents the attention between ğ‘’and ğ‘’in ğ‘¥-th layer noted as: whereğ‘“ (Â·)indicates a LeakyReLU activation and||indicates the concatenation.ğ’˜is the weighting vector. Note that theğ‘is a dynamic neighbor set which is randomly generated based on the edge weight in Sec. 3.2. We conduct a two-layer GAT to generate the aggregated node representationsğ’†for all nodes (ğ’†andğ’†for the source and target domains). It is also not diî€œcult to conduct other GNN models such as LightGCN [12] in this module. 3.3.2 Neighbor-similarity Based Optimization. In practical CDR scenarios, users often have fewer historical behaviors on items in (cold-start) target domains. Conventional embedding-based matching methods such like Matrix factorization (MF) [18] cannot get suî€œcient supervised information from the sparse user-item interactions, and thus cannot learn reliable user and item representations for matching. To capture additional information from behavior, session, taxonomy, semantics, and data source aspects, we conduct the neighbor-similarity based loss [20] on the diversiî€›ed preference network. As shown in Fig. 2, this loss projects all nodes into the same latent space, making all nodes similar with their neighbors. It regards all types of edges as unsupervised information to guide the training besides user-item interactions. Formally, the neighborsimilarity based loss ğ¿is deî€›ned as follows:îƒ•îƒ•îƒ• ğ¿= âˆ’(âˆ’ log(ğœ (ğ’†ğ’†)) + log(ğœ (ğ’†ğ’†))).(3) ğ’†is theğ‘–-th aggregated node representation, andğ‘’is a sampled neighbor of ğ‘’. ğ‘’is a randomly selected negative sample of ğ‘’. We choose the neighbor-similarity based loss for the following advantages: (1)ğ¿makes full use of all types of interactions between heterogeneous objects in matching, which contain signiî€›cant information from user behaviors (U-I edges), sessions (I-I edges), item taxonomies (T-I and C-I edges), data sources (M-I edges) and semantics (W-I edges). It helps to capture user diverse preferences to balance accuracy and diversity in matching. If we only consider U-I edges, this loss will degrade into the classical MF. (2) CDR in matching should deal with long-tail items.ğ¿brings in additional information for long-tail items that can beneî€›t cold-start domains. (3) We conduct a cross-domain multi-channel matching strategy in online for diversity. This embedding-based retrieval strategy also depends on heterogeneous node embeddings optimized byğ¿to retrieve similar items in the (cold-start) target domain (see Sec. 4 for more details). Theğ¿loss exactly î€›ts for the online multi-channel matching, and also well cooperates with the diversiî€›ed preference network and the inter-CL losses. We cannot conduct complicated user-item interaction calculations in Eq. (3), since we rely on the fast embedding-based retrieval in matching for eî€œciency. Contrastive learning is a widely-used SSL method that can make full use of unlabelled data via its pair-wise training. In CCDR, we conduct two types of CL tasks. Theintra-domain contrastive learning (intra-CL)is conducted inside the target domain to learn better node representations, while theinter-domain contrastive learning (inter-CL)is adopted across the source and target domains to guide a better knowledge transfer. Figure 2: The neighbor-similarity loss and the intra-CL loss. In intra-CL, we conduct a sub-graph based data augmentation for each node aggregation, which could be regarded as a dynamic node/edge dropout in classical graph augmentation [37]. Precisely, for a nodeğ‘’, we sample two neighbor setğ‘andğ‘to conduct the GNN aggregation, and receive two node representationsğ’†and ğ’†.ğ’†is regarded as the positive instance ofğ’†in intra-CL, with a diî€erent sub-graph sampling focusing on diî€erent neighbors ofğ‘’. Similar to [2], we randomly sample from other examplesğ’†in the same batchğµofğ‘’to get the negative samplesğ‘’. We do not use all examples inğµas negative samples for eî€œciency. In this case, the popularity bias is partially solved [36]. Formally, we follow the InfoNCE [24] to deî€›ne the intra-CL loss ğ¿as follows: ğ¿= âˆ’logexp(sim(ğ’†, ğ’†)/ğœ)Ãexp(sim(ğ’†, ğ’†)/ğœ).(4) ğ‘†indicates the negative samples ofğ‘’inğµ.ğœis the temperature. sim(ğ’†, ğ’†)measures the similarity betweenğ’†andğ’†, which is calculated as their cosine similarity. With the intra-CL loss, longtail nodes can also get training opportunities via SSL. The inter-CL aims to improve the knowledge transfer across diî€erent domains via various types of nodes and edges in the diversiî€›ed preference network. Precisely, we design three inter-domain CL tasks via aligned users, taxonomies, and neighbors as in Fig. 3. 3.5.1 User-based Inter-CL. Most conventional CDR methods [23] take aligned users as their dominating mapping seeds across domains. We follow this idea and conduct a user-based inter-CL task. ttTaxonomy-based inter-CL: e eeNeighbor-based inter-CL: Figure 3: Three inter-CL tasks across diî€erent domains. Each userğ‘¢has two user representationsğ’–andğ’–in the source and target domains learned in Sec. 3.3. Although users may have diî€erent preferences and behavior patterns in two domains, it is still natural that the source-domain representationğ’–should be more similar with its target-domainğ’–than any other representations ğ’–. We deî€›ne the user-based inter-CL loss ğ¿as follows: ğ¿= âˆ’logexp(sim(ğ’–, ğ’–)/ğœ)Ãexp(sim(ğ’–, ğ’–)/ğœ).(5) ğ‘†is the sampled negative set collecting from all other users except ğ‘¢. sim(Â·, Â·) indicates the cosine similarity. 3.5.2 Taxonomy-based inter-CL. Diî€ering from some classical CDR methods [17], CCDR builds a diversiî€›ed preference network that introduces more bridges across diî€erent domains. We assume that the same tag/category/word in diî€erent domains should have the same meanings. Hence, we design a taxonomy-based inter-CL similar to the user-based CL. we take the aggregated node representation pair (ğ’•,ğ’•) of the same taxonomyğ‘¡in two domains as the positive pair, where ğ‘¡could be tags, categories, and words. We have: ğ‘†is the sampled negative set ofğ‘¡from all other taxonomies with the same type. We can set diî€erent temperatures for taxonomies and users if we want to sharpen the diî€erences of some types.ğ¿ functions as a supplement to the original user-based mapping. 3.5.3 Neighbor-based inter-CL. Besides the explicit alignments of users and taxonomies across domains, there are also some essential objects such as items that do not have explicit mapping. We aim to bring in more implicit cross-domain knowledge transfer paths between unaligned nodes in two domains. We suppose that similar nodes in diî€erent domains should have similar neighbors (e.g., similar items may have similar users, taxonomies, and producers). Hence, we propose a neighbor-based inter-CL, which builds indirect (multi-hop) connections between objects in diî€erent domains. Precisely, we deî€›neğ¸as the overall aligned node set (including users, tags, categories, and words). The neighbor-based inter-CL lossğ¿is formalized with all aligned nodesğ‘’âˆˆ ğ¸andğ‘’â€™s neighbor set ğ‘in the target domain as follows: ğ¿= âˆ’logÃexp(sim(ğ’†, ğ’†)/ğœ).(7) Inğ¿, for an aligned nodeâ€™s representationğ’†in the source domain, its target-domain neighborâ€™s representationğ’†is the positive instance, while other target-domain representationsğ’†are negative. It is reasonable since related objects should be connected in the diversiî€›ed preference network and learned to be similar under the neighbor-similarity based loss in Eq. (3). It is also convenient to extend the current positive samplesğ‘’âˆˆ ğ‘to multi-hop neighbors for better generalization and diversity in CDR. This neighbor-based inter-CL greatly multiplies the diversiî€›ed knowledge transfer paths between two domains, especially for the cold-start items. For example, through theğ‘¡ğ‘ğ‘”â†’ ğ‘¡ğ‘ğ‘”â†’ ğ‘–ğ‘¡ğ‘’ğ‘š path, the cold-startğ‘–ğ‘¡ğ‘’ğ‘šâ€™s representation in the target domain can be directly inî€uenced by fully-trained representations in the source domain. Moreover, the similarities between diî€erent types of source-domain node representations and the target-domain item representations are directly used in the online multi-channel matching for diversiî€›ed retrieval, which will be discussed in Sec. 4. Finally, we combine all three CL losses from aligned user, taxonomy, and neighbor aspects to form the inter-CL loss ğ¿as: Following classical CL-based recommendation models [45], we also conduct a multi-task optimization combining the source-domain matching lossğ¿, the target-domain matching lossğ¿, the intraCL loss ğ¿, and the inter-CL loss ğ¿as follows: ğœ†, ğœ†, ğœ†, ğœ†are loss weights set as 1.0,1.0,1.5,0.6 according to the grid search (see Sec. 5.7 for more details). We have deployed CCDR on the cold-start matching module in a well-known recommendation system named WeChat Top Stories. A good CDR-based cold-start matching module should have the following key characteristics: (1) making full use of user behaviors and item features in the source and target domains, (2) capturing user diverse preferences from diî€erent aspects, and (3) balancing accuracy, diversity and eî€œciency. To achieve these, we propose a new cross-domain multi-channel matching in online. Speciî€›cally, we conduct six channels including user, item, tag, category, media, and word channels to retrieve items in the target domains via node representations learned by Eq. (9). We rely on the user historical behavior sequenceğ‘ ğ‘’ğ‘ = {ğ‘‘, ğ‘‘, Â· Â· Â· , ğ‘‘}to capture userâ€™s interests, whereğ‘‘is theğ‘–-th clicked item andğ‘›is the max length. In the item channel, we directly use the node representations of all items inğ‘ ğ‘’ğ‘to retrieve similar items in the target domain. Formally, we deî€›ne the scoreğ‘ of theğ‘–-th target-domain itemÂ¯ğ‘‘ in the item channel as follows:îƒ• ğ‘ =ğ‘ ğ‘–ğ‘š(Â¯ğ’…, ğ’…) Ã— ğ‘ ğ‘ğ‘¡ğ‘–ğ‘  ğ‘“Ã— ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘›ğ‘ğ‘¦Ã— ğ‘§(ğ‘–, ğ‘—).(10) ğ‘ ğ‘–ğ‘š(Â¯ğ’…, ğ’…)is the cosine similarity between the clicked itemğ‘‘in user historical behaviors and the item candidateÂ¯ğ‘‘in the target domain, whereÂ¯ğ’…andğ’…are aggregated item embeddings trained by Eq. (9).ğ‘ ğ‘ğ‘¡ğ‘–ğ‘  ğ‘“measures the posterior user satisfaction onğ‘‘, which is roughly calculated as the complete rate of item contents.ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘›ğ‘ğ‘¦ models the temporal factors of historical items, which decays exponentially from the short term to the long term (ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘›ğ‘ğ‘¦=0.95). For online eî€œciency, each item inğ‘ ğ‘’ğ‘only recommends its top 100 nearest items.ğ‘§(ğ‘–, ğ‘—)equals 1 only if the target-domain itemÂ¯ğ‘‘appears in the top 100 nearest items ofğ‘‘, and otherwiseğ‘§(ğ‘–, ğ‘—) =0. We pre-calculate the similarities and index the top nearest items for all nodes in oî€Ÿine to further accelerate the online matching. To capture user diverse preferences from diî€erent aspects, we further conduct the tag, category, media and word channels similar to the item channel. Taking the tag channel for instance, we build a historical tag sequenceğ‘ ğ‘’ğ‘= {ğ‘‡,ğ‘‡, Â· Â· Â· , ğ‘‡}according to the item sequence{ğ‘‘, ğ‘‘, Â· Â· Â· , ğ‘‘}, whereğ‘‡is the tag set ofğ‘‘. All tags inğ‘ ğ‘’ğ‘retrieve top 100 nearest items in the target domains as candidates. Similar to Eq. (10), the score of theğ‘–-th target-domain itemÂ¯ğ‘‘in the tag channel ğ‘ is deî€›ned as follows: ğ‘ =ğ‘ ğ‘–ğ‘š(Â¯ğ’…, ğ’•) Ã— ğ‘ ğ‘ğ‘¡ğ‘–ğ‘  ğ‘“Ã— ğ‘Ÿğ‘’ğ‘ğ‘’ğ‘›ğ‘ğ‘¦Ã— ğ‘§(ğ‘–, ğ‘—, ğ‘˜).(11) ğ‘ ğ‘–ğ‘š(Â¯ğ’…, ğ’•)is the cosine similarity betweenÂ¯ğ’…and the aggregated tag representationğ’•.ğ‘§(ğ‘–, ğ‘—, ğ‘˜)is the tagâ€™s indicator.ğ‘§(ğ‘–, ğ‘—, ğ‘˜) =1 only if the tagğ‘¡belongs to theğ‘—-th itemğ‘‘inğ‘ ğ‘’ğ‘, andÂ¯ğ‘‘locates in the top 100 nearest items ofğ‘¡. Other category, media and word channels are the same as the tag channel, generating their corresponding scoresğ‘ ,ğ‘ andğ‘ . As for the user channel, we directly depend on the userâ€™s gender-age-location attribute tripletâ€™s (i.e., the user group in Sec. 3.2) node representations to retrieve top nearest items according to the cosine similarity score ğ‘ forÂ¯ğ‘‘. Finally, all top items retrieved by six heterogeneous channels are combined and reranked via the aggregated score ğ‘ as: It is easy to set and adjust the hyper-parameters of heterogeneous channelsâ€™ weights for the practical demands and the preferences of systems. We rank top target-domain items viağ‘ , and select top 500 items as the î€›nal output of our multi-channel matching, considering both matching accuracy and memory/computation costs. We conclude the feasibility and advantages of our cross-domain multi-channel matching as follows: (1) These multiple matching channels rely on the similarities between the target-domain items and heterogeneous nodes, which is consistent with the neighborsimilarity based loss and the inter-CL losses. (2) The multi-channel matching makes full use of all heterogeneous information to generate diversiî€›ed item candidates, which is essential in cold-start matching. (3) We pre-calculate the indexes for the top nearest items of all nodes, which greatly reduces the online computation costs. The online computation complexity of CCDR isğ‘‚ (log(600ğ‘›)+600ğ‘›) (ğ‘›is the length of user historical behavior). More details of online deployment and eî€œciency are given in Appendix A.1. In this section, we conduct suî€œcient experiments to answer the following research questions: (RQ1): How does CCDR perform against diî€erent types of competitive baselines on metrics of matching (see Sec. 5.4)? (RQ2): How does CCDR perform in online evaluation on real-world recommendation systems (see Sec. 5.5)? (RQ3): What are the eî€ects of diî€erent components in CCDR (see Sec. 5.6)? CCDR relies on item-related taxonomy, semantic, and producer information for CDR in matching, while no large-scale public CDR dataset is capable for this setting. Therefore, we build a new CDR dataset CDR-427M extracted from a real-world recommendation system named WeChat Top Stories, which contains a source domain and two target domains. Speciî€›cally, we randomly select nearly 63 million users, and collect their 427 million behaviors on 3.0 million items. We split these behaviors into the train set and the test set using the chronological order. We also bring in 187 thousand tags, 356 categories, 56 thousand medias, and 207 thousand words as additional item-related information. All data are preprocessed via data masking to protect the userâ€™s privacy. To simulate diî€erent CDR scenarios, we evaluate on two target domains having diî€erent cold-start degrees. The î€›rst is a few-shot target domain, where most users only have several behaviors. The second is a strict cold-start domain, which is more challenging since all user behaviors on items in the train set are discarded [27]. Following Sec. 3.2, we build three diversiî€›ed preference networks for all domains separately via the train set and all item-related information. The statistics of diversiî€›ed preference networks in three domains are in Table 1. Table 1: Statistics of three domains in CDR-427M. few-shot target domain 2.38M 0.39M 29.8M 10.8M 4.99M We implement several competitive baselines focusing on the matching module and cross-domain recommendation for comparisons. Classical Matching Methods. We implement three competitive matching models as baselines. We do not compare with tree-based models [54], for they cannot be deployed in cold-start domains. All user behaviors of two domains are considered in these models. â€¢ Factorization Machine (FM) [Rendle 2010].FM [29] is a classical embedding-based matching model. It captures the feature interactions between users and items for embeddingbased retrieval under the two-tower architecture [4]. â€¢ AutoInt [Song et al. 2019].AutoInt [32] is a recent method that utilizes self-attention to model feature interactions. It also adopt the two-tower architecture for matching. â€¢ GraphDR+ [Xie et al. 2021a].GraphDR [41] is an eî€ective graph-based matching model. The single-domain model of CCDR could be considered as an enhanced GraphDR with diî€erences in node aggregation and multi-channel matching specially designed for CDR. We directly conduct the singledomain model of CCDR on the joint network containing both source and target domains, noted as GraphDR+. Table 2: Results of matching-related metrics on CDR-427M. All improvements of CCDR are signiî€›cant (t-test with ğ‘ < indicates that these models are based on the same single-domain model (noted as GraphDR+) in CCDR. Cross-domain/Multi-domain Methods. We also implement two representative CDR models and one multi-domain matching model as baselines. We do not compare with CDR models like CoNet [14], since they cannot be directly used in matching for eî€œciency. â€¢ EMCDR+ [Man et al. 2017].EMCDR [23] is a classical CDR model that directly learns the embedding mapping of users between two domains. For fair comparisons, we use the same single-domain model and multi-channel matching in CCDR for learning and serving, noted as EMCDR+. â€¢ SSCDR+ [Kang et al. 2019].SSCDR [17] is regarded as an enhanced EMCDR, which adopts a semi-supervised loss to learn the mapping of items. We also follow the same settings of EMCDR to get SSCDR+. Since the strict cold-start domain has no user-item behaviors, we use aligned taxonomies to learn cross-domain mappings in EMCDR+ and SSCDR+. â€¢ ICAN [Xie et al. 2020b].ICAN [42] is the SOTA model in multi-domain matching, which is the most related work of our task. It highlights the interactions between feature î€›elds in diî€erent domains for cold-start matching. Knowledge Distillation/Contrastive Learning Methods. We further propose two enhanced versions of the single-domain matching model in CCDR (i.e., GraphDR+) for more challenging comparisons. â€¢ Sub-graph CL.We build a sub-graph CL method based on GraphDR+. It considers the intra-CL loss with a sub-graph augmentation in Eq. (4) inspired by [28,37]. It can be viewed as an ablation version of CCDR without the inter-CL. â€¢ Cross-domain KD.We further propose a cross-domain knowledge distillation (KD) model. This model also follows the single-domain model of CCDR, learning the cross-domain mapping via the Hint loss [31] between aligned nodes in two domains (i.e., user, tag, category, and word). In the single-domain model of CCDR, the input dimensions of all nodes are 128, and the output dimensions are 100. We conduct a weighted neighbor sampling to select 25 and 10 neighbors for the î€›rst and second layersâ€™ aggregations. The edge weight is proportional to the mutual information between its two nodes to make sure that diî€erent types of interactions can have comparable frequencies. In online matching, we use the top 200 most recent behaviors. All graph-based models have the same online matching strategy. The batch sizes and the negative sample numbers of the intra-CL, inter-CL, and neighbor-similarity based losses are 4,096 and 10. The temperatureğœis set to be 1. For all models, we conduct a grid search to select parameters. Parameter analyses of CL loss weights are given in Sec. 5.7. All models share the same experimental settings and multi-domain behaviors for fair comparisons. 5.4.1 Evaluation Protocols. We evaluate on the few-shot and strict cold-start domains separately. All models select topğ‘items from the overall corpora for each test instance. Following classical matching models [33,41,42], we utilize the topğ‘hit rate (HIT@N) as our evaluation metric. To simulate the real-world matching systems, we concentrate on largerğ‘as 50, 100, 200, and 500 (we retrieve top 500 items in online). We should double clarify that CCDR focuses on CDR in matching, which cares whether good items are retrieved, not the speciî€›c ranks that should be measured by the ranking module. Hence, HIT@N is much more suitable for matching than ranking metrics such as AUC and NDCG. We also evaluate the diversity via a classical aggregate diversity metric named item coverage [13]. 5.4.2 Experimental Results. From Table 2 we can observe that: (1) CCDR achieves signiî€›cant improvements over all baselines on all HIT@N in both two domains, with the signiî€›cance level ğ‘ <0.01 (the deviations of CCDR are withinÂ±0.0004 in HIT@500). It indicates that CCDR can learn high-quality matching embeddings and well transfer useful knowledge to the target domain via CL. The improvements of CCDR mainly derive from three aspects: (a) The intra-CL enables more suî€œcient and balanced training via SSL with selected negative samples, which successfully alleviates the data sparsity and popularity bias issues. (b) The inter-CL builds interactions across diî€erent domains via three CL tasks, which multiplies the knowledge transfer via heterogeneous bridges. (c) The diversiî€›ed preference network, CL losses, and multi-channel matching cooperate well with each other. The similarities used in online matching are directly optimized via losses in Eq. (9). (2) CCDR has large improvement percentages on the challenging strict cold-start domain (55% improvement on HIT@500 over SSCDR+), where users have no behaviors on target items. It is natural since the combination of the diversiî€›ed preference network and user/taxonomy/neighbor based inter-CL tasks can transfer more diversiî€›ed preferences via more cross-domain bridges. Moreover, comparing with diî€erent CCDR models, we î€›nd that both intra-CL and inter-CL are eî€ective, while inter-CL plays a more important role in CDR. We also î€›nd that CCDR has 4.2% and 6.0% improvements on the diversity metric item coverage [13] compared to the best-performing GraphDR+ in two domains. It indicates that CCDR has better performances on the diversity via CL tasks. (3) Among baselines, we î€›nd that ICAN performs better in the few-shot domain, while SSCDR+ performs better in the strict coldstart domain. It is because that ICAN strongly relies on the feature î€›eld interactions between behaviors in diî€erent domains, which are extremely sparse or even missing in the strict cold-start scenarios. In contrast, SSCDR+ beneî€›ts from cross-domain mapping. Moreover, classical matching methods such as GraphDR+ perform worse than CCDR. It implies that simply mixing behaviors in two domains may not get good performances, since the unbalanced domain data will confuse the user preference learning in the target domain. (4) Comparing with diî€erent CDR methods, we observe that the CL-based methods are the most eî€ective compared to knowledge distillation (e.g., cross-domain KD) and embedding mapping (e.g., EMCDR+ and SSCDR+). It is because that (a) contrastive learning can provide a suî€œcient and balanced training via SSL, and (b) CCDR conducts knowledge transfer via not only aligned users, but also taxonomies and neighbors. In this case, the popularity bias and data sparsity issues in the CDR part can be largely alleviated. 5.5.1 Evaluation Protocols. To verify the eî€ectiveness of CCDR in real-world scenarios, we conduct an online A/B test on a wellknown online recommendation system named WeChat Top Stories. Precisely, we deploy CCDR and several competitive baselines in the matching module of a relatively cold-start domain as in Sec. 4, with the ranking module unchanged. The online baseline is the GraphDR+ (target) model trained solely on the target domain. In online evaluation, we focus on the following three online metrics in the target domain: (1) CTR, (2) average user duration per capita, and (3) average share rate per capita. We conduct the A/B test for 8 days, with nearly 6 million users inî€uenced by our online evaluation. CCDR (inter-CL) CCDR (inter-CL+intra-CL) +14.368% +6.623% +10.401% Table 3: Online A/B tests on WeChat Top Stories. 5.5.2 Experimental Results. Table 3 shows the online improvement percentages of all models. We can î€›nd that: (1) CCDR signiî€›cantly outperforms all models in three metrics with the signiî€›cance levelğ‘ <0.01. Note that all models are based on the same single-domain model in CCDR (i.e., GraphDR+). It reconî€›rms the eî€ectiveness of the intra-domain and inter-domain contrastive learning. We jointly consider multiple behaviors such as click, share and like to build the diversiî€›ed preference network, and use a neighbor-similarity based loss to learn user diverse preferences. Hence, CCDR has improvements on diî€erent metrics, which reî€ects userâ€™s real satisfaction more comprehensively. (2) Comparing with the base model that only considers the target domain, we know that the source domainâ€™s information is essential. Looking into the diî€erences among GraphDR+, Sub-graph CL (i.e., CCDR (intra-CL)), and CCDR (inter-CL), we can î€›nd that both intraCL and inter-CL are eî€ective in online scenarios. Moreover, CCDR models outperform GraphDR+ and Cross-domain KD, which also veriî€›es the advantages of inter-CL over simple multi-domain mixing and cross-domain knowledge distillation in knowledge transfer. In this subsection, we further compare CCDR with its several ablation versions to show the eî€ectiveness of diî€erent CL tasks. Table 4 displays the HIT@N results on both few-shot and strict cold-start domains. We î€›nd that: (1) Both intra-CL and inter-CL are essential in few-shot and cold-start domains. Inter-CL contributes the most to the CDR performances, since it is strongly related to the knowledge transfer task in CDR and î€›ts well with the neighbor-similarity based loss of the single-domain model. (2) The intra-CL task also signiî€›cantly improves the matching in CDR, while it just achieves slight improvements on the strict cold-start domain. The power of intra-CL will be multiplied when there are more user behaviors in the target domain. (3) From the second ablation group, we observe that all three inter-CL tasks can provide useful information for CDR. We observe that the user-based inter-CL functions well in the few-shot domain (since it has more user-related interactions), while taxonomy-based inter-CL achieves higher improvements in the cold-start domain. Note that CCDR does not conductğ¿and use user channel in the strict cold-start domain, since the cold-start user nodes are isolated in the target domain with no behaviors. We further conduct two model analyses on diî€erent weights of the intra-CL and inter-CL losses. Fig. 4 displays the HIT@500 results of diî€erent intra- and inter- CL weights (ğœ†andğœ†) on both fewshot and strict cold-start domains. We can î€›nd that: (1) as the loss weight increases, the HIT@500 results of both intra-CL and interCL losses î€›rst increase and then decrease. The best parameters are ğœ†=1.5, ğœ†=0.6. Note that the parameter analysis is carried out around the optimal parameter point. (2) The performance trends of two CL loss weights are relatively consistent on both few-shot and strict cold-start domains. Moreover, CCDR models with diî€erent CL loss weights still outperform all baselines, which veriî€›es the robustness and usability of CCDR in real-world scenarios. Figure 4: Results of diî€erent intra-/inter- CL loss weights. In this work, we propose a novel CCDR framework to deal with CDR in matching. We adopt the intra-CL to alleviate the data sparsity and popularity bias issues in matching, and design three inter-CL tasks to enable more diverse and eî€ective knowledge transfer. CCDR achieves signiî€›cant oî€Ÿine and online improvements on diî€erent scenarios, and is deployed on real-world systems. In the future, we will explore more sophisticated inter-CL tasks to further improve the eî€ectiveness and diversity of knowledge transfer. We will also try to introduce the idea of inter-CL to other cross-domain tasks.