Graph Neural Networks (GNNs) have shown success in learning from graph stru c tured data containing node/ed ge feature information, with application to social networks, recommendation, fraud detection and knowledge graph reasoning. In this regard, various strategies have been proposed in the past t o improve the expressiveness of GNNs. For example, one straightforward option is to simply increase the parameter size by either expanding the hidden d imension or increasing the number of GNN layers. However, wider hidden layers can easily lead to overÔ¨Åtting, and incrementally adding more GNN layers can potentially result in over-smoothing. In this paper, we present a mo del-agnostic methodology, namely Network In Graph Neural Network (NGNN ), that allows arbitrary GNN models to increase their model capacity by making the model deeper. However, instead of adding or widening GNN layers, NGNN deepens a GNN model by inserting non-linear feedforward neural network layer(s) within each GNN layer. Although, some works mentioned that adding MLPs within GNN layers could beneÔ¨Åt the performance, they did not systematically analyze the reason for the improvement, nor evaluate with numerous GNNs o n large-scale graph datasets. In this paper, we demonstrate that NGNN can keep the model stable against either node feature or graph structure perturb ations through an analysis of it as applied to a GraphSage base GNN on ogbn-products data. Furthermore, we take a wideranging evaluation of NGNN on both node classiÔ¨Åcation and link prediction tasks and show that NGNN works reliably across diverse GNN architectures. For instance, it improves the test accuracy of GraphSage on the ogbn-products by 1.6% and improves the hits@100 score of SEAL on ogbl-pp a by 7.08% and the hits@20 score of GraphSage+Edge-Att r on ogbl-ppi by 6.22%. And at the time of this submission, it achieved two Ô¨Årst places on the OGB link prediction leaderboard. ‚Ä¢ Computer systems organization ‚Üí Embedded systems; Redundancy; Robotics; ‚Ä¢ Networks ‚Üí Network reliability. Graph Neural Network, Link Prediction, Node ClassiÔ¨Åcation daviwipf@amazon.com (a) The test accuracy of GraphSage with diÔ¨Äerent settings. (b) The model parameter sizes of GraphSage with diÔ¨Äerent settings. Figure 1: The test accuracy of GraphSage on ogbn-products with diÔ¨Äerent number of GNN layers (from 2 to 4) and diÔ¨Äerent hidden dimension sizes (from 128 to 1024) and the corresponding model parameter sizes. Graph Neural Networks (GNNs) captu re local graph structure and feature information in a trainable fashion to derive powerful node representations. They have shown promising success on multiple graph-based machine learning tasks [10, 25, 35] and are widely adopted by various web applications including social network [24, 35], recommendation [1, 6, 38], fraud detection [16, 21, 30], etc. Various strategies have been proposed to improve the expressiveness of GNNs [8, 26, 29, 34]. One natural candidate for improving the performance of a GNN is to increase its parameter size by either expanding the hidden dimension or the number of GNN layers. However, this can result in a large computational cost with only a modest performance gain. As a representative example, Figure 1 displays the performance of GraphSage [ 8] under diÔ¨Äerent settings on the ogbn-products dataset and the corresponding model parameter sizes. From these results, it can be seen that either increasing t he hidden dimension or increasing the number of GNN layers increases the model p arameter size exponentially, but brings little performance improvement in tersm of test accuracy. For example, in order to improve the accuracy of a 3-layer GraphSage model by 1%, we need to add 2.3√ó more parameters (by increasing the hidden dimension from 256 to 512). Furthermore, with a larger hidden dimension a model is more likely to overÔ¨Åt the training data. On the other head, stacking multiple GNN layers may oversmooth the features of nodes [3, 23]. As shown in Figure 1a, GrageSage reaches its peak performance with only 3 GNN layers and a hidden dimension of 512. Inspired by the Network-in-Network architecture [20], we present Network-in-Graph Neural-Network (NGNN ), a model agnostic methodology that allows arbitrary GNN models to increase their model capacity by making the model deeper. However, instead of adding more GNN layers, NGNN deepens a GNN model by inserting nonlinear feedforward neural network layer(s) within each GNN layer. This leads to a much smaller memory footprint t han recent alternative deep GNN architectures [18, 19] and can be applied to all kinds of GNN models with various training methods including fullgraph training, neighbor sampling [8], cluster-based sampling [5] and local subgraph sampling [40]. Thus, it can easily scale to large graphs. Moreover, analysis of NGNN in conjunction with GraphSage on perturbed ogbn-products showed that NGNN is a cheap yet eÔ¨Äective w ay to keep the model stable against either node feature or graph structure perturbations. In this work, we applied NGNN to GCN [14], GraphSage [8], GAT [29] and AGDN [28] and SEAL [40]. We also combine the proposed technique with diÔ¨Äerent mini-batch training methods including neighbor sampling, graph clustering and local subgraph sampling. We conduct ed comp rehensive experiments on several large-scale graph datasets for both node classiÔ¨Åcation and link prediction leading to the following conclusions (which hold as of the time of this submission): ‚Ä¢ NGNN improves the performance of GraphSage and GAT and their variants on node classiÔ¨Åcation datasets includ ing ogbn-products, ogbn-arxiv, ogbn-proteins and reddit. It improves the test accuracy by 1.6% on the ogbn-products datasets for GraphSage. Furthermore, NGNN with AGDN+BoT+selfKD+C&S [13] achieves the forth place on the ogbn-arxiv leaderboardand NGNN with GAT+BoT [3 2] achieves second place on the ogbn-proteins leaderboard with many fewer model parameters. ‚Ä¢ NGNN improves the performance of SEAL, GCN and Graph- Sage and their variants on link prediction datasets including ogbl-collab, ogbl-ppa and ogbl-ppi. For example, it increases the test hits@100 score by 7.08% on the ogbl-ppa dataset for SEAL, which outperforms all the state-of-the-art approaches on the ogbl-ppa leaderboardby a substantial margin. Furthermore, NGNN achieves an improvement of the test hits@20 score by 6.22% on the ogbl -ppi dataset for GraphSage+EdgeAttr, which also takes the Ô¨Årst place on the ogbl-ppi learderboard. ‚Ä¢ NGNN improves the performance of GraphSage and GAT under diÔ¨Äerent training methods including full-graph training, neighbor sampling, graph clustering, and subgraph sampling. ‚Ä¢ NGNN is a more eÔ¨Äective way o f improving the model performance t han expanding the hidden dimension. It takes less parameter size and less training time to get better performance than simply doubling the hidden dimension. In summar y, we present NGNN , a method that deepens a GNN model without adding extra GNN message-passing layers. We show that NGNN signiÔ¨Åcantly improves the performance of vanilla GNNs on various datasets for both node classiÔ¨Åcation and link prediction. We demonstrate the generality of NGNN by applying them to various GNN architectures. Deep models have been widely studied in various domains including computer vision [9, 27], natural language processing [2], and speech recognition [43]. VGG [27] investigates the eÔ¨Äect of the convolutional neural network depth on its accuracy in the largescale image recognition setting. It demonstrates t he depth of representations is essential to the model p erformance. But when the depth grows, the accuracy will not always grow. Resnet [9] eases the diÔ¨Éculties on training the deep model by introducing residual connections between input and output layers. DenseNet [12] takes this idea a step further by adding connections across layers. GPT3 [2] presents an autoregressive language model with 96 layers that achieves SOTA performance on various NLP tasks. Even so, while deep neural networks have achieved great success in various domains, t he use of d eep models in graph representation leaning is less well-established. Most recent works [17‚Äì19] attempt to train deep GNN models with a large number of parameters and achieved SOTA performance. For example, De epGCN [18] adapts the concept of residual connections, d ense connections, and dilat ed convolutions [37] to training very deep GCNs. H owever DeepGCN and its successor DeeperGCN [19] have large memory footprints during model training which can be subject to current hardware limitations. RevGNN [17] explored grouped reversible graph connections to train a deep GNN and has a much smaller memory footprint. However, RevGNN can only work with ful l-graph training and cluster-based mini-batch training, which makes it diÔ¨Écu lt t o work with other methods designed for large scale graphs such as neighbor sampling [8] and layer-wise sampling [4]. In contrast, NGNN deepens a GNN model by inserting non-linear feedforward layer(s) within each GNN layer. It can be applied to all kinds of GNN models with various training methods including full-graph training, neighbor sampling [8], layer-wise sampling [ 4] and cluster-based sampling [5]. Xu et al. [34] used Multilayer Perceptrons (MLPs) to learn the injective functions of the Graph Isomophism Network (GIN) model and showed its eÔ¨Äectiveness on graph classiÔ¨Åcation tasks. But they did not show whether adding an M LP within GNN layers works eÔ¨Äectively across wide-ranging node classiÔ¨Åcation and link prediction tasks. Additionally, You et al . [36] mentioned that adding (a) Gaussian noise is concatenated with node features. Figure 2: The test a ccuracy (%) of GraphSage, GraphSage with the hidden dimension of 512 (GraphSage-512), 4layer GraphSage (GraphSage-4layer), GraphSage with one additional non-linear layer in each GNN layer (NG NNGraphSage-1) and GraphSage with two addit ional nonlinear layers in each GNN layer (NGNN-GraphSage -2) on ogbn-product with randomly added Gaussian noise in node features. B y default, a GraphSage model has three GNN layers with a hidden dimension of 256. MLPs within GNN layer co uld beneÔ¨Åt the performance. However, they did not systematically analyze the reason for the performance improvement introduced b y extra non-linear layers, nor evaluate with numerous SOTA GNN architectures on large-scale graph datasets for both node cl assiÔ¨Åcation and link prediction tasks. A graph is composed of nodes and edges G = (V, E ) , where V = (ùë£, . . . , ùë£) is the set of ùëÅ nodes and E ‚äÜ V √ó V is the set of edges. Furthermore, A ‚àà {0, 1}denotes the co rresponding adjacency matrix of G. Let X ‚àà Rbe the node feature space such that X = (ùë•, . . . , ùë•)where ùë•represents the node feature of ùë£. Formally, the (ùëô + 1)-th layer of a GNN is deÔ¨Åned as: Figure 3: The model performance of GraphSage, GraphSage with one additional non-linear layer in each GNN layer (NGNN-GraphSage-1) and GraphSage wit h two additional non-linear layers in each GNN layer (NGNN-GraphSage-2) on ogbn-product with rand omly added noise edges. where the function ùëì(G, ‚Ñé) is determined by learnable parameters ùë§ and ùúé(¬∑) is an optional activation function. Additionally, ‚Ñérepresents the emb eddings of the nodes in the ùëô-th layer, and ‚Ñé= ùëã when ùëô = 1 . With an ùêø-layer GNN, the node embeddings in the last layer ‚Ñéare used by downstream tasks like node classiÔ¨Åcation and link prediction. Inspired by the network-in-network architecture [20], we deepen a GNN model by inserting non-linear feedforward neural network layer(s) within each GNN layer. The (ùëô + 1)-th layer in NGNN is thus constructed as: The calculation of ùëîis deÔ¨Åned layer-wise as: where ùë§, . . . , ùë§are learnable weight matrices, ùúé (¬∑) is an activation function, and ùëò is the number of in-GNN non-linear feedforward neural network layers. T he Ô¨Årst in-GNN layer takes the output of ùëìas input and performs the non-linear transformation. In this section, we demonstrate that a NGNN architecture can better handle bot h noisy node features and noisy graph structures relative to its vanilla GNN counterpart. Remark 1. GNNs work well when th e in put features consist of distinguishable true features and noise. But when the true features are mixed with noise, GNNs can struggle to Ô¨Ålter out the noise, especially as the noise level increases. GNNs follow a neural message passing scheme [7] to aggregate information from neighbors of a target node. In doing so, they can perform noise Ô¨Åltering and learn from the resulting signal when the noise is in some way distinguishable from true features, such as when the latter are mostly low-frequency [22]. However, when the noise level becomes too large and is mixed with true features, it cannot easily be reduced by GNNs [11]. Figure 2 demonstrates this scenario. Here we randomly added Gaussian noise N = ùëÅ (0, ùúé) to node features in ogbn-products data, where ùúé is the standard deviation ranging from 0.1 to 5.0. We adopt two diÔ¨Äerent methods for adding noise: 1) X = [X|N ] as shown in Figure 2a, where | is a concatenation operation, and 2) X = [X + N ] as shown in Figure 2 b. We trained GraphSage models (using the DGL [31] implementation) under Ô¨Åve diÔ¨Äerent settings: 1) baseline GraphSage with default 3-layer structure and hidden dimension of 256, denoted as GraphSage; 2) GraphSage with the hidden dimension increased to 512, denoted as GraphSage-512; 3) 4-layer GraphSage, denoted as GraphSage-4layer; 4) GraphSage with o ne additional non-linear layer in each GNN layer, denoted as NGNN-GraphSage1 and 5) GraphSage wit h two additional non-linear layers in each GNN layer, denoted as NGNN-GraphSage-2. In all cases we used ReLU as the activation function. As show n in Figure 2a, Grap hSage performs well when the noise is highly distinguishable from the true features. But the performance starts dropping when the noise is mixed with the true features and decays faster when ùúé be comes larger than 1.0 as shown in Figure 2b. The same scenario happens with the gfNN model [22], which is formed by transforming input node features via muliplications of the adjacency matrix followed by application of a single MLP block. This relatively simple model was shown to be more noise tolerant than GCN and SGC [33]; however, the p erformance of gfNN turns out to be much lower t han the baseline GraphSage model in our experiments, so we do not present results here. Remark 2. NGNN is a cheap yet eÔ¨Äective way t o form a GNN architecture that is stable against node feature perturbations. One potential way to improve the denoising capability of a GNN model is to increase the parameter count via a larger hidden dimension. As shown in Figure 2b, GraphSage-512 does perform better than the baseline GraphSage. But it is also more expensive as its parameter size (675,887) is 3.27√ó larger than that of baseline GraphSage (206,895). And it is st ill not as eÔ¨Äective as either NGNN model, both of which use considerably fewer parameters (see below) and yet have more stable performance as the noise level increases. An alternative strategy for increasing the model p ar ameter count is to add more GNN layers. As shown in Figure 2b, by adding one more GNN layer, GraphSage-4layer does outperform baseline GraphSage when ùúé is smaller than 4.0. However, as a deeper GNN potentially aggregates more noisy information from its ùêø hop neighbors [39], the performance of GraphSage-4layer drops below baseline GraphSage when ùúé is 5.0. In contrast to the above two methods, NGNN-GraphSage achieves much better performance as shown in Figure 2b with fewer parameters (272,687 for NGNN-GraphSage-1 and 338,479 for NGNNGraphSage-2) than GraphSage-512 and without introducing new GNN layers. It can help maintain model performance when ùúé is smaller than 1.0 and slow the downward trend when ùúé is larger than 1.0 compared to the other three counterparts. Remark 3. NGNN with GNNs can also keep the model stable against graph structure perturbation. We now show that b y applying NGNN to a GNN, it can better deal with graph structure data perturbation. For this pu rpose, we randomly ad ded ùêæ edges to the original graph of ogbn-products, where ùêæ is the ratio of newly added noise edges to the existing edges. For example ùêæ = 0.01 means we randomly added 618.6K edges.We trained 3-layer GraphSage models with a hidden dimension of 256 under three diÔ¨Äerent settings: 1) vanilla GraphSage, denoted as GraphSage; 2) GraphSage with one additional non-linear layer in each GNN layer, denoted as NGNN -GraphSage1 and 3) GraphSage wit h two additional non-linear layers in each GNN layer, denoted as NGNN -GraphSage-2. Figure 3 shows the results. It can be seen that, NGNN can help preserve the model performance when ùêæ is smaller than 0.01 and ease the trend of performance downgrade after ùêæ is larger than 0.01 comparing to vanilla Grap hSage. We next provide experimental evidence to show that NGNN works will with various GNN architectures for both node classiÔ¨Åcation and link prediction tasks in Sections 4.2 and 4.3. We also show that NGNN works with diÔ¨Äerent training methods in Section 4.4. Finally, we discuss the impact of diÔ¨Äerent NGNN settings in Sections 4.5 and 4.6. Datasets. We conducted experiments on seven datasets, including ogbn-products, ogbn-arxiv and ogbn-proteins from ogbn [10] and redditfor node classiÔ¨Åcation, and ogbl-collab, o gbl -ppa and ogbl-citaiton2 from ogbl [10] for link prediction. The detailed statistics are summarized in Table 1. We evaluated the eÔ¨Äectiveness of NGNN by applying it to various GNN models including GCN [14], Graphsage [8], Graph Attention Network (GAT) [29], Adaptive Graph DiÔ¨Äusion Networks (AGDN) [28], and SEAL [4 2] and their variants. Table 2 presents Table 4: Performance (as measured by classiÔ¨Åcation accuracy and ROC -AUC for ogbn-arxiv and ogbnproteins, respectively) of NGNN combined with bag of tricks on ogbn-arxiv and ogbn-proteins. ogbn-arxivAGDN+BoT ogbn-proteinsGAT+BoT all the baseline models. We directly followed the implementation and conÔ¨Åguration of each baseline model from the OGB [10] leaderboard and added non-linear layer(s) into each GNN layer for NGNN . Table 11 presents the detail conÔ¨Åguration of each model. All models were trained on a single V100 GPU with 32GB memory. We report average performance over 10 runs for all models except SEAL related models. As training SEAL models is very expensive, we took 5 runs instead. Firstly, we analyzed how NGNN improves the performance of GNN models on node classiÔ¨Åcation tasks. Table 3 presents the overall results. It can be seen that NGNN-based models outperform their baseline models in most of the cases. Notably, NGNN tends to performs well w ith GraphSage. It improves the test accuracy of GraphSage on ogbn-products and ogbn-arxiv by 1.61 and 0.62 respectively. It also improves the ROC-AUC score of GraphSage on ogbnproteins by 0 .63 . But as the baseline performance of reddit dataset is quite high, not surprisingly, the overall improvement of NGNN is not signiÔ¨Åcant. We further analysis the p erformance of NGNN combined with bag of tricks [32] on ogbn-arxiv and ogbn-proteins in Table 4. It can be seen that NGNN-based mo dels o utperform their vanilla counterparts. NGNN with AGDN+BoT+self-KD+C&S even achieves the Ô¨Årst place over all the methods with no extension to the input data on the ogbn-arxiv leaderboard as of the time of this submission (The forth place on the entire ogbn-arxiv leaderboard). NGNN with GAT+BoT also achieves the second place on the ogbn-proteins leaderboard with 5.83 times fewer parameters compared with the current leading method RevGNN-Wide. Secondly, we analyzed how NGNN improves the performance of GNN models on link prediction tasks. Table 5 presents the results on the ogbl-collab, ogbl-ppa and ogbl -ppi datasets. As shown in the tables, the performance improvement of NGNN over SEAL models is signiÔ¨Åcant. NGNN improves the hit@20, hit@50 and hit@100 of SEAL-DGCNN by 4.72%, 4.67% and 7.08% respectively on ogblppa. NGNN with SEAL-DGCNN achieves the Ô¨Årst place on the ogbn-ppa leaderbo ard with an improvement of hit@100 by 5.82% over the current leading method MLP+CN&RA&AA. Furthermore, NGNN with GraphSage+EdgeAttr achieves the Ô¨Årst place on the ogbl-ddi leaderboard with an improvement of hit@20 by 5.47% over the cur rent leading method vanilla GraphSage+EdgeAttr. As GraphSage+EdgeAttr only provided the p erformance on ogbl-ppi, we do not compare its p erformance on other datasets. NGNN also works with GCN and GraphSage on link prediction tasks. As shown in the tables, It improves the performance of GCN and GraphSage in all cases. In particular, it improves the hit@20, hit@50 and hit@100 of GCN by 1.64%, 4.21% and 6.57% respectively on ogbl-ppa. Table 6: Tes t accuracy (%) of GraphSage and GAT with and without NGNN trained with diÔ¨Äerent tra ining methods on ogbn-products. Table 7: Test accuracy (%) of GraphSage and GAT with diÔ¨Äerent number of non-linear layers added into GNN layers on ogbn-products. Table 8: The paramet er size of each model in Table 7. NGNN -1layer 514,15 2 1,559,656 5,248,104 NGNN -2layer 518,24 8 1,576,040 5,313,640 NGNN -4layer 526,44 0 1,608,808 5,444,712 Table 9: The single training epoch t ime of GraphSage with diÔ¨Äerent number of non-linear layers added into GNN layers on ogbn-products. NGNN -1layer 19.57¬±1.04 20.98¬±0.47 29.07¬±0.69 NGNN -2layer 19.25¬±0.87 21.36¬±0.48 30.01¬±0.13 NGNN -4layer 19.79¬±0.72 24.41¬±0.38 32.33¬±0.19 Finally, we presents the eÔ¨Äectiveness of using NGNN with diÔ¨Äerent training methods including full-graph training, neighbor sampling and cluster-based sampling. Table 6 presents the results. It can be seen that NGNN improves the performance of GraphSage and GAT with all kinds of training methods on ogbn-products. It is worth mentioning that NGNN also works with local subgraph sampling method proposed by SEAL [40] as shown in Section 4.3. We studied the eÔ¨Äectiveness of adding multiple non-linear layers to GNN layers on ogbn-products using GraphSage and GAT. Table 7 presents the results. The baseline model is a three-layer GNN model. We applie d 1, 2 or 4 non-linear layers to each hidden GNN layer denoted as NGNN-1layer, NGNN-2layer and NGNN-4layer respectively. The GAT models use eight attention heads and all heads share the same NGNN layer(s). Table 7 presents the result. As shown in the table, NGNN-2layer always performed best with diÔ¨Äerent hidden sizes in most of the cases. This reveals that adding non-linear layers can be eÔ¨Äective, but the eÔ¨Äect may vanish signiÔ¨Åcantly when we continuously add more layers. The reason is straightforward, given that adding more non-linear layers can eventually cause overÔ¨Åtting. We also observe that deeper models can achieve better performance with many fewer trainable parameters than w ider models. Table 8 presents the model parameter size of each model. As shown in the table, the parameter size of GraphSage with NGNN-2layer Table 10: Test accu racy (%) of GraphSage and GAT when applying NGNN on diÔ¨Äerent GNN layers on ogbn-product. and a hidden size of 256 is 338,479 which is 2√ó smaller than the parameter size of vanilla GraphSage with a hidden-size of 512, i.e., 675,887. And its performance is much better than vanilla GraphSage with a hidden size of 512. Furthermore, we also obser ve that adding NGNNlayers only slightly increase the model training time. Table 9 presents the single tr aining epoch time of Gr ap hSage under diÔ¨Äerent conÔ¨Ågurations. As shown in the table, the epoch time of GraphSage with NGNN-2layer and a hidden size of 256 is only 3.1% longer than that of vanilla GraphSage with the same hidden size. However the corresponding parameter size is 1 .63 √ó larger. Finally, we studied the eÔ¨Äectiveness of app lying NGNN to only the input GNN layer (NGNN-input), only the hidden GNN layers (NGNN-hidden), only the output GNN layer (NGNN-output) and all the GNN layers on the ogbn-products dataset using GraphSage and GAT. The baseline model is a three-layer GNN model. The hidden dimension size is 25 6 and 128 for GraphSage and GAT respect ively. Table 10 presents the results. As t he table shows, o nly applying NGNN to the output GNN layer brings littl e or no beneÔ¨Åt. While applying NGNN to hidden and input GNN layers can improve the model performance, especially applying NGNN to hidden layers. It demonstrates that the beneÔ¨Åt of NGNN mainly comes from ad ding additional non-linear layers into the input and hidden GNN layers. We present NGNN, a model agnostic methodology that allows arbitrary GNN models to increase t heir model capacity by inserting non-linear feedforward neural network layer(s) inside GNN layers. Moreover, unlike existing deep GNN approaches, NGNN does not have large memory overhead and can work with various tr aining methods including neighbor sampling, graph clustering and local subgraph sampling. Empiricall y, we demonstrate that NGNN can work with various GNN models on both node classiÔ¨Åcation and link prediction tasks and achieve state-of-the-art results. Future work includes evaluating NGNN on more GNN models and investigating whether NGNN can work on broader graph-related prediction tasks. We also plan to explore methodologies to make a single GNN layer deeper in the future. Table 11: Model settings of NGNN models. The column of NGNN position presents where we put the non-linear layers. hidde n-only means only applying NGNN to the hidden GNN layers, input-only me ans only applying NGNN to the input layer, all-layer means applying NGNN to all the G NN layers.. The column of NGNN set ting presents how we organize each NGNN layer. For example, 1-relu+1-sigmoid mea ns NGNN contains one feedforward neural network with ReLU as its activation function followed by another feedforward neural network with Sigmoid as its activation function and 2-relu means NGNN contains two feedforward neural network layers with ReLU as the activation function of each layer.