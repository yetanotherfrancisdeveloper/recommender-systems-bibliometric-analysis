User interest exploration is an important and challenging topic in recommender systems, which alleviates the closed-loop eî€ects between recommendation models and user-item interactions. Contextual bandit (CB) algorithms strive to make a good trade-oî€ between exploration and exploitation so that usersâ€™ potential interests have chances to expose. However, classical CB algorithms can only be applied to a small, sampled item set (usually hundreds), which forces the typical applications in recommender systems limited to candidate post-ranking, homepage top item ranking, ad creative selection, or online model selection (A/B test). In this paper, we introduce two simple but eî€ective hierarchical CB algorithms to make a classical CB model (such as LinUCB and Thompson Sampling) capable to explore usersâ€™ interest in the entire item space without limiting to a small item set. We î€›rst construct a hierarchy item tree via a bottom-up clustering algorithm to organize items in a coarse-to-î€›ne manner. Then we propose a hierarchical CB (HCB) algorithm to explore usersâ€™ interest on the hierarchy tree. HCB takes the exploration problem as a series of decisionmaking processes, where the goal is to î€›nd a path from the root to a leaf node, and the feedback will be back-propagated to all the nodes in the path. We further propose a progressive hierarchical CB (pHCB) algorithm, which progressively extends visible nodes which reach a conî€›dence level for exploration, to avoid misleading actions on upper-level nodes in the sequential decision-making process. Extensive experiments on two public recommendation datasets demonstrate the eî€ectiveness and î€exibility of our methods. â€¢ Information systems â†’ Recommender systems;â€¢ Computing methodologies â†’ Sequential decision making. ACM Reference Format: Yu Song, Jianxun Lian, Shuai Sun, Hong Huang, Yu Li, Hai Jin, and Xing Xie. 2018. Show Me the Whole World: Towards Entire Item Space Exploration for Interactive Personalized Recommendations. In Woodstock â€™18: ACM Symposium on Neural Gaze Detection, June 03â€“05, 2018, Woodstock, NY . ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/1122445.1122456 Recommender systems help users to easily î€›nd their favorite items from massive candidates. Typically, recommender models, such as collaborative î€›ltering [15] and DeepFM [9], exploit usersâ€™ historical behaviors to learn usersâ€™ preference for future recommendations. Recommender systems with only exploitation models usually suî€er from closed-loop eî€ects [11]: users mostly only interact with the items recommended by the system; the system further consolidates usersâ€™ proî€›les with their interacted items recommended by the deployed model. Therefore, as time goes on, the system will be biased to a small, exposed set of interests for each user and keep recommending a limited range of items to a same user. Contextual multi-armed bandit algorithms, such as LinUCB [16], are classical methods that leverage side information to provide a good trade-oî€ between exploration and exploitation, so that the closed-loop eî€ect can be alleviated. Items are treated as arms and the recommender model is treated as an agent. Basically, at each round, the agent chooses one arm which has the biggest potential fromğ¾arm candidates, then receives a corresponding reward based on user-item interaction. The goal is to maximize the cumulative reward overğ‘‡rounds. However, these algorithms hold a premise thatğ¾is small, so enumerating all arm candidatesâ€™ scores and pick up the best one is feasible. The premise is true for a few scenarios where the candidates are naturally small, for example, homepage breaking news ranking, ads creative ranking and online model selection. In the scenario of general recommender systems, to fully explore usersâ€™ potential interests and truly alleviate the closed-loop eî€ect, the arms candidates are the entire item repository, which usually contains millions or even billions of items. Classical bandit methods become infeasible due to the high computation cost of enumerating every one of the arms. To address the challenge, we î€›rst propose a generic hierarchical contextual bandit (HCB) algorithm to eî€œciently explore the interests of users for large-scale recommendation scenarios. Tree structure are widely employed to partition the search space to reduce the computational cost [12,26,40,41]. HCB uses a tree structure as the index for coarse-to-î€›ne retrieval. For example, in e-commerce scenario, â€œApparel > Clothing > Womenâ€™s Clothing > Dressesâ€ is a path from general apparel to womenâ€™s dresses. Instead of using the category taxonomy of items, we utilize a bottom-up clustering method on item embeddings to organize items as a hierarchy tree, on which each node contains a group of semantically similar items. As a result, the number of items associated to each node on the hierarchy tree can be balanced, and usersâ€™ collaborative behaviors (such as co-click relations) can be encoded to form the hierarchy tree. HCB leverages the hierarchical information and turns the interest exploration problem into a series of decision-making problems. Starting from the root of the tree, on each non-leaf node, HCB performs a bandit algorithm among the children arms to choose a child node until a leaf node is reached. Afterward, another bandit algorithm is responsible for recommending an item from the leaf node to the user and collect her feedback as reward. The reward will be back-propagated along the path to adjust the estimation of usersâ€™ interest towards the hierarchy tree. The process of HCB is like a depth-î€›rst search (DFS) idea. However, selecting a path in this DFS manner may cause new uncertainties, especially for a deep tree. First, if the selection of the parent node is misleading, all the subsequent choices will be impacted, which we call the error propagation. Second, since user interests are usually diverse, it is possible that the user is interested in many child nodes located in diî€erent parts of the tree. Therefore, we further propose a progressive HCB (pHCB) algorithm to reduce uncertainties and enhance the capacity of recommendation. Like the process of breadth-î€›rst search (BFS), pHCB explores items in an adaptive top-down manner. On the one hand, it gradually maintains a limited number of nodes as a receptive î€›eld. If one node has been explored multiple times and the userâ€™s interest on this node has been veriî€›ed, the nodeâ€™s children nodes will be included to the receptive î€›led while the current node will be removed. On the other hand, pHCB learns user interests of diî€erent aspects by performing a bandit algorithm with visible nodes in the receptive î€›eld as arms. Consequently, the pHCB avoids greedily selecting only one node at each level to improve the HCB. To summarize, we make the following contributions: â€¢We highlight the importance of exploring usersâ€™ interests in the entire item space to truly alleviate the closed-loop eî€ect in personalized recommender systems. To the best of our knowledge, it is the î€›rst attempt to implement CB models on millions of items. â€¢Two simple yet eî€ective algorithms, i.e. HCB and pHCB, are proposed to explore potential interests of users eî€œciently through a hierarchy item tree. â€¢We conduct experiments on two large-scale recommendation datasets. Results show the superiority of HCB and pHCB over baselines, as well as the î€exibility to integrate with diî€erent exploration methods such as LinUCB, Thompson Sampling andğœ–greedy. In addition, we design an experiment to verify that thanks to the exploring mechanism, both HCB and pHCB can eî€ectively alleviate the closed-loop eî€ects in recommender systems and learn better user proî€›les in the long term. It is the î€›rst work to study entire space user interest exploration. Our work is relevant to two lines of research, and we will review them separately. Contextual bandit algorithms aims to seek a balance between exploration and exploitation, which have been used in several applications, such as recommender systems [21], dynamic pricing [22], quantitative î€›nance [30] and so on. [4] reviews the existing practical applications of contextual bandit algorithms. By assuming the payoî€ model is linear, LinUCB [16] and Thompson Sampling [2] and two representative methods for solving contextual bandit problems. Beyond them, a variety of algorithms have been proposed to optimize the performance or learning speed. For example, ConUCB [39] introduces conversations between the agent and users to ask whether the user is interested in a certain topic occasionally. HATCH [38] considers the resource consumption of exploration and proposes a strategy to conduct bandit exploration with budget limitation. SMAB [6] considers two aspects, one is to maximize the cumulative rewards and the other is to decide how many arms to be pulled so as to reduce the exploration cost. GRC [36] develops a graph regularized cross model to leverage the non-linearity of neural networks for better estimating the rewards. Diî€erent from them, our work commits to eî€œciently explore user interests in the entire space, rather than from a small subset of items. In the past few years, cluster-of-bandit algorithms have attracted the attention of some scholars. Generally, cluster-of-bandit algorithms aim to model the dependency since the items or users are always related to each other. As a result, cluster-of-bandit algorithms achieve better cumulative rewards than traditional contextual bandit algorithms due to knowledge sharing. For example, CLUB [8], DYNUCB [23], CAB [7] and COFIBA [17] assign users with similar interests into a same subset to make decisions together, thus it make contributions to accelerate the learning speed. Diî€erent from them, this paper focus on modeling the item dependency. ICTRTS and ICTRUCB [34] explicitly model the item dependencies via clustering ğ¶â„ (ğ‘›) The set of child nodes of node ğ‘› ğ‘ƒğ‘ (ğ‘›) The parent node of node ğ‘› ğ‘¿The (static) embedding features of arm ğ‘, ğ‘¿âˆˆ R ğ‘–(ğ‘¡) The selected arm by policy ğœ‹ at the ğ‘¡-th round ğ‘Ÿ(ğ‘¡) Reward of policy ğœ‹ at the ğ‘¡-th round ğœ½, ğœ½Learnable parameter of userğ‘¢. A superscript indicates of arms, but they are only designed for context-free bandits. Similarly, [25] uses a taxonomy structure to exploit arm dependencies with context-free bandits. Considering that context-free bandits cannot utilize the abundant side information for making decisions, their exploration ability has yet to be improved. HMAB [33] leverages a tree-structured hierarchy constructed by domain experts to design a hierarchical multi-armed bandit algorithm for online IT ticket automation recommendation. However, domain knowledge is hard to collect and HMAB can not be applied to large-scale recommender systems because it needs to traverse all the paths in the tree. Moreover, HMAB aims to learn latent parameters for the nodes in the hierarchy tree, which is totally diî€erent from our goal of exploring usersâ€™ latent interests. Distributed bandit algorithms, such as DCCB [14] and DistCLUB [20], aim to speed up the computation by distributing the workloads in parallel. However, these methods do not address the issue of searching from tremendous items, the computational cost is still too expensive for responding usersâ€™ requests in an online manner (for example, how to response 100 usersâ€™ concurrent requests within 10 milliseconds in a scenario involving one million items). In summary, compared with existing cluster-of-bandit algorithms, our HCB and pHCB algorithms leverage a bottom-up clustering method to build a hierarchical tree of items, then explore usersâ€™ potential interests in the entire space of items based on the item hierarchy. We start by introducing the multi-armed bandit algorithms and the motivations of this paper. For better readability, we summarize most of the notations used throughout the paper in Table 1. The recommender system is regarded as an agent, where there are ğ‘€users andğ‘items. At each roundğ‘¡ =1,2, Â·Â· Â· ,ğ‘‡of interactions, given a userğ‘¢, the agent recommends an itemğ‘–(ğ‘¡)to the user according a policyğœ‹. Then the agent receives a feedbackğ‘Ÿ(ğ‘¡)from the user, for example, if the user clicks on the itemğ‘–(ğ‘¡),ğ‘Ÿ(ğ‘¡)is 1 and otherwise it is 0. The optimal policy is denoted byğœ‹. The goal is to learn a good policyğœ‹, so that the cumulative regret over ğ‘‡ rounds, which is deî€›ned as below, is minimized: In practice, due to the absence of the optimal policyğœ‹, we maximizeÃ the cumulative rewardğ‘¬ [ğ‘Ÿ(ğ‘¡)]instead, because maximizing cumulative reward equals to minimizing cumulative regret [17,33, 37]. At the core of bandit algorithms is to î€›nd an optimal trade-oî€ between exploitation (to recommend fully based on user proî€›les learned from user interaction history) and exploration (î€›nd out the new items which user may potentially love better), so that users diverse new interests have a certain chance to expose, meanwhile the system wonâ€™t waste too many resources on items that users are not interested in. Letâ€™s consider the (user-centric) LinUCB [16] algorithm. Each item is regarded as an arm. Atğ‘¡-th round, when receiving a user visit request, the agent selects an arm ğ‘(ğ‘¡) by: The policyğœ‹of LinUCB is a linear function between the feature vectorğ’™and user hidden parameterğœƒ, where the estimated reward isğ‘…(ğ‘¡) = ğœ½ğ’™+ğœ‚,ğœ‚is a Gaussian random variable representing environmental noise, whose mean is zero and variance isğœâ‰¤1, The upper boundğ¶(ğ‘¡)measures the uncertainty of the reward estimation. The key point lies in how to determine the parameter ğœ½and the upper bound ğ¶(ğ‘¡). With LinUCB, we have: whereğ‘«âˆˆ Ris the matrix of interacted armsâ€™ features up to timeğ‘¡,ğ›¼is a hyper-parameter to control the probability that the boundğ¶(ğ‘¡)holds,ğ’“âˆˆ Ris the user response vector up to timeğ‘¡. However, as revealed in Eq.(2), LinUCB needs to enumerate and calculate the score for every arm and then select the best one. In a modern recommender system, the number of items is usually very large (millions or even billions), which makes it impossible to calculate scores for all item. Thus, in the research community, a typical setting for existing literature is to randomly sample a small numberğ¾(such as 50) arms from the entireğ‘arms at time ğ‘¡, and perform LinUCB on this small arm setA; in industry, the bandit algorithms can only be applied to scenarios whose candidate pool is small, such as post-ranking stage of a recommender system, homepage most popular item ranking, ad creative ranking, etc. We argue that in order to fully explore usersâ€™ potential interest, it is better to place the bandit module in the item retrieval stage (aka the recall stage) of a recommender system, where the candidate pool is the entire item set. Otherwise, in the post-ranking stage of a recommender system, the candidates are actually proposed by recommendation models and are strongly related to usersâ€™ past behaviors. Thus, it is less meaningful to explore usersâ€™ interest in the latter stages of a recommender system. To fully alleviate the closed-loop eî€ect, in this paper, we advocate to explore usersâ€™ interest in the entire space of item repository. However, to the best of our knowledge, there is no work studying how to make the bandit algorithm like LinUCB î€›t for a large candidate set. To address the challenge, we propose to use a tree structure to partition the entire item space into multiple sub-spaces and build the hierarchical dependencies among items, to accelerate the exploration. Formally, we deî€›ne the Framework 1: Framework1.Tree-based ExplorationThe entire item set can be organized as a hierarchical tree structureH, where nodes are linked to a subset of items that share some common topics or user interests, and nodes moving from top to bottom reî€ects the topics/interests partition being coarse-to-î€›ne. During the tree-based exploration, we will î€›rst select a node according to some mechanism, then select an item from the candidates linked to this node. The user fee dback on the selected item will not only update the item-wise user preference estimation, but also update the node-wise user preference estimation along the hierarchical path. The tree structure plays a signiî€›cant role in designing hierarchical bandit algorithms. Item category taxonomy can serve as the hierarchy tree. However, due to the imbalanced number of items under diî€erent category and lack of leveraging of usersâ€™ collective behaviors, simply using the category taxonomy may lead to suboptimal performance, which is veriî€›ed in Section 5. In view of this, we î€›rst learn item embeddings based on item content and user co-click behaviors, then design a bottom-up clustering method based on K-Means clustering algorithm [18] to form a hierarchy tree for modeling dependencies among items. Speciî€›cally, to construct a tree structure withğ¿levels, at î€›rst,ğ‘ items are clustered intoğ‘˜diî€erent subsets based on the similarity of item embeddings. We treat each subset as a new node on the tree, with an embedding vector being the average of all item embeddings belonging to this node. Afterward, theseğ‘˜nodes will be further clustered intoğ‘˜diî€erent subsets using K-Means and each subset will be treated as a new node on the tree, forming a parent-children relation. This step will be repeated several times until the depth of the tree structure researchesğ¿. As a result, the constructed tree structure, denoted byH, contains{ğ‘˜, ğ‘˜, ğ‘˜, Â·Â· Â· , ğ‘˜}nodes at each level, whereğ‘˜=1 because only a root node appears at the î€›rst level. Intuitively, items within the same node are more similar to each other, thus the clustering results reî€ect the dependencies among items. InH, only the root node does not have parent node, and leaf nodes have no children nodes. In this section, we introduce the proposed hierarchical contextual bandit (HCB) algorithm, which empowers a base bandit model to explore over the entire space of item repository. Our algorithm can be generalized to diî€erent bandit models, without loss of generality, we take LinUCB as the base model to explain the algorithm for clarity. There are two types of arms: nodes on the hierarchy treeH and items mounted to the leaf nodes. Each node onHrepresents a certain group of items. The feature vector of a leaf node is the average pooling of items mounted to it, and a non-leaf nodeâ€™s feature vector is the average pooling of its children nodesâ€™ feature vectors. The HCB algorithm makes decisions sequentially, starting from the root node to a leaf node. At any non-leaf nodeğ‘›(ğ‘¡)atğ‘™-th level, the policyğœ‹selects one of the child nodes fromğ¶â„(ğ‘›(ğ‘¡)) by assuming the expected reward of an arm is linear in its feature vector, which isğœ½ğ‘¿, andğœ½is the latent parameters of a given userğ‘¢towards the nodes at levelğ‘™,ğ‘«âˆˆRis the matrix comprised of interacted items atğ‘™-th level, each row ofğ‘« represents an itemâ€™s feature vector. Applying ridge regression to the training samples to estimate the coeî€œcients, we have: Whereğ‘°is an identity matrix andğ’“is the vector of historical rewards at node levelğ‘™. LinUCB also considers conî€›dence interval to better estimate the arm payoî€. Letğ‘¨= ğ‘«ğ‘«+ğ‘°. According to [32], with probability 1 âˆ’ ğ›¿, the upper bound is: for anyğ›¿ >0 andğ›¼ =1+ln(2/ğ›¿)/2. In this way, the LinUCB algorithm tends to select an arm with: If policyğœ‹recommendsğ‘–(ğ‘¡)to a given user and receives the rewardğ‘Ÿ(ğ‘¡), similar as [40], then each node onğ‘ƒğ‘ğ‘¡â„(ğ‘Ÿğ‘œğ‘œğ‘¡ â†’ ğ‘›(ğ‘¡))also receives the same rewardğ‘Ÿ(ğ‘¡). Therefore, the rewards of all selected nodes can be obtained, we can update the learnable parameters{ğœ½,ğœ½,ğœ½,Â·Â·Â·,ğœ½}at each level (whereğœ½ means the parameter towards item arms, the otherğœ½means parameters towards node arms), which can be formulated as: Whereğ‘¨andğ’ƒare initialized asğ‘‘-dimensional identity matrix and zero vector respectively.ğ‘¿is the contextual embedding of the selected node at ğ‘™-th level. The pseudo-code of HCB is provided in Algorithm 1. To illustrate HCB, we oî€er a toy example shown in Figure 1. It has three layers in the hierarchy tree. The agent makes three decisions sequentially, and î€›nally select the path {ğ´,ğ¶, ğ¼, ğ‘ƒ}. Then the agent will launch another bandit selection among the items mounted to the leaf node ğ‘ƒ. The reward on the selected item will impact the parameter estimation on the hierarchy tree{ğœ½,ğœ½,ğœ½,ğœ½}, by updating the reward history ğ’“and interaction history ğ‘«. Output: The policy ğœ‹. node to user ğ‘¢ with Eq. (2); Figure 1: An illustration of HCB. The policy selects a path { A, C, I, P } from root to a certain leaf node. The HCB learns the interests of each user via a sequential decisionmaking processes and always select the item from the arriving leaf node, which may lead to two problems: (1) the decisions made in upper levels severely impact the scope of lower-level nodes. Once the policy makes a bad decision at a certain level, the rest selections are all sub-optimal. The issue is especially true when the tree hierarchy is deeper. We call this phenomenon error propagation; (2) Users may be interested in more than one child node, thus the greedy selection may fail to capture the comprehensive interests of users. Therefore, we further propose a progressive hierarchical contextual bandit (pHCB) algorithm for exploration in another manner on the tree. The main idea is that the policy continuously expands the receptive î€›eld from top to bottom according to the feedback obtained from historical exploration. We î€›rst give a deî€›nition of receptive î€›eld as follows. Definition1.Receptive î€›eldis a personalized set of nodes representing the current potential interests for each user to explore. At the î€›rst round, the receptive î€›eld only consists of the root node (or is set with prior knowledge). With the exploration process progressing, the receptive î€›eld will be expanded (and reduced) when predetermined conditions are met in an adaptive top-down manner. The nodes in the receptive î€›eld are called visible nodes. In HCB, only the leaf node is associated with a set of items. In contrast, in pHCB we allow the policy to select a non-leaf node and then recommend an item from the item set associated with the non-leaf node. Hence, we have the Deî€›nition. 2 to deî€›ne the item set of each non-leaf node. Definition2. Given a non-leaf nodeğ‘›and the set of child nodes ğ¶â„(ğ‘›), the item set of nodeğ‘›will be the union of item sets of the nodes inğ¶â„(ğ‘›), that isğ¼ (ğ‘›) = ğ¼ (ğ‘›) âˆª ğ¼ (ğ‘›) âˆª Â·Â·Â· âˆª ğ¼ (ğ‘›)and ğ¶â„(ğ‘›) = {ğ‘›, ğ‘›, Â·Â· Â· , ğ‘›}. Atğ‘¡-th round, the agent faces a userğ‘¢whose receptive î€›eld is denoted asV(ğ‘¡). pHCB algorithm treats each node inV(ğ‘¡)as an arm, and selects the arm (denoted asğ‘›(ğ‘¡)) with highest estimated reward according to Eq.(2). Then another LinUCB algorithm is used to select one itemğ‘–(ğ‘¡)from the selectedğ¼ (ğ‘›(ğ‘¡))and collect the feedback from the user. pHCB directly selects an arm from the receptive î€›eld without performing sequential decision-making processes, which avoids the aforementioned concerns of HCB. Here we oî€er an example in Figure 2 for illustrating the expanding process. Assuming at roundğ‘‡, the receptive î€›eld of the userğ‘¢consists of three nodes:ğµ,ğ¶andğ·. In the next several rounds, if nodeğ¶is selected multiple times and received several positive rewards, making it meet the conditions of expansion, its children nodesğº, ğ», ğ¼will then be added into the receptive î€›eld to replaceğ¶. As a result, at roundğ‘‡, the receptive î€›eld includes nodesğµ, ğ·, ğº, ğ», ğ¼. In this way, pHCB expands the receptive î€›eld from coarse to î€›ne and gradually discovers the interests of users. A critical mechanism of pHCB is how to expand the receptive î€›eld. Since the tree nodes are organized in diî€erent granularity, the nodes at top levels represent the coarse interests of users while the nodes at bottom levels depict speciî€›c interests of users. We want the receptive î€›eld be able to quickly converge to the leaf nodes, thus we set the expansion conditions as follows: for a non-leaf node ğ‘›at theğ‘™-th level ofH, if (1) it has been selected at leastâŒŠğ‘Â¤logğ‘™âŒ‹ times, and meanwhile (2) the average reward on this node is larger thanğ‘Â¤logğ‘™(0â‰¤ ğ‘ â‰¤1), then we expand the receptive î€›led by replacing it with its children.ğ‘andğ‘are hyper-parameters. The log ğ‘™means that the nodes at a top-level are easier to be expanded than those at a low level. One can also design more î€exible rules for expansion according to the actual application scenario. Overall, the pseudo-code of the pHCB is available in Algorithm 2. As deî€›ned in Eq.(1), the regret is deî€›ned as the diî€erence between the expected reward under hindsight knowledge and the actual reward under the algorithm. The regret bound we would like to obtain is established on a premise that the clustering structure is known to the algorithm ahead of time, which is consistent with the scenario of this paper. In this case, each cluster is viewed as an independent arm, according to [1,3,5], the regret bound is up to logarithmic termsln(ğ‘‡ ),ln(ğ‘ ), andln(1/ğ›¿). By hiding the Output: The policy ğœ‹. Figure 2: An illustration of pHCB. At round ğ‘‡, the receptive î€›eld consists of nodes B, C and D; After several trials, at round ğ‘‡, node C meets the conditions of expansion, so the receptive î€›eld changes to nodes B, D, G, H and I logarithmic factors with notationeO, the cumulative regret overğ‘‡ rounds is bounded with probability 1 âˆ’ğ›¿ as: In Eq.(9), we shall assume that||ğ‘¿|| =1 for all clusters.âˆš Then as proven by [8], one can replaceğ‘‡of each arm by a termî± formulated asâˆšğ‘‡ (+), where|ğ‘‰|is the size ofğ‘˜-th cluster andğ‘is the total number of items. As a result, the regret bound becomes: In Eq.(10), according to [8], the worst case occurs when each cluster has the same size, leading to the regret bound: Here we î€›rst discuss the regret bound for HCB. For simplicity, we assume that the number of clusters are reducedğ‘štimes that of the previous level. At the beginning, each item is treated as a cluster, i.e. the number of clusters should beğ‘. In this case, the regretî€âˆšâˆšî€‘ bound holdseO(ğœğ‘‘ +ğ‘‘) log(ğ‘ )ğ‘šğ‘‡. For pHCB, the receptive î€›eld expands in a progressive manner, assuming the î€›nal size ofî€âˆšâˆšî€‘ receptive î€›eld isğ‘Ÿ, the regret bound is at mosteO(ğœğ‘‘ +ğ‘‘)ğ‘Ÿğ‘‡. As proven in [5], if the arm set is î€›xed over time and containsğ‘ arms, the regret bound of the contextual bandits with linear payoî€âˆš functions is up toO(ğ‘‡ğ‘‘ln(ğ‘ğ‘‡ ln(ğ‘‡ )/ğ›¿))). It is signiî€›cantly larger than the regret bound of HCB and pHCB due to the higher order of logarithmic terms andğ‘‘ â‰ª ğ‘,ğ‘š â‰ª ğ‘andğ‘Ÿ â‰ª ğ‘in most instances. Therefore, our proposed algorithms can improve the exploration eî€œciency substantially by reducing the cumulative regret. Moreover, the exploration time complexity is reduced from O(ğ‘ ) to O(log(ğ‘ )) with the help of hierarchy. 5.1.1Datasets. We perform experiments on two public recommendation benchmark datasets, basic statistics are shown in Table 2. â€¢ MIND[35]: The MIND dataset is the largest public benchmark dataset for news recommendations so far, which is constructed from the click logs of Microsoft News. We use Sentence BERT [27] to train news embeddings from their contents, and adopt a GRU [24] as the user model to î€›ne-tune news embeddings from the sequence of click logs. â€¢ Taobao: The Taobao dataset is constructed from user behaviors of Taobao for E-commerce recommendations. Similar to MIND dataset, we also utilize GRU to learn item embeddings from the sequence of user behavior logs. 5.1.2Baselines. We compare the proposed algorithms against the following related and competitive bandit algorithms: â€¢ LinUCB[16] is a classical contextual bandit algorithm. It only works with item-level recommendation. â€¢ HMAB[33] organizes arms into hierarchy tree purely by domain knowledge. It utilizes category information to model the dependencies among items. Then the algorithm selects a path from root node to a leaf node, and the leaf node is an item. â€¢ ICTRUCB[34] formulates the item dependencies as the clusters on arms. Diî€erent from our methods, it does not consider the hierarchy. â€¢ ConUCB[39] utilizes key-terms to organize items into diî€erent subsets to represent dependencies among items. The algorithm occasional conversation with users and leverages conversational feedbacks on key-terms from users to accelerate the speed of bandit learning. 5.1.3Our Methods and Variants. Our goal is to propose a generic algorithm that can empower diî€erent bandit models to be more eî€ective on large-scale item set exploration. Therefore, our main experiments contain two groups: î€›rst, with LinUCB as the base bandit model, to compare our algorithms (i.e., HUCB and pHUCB) with the aforementioned baselines (because most of the listed baselines are based on LinUCB); second, with three diî€erent base bandit models, including LinUCB, Thompson Sampling (TS) [2], andğœ–-greedy, to verify whether our algorithms are eî€ective under diî€erent settings. In the second group of experiments, we also compare two variants of our models: â€¢ CB-Category: It borrows the idea from HMAB [33] by using the prior knowledge, i.e. the category taxonomy, to assign items into diî€erent subsets. Each subset will be treated as an arm, the policy î€›rst selects a subset and then recommends an item from the subset to users with a base bandit algorithm. â€¢ CB-Leaf: It is a variant of ICTRUCB [34]. Since the ICTRUCB is designed for context-free bandits, to keep a fair comparison, we also utilize K-Means clustering on item embeddings to assign items into diî€erent subsets. Here, we treat the leaf nodes ofH as the clustering results. Each leaf node will be treated as an arm, the policy î€›rst selects a leaf node and then recommends an item from the node to users with a base bandit algorithm. Here, CB- can take a value from { LinUCB, Thompson Sampling (TS), andğœ–-greedy }, our experiments are separated into three groups. For example, if the CB- model is LinUCB, then the involved variants are LinUCB-Category, LinUCB-Leaf, and our î€›nal models are HUCB and pHUCB. For all models, including our proposed ones and baselines, we set the maximum times of score-computing per round to 50 for a fair comparison. For example, in LinUCB, if the number of arm candidates is 1000, then originally we need to compute 1000 scores per round to select the best one in estimation, which exceeds our budget, so we will randomly sample 50 arms from the 1000 arms and then perform the LinUCB on the small set. For hierarchical CB methods, the budget is evenly distributed to each level, e.g., for pHCB, we have two levels of bandit, then each level will get a budget of 25. 5.1.4Evaluation. We evaluate the performance of diî€erent bandit algorithms with oî€-policy user simulator evaluation. To reduce the biases of the simulation, we utilize the IPS estimator [10,28,29], which is a standard method used for oî€-policy evaluation of bandit algorithms. IPS learns to re-weigh the training samples by the propensity score to learn an unbiased simulator. The simulator is trained on the whole data to make the best use of information. This evaluation enables us to compare the performance of candidate hypothetical policies without expensive online A/B tests. Speciî€›cally, the simulator learns the unbiased embeddings of users and items, and the reward of a userğ‘¢towards an itemğ‘–is derived from the inner product of their embeddings. 5.1.5Reproducibility. For MIND dataset, each item is represented by a 64-dimensional embedding vector. The dataset has been split for training/validation/test, only the click logs of the training set is used for learning item embeddings and building tree structures. The users without history logs or impression logs are removed. The click is treated as positive feedback and non-click is treated as negative feedback. The hierarchy tree structure is set to { 1, 100, 10000 }, which means there is 10000 leaf nodes, and only one layer of non-leaf nodes. For Taobao dataset, each item is represented by a 32-dimensional embedding vector. As the same method does in [40], we remove the users who have less than 10 behaviors or the items appearing less than 10 times. We randomly select 10000 users for testing and 1000 users for validation, and the behavior logs of the rest users are used for learning item embeddings. The click is treated as positive feedback and the negative feedback is generated via negativing sampling. The hierarchy tree structure is set to { 1, 50, 5000, 50000 }, so that number of items mounted to a leaf node is less than 100. For the LinUCB-based algorithms, all learnable parameters are initialized as all zero matrices or vectors, and the hyper-parameter ğ›¼is set as 0.5. The Gaussian prior is used to design Thompson Sampling-based algorithms for contextual bandit. Forğœ–-greedybased algorithms, theğœ–is set as 0.05. With the help of validation set, the hyper-parametersğ‘andğ‘of pHCB and its variants are set as 10 and 0.1 respectively. Follow the setting in [39], we consider a general conî€›guration in which at each round, the computational cost is limited as 50. Note here the arms can be nodes or items depending on diî€erent bandit algorithms. The learning rate for training GRU is 0.001, the hidden size is the same as the embedding size, and optimizing with Adam optimizer [13]. All algorithms are implemented with Python and PyTorch, and repeated 10 times to report the average performance. The code and processed datasets will be released after acceptance of the paper for easy reproducibility. ICTRUCB 5.60 300.83 709.05 10.98 135.99 290.33 5.2.1Comparison with Baselines. Since all baseline algorithms are on the basis of LinUCB, we also choose LinUCB as the base algorithm for HCB and pHCB to keep a fair comparison. Note that HCB and pHCB can work with diî€erent base algorithms, and we discuss their generality in sec. 5.2.2. We compare the cumulative rewards over 100/1000/2000 roundsof diî€erent algorithms Figure 3: Cumulative rewards of our algorithms and variants based on LinUCB, Thompson Sampling and ğœ–-greedy, on the MIND dataset and Taobao dataset, respectively. in Table 3, and the best results are presented in bold. As can be seen, our proposed algorithms, HUCB and pHUCB, outperform all baselines across diî€erent datasets consistently at diî€erent rounds, and pHUCB is generally better than HCB. For example, on the largest TaoBao dataset, At 100/1000/2000 rounds, the performance of pHUCB was improved by 54.3%, 268.7%, and 280% compared with LinUCB, respectively. Although HMAB, ICTRUCB, and ConUCB achieve higher cumulative rewards over LinUCB, there is still a considerable gap between these algorithms and ours. The superiority of pHUCB over HUCB further veriî€›es that by expanding the receptive î€›eld in a progressive manner, the pHCB algorithm is able to better discover the comprehensive interests of users. 5.2.2Flexibility and Variants Study. Next, we report the cumulative reward of our algorithms and their variants, in Figure 3, based on three diî€erent base bandit algorithms. From the experimental results, we have the following observations. â€¢Constructing item dependencies in the form of clusters indeed helps a lot in accelerating the exploration. This can be veriî€›ed from that both our algorihtms (HCB and pHCB) and their variants (CB-category and CB-Leaf) outperform the corresponding base bandit model. â€¢HCB and pHCB achieve the highest cumulative rewards on both two datasets in most of the cases, indicating that the proposed hierarchical algorithms are eî€ective. As we can see, the performance of baseline algorithms has a noticeable gap between our proposed algorithms. This result is reasonable since our proposed methods introduce the hierarchy knowledge to take the item dependencies into consideration, which greatly improves the eî€œciency of exploration. Although the two variants, such as CB-Category and CB-Leaf, also organize items into diî€erent clusters, they fail to model the coarse-to-î€›ne item dependencies as tree structures. Apart from that, the pHCB algorithm beats other methods, which veriî€›es that the progressive exploration can adaptively discover the diverse interests of users with a receptive î€›eld. â€¢Our algorithms have strong î€exibility. We have tested the performance with base models varying in { LinUCB, Thompson Sampling,ğœ–-greedy }, the conclusions are consistent, which proves our proposed frameworks can well generalize to various bandit algorithms. 5.2.3Parameter Sensitivity. In this subsection, we study the impacts of hyper-parameters. Since the pHCB performs best in most of the cases, we particularly study the key hyper-parameter of it, i.e.,ğ‘andğ‘, which control the expansion conditions:ğ‘determines the number of trails at one arm andğ‘indicates the threshold of average reward for expanding child nodes. To study their impacts, we take pHUCB as an example and varyğ‘from{0,5,10,15,20,25} (at round 1000) will be aî€ected. As shown in Figure 4, diî€erent hyper-parameters have a noticeable inî€uence on the cumulative reward of pHUCB algorithm over 1000 rounds. For MIND dataset, ifğ‘is too small orğ‘is too large, the cumulative rewards become worse since the former makes the expansion conditions unreliable and the latter makes the receptive î€›eld diî€œcult to expand. As for the Taobao dataset, the trend of impacts caused byğ‘is similar to the MIND dataset. Meanwhile, the model is less sensitive with parameterğ‘. Overall, from Figure 4 we learn that a suggested conî€›guration is ğ‘ = 0.1 and ğ‘ = 10. 5.2.4Alleviate Closed-Loop Eî€›ects. Typically, recommender models trained from historical logs are designed for exploitation purposes, here we called them exploitation models. Such recommendation systems often suî€er from closed-loop eî€ects [11] because they only learn usersâ€™ interests from historical logs, but they do not have the ability to explore new interests of users. In contrast, the contextual bandit algorithms are more eî€ective to break the information cocoons with exploration strategies. In order to verify that our model is able to explore the potential interests of users thus Figure 4: Eî€e ct of hyper-parameters of pHUCB. Transformer 1.3770.6950.6830.546 alleviate the closed-loop eî€ects, we design an additional experiment as follows. We select three exploitation models, i.e., Linear model, GRU, and Transformers [31] as baselines. These exploitation models are î€›rst pre-trained by the historical logs of existing users to get the deployed models. Then, we use exploitation models as well as our proposed models as "deployed models" to serve users. Speciî€›cally, in this stage, for each new user (whose logs are not used in the î€›rst pre-trained stage), we randomly sample only three clicked items as visible historical logs for generating her initial user embedding with a deployed model. Then we can recommend two hundred items to the user with a deployed model and collect the userâ€™s feedback. Note here the user embedding will be refreshed once the model receives positive feedback. This stage is performed for every deployed model respectively. The third stage is about evaluating the quality of impression logs produced by the deployed models. We utilize the collected historical logs together with all the rest historical logs of existing users (which are used in the î€›rst stage) as training samples to train an evaluating model (here we use the matrix factorization (MF) model as the evaluating model, each user and item will be mapped to an embedding vector. MF-based collaborative î€›ltering method is one of the most popular models for personalized recommendations) and evaluate the trained model on the same test samples for a fair comparison. In order to prove the advantages of our bandit algorithms in exploring user interests, we select two hundred of test users with the most diversiî€›ed interests as new users: we calculate the Gini impurity [19] of the historical items clicked by the user according to the category of items. Obviously, the larger the Gini impurity, the more diverse the interests of the user. The users of the training set are treated as existing users. We report the test log loss (LogLoss) and area under curve (AUC) score in Table 4. We can observe that both our methods, HUCB and pHUCB, achieve much higher performance than exploitation models including the Linear model, GRU, and Transformers, which demonstrates that our proposed models can eî€ectively help to alleviate the closed-loop eî€ects in recommender systems. In this paper, we propose a general hierarchical bandit framework for entire space user interest exploration. Speciî€›cally, we design two algorithms, i.e., HCB and pHCB. The HCB algorithm makes a sequence of decision-making tasks to î€›nd a path from the root to a leaf node, while the pHCB progressively expands the receptive î€›eld in a top-down manner to explore the user interests, which is more î€exible and also achieves more satisfactory results. Extensive experiments are conducted to demonstrate the eî€ectiveness of the proposed framework on two real-world datasets with three diî€erent base bandit algorithms. In the future, we plan to combine our methods with the start-of-the-art deep learning methods to estimate reward for making more reasonable decisions. Moreover, we assume the items are static in this paper by î€›xing the tree structure unchanged. It would be interesting to extend the proposed frameworks to the non-static setting, which has not been well studied yet.