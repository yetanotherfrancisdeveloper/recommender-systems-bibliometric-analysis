Recent research has achieved impressive progress in the sessionbased recommendation. However, information such as item knowledge and click time interval, which could be potentially utilized to improve the performance, remains largely unexploited. In this paper, we propose a framework calledKnowledge-enhancedSessionbased Recommendation withTemporalTransformer (KSTT) to incorporate such information when learning the item and session embeddings. Speciî€›cally, a knowledge graph, which models contexts among items within a session and their corresponding attributes, is proposed to obtain item embeddings through graph representation learning. We introduce time interval embedding to represent the time pattern between the item that needs to be predicted and historical click, and use it to replace the position embedding in the original transformer (called temporal transformer). The item embeddings in a session are passed through the temporal transformer network to get the session embedding, based on which the î€›nal recommendation is made. Extensive experiments demonstrate that our model outperforms state-of-the-art baselines on four benchmark datasets. session-based recommendation, temporal transformer, knowledge graph ACM Reference Format: Rongzhi Zhang,Yulong Gu,Xiaoyu Shen, Hui Su. 2021. Knowledgeenhanced Session-based Recommendation with Temporal Transformer. In Proceedings of ACM Conference (Conferenceâ€™17). ACM, New York, NY, USA, 5 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn Session-based recommendation is a classic problem in recommendation systems [5]. It aims to predict the next items users will choose based on their historical behaviors within the current session. Recently, many researchers have used the idea of Natural Language Processing(NLP) task(like [1â€“3,12,16,17,28]) for reference to provide new ideas for session-based recommendation [21,24,27]. For session-based recommendation, the model architecture usually consists of two key components: (1)Item Encoderthat projects each item into a low dimensional vector. In doing so, Smirnova and Vasile[18]concatenated the one-hot embedding of the item and its context representation into recurrent neural networks (RNNs). Wu et al. [23]further considered the temporal relationships of items. (2)Session Encoderwhich encapsulates all items within the current session into a compact vector. Hidasi et al. [4]proposed a session embedding model based on Gated Recurrent Units (GRUs). Li et al. [8]and Liu et al. [11]used attention mechanisms to capture usersâ€™ local and global preferences. Recently, Xu et al. [25] further improved the recommendation performance by utilizing self-attention networks. Despite the impressive success, two limitations exist in current models. (1) Item attributes involve important knowledge, and the context information of the item can be obtained by modeling them, which is beneî€›cial to item recommendations. However, existing approaches ignore them when building the item encoder. (2) The session embedding ignores the time interval between behaviors. The time interval between two adjacent clicks in the userâ€™s click history is varied. When the time interval is diî€erent, the product that the user clicks next should also have a diî€erent relationship with the previous behavior. For example, if the user is currently buying beer, the click within 3 minutes may be a beer of a diî€erent brand, but after 10 minutes, they may be browsing products such as beer glasses. Existing self-attention networks like Transformers Vaswani et al. [22]are powerful at modelling sequential data with position embedding, but they fail to encode the time interval information for predictions, resulting in unsatisfactory results in session-based recommendations. In this paper, we propose a Knowledge-enhanced Session-based Recommendation with Temporal Transformer (KSTT) to improve the performance of session-based recommendations. In comparison with state-of-the-art methods, our KSTT is equipped with two mechanisms to address the above-mentioned limitations: (1) We build a knowledge graph based on the attributes of the items and the order of clicks between the items, and use the graph neural network (GNN) to obtain the global context information of each node. (2) We use a well-designed mapping function to embed the timestamp into k-dimension vectors for modeling the time and time interval information. The mapping function is based on recent researches [6,26], which replaces position embedding with time embedding to reveal more time patterns and recurrent attention. In summary, our contributions are listed as follows. â€¢We introduce the knowledge graph into session-based item recommendation, and use GNN to integrate the information of adjacent nodes in the graph. â€¢We introduce temporal transformer into session-based recommendation and testify its eî€ectiveness. â€¢We conduct extensive experiments on four benchmarks datasets to demonstrate that KSTT can signiî€›cantly overcome stateof-the-art baselines. In this paper, we propose a framework called Knowledge-enhanced Session-based Recommendation with Temporal Transformer (KSTT) for Session-based Recommendation. Given the training set, it has itemsV = {ğ‘£, ğ‘£, ..., ğ‘£}and attributesA = {ğ‘, ğ‘, ..., ğ‘}. Sessionğ‘ = {ğ‘£, ğ‘£, ..., ğ‘£}means that theğ‘–-th session contains a transaction with itemğ‘£at timeğ‘¡. The architecture of KSTT is demonstrated in Figure 1. Firstly, the Item encoder in KSTT learns the embeddings of items using Knowledge-enhanced Graph Neural Network. The nodes in the knowledge graph contains two types: items(ğ‘£) and itemsâ€™ attributes(ğ‘). The edges in the graph represent the relations(ğ‘Ÿ) between nodes. Secondly, the Session encoder in KSTT exploits Temporal Transformer using time encoderğœ™to learn session representation. Finally, KSTT calculates the prediction scoresË†ğ‘¦ for the next item. The item encoder uses Knowledge-enhanced Graph Neural Network to learn item embeddings based on the knowledge graph. Knowledge Graph ConstructionPrevious methods [20,23,25] usually construct the session graph based on the sequential relationship between items. In this work, we propose the idea of constructing a directed KG based on the relationships in the session as well as the knowledge (e.g. attributes) of items. We construct the KG, where the nodes are items and these itemsâ€™ related attributes and the edges are the relationships between the nodes including sequential edges and semantic edges. The sequential edges are denoted as(ğ‘£, ğ‘Ÿ, ğ‘£),1â‰¤ ğ‘¡ < ğ‘šand c is a î€›xed number.ğ‘Ÿis the sequential relationship of usersâ€™ behaviors, and each edge represents that the user clicks the itemğ‘£after consuming itemğ‘£. The semantic edges are denoted as(ğ‘£, ğ‘Ÿ, ğ‘),0< ğ‘— â‰¤ ğ‘˜ +1 and 0 < k < K which represent itemğ‘£and attributeğ‘are connected with relation ğ‘Ÿ. Knowledge-enhanced Graph Neural NetworkWe exploit our proposed Knowledge-enhanced Graph Neural Network to learn the embeddings of items in the KG. We perform the knowledge graph embedding method TransR [10] to learn the semantic information and exploit Graph Convolutional Neural Networks [7] to aggregate information from neighbors to capture context information between nodes. We use the(head,relation,tail)entity triplets to learn the semantic representation of nodes. For each triplet, the embeddings of the head entityâ„, relationğ‘Ÿand tail entityğ‘¡areğ‘‘dimension vector updated by graph convolutional neural network [7]. The update function of nodes in one graph convolutional layer can be represented as:ğ‘’= norm(LeakyReLU(ğºğ¸ğ‘Š )), whereğ‘’ is the representation of entityâ„orğ‘¡.ğ¸is the embeddings for all entities andğºis the adjacent matrix of knowledge graph. Graph convolution is computed through gathering the response from each neighbour, and multiple graph convolutional layers are stacked to extract high-level representation for nodes. Therefore, graph convolution is suitable for exchanging information in neighborhoods. The Session Encoder exploits Temporal Transformer to capture the relationships among items in a session, and learn the representation of the session. Temporal TransformerIn the Transformer architectures [22], position embeddings are used to model the position information in one sequence. However, for session-based recommendation, this method only models the sequential order instead of time interval information between behaviors. In this work, we proposed Temporal Transformer to deal with this issue. Firstly, we encode each behavior in the session. For each historical behavior, we calculate the corresponding time interval, which is diî€erence of timestamp between this behavior and prediction time. Then, we exploit the time encoder to encode the time interval into low dimensional vector. The embedding of the behaviorğ‘£is deî€›ned asğ¸(ğ‘£), computed by the sum of item embeddingğ¼ğ¸(ğ‘£)and time embedding ğ‘‡ ğ¸(ğ‘£). Secondly, as Transformer did [22], we exploit Multi-head Self-Attention Networks and Feed Forward neural networks(FFN) to learn the representation of the session. The calculation method is deî€›ned asğ¿= FNN(MultiHead(ğ¿, ğ¿, ğ¿)), where ğ‘€ğ‘¢ğ‘™ğ‘¡ğ‘–ğ»ğ‘’ğ‘ğ‘‘ (Â·, Â·, Â·)is a multi-head self-attentive network that takes a query matrix, a key matrix, and a value matrix as input.ğ¿is initialized session representation including behavior embeddings ğ¸(ğ‘£) of this session. Time EncoderWe investigate three time embedding methods as time encoder: Time Bucket embedding method, Time2vec [6] and Mercer time embedding method [26]. Time Bucket Embedding method(TBE).This method uses log and î€oor function to discretize time into one-hot representation. Time2vec(T2v).T2v is designed to capture both periodic and non-periodic patterns in time [6]. The embedding is calculated using following equation where ğ‘– is the dimension. Mercer Time Embedding(MTE).Mercer Time Embedding(MTE) learns time representation with transition-invariant property [26]. The detailed Mercer time embedding function is given as follows: Î¦(ğ‘¡) = [Î¦(ğ‘¡), Î¦(ğ‘¡), ..., Î¦(ğ‘¡)] whereğ‘andğœ”represent the parameters.ğœ”should be initialized by uniformly distribution and ğœ”should be initialized as 0. In summary, these time embedding methods capture diî€erent time patterns within one session which helps attention mechanism to learn session embedding. We adopt the two phases with diî€erent loss functions to optimize KSTT. In one phase, we mainly optimize the loss function based on Lin et al. [10]. After obtaining entity embeddings from knowledge graph, we also utilize a projection matrixMto project the head entitieshand tail entitiestfrom the entity space to the relation space, represented asMhandMtrespectively. Then we try to minimize the diî€erence betweenMh + randMt, formulated as the score functionğ‘”(â„, ğ‘¡) = âˆ¥Mh + r âˆ’ Mtâˆ¥. A lower score means that the triplet is more likely to be true. We sample head entities from all training nodes with diî€erence relation and obtain a batch containing positive and negative pairs. We minimize the scores of negative pairs and maximize that of positive pairs for learning knowledge graph embeddings as follows.îƒ• whereğ‘‡= {(â„, ğ‘Ÿ, ğ‘¡, ğ‘¡) | (â„, ğ‘Ÿ, ğ‘¡) âˆˆ G, (â„, ğ‘Ÿ, ğ‘¡) âˆ‰ G},(â„, ğ‘Ÿ, ğ‘¡)is a random sampled negative triplet by replacingğ‘¡with another entity ğ‘¡in the knowledge base, and ğœ (Â·) is the sigmoid function. In the other phase, we aim to predict the next items users will buy. First, we obtain the session representationsfor each user from temporal transformer. Then we can compute the scoreË†zfor each itemğ‘£byË†ğ‘§ = ğ‘ ğ‘£.we apply a softmax function overË†ğ‘§and obtain the î€›nal output vectorË†ywhich is the probability distribution of the items. we optimize KSTT with the cross-entropy of the prediction Ë†y and the ground truth y. where ğ‘› is the number of training data. We jointly optimize the session-based recommendation and knowledge graph embedding by minimizing the following loss function:L= L+ L+ğœ†||Î˜||, whereÎ˜is the parameters of our model, andğœ†is the coeî€œcients of theğ¿regularization. DatasetsWe choose three representative benchmark Yoochoose, Diginetica and Last-fm, to evaluate our model.Yoochooseis used as a challenge dataset for RecSys Challenge 2015. It records the click-streams from an E-commerce website within 6 months. As previous did, we split the session sequences by î€›xed cutoî€ to generate the train and test datasets, and then further split the train data by the most recent partition 1/4 and 1/64.Digineticais used as a challenge dataset for CIKM Cup 2016. It contains the transaction data with the basic information of the products and transaction. Last-fmis a music dataset collected from the Last.fm API, which has been widely applied for music artist recommendations. After preprocessing, we keep the top 40,000 most popular artists and î€›lter out sessions that are longer than 50 or shorter than 2 items. BaselinesIn order to valid the eî€ectiveness of our proposed KSTT model, we compare KSTT with the following representative methods: POP always recommends the most popular items in the whole training set.S-POPalways recommends the most popular items in the current session.Item-KNNcomputes the similarity between each pair of items with their cosine distance in sessions. BPR-MF[14] calculates a pairwise ranking loss with a BPR objective function.FPMC[15] is a hybrid model for the next-basket recommendation based on the personalized transition matrix tube. GRU4REC[4] stacks multiple GRU layers to encode the session sequence into a latent vector, where a ranking loss is used to train the model.NARM[8] incorporates an attentive network to combine states of RNN.STAMP[11] uses attention layers to replace all RNN encoders in previous work.RepeatNet[13] proposes an encoderdecoder architecture to solve the reputation consumption problem. SR-GNN[23] applies a gated graph convolutional layer [9] to obtain item embeddings.GC-SAN[25] introduces a self-attention network to the session encoder. Parameters and Metric MeasureThe item embedding size and the dropout ratio are set to 100 and 0.1 in all the methods. We use Adam as our optimizing algorithm and set the learning rate as 0.001. The batch size is set to 246. To speed up the converge in the training process, we set the mini-batch size as 256 in session graph and 512 in the knowledge graph. For the evaluation metrics, we use R@K (Recall@K) and MRR@K (Mean Reciprocal Rank@K) to measure the recommendation performance. Table 1 lists the performance of the proposed model and other stateof-art session-based recommendation methods. All of the methods fall into three categories: classical methods, DNN-based methods and GNN-based methods. Considering POP and S-POP, they ignore the complex relationship between items and make recommendation based on occurrence number. Even so, S-POP still outperforms other methods demonstrating the importance of session contextual information. Item-KNN achieves better results than FPMC. Item-KNN Table 1: Performance comparisons on benchmark datasets. utilizes the similarity between items without considering sequential information but MC-based methods often consider adjacent information. These demonstrate that inter-session information is more important than information of neighbors. In particular, DNN-based methods, explicitly model the usersâ€™ global behavioral preferences by local and global representation leading to superior performance against traditional methods. GNNbased methods are state-of-the-arts. They model the transactions within a session as a graph, which outperforms other methods signiî€›cantly. Our proposed method KSTT is more powerful to model behaviors in the session and achieves the best performance on all four datasets in terms of R@20 and MRR@20, especially for Yoochoose dataset which most have 1.05% improvement for R@20 and 0.83% for MRR@20 . In this subsection, we conduct the ablation studies to verify the eî€ectiveness of Knowledge-enhanced Graph Neural Network (KGNN) and Temporal Transformer (TT) combining Self-Attention Neural Networks (SAN) and Time Encoder (TE). We conduct the experiments on Yoochoose1/64 and Diginetica and choose SRGNN as baseline. The experimental results are listed in Table 2, where+ meansaddingthis module orreplacingthe original similar module on the baseline. We can draw the following conclusions: (1)For KGNN: KGNN module indeed helps GNN achieve better performance, evidencing that +KGNN overcomes SRGNN that only uses a session graph. In particular, the KGNN module brings 1.44 (50.73 to 52.17) improvement in R@20 and 0.39 (17.59 to 17.88) gains in MRR@20 on Diginetica and the improvement on Yoochoose1/64 is 0.26 and 0.17, respectively. Since Diginetica has more attributes than Yoochoose, the former has more knowledge than the latter. Therefore, we can conclude that more knowledge information can bring better improvement. (2)For TT: We use three time embedding functions. The results show that all the time representation methods help session encoder with more information, especially TBE. This demonstrates that TBE has superiority the inter-session time pattern for session-based recommendation. In summary, our proposed modules can signiî€›cantly improve the baseline model, which result in the state-of-the-art performance. Session-based recommendation, which is an emerging topic in the recommendation system, has attracted interests of many researchers from both academia and industry. Hidasi et al. [4]use a gated recurrent unit model to model the userâ€™s behaviors in each single session. Li et al. [8] utilizes attention mechanisms to fusion the local interests and global usersâ€™ preference. Liu et al. [11]further takes the long/short term memory into account when constructing a neural attention model. Graph Neural Networks(GNN), which can capture both graph structure and nodesâ€™ attributes in the graph, have shown its superiority in many applications. Song et al. [19]represents the interactions between users and items as a bipartite graph. Li et al. [9]proposed Gated-GSNN for sequential learning in graph structure. Wu et al. [23], which constructs item-item graph in each session, is a pioneering work that î€›rst use GNN to capture the nonlinear relationship between items in a session. It generates session embedding using attentive networks and predict the next item. Xu et al. [25]uses transformer to learn inter-item dependencies and generate session embedding for prediction. In this paper, we present a novel architecture that incorporates knowledge graph and time interval information into session-based recommendation. The proposed method leverages knowledge-enhanced graph neural networks to obtain informative item embedding and uses temporal transformer to learn session representation effectively. Comprehensive experiments show that the proposed approach can consistently outperform state-of-art methods.