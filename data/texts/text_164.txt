The observed ratings in most recommender systems are subjected to popularity bias and are thus not randomly missing. Due to this, only a few popular items are recommended, and a vast number of non-popular items are hardly recommended. Not suggesting the non-popular items lead to fewer products dominating the market and thus oî€ering fewer opportunities for creativity and innovation. In the literature, several fair algorithms have been proposed which mainly focused on improving the accuracy of the recommendation system. However, a typical accuracy measure is biased towards popular items, i.e., it promotes better accuracy for popular items compared to non-popular items. This paper considers a metric that measures the popularity bias as the diî€erence in error on popular items and non-popular items. Motivated by the fair boosting algorithm on classiî€›cation, we propose an algorithm that reduces the popularity bias present in the data while maintaining accuracy within acceptable limits. The main idea of our algorithm is that it lifts the weights of the non-popular items, which are generally underrepresented in the data. With the help of comprehensive experiments on real-world datasets, we show that our proposed algorithm outperforms the existing algorithms on the proposed popularity bias metric. â€¢ Information Filtering Systems â†’ Recommender systems. Recommender systems, Popularity Bias, Fairness ACM Reference Format: Ajay Gangwar and Shweta Jain. 2021. An Adaptive Boosting Technique to Mitigate Popularity Bias in Recommender System. In ,. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/1122445.1122456 Recommendation systems have become an essential part of our lives, from movies we watch on OTT platforms to shopping on e-commerce websites to the news we read on the internet. In this information age, where the information is available in abundance or huge volumes, recommendation systems make our lives simpler by î€›ltering information for us according to our taste. Recommender systems recommend relevant items to the users based on the userâ€™s previous data. The primary goal of recommendation engines is to identify the items that a particular customer might buy or be interested in based on the previous ratings. They Indian Institute of Technology, Ropar do so by anticipating the ratings that users would give to the items, and the more data we have for a user, the more precisely we can predict their ratings. Netî€ix (movie recommendation system), Spotify (music recommender system), and Amazon (product recommendation system) are some examples of recommendation systems that we see in our daily lives. Although recommender systems are quite popular, there are concerns regarding the fairness of these systems amongst the research community. Recommender systems primarily face fairness issues at two levels, user-level [9,18] and item level [20,24]. This paper considers the most important fairness issue, namely the popularity bias at the item level. The items rated by most of the users or have received high ratings are known as popular items, and the items rated by very few users or not rated at all are known as non-popular items. The underlying notion behind popularity bias is that people are more inclined to oî€er comments on mainstream or popular products than on non-popular items. As a result, reported user response is skewed towards popular products rather than genuine user interest. Thus, frequently rated items or popular items receive a lot of exposure, but less popular or niche items are underrepresented in a recommendation. Recommending only popular items is problematic for two reasons: î€›rst, not everyone wants to follow the taste of the mainstream crowd, and second, it makes it harder for new goods to gain user attention. One example of how just suggesting popular things aî€ects companies is that a lawsuit was recently î€›led in the United States against Google for only showing advertising for popular items and not giving opportunities to less popular and newer items on the market [25]. Because just a few items are displayed most of the time, this sort of activity generates a monopoly in the market, which is unsuitable for any î€›rm. It will hamper the possibilities of creativity and innovation in the products or items. Exploring and mitigating popularity bias in recommender systems is not a new problem and have been previously explored in many works [4,5,12,27,30] . However, the existing works focus either on improving the accuracy of the overall recommendation system or exploring the non-popular items through diversiî€›cation. Since more ratings are available for the popular items, the accuracy is inherently biased towards the popular items. A highly accurate recommender system may have very high accuracy on popular items but a very low accuracy on non-popular items. On the other hand, naive diversiî€›cation may lead to poor accuracy of the overall recommender system. In this work, we deî€›ne popularity bias as the diî€erence between errors on non-popular items and popular items. A fair recommender system should perform equally well on both popular and non-popular items thus ensuring a balanced accuracy of the recommender system. The main aim is to reduce popularity bias from the data and recommend relevant items to the user based on prior data while keeping the error as low as possible. The main contributions of this paper are as follows: â€¢We provide a metric that quantitatively measures the popularity bias present in the data. The previous papers have used error as a metric that is inherently biased towards the popular items and thus may lead to poor accuracy on non-popular items. â€¢To reduce the popularity bias, we propose a novel algorithm, namely FairBoost, that signiî€›cantly reduces the popularity bias present in the data without deteriorating on the error. â€¢We compare our proposed algorithm with the existing algorithms that claim to remove popularity bias and show that our algorithm outperforms the existing algorithms on the proposed popularity bias metric. Fairness in recommender systems has been the center of discussion for a long time since these systems suî€er from diî€erent types of biases present in the data. Due to these biases, it can not represent the entire population, and due to this, the results produced by recommender systems are biased, so there is an utmost need to solve this problem. The most common types of biases are gender bias [9,18], racial bias[19], selection bias [20,24], exposure bias [28,31], position bias [8,11], and popularity bias [4,5,12,27,30]. The focus of this paper is on popularity bias. It is crucial to recommend non-popular items as they are the ones that are less likely to be discovered. Abdollahpouri et al. [5] have proposed a customized re-ranking diversiî€›cation strategy that aims to boost non-popular items representation while retaining acceptable recommendation accuracy. The focus in [5] was to achieve a trade-oî€ between coverage of popular and non-popular items. To achieve this coverage, non-popular items that consumers could appreciate were randomly selected and presented to the user without being displayed in some order of preference. Thus the proposed method could result in poor accuracy for non-popular items. To address the popularity bias, Huang et al. [12]and Schnabel et al. [27]employed the Inverse Propensity Score (IPS) method. Propensity-based techniques have previously been used in causal inference and observational studies but are used for the î€›rst time in the recommender system by Schnabel et al. [27]. To apply the IPS approach, we need propensities, and the authors in [27] have discussed primarily two propensity estimation models, via Naive Bayes, and the other is via logistic regression. Saito[24]has also discussed a straightforward approach to measure the propensities. However, the performance of these propensity-based methods is heavily inî€uenced by the model used to estimate propensity. The algorithms require actual probabilities to function correctly which are diî€œcult to estimate [24]. As a result, a more eî€ective strategy is required. Furthermore, all of the above works focus on reducing recommender system errors without explicitly being fair to non-popular items.Two exceptions to this are [6,32]. [6] mitigate the issue of popularity bias by providing a metric similar to the individual fairness metric used in machine learning. The idea is to equalise the outcomes across all individual items. On the other hand, [32] focuses on group fairness by grouping the items together and equalizing the true positives across all groups. Our work focuses on group fairness, and instead of just equalizing true positives, we sought to equalize the total error across the groups. Machine learning algorithms typically discriminate based on biased historical data. Data manipulation is used in certain preprocessing techniques to address this discrimination problem. Massaging [15] , re-weighting [7] , uniform or preferential sampling [16] , and data augmentation [13] are examples of preprocessing methods that might have an impact on data distribution. By altering instance labels, giving diî€erent weights, under or oversampling instances, and producing pseudo instances, they attempt to correct imbalances between protected and non-protected groups within the data. Iosiî€›dis and Ntoutsi[14]demonstrated Adafair, an Adaboost-based fairness-aware classiî€›er that adjusts the weights of the instances in each subsequent round while explicitly addressing class imbalance by optimizing the number of ensemble models for balanced classiî€›cation error. Adapting this idea, we examine the use of boosting algorithm in the recommender system. The idea is to increase the weights of non-popular items to maintain the balance between popular and non-popular items. We show that this technique can signiî€›cantly reduce the popularity bias present in the system. Let us consider the set of usersğ‘ˆand the set of itemsğ‘€, with the number of users|ğ‘ˆ | = ğ‘˜and the number of items|ğ‘€| = ğ‘™. ğ´ = ğ‘ˆ Ã— ğ‘€represents the collection of (user, item) pairs with each entryğ´denoting the true rating provided by userğ‘¢to itemğ‘š. Integer values ranging from 1 to 5 can be used for representing each entry, with 1 representing low interest and 5 indicating strong interest. Because consumers only rate a small portion of the items, many ratings are typically unknown. The main aim is to develop an algorithm that generates the best possible predicted rating matrix Ë†ğ´. In the predicted rating matrixË†ğ´, each entryË†ğ´denotes the probable rating given by userğ‘¢to itemğ‘š. To achieve the goal, ideal loss function to î€›nd predicted rating matrix is deî€›ned as: where ğ›¿can be taken as mean squared error (MSE) i.e. or minimum absolute error (MAE) or i.e. The ideal loss function cannot be calculated since the majority of the elements in the actual rating matrix are missing. Therefore, to build an eî€ective recommender system, we need to estimate the ideal loss as accurately as possible using only the ratings present in the true rating matrix. The simplest straightforward estimator for estimating ideal loss, also known as a naÃ¯ve estimator is used which is deî€›ned as follows: whereğµis an observed rating matrix and each entryğµ=1 if user ğ‘¢ has given rating to item ğ‘š otherwise ğµ= 0. The loss over observed ratings is calculated using naive loss, which is deî€›ned as the sum of loss overall user-item pairs in the data divided by the total number of user-item pairs in the true rating matrix. It is easy to see that when the data are missing fully at random, this naÃ¯ve estimator is unbiased i.e. However, when the data are not missing completely at random which happens in the presence of popularity bias where there is more rating available for popular items as opposed to that of non popular items, it is shown that the naive loss may not exactly correspond to the ideal loss function [27, 29] i.e. One popular approach to mitigate the popularity bias is by using Inverse Propensity Score (IPS) approach [12,27]. The main idea behind this approach is to build a pseudo missing completely at random dataset by weighting all the observed ratings by the inverse of their propensity score. It is easy to theoretically show that IPS loss is an unbiased estimator and hence can be used to remove popularity bias [24]. The IPS estimatorâ€™s unbiasedness is desired; nevertheless, this feature is dependent on the true propensity scores which need to be approximated using various approaches. These methods majorly suî€ers from two problems [23]. One, the IPS estimator is no longer an unbiased estimator if a propensity estimation model is not stated appropriately. Another, the inverse of the propensities may be substantial, so the IPS estimator has a high variance problem. As a result, building learning methods that are resilient to misspeciî€›cation of propensity and estimator variation is crucial for applying the approaches to real-world MNAR situations. Saito[24]devised an asymmetric tri-training technique based on the asymmetric tri-training approach used in unsupervised domain adaptation to tackle the challenges that an IPS approach faced. It employs three rating predictors, two of which were used to create a pseudo rating dataset and the third of which was used to train the model on these pseudo ratings. The problem with this method is that the size of the dataset reduces after the algorithm is applied, making it impossible to accurately estimate the ratings of all the items. Because data in MNAR datasets are typically sparse, we must make appropriate use of it in order to predict ratings. Boosting technique on the other hand would be ideal because they utilise the majority of their data in diî€erent iteration stages, and one can also use upweighting to boost non-popular items in the same way that incorrectly categorised points are boosted in subsequent iterations. So far, the research community has been focusing on mitigating the popularity bias so as to improve the overall accuracy of the system. As mentioned in the Introduction section, resolving popularity bias as a standalone problem is highly required in order to prevent monopoly in the system. A typical accuracy measure is not a good metric for popularity bias because it is biased towards popular items, i.e., it promotes better accuracy for popular items compared to non-popular items. This is because more number of ratings are given to popular items. One naive way to resolve popularity bias is to recommend nonpopular items randomly thereby exploring these items. Another possibility to avoid the monopoly is through diversiî€›cation [17,21] which ensures that all group of items must be recommended some number of times. However, both naive or diversiî€›cation methods could potentially lead to dissatisfaction amongst the users of the recommendation system. Thus, we need a metric that not only ensures the accuracy of the overall system, but also prevent the issue of popularity bias. We deî€›ne the popularity biasedness of any algorithm by the diî€erence in the error it achieves between the non-popular items and popular items. LetPSbe the set of popular items and let N PSbe the set of non-popular items. We employ the threshold ğœto identify which of the items are popular and which are not. The set of all those items that obtained more than the threshold number of ratings will be regarded popular, whereas the set of items that received less than the threshold number of ratings will be considered non-popular.ğœis a dataset-speciî€›c parameter which can be taken as input. Then the popularity biasedness of the algorithm with predicted rating matrixË†ğ´ is deî€›ned as: whereğ›¿can be taken as MSE(mean squared error) or MAE(minimum absolute error). This popularity bias metric separates the items into two groups: popular and non-popular. After that, the mean error is computed separately for non-popular and popular sets. Finally, the diî€erence between them is used to determine the popularity bias ğ‘ƒğµ(Ë†ğ´, ğœ). The goal is to minimize theğ‘ƒğµ(Ë†ğ´, ğœ)to reduce the eî€ect of popularity bias. Boosting is a method of creating a powerful learner by combining several weak base learners. Adaboost[26] is one such approach, which iteratively calls weak base learners after modifying the weights based on misclassiî€›ed data points in each iteration. Because it separates the learning problem into numerous sub-problems and then combines their answers into an overall model, we believe boosting is a good î€›t for our problem. In sub-models, the popularity bias problem is easier to address than in the entire complicated model. As popular items frequently appear in the recommender systemâ€™s results, the weights of the non-popular items must be increased in a way such that it shows up in the recommender systemâ€™s results. In the classiî€›cation setting, the AdaBoost algorithm internally boosts the weights of erroneously categorised data points. The way we increase the weights of incorrectly classiî€›ed data points in Adaboost, we increase the weights of non-popular items while keeping the accuracy on these items in mind. Finally, we adjust the reweighting procedure of the AdaBoost algorithm to make it more fair and obtain the FairBoost Algorithm. Adaboost is not a novel technique for recommender systems; it was previously used to improve accuracy when using a decision tree as the base learner [10]. However, we are the î€›rst to use and modify this notion to reduce popularity bias in recommender systems. Algorithm 1 depicts the training phase of FairBoost. To establish its signiî€›cance in the training dataset, FairBoost also provides weight to each training example. When the given weights are high, that set of training user-item pairs is more likely to inî€uence the training set. Similarly, user-item pairs with low weights will have a little impact on the training dataset. At î€›rst, all of the user-item pairs will be given same weight of 1/ğ‘›, whereğ‘›is the number of user item pairs. Fairboost additionally employs a popularity biasrelated cost denoted byğ‘ğ‘œğ‘ ğ‘¡for each user-item pair(ğ‘¢, ğ‘š), which try to maintain similarity and reduce the popularity bias that exists between popular and non-popular sets for current learners. We initializeğ‘ğ‘œğ‘ ğ‘¡to zero for all(ğ‘¢, ğ‘š). Then the user item pairs are sampled using sample weightsğ‘¤, and a weak learnerË†ğ´ is trained on these sampled points. Letğ‘†is the set of sampled user-item pairs atğ‘—iteration. The error rateğ‘’ğ‘Ÿğ‘Ÿis computed using: whereğ‘¤is the weight of(ğ‘¢, ğ‘š)user-item pair in the sampled user-item pairs setğ‘†,ğ´is the actual rating of user-item pair andË†ğ´is the rating predicted by the current base learner for user-item pair(ğ‘¢, ğ‘š). Following that, we use the following formula to calculate the the weightğ›¼depicting the inî€uence of the base learner ğ‘— in predicting ratings. After that popularity biasğ‘ƒğµ(Ë†ğ´, ğœ)is computed for the current base learner using Equation (4). Next popularity bias related cost,ğ‘ğ‘œğ‘ ğ‘¡is computed for all the user-item pairs(ğ‘¢, ğ‘š)in the sampled user-item pairs. Since we conduct random sampling every time, popular items become nonpopular in some iterations and non-popular items become popular in other iterations. We seek to achieve similarity between popular and non-popular sets or eliminate popularity bias. As a result, in the current iteration, a popularity bias-related cost is employed to preserve this similarity or reduce popularity bias.ğ‘ğ‘œğ‘ ğ‘¡is given by : It should be noted thatğœ–is a hyperparameter used to set a bound on the diî€erence between true and predicted ratings, andğœ–is a hyperparameter used to set a bound on popularity bias. Both of these parameters must be tuned for the algorithm to work properly. For all user-item pairs, if item belongs to popular set, popularity biasğ‘ƒğµ(Ë†ğ´, ğœ)is greater than zero and if diî€erence between true and predicted rating is greater thanğœ–and absolute value of popularity bias for current base learner is greater thanğœ–, thenğ‘ğ‘œğ‘ ğ‘¡ assigned to the pair is|ğ‘ƒğµ(Ë†ğ´, ğœ) |. Here, a popular item has become unpopular, therefore we are weighing it more. For all user-item pairs, if item belongs to non-popular set, popularity biasğ‘ƒğµ(Ë†ğ´, ğœ) is less than zero and if diî€erence between true and predicted rating is greater thanğœ–and absolute value of popularity bias for current base learner is greater thanğœ–, thenğ‘ğ‘œğ‘ ğ‘¡assigned to the pair is |ğ‘ƒğµ(Ë†ğ´, ğœ) |. We are upweighting this item because it is already unpopular. And for all other user-item pairsğ‘ğ‘œğ‘ ğ‘¡of zero is assigned. Then the weights are updated for the next round after computing costs using : whereğ‘is a factor used for normalizing weights. The user-item pairs having the larger error are given greater weight, so they can be predicted accurately in the following iteration. Weights are updated for all sampled user-item pairs by multiplyingğ‘¤with exponential of product of current base learner weight and diî€erence between true and predicted rating so that the examples that are having more error gets more weight in next round and the examples having less errors gets less weight in next round. Weights are also multiplied by (1+ğ‘ğ‘œğ‘ ğ‘¡). It is done for all those examples that were treated unfairly during current round. And once the number of rounds is reached, the algorithm converges. The algorithm will generate a number of base learners equal to the number of rounds, which will then be merged using the weightsğ›¼or the amount of inî€uence they have to produce the estimatorË†ğ´as an output. Now, in order to forecast the rating of a new user-item pair, it will go through all of the base learners. The predicted ratings from all these learners are weighted with the corresponding weights of the base learners and then combined to provide the expected rating for the new user-item pair. ğ‘is a factor used for normalizing weights. 4.1.1 Datasets. To show the eî€ectiveness of our proposed algorithm, we have used the following realworld datasets. In all the datasets, training set and testing set have been created by considering 80% and 20% ratings respectively. â€¢Netî€ix dataset[2] The Netî€ix dataset consists of about 100 million MNAR î€›ve-star movie ratings (missing not at random). There are 480189 users and 17770 movies involved. Due to computational restrictions, we took 10 million of the most recent ratings by sorting them according to time. There are 370811 users and 1962 movies in the training set, whereas there are 258603 users and 1962 movies in the test set. â€¢ Yahoo dataset[3] This dataset captures the preferences of the Yahoo! Music community and comprises almost 717 million ratings on 136 thousand songs provided by 1.8 million users. The data was gathered between 2002 and 2006. The ratings are on a scale of 1 to 5, with 1 being the lowest and 5 being the highest. We have sampled around 10 Million ratings from this set. Training set consists of 23179 users and 136737 songs while test set consists of 23179 users and 63261 movies. â€¢ Amazon dataset[22] This is an Amazon Movies and TV dataset which consists of around 4.6 million ratings. The ratings are on a scale of 1 to 5, with 1 being the lowest and 5 being the highest. There are 2088620 users and 200941 items involved. There are 1666901 users and 188083 items in the training set, whereas there are 562187 users and 80112 items in the test set. â€¢ Movielens dataset[1] This dataset is made up of 100K î€›ve-star movie ratings(missing not at random) gathered from a movie recommendation service . There are 1682 movies and 943 people involved. The data has been sorted by date. The dataset is divided into a training set and a test set, with the training set consisting of the previous 80% of user-item pairs and the test set consisting of the rest or most recent 20% of user-item pairs. There are 751 users and 1616 movies in the training set, whereas there are 301 users and 1448 movies in the test set. 4.1.2 Compared methods. We conducted comprehensive testing on the above mentioned datasets to demonstrate that our proposed algorithm lowers the systemâ€™s popularity bias. Three algorithms were tested and their results were compared. We compared the Fairboost algorithm to that of Matrix Factorization with Inverse Propensity scoring [27] and Asymmetric tri-training [24]. The previous papers used error as a measure and focused on lowering the error rather than explicitly addressing the systemâ€™s popularity bias. We compared several methods using the proposed popularity bias metric in Equation ( 4). We keep the value ofğœto be 100 which means if an item received rating from more than 100 users, then we call that item as popular item. 4.1.3 Hyperparameter Tuning. To adjust the parametersğœ–and ğœ–, we used a random search cv hyperparameter tuning procedure. Both parameters were tuned in the range [10, 1]. The results from all the datasets are summarised in Table 1 which displays the results of 10 iterations of all the algorithms. Table 1 can be used to make the following observations. The popularity bias was reduced when IPS was used in conjunction with matrix factorization. However, the error on both popular and non-popular items had increased when compared to the baseline matrix factorization algorithm on all the datasets. Another î€›nding was that the propensity estimation model chosen had a signiî€›cant impact on the performance of propensity-based unbiased estimation approaches. We have used naive Bayes propensity estimation method for the comparison as it gave us the least value of popularity bias. The other methods such as user-item propensity gave us a good accuracy, however, the popularity bias was quite high. Another observation is that the popularity bias was also reduced when asymmetric tri-training was used when compared to matrix factorization based methods, as shown in table 1. As popular items appear frequently in the recommender systemâ€™s results. So, in order for non-popular items to show up in the recommender systemâ€™s results, their weightage must be increased in some way. In the classiî€›cation setting, the AdaBoost algorithm internally boosts the weightage of wrongly categorised data points, so we thought weâ€™d give that a try in ours because we also want to give non-popular items more weightage. Table 1 shows that after applying Adaboost, the popularity bias has decreased on almost all of the datasets as compared to previously implemented methods. Inspired by how well Adaboost performed, we modiî€›ed it to create the FairBoost algorithm (Algorithm 1). When running the FairBoost algorithm, we employed matrix factorization as our underlying base learner. On all the datasets, Table 1 shows that our proposed algorithm Fairboost signiî€›cantly reduced the popularity bias when compared to other algorithms. In the following steps, the FairBoost algorithm tries to make non-popular items popular. We increase the weights of non-popular items in our algorithm in the following steps to make them popular, similar to how Adaboost increases the weights of inaccurately classiî€›ed data points. Figures 1a, 1b and 1c show that the popularity bias decreases as the number of estimators increases. This is because the FairBoost algorithm inherently gives more weight to non-popular items, causing the graph to decrease. In the case of the movielens dataset, we got a zig-zag graph, as shown in Fig 1d. This is because the data is nearly fair; as the table 1 shows, the error diî€erence is extremely small, and ratings are also uniformly distributed. Figures 2a, 2b, 2c and 2d show that the overall inaccuracy increases slightly. This is due to the fact that in succeeding steps, popular items weights are given less weight. As a result, FairBoost Algorithm minimises the popularity bias in each subsequent step while keeping the error increase within acceptable bounds. Adequate coverage of non-popular or long-tail items is critical to any businessâ€™s success. Because almost all users are familiar with popular items, a recommender systemâ€™s ability to recommend non-popular items will determine how well it introduces users to new experiences and products; however, it is well known that recommender systems are biased towards popular items. This paper proposed and compared FairBoost, an adaptive boosting algorithm, to previously implemented approaches for mitigating popularity bias in recommender systems. We also proposed a new metric to quantify popularity bias, because the error metric was insuî€œcient for this purpose. On the four datasets, we were able to demonstrate that the FairBoost algorithm signiî€›cantly reduces the popularity bias compared to other algorithms while keeping the (c) Amazon dataset(d) Movielens dataset error as low as possible. One exciting area for future study would be to test this algorithm on other sorts of biases that might exist in the system such as selection bias, gender bias, and so on. Another thing we can try is to test the algorithm with diî€erent base learners.