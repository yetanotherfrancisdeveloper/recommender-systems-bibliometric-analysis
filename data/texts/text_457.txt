Extreme multi-label classiî€›cation (XMLC) refers to the task of tagging instances with small subsets of relevant labels coming from an extremely large set of all possible labels. Recently, XMLC has been widely applied to diverse web applications such as automatic content labeling, online advertising, or recommendation systems. In such environments, label distribution is often highly imbalanced, consisting mostly of very rare tail labels, and relevant labels can be missing. As a remedy to these problems, the propensity model has been introduced and applied within several XMLC algorithms. In this work, we focus on the problem of optimal predictions under this model for probabilistic label trees, a popular approach for XMLC problems. We introduce an inference procedure, based on theğ´-search algorithm, that eî€œciently î€›nds the optimal solution, assuming that all probabilities and propensities are known. We demonstrate the attractiveness of this approach in a wide empirical study on popular XMLC benchmark datasets. â€¢ Computing methodologies â†’ Super vised learning by classiî€›cation. extreme classiî€›cation, multi-label classiî€›cation, propensity model, missing labels, probabilistic label trees, supervised learning, recommendation, tagging, ranking ACM Reference Format: Marek Wydmuch, Kalina Jasinska-Kobus, Rohit Babbar, and Krzysztof DembczyÅ„ski. 2021. Propensity-scored Probabilistic Label Trees. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR â€™21), July 11â€“15, 2021, Virtual Event, Canada. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3404835.3463084 Extreme multi-label classiî€›cation (XMLC) is a supervised learning problem, where only a few labels from an enormous label space, reaching orders of millions, are relevant per data point. Notable examples of problems where XMLC framework can be eî€ectively leveraged are tagging of text documents [8], content annotation for multimedia search [9], and diverse types of recommendation, including webpages-to-ads [5], ads-to-bid-words [2,19], users-toitems [23,28], queries-to-items [17], or items-to-queries [7]. These practical applications impose new statistical challenges, including: 1) long-tail distribution of labelsâ€”infrequent (tail) labels are much harder to predict than frequent (head) labels due to the data imbalance problem; 2) missing relevant labels in learning dataâ€”since it is nearly impossible to check the whole set of labels when it is so large, and the chance for a label to be missing is higher for tail than for head labels [11]. Many XMLC models achieve good predictive performance by just focusing on head labels [22]. However, this is not desirable in many of the mentioned applications (e.g., recommendation and content annotation), where tail labels might be more informative. To address this issue Jain et al. [11]proposed to evaluate XMLC models in terms of propensity-scored versions of popular measures (i.e., precision@ğ‘˜, recall@ğ‘˜, and nDCG@ğ‘˜). Under the propensity model, we assume that an assignment of a label to an example is always correct, but the supervision may skip some positive labels and leave them not assigned to the example with some probability (diî€erent for each label). In this work, we introduce the Bayes optimal inference procedure for propensity-scored precision@ğ‘˜for probabilistic classiî€›ers trained on observed data. While this approach can be easily applied to many classical models, we particularly show how to implement it for probabilistic label trees (PLTs) [12], an eî€œcient and competitive approach to XMLC, being the core of many existing state-of-the-art algorithms (e.g., Parabel [18], extremeText [24], Bonsai [15], AttentionXML [25], napkinXC [13], and PECOS that includes XR-Linear [26] and X-Transformers [7] methods). We demonstrate that this approach achieves very competitive results in terms of statistical performance and running times. In this section, we state the problem. We î€›rst deî€›ne extreme multilabel classiî€›cation (XMLC) and then the propensity model. LetXdenote an instance space, and letL = [ğ‘š]be a î€›nite set ofğ‘šclass labels. We assume that an instanceğ’™ âˆˆ Xis associated with a subset of labelsLâŠ† L(the subset can be empty); this subset is often called the set of relevant or positive labels, while the complementL\Lis considered as irrelevant or negative for ğ’™. We identify the setLof relevant labels with the binary vector ğ’š = (ğ‘¦, ğ‘¦, . . . ,ğ‘¦), in whichğ‘¦=1â‡” ğ‘— âˆˆ L. ByY = {0,1}we denote the set of all possible label vectors. In the classical setting, we assume that observations(ğ’™,ğ’š)are generated independently and identically according to a probability distributionP(ğ’™, ğ’š)deî€›ned on XÃ—Y. Notice that the above deî€›nition concerns not only multi-label classiî€›cation, but also multi-class (whenâˆ¥ğ’šâˆ¥=1) andğ‘˜-sparse multi-label (whenâˆ¥ğ’šâˆ¥â‰¤ ğ‘˜) problems as special cases. In case of XMLC we assumeğ‘što be a large number (e.g.,â‰¥10), andâˆ¥ğ’šâˆ¥to be much smaller than ğ‘š, âˆ¥ğ’šâˆ¥â‰ª ğ‘š. The problem of XMLC can be deî€›ned as î€›nding a classiî€›er ğ’‰(ğ’™) = (â„(ğ’™), â„(ğ’™), . . . , â„(ğ’™)), from a function classH:X â†’ R, that minimizes the expected loss or risk: whereâ„“ (ğ’š,Ë†ğ’š)is the (task) loss. The optimal classiî€›er, the so-called Bayes classiî€›er, for a given loss functionâ„“is:ğ’‰= arg minğ¿(ğ’‰) . In the case of XMLC, the real-world data may not follow the classical setting, which assumes that(ğ’™,ğ’š)are generated according toP(ğ’™, ğ’š). As correct labeling (without any mistakes or noise) in case of an extremely large label set is almost impossible, it is reasonable to assume that positive labels can be missing [11]. Mathematically, the model can be deî€›ned in the following way. Letğ’šbe the original label vector associated withğ’™. We observe, however, Ëœğ’š = (Ëœğ‘¦, . . . ,Ëœğ‘¦) such that: P(Ëœğ‘¦= 1 | ğ‘¦= 1) = ğ‘, P(Ëœğ‘¦= 0 | ğ‘¦= 1) = 1 âˆ’ ğ‘,(2) P(Ëœğ‘¦= 1 | ğ‘¦Ëœğ‘¦= 0 | ğ‘¦= 0) = 1 , whereğ‘âˆˆ [0,1]is the propensity of seeing a positive label when it is indeed positive. All observations in both training and test sets do follow the above model. The propensity does not depend onğ’™. This means that for the observed conditional probability of labelğ‘—, we have: Let us denote the inverse propensity byğ‘, i.e.ğ‘=. Thus, the original conditional probability of label ğ‘— is given by: ğœ‚(ğ’™) = P(ğ‘¦= 1 | ğ’™) = ğ‘P(Ëœğ‘¦= 1 | ğ’™) = ğ‘Ëœğœ‚ Therefore, we can appropriately adjust inference procedures of algorithms estimatingËœğœ‚(ğ’™)to act optimally under diî€erent propensity-scored loss functions. Jain et al. [11]introduced propensity-scored variants of popular XMLC measures. For precision@ğ‘˜ it takes the form: whereË†Lis a set ofğ‘˜labels predicted byğ’‰forğ’™. Notice that precision@ğ‘˜ (ğ‘@ğ‘˜) is a special case of ğ‘ğ‘ ğ‘@ğ‘˜ if ğ‘= 1 for all ğ‘—. We deî€›ne a loss function for propensity-scored precision@ğ‘˜as â„“= âˆ’ğ‘ğ‘ ğ‘@ğ‘˜. The conditional risk for â„“is then: The above result shows that the Bayes optimal classiî€›er for ğ‘ğ‘ ğ‘@ğ‘˜is determined by the conditional probabilities of labels scaled by the inverse of the label propensity. Given that the propensities or their estimates are given in the time of prediction,ğ‘ğ‘ ğ‘@ğ‘˜is optimized by selectingğ‘˜labels with the highest values ofğ‘Ëœğœ‚(ğ’™). Conditional probabilities of labels can be estimated using many types of multi-label classiî€›ers, such as decision trees, k-nearest neighbors, or binary relevance (BR) trained with proper composite surrogate losses, e.g., squared error, squared hinge, logistic or exponential loss [1,27]. For such models, where estimates ofËœğœ‚(ğ’™) are available for allğ‘— âˆˆ L, application of the Bayes decision rule for propensity-scored measures is straightforward. However, in many XMLC applications, calculating the full set of conditional probabilities is not feasible. In this section, we introduce an algorithmic solution of applying the Bayes decision rule forğ‘ğ‘ ğ‘@ğ‘˜to probabilistic label trees (PLTs). We denote a tree byğ‘‡, a set of all its nodes byğ‘‰, a root node by ğ‘Ÿ, and the set of leaves byğ¿. The leafğ‘™âˆˆ ğ¿corresponds to the labelğ‘— âˆˆ L. The parent node ofğ‘£is denoted bypa(ğ‘£), and the set of child nodes byCh(ğ‘£). The set of leaves of a (sub)tree rooted in node ğ‘£ is denoted by ğ¿, and path from node ğ‘£ to the root by Path(ğ‘£). A PLT uses treeğ‘‡to factorize conditional probabilities of labels, ğœ‚(ğ‘¥) = P(ğ‘¦=1|ğ’™),ğ‘— âˆˆ L, by using the chain rule. Let us deî€›ne an event thatLcontains at least one relevant label inğ¿:ğ‘§= (|{ğ‘—: ğ‘™âˆˆ ğ¿} âˆ© L| >0). Now for every nodeğ‘£ âˆˆ ğ‘‰, the conditional probability of containing at least one relevant label is given by: whereğœ‚(ğ’™, ğ‘£) = P(ğ‘§=1|ğ‘§=1, ğ’™)for non-root nodes, and ğœ‚(ğ’™, ğ‘£) = P(ğ‘§=1| ğ’™)for the root. Notice that (6) can also be stated as recursion: and that for leaf nodes we get the conditional probabilities of labels: To obtain a PLT, it suî€œces for a givenğ‘‡to train probabilistic classiî€›ers fromH:Râ†¦â†’ [0,1], estimatingğœ‚(ğ’™, ğ‘£)for allğ‘£ âˆˆ ğ‘‰. We denote estimates ofğœ‚byË†ğœ‚. We index this set of classiî€›ers by the elements of ğ‘‰as ğ» = {Ë†ğœ‚(ğ‘£) âˆˆ H : ğ‘£ âˆˆ ğ‘‰}. An inference procedure for PLTs, based on uniform-cost search, has been introduced in [12]. It eî€œciently î€›ndsğ‘˜leaves, with highest Ë†ğœ‚(ğ’™)values. Since inverse propensity is larger than one, the same method cannot be reliably applied to î€›nd leaves with theğ‘˜highest products ofğ‘andË†Ëœğœ‚(ğ’™). To do it, we modify this procedure to an ğ´-search-style algorithm. To this end we introduce cost function ğ‘“ (ğ‘™, ğ’™) for each path from the root to a leaf. Notice that: ğ‘Ë†Ëœğœ‚(ğ’™) = expâˆ’âˆ’ log ğ‘âˆ’logË†Ëœğœ‚(ğ’™, ğ‘£). (9) This allows us to use the following deî€›nition of the cost function:îƒ• ğ‘“ (ğ‘™, ğ’™) = logğ‘âˆ’ log ğ‘âˆ’logË†Ëœ whereğ‘= maxğ‘is a natural upper bound ofğ‘Ë†Ëœğœ‚(ğ’™)for all paths. We can then guide the A*-search with functionË†ğ‘“ (ğ‘£, ğ’™) = ğ‘”(ğ‘£, ğ’™) + â„(ğ‘£, ğ’™), estimating the value of the optimal path, where:îƒ• is a cost of reaching tree node ğ‘£ from the root, and: is a heuristic function estimating the cost of reaching the best leaf node from node ğ‘£. To guarantee that ğ´-search î€›nds the optimal solutionâ€”top-ğ‘˜labels with the highestğ‘“ (ğ‘™, ğ’™)and thereby top-ğ‘˜ labels with the highestğ‘Ë†Ëœğœ‚(ğ’™)â€”we need to ensure thatâ„(ğ‘£, ğ’™)is admissible, i.e., it never overestimates the cost of reaching a leaf node [21]. We also would like â„(ğ‘£, ğ’™) to be consistent, making the ğ´-search optimally eî€œcient, i.e., there is no other algorithm used with the heuristic that expands fewer nodes [21]. Notice that the heuristic function assumes that probabilities estimated in nodes in a subtree rooted inğ‘£are equal to 1. Sincelog1=0, the heuristic comes to î€›nding the label in the subtree ofğ‘£with the largest value of the inverse propensity. Algorithm 1 outlines the prediction procedure for PLTs that returns the top-ğ‘˜labels with the highest values ofğ‘Ë†Ëœğœ‚(ğ’™). We call this algorithm Propensity-scored PLTs (PS-PLTs). The algorithm is very similar to the original Uniform-Cost Search prediction procedure used in PLTs, which î€›nds the top-ğ‘˜labels with the highestË†ğœ‚(ğ’™). The diî€erence is that nodes in PS-PLT are evaluated in the ascending order of their estimated cost valuesË†ğ‘“ (ğ‘£, ğ’™)instead of decreasing conditional probabilitiesË†ğœ‚(ğ’™). Theorem 1.For anyğ‘‡, ğ», ğ’’, andğ’™the Algorithm 1 is admissible and optimally eî€œcient. Proof. ğ´-search î€›nds an optimal solution if the heuristicâ„is admissible, i.e., if it never overestimates the true value ofâ„, the cost value of reaching the best leaf in a subtree of nodeğ‘£. For node ğ‘£ âˆˆ ğ‘‰ , we have: â„(ğ‘£, ğ’™) = logğ‘âˆ’ log maxğ‘âˆ’logË†Ëœğœ‚(ğ’™, ğ‘£) . SinceË†Ëœğœ‚(ğ’™, ğ‘£) âˆˆ [0,1]and thereforelogË†Ëœğœ‚(ğ’™, ğ‘£) â‰¤0, we have that â„(ğ‘£, ğ’™) â‰¥ â„(ğ‘£, ğ’™), for all ğ‘£ âˆˆ ğ‘‰, which proves admissibility. ğ´-search is optimally eî€œcient ifâ„(ğ‘£, ğ’™)is consistent (monotone), i.e., its estimate is always less than or equal to the estimate for any child node plus the cost of reaching that child. Since we have thatmaxğ‘â‰¥ maxğ‘, and the cost of reaching ğ‘£frompa(ğ‘£)isâˆ’ log(Ë†Ëœğœ‚(ğ’™, ğ‘£))which is greater or equal 0, it holds that â„(pa(ğ‘£), ğ’™) â‰¤ â„(ğ‘£, ğ’™) âˆ’ log(Ë†Ëœ The same cost functionğ‘“ (ğ‘™, ğ’™)can be used with other tree inference algorithms (for example discussed by Jasinska-Kobus et al. [13]), including beam search [16], that is approximate method for î€›ndingğ‘˜leaves with highestË†ğœ‚(ğ’™). It is used in many existing label tree implementations such as Parabel, Bonsai, AttentionXML and PECOS. We present beam search variant of PS-PLT in the Appendix. In this section, we empirically show the usefulness of the proposed plug-in approach by incorporating it into BR and PLT algorithms and comparing these algorithms to their vanilla versions and stateof-the-art methods, particularly those that focus on tail-labels performance: PFastreXML [11], ProXML [4], a variant of DiSMEC [3] with a re-balanced and unbiased loss function as implemented in PW-DiSMEC [20] (class-balanced variant), and Parabel [18]. We conduct a comparison on six well-established XMLC benchmark datasets from the XMLC repository [6], for which we use the original train and test splits. Statistics of the used datasets can be found in the Appendix. For algorithms listed above, we report results as found in respective papers. Since true propensities are unknown for the benchmark datasets, as trueğ’šis unavailable due to the large label space, for empirical evaluation we model propensities as proposed by Jain et al. [11]: whereğ‘is the number of data points annotated with labelğ‘—in the observed ground truth dataset of sizeğ‘, parametersğ´andğµare speciî€›c for each dataset, andğ¶ = (log ğ‘ âˆ’1)(ğµ +1). We calculate propensity values on train set for each dataset using parameter values recommended in [11]. Values ofğ´andğµare included in Table 1. We evaluate all algorithms with both propensity-scored and standard precision@ğ‘˜. Algorithm 1 PS-PLT.PredictTopLabels(ğ‘‡, ğ», ğ’’, ğ’™, ğ‘˜) Ë†ğ’š = 0, ğ‘= maxğ‘Ë†ğ’š vector to all zeros, ğ‘ , ğ’™) = âˆ’ logË†Ëœğœ‚ (ğ’™, ğ‘Ÿ Ë†ğ‘“ (ğ‘Ÿ, ğ’™) = ğ‘”(ğ‘Ÿ, ğ’™) + log ğ‘âˆ’ log maxğ‘ Table 1: PS-PLTs and PLTs compared to other state-of-the-art algorithms on propensity-scored and standard precision@{1,3,5} [%]. The best result for each measure is in bold. The best result in the group of sub-linear methods (the last 4 methods) is underlined. Table 2: PS-PLT and PLT average CPU train and prediction time compared to other state-of-the-art algorithms. We modiî€›ed the recently introduced napkinXC [13] implementation of PLTs,which obtains state-of-the-art results and uses the Uniform-Cost Search as its inference method. We train binary models in both BR and PLTs using the LIBLINEAR library [10] with ğ¿-regularized logistic regression. For PLTs, we use an ensemble of 3 trees built with the hierarchical 2-means clustering algorithm (with clusters of size 100), popularized by Parabel [18]. Because the tree-building procedure involves randomness, we repeat all PLTs experiments î€›ve times and report the mean performance. We report standard errors along with additional results for popular ğ¿-regularized squared hinge loss and for beam search variant of PS-PLT in the Appendix. The experiments were performed on an Intel Xeon E5-2697 v3 2.6GHz machine with 128GB of memory. The main results of the experimental comparison are presented in Table 1. Propensity-scored BR and PLTs consistently obtain better propensity-scored precision@ğ‘˜. At the same time, they slightly drop the performance on the standard precision@ğ‘˜on four and improve it on two datasets. There is no single method that dominates others on all datasets, but PS-PLTs is the best sub-linear method, achieving best results onğ‘ğ‘ ğ‘@{1,3,5}in this category on î€›ve out of six datasets, at the same time in many cases being competitive to ProXML and PW-DiSMEC that often require orders of magnitude more time for training and prediction than PS-PLT. In Table 2, we show CPU train and test times of PS-PLTs compared to vanilla PLTs, PfasterXML, ProXML and PW-DiSMEC on our hardware (approximated for the last two using a subset of labels). In this work, we demonstrated a simple approach for obtaining Bayes optimal predictions for propensity-scored precision@ğ‘˜, which can be applied to a wide group of probabilistic classiî€›ers. Particularly we introduced an admissible and consistent inference algorithm for probabilistic labels trees, being the underlying model of such methods like Parabel, Bonsai, napkinXC, extremeText, AttentionXML and PECOS. PS-PLTs show signiî€›cant improvement with respect to propensityscored precision@ğ‘˜, achieving state-of-the-art results in the group of algorithms with sub-linear training and prediction times. Furthermore, the introduced approach does not require any retraining of underlining classiî€›ers if the propensities change. Since in realworld applications estimating true propensities may be hard, this property makes our approach suitable for dynamically changing environments, especially if we take into account the fact that many of PLTs-based algorithms can be trained incrementally [12,14,24,25]. Computational experiments have been performed in Poznan Supercomputing and Networking Center.