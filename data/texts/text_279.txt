There is a soaring interest in the news recommendation research scenario due to the information overload. To accurately capture usersâ€™ interests, we propose to model multi-modal features, in addition to the news titles that are widely used in existing works, for news recommendation. Besides, existing research pays little attention to the click decision-making process in designing multi-modal modeling modules. In this work, inspired by the fact that users make their click decisions mostly based on the visual impression they perceive when browsing news, we propose to capture such visual impression information with visual-semantic modeling for news recommendation. Speciî€›cally, we devise the local impression modeling module to simultaneously attend to decomposed details in the impression when understanding the semantic meaning of news title, which could explicitly get close to the process of users reading news. In addition, we inspect the impression from a global view and take structural information, such as the arrangement of diî€erent î€›elds and spatial position of diî€erent words on the impression, into the modeling of multiple modalities. To accommodate the research of visual impression-aware news recommendation, we extend the text-dominated news recommendation dataset MIND by adding snapshot impression images and will release it to nourish the research î€›eld. Extensive comparisons with the state-of-the-art news recommenders along with the in-depth analyses demonstrate the effectiveness of the proposed method and the promising capability of modeling visual impressions for the content-based recommenders. â€¢ Information systems â†’ Recommender systems. ACM Reference Format: Jiahao Xun, Shengyu Zhang, Zhou Zhao, Jieming Zhu, Qi Zhang, Jingjie Li, Xiuqiang He, Xiaofei He, Tat-Seng Chua, Fei Wu. 2021. Why Do We Click: Visual Impression-aware News Recommendation . In Proceedings of the 29th ACM International Conference on Multimedia (MM â€™21), October 20â€“24, 2021, Virtual Event, China. ACM, New York, NY, USA , 10 pages. https://doi.org/10.1145/3474085.3475514 Nowadays, online content sharing platforms have changed the way of people reading news in a mobile and digital manner. News production sources have been extremely enlarged on such platforms, such as Microsoft Newsand Google News, that users could suî€er from information overload due to the overwhelming amount of news. To mitigate information overload and improve user experiences, personalized news recommender systems are devised to make it easy for users to î€›nd the news of their interests. The challenging and open-ended nature of news recommendation lends itself to diverse advances in the literature [18,25,26,30,31,34,38,48]. Recently, Okura et al., [18] learn to represent historically interacted news for a user via a denoising autoencoder and RNNs in the recommender system of Yahoo! JAPAN. Wang et al., [25] learn to obtain multi-level user representations with stacked dilated convolutions. Despite signiî€›cant progress made with these advances, they solely use the textual contents of news titles to represent usersâ€™ interests and ignore the digital newsâ€™s multi-modal nature. As shown in Figure 1, online news might contain a variety of modalities or î€›elds, i.e., title, body, video, soundtrack, image, and category. Thus, we derive inspirations from many other domains [8,9,13â€“15,17,23,41â€“45] and propose to incorporate multi-modal information for an in-depth understanding of usersâ€™ preferences on news. Figure 1: An illustration of impression-aware news recommendation. (a) The interface that users are browsing. (b) Before making click decisions, users typically have the semantic understanding of news title and visual impression in mind. (c) Impression-aware recommendation takes the î€›negrained visual cues and the global structures into account. Recently, advances in other domains and applications have demonstrated great successes of multi-modal recommender systems [3,5, 12,27â€“29,39,46]. For example, Wei et al., [29] propose to model individual user-item interactions for each modality and use graph convolutional networks [11] to learn modality-speciî€›c representations. Following this work, Wei et al., [28] propose to reî€›ne the user-item graph connections for each modality and thus leverage modality-speciî€›c network structures, which also helps mitigate the implicit feedbacks. Zhao et al., [46] propose to learn multi-modal heterogeneous network representation and incorporate user proî€›les, social relationships, textual description, and video posters for video recommendation. Despite their successes on real-world datasets, we argue that these methods have two major deî€›ciencies. Firstly, they typically introduce all available modalities without evaluating and explaining which modalities are essential for clickthrough-rate (CTR) prediction. However, introducing more features would not necessarily mean being more eî€ective since such features might lead to expensive computation, more over-î€›tting, and even more noise. Secondly, most of them tend to leverage modalities with generic architectures with less recommendation-speciî€›c or application-speciî€›c designs. Towards this end, we propose to investigate which modalities we should incorporate for news recommendation and design fusion modules with highly application-speciî€›c insights. We aim to answer the question, "why do users click" and start with the perspective that the userâ€™s click decision is mostly based on his/her inherent interest and the visual impression delivered by the news. The visual impression can be the visual-semantic information he/she perceives when browsing the news application. In this paper, we treat the visual region of the news displayed on the user interface of news applications as the visual impression (as shown in Figure 1). Therefore, our work aims to model such multi-modal visual impression information to improve the click-through-rate prediction. We contend that other modalities or î€›elds such as news body and soundtrack are inaccessible before users clicking the news. Recommender systems might draw false conclusions when spuriously connecting these modalities to usersâ€™ interests. Furthermore, we leverage the layout information such as relative positions, relative sizes, and styles as guidance for multi-modal fusion, i.e., a news recommendationspeciî€›c design. To be speciî€›c, we devise theIMpression-aware multi-modal newsRecommendation framework, denoted as IMRec. IMRec comprises two key components: (1) The global impression module that not only fuses the multi-modal content features under the guidance of news layout but also enhances the global item representations. (2) The local impression module that models the correlation of each title word and other impression units, such as visual title words and visual images. In this way, our model bridges the gap between semantic understanding and visual impressions for each news in a î€›ne-grained manner. To the best of our knowledge, this work is one of the initiatives to investigate impression-aware recommendation, and there is currently no news recommendation dataset suitable for this research. To this end, we construct a large-scale impression-aware news recommendation dataset, IM-MIND, by adding the snapshot impression images into a text-dominated benchmark, MIND [36]. We conduct in-depth experimental analyses along on both quantitative and qualitative results, which have demonstrated the eî€ectiveness and necessity of modeling visual impressions for news recommendation. The highlights of this work are summarized as follows: â€¢We discuss why users click a news article at an intuitive level and propose to investigate impression-aware news recommendation, which guides the modality selection and model design better for click-through-rate prediction. â€¢We propose the IMRec framework that comprehensively exploits the visual impression features in a global-local manner and bridges the gap between the semantic meaning of news title and the visual impression users perceive before clicking. â€¢We contribute the visual images of news impressions to MIND dataset to facilitate this line of research and demonstrate the eî€ectiveness of IMRec framework with extensive experiments. Noteworthy, for ease of modeling, we currently model the visual impression of each news independently because the surrounding news articles can only be obtained after they are î€›nally ranked and displayed to the users. During training and inference, we could simulate the visual impression of each news through the software of UI interface. In recent years, the explosively growing amount of digital news calls for eî€ective news recommender systems which enable personalized news suggestions. Both natural language processing and data mining research î€›elds [20,32,47] have witnessed deep learning based modelsâ€™ successes in extracting semantic content features and mining user preferences accordingly [1,7,18,25,26,30,31,33, 35,48]. Diverse models concerning RNNs [18], attention mechanisms [30,31,48], dilated convolution [25], graph neural networks [7], and knowledge distillation [26] are explored. Typically, Wu et al., [34] leverage both self-attention mechanisms [24] and additive attention [4] to represent words with one news and multiple news with the userâ€™s historical interactions. FIM is a state-of-the-art recommendation model proposed by Wang et al., [25] that captures î€›ne-grained interest matching signals using dilated convolutions. However, most of these works solely model the news title and disregard other modalities that might highly contribute to userâ€™s click behavior, such as the news cover image. Towards this end, we propose to incorporate necessary modalities and design news recommendation-speciî€›c architectures. Online content sharing platforms are becoming rich in modalities due to the rapidly developing network communication technologies. Therefore, as a nascent research î€›eld, multi-modal recommendation attracts increasing attention [3,39,40] recently with applications in various domains such as music recommendation [12], location recommendation [27], movie recommendation [46], micro-video recommendation [28,29], and fashion recommendation [5]. Noteworthy, MMGCN [29] and GRCN [28] construct a user-item bipartite graph and conduct information propagation and embedding learning for each modality. GRCN diî€ers from MMGCN by reî€›ning each graphâ€™s connections and denoising implicit feedback in the î€›ne-grained modality level. Despite great successes, we argue that most of them model multiple modalities without domain knowledge guidance. To be speciî€›c, most architectures disregard a fundamental question, "why do users click" and fail to model the impression which can be necessary for usersâ€™ decision making. In this paper, we propose the impression-aware recommendation framework designed especially for news recommendation by explicitly modeling usersâ€™ click decision-making processes. Following the common practice in modern news recommender systems [31,34], we formulate news recommendation as a sequential recommendation problem and speciî€›cally focus on the news click-through-rate (CTR) prediction. We useğ‘¢to denote one user andI= {ğ‘’}to denote the sequence of news historically clicked by userğ‘¢on an online news platform.Iis ordered by the click time beforehand. News CTR prediction aims to predict whether the userğ‘¢will click a candidate newsğ‘with the binary label denoted asğ‘¦âˆˆ {0,1}. A deep learning based news recommendation modelğ‘“ (Â·)takes a pair of userğ‘¢and candidate news ğ‘as input and predicts a probabilityË†ğ‘¦ = ğ‘“ (ğ‘¢, ğ‘)indicating how likely the click will happen. During testing and serving, candidate news will be ranked by the probabilities and displayed on the news platform with positions consistent with the ranks. To explicitly model usersâ€™ click decision-making processes (depicted in Figure 1) and bridge the gap between semantic understanding Figure 2: Schematic illustration of impression-aware news recommendation framework applied to NRMS. We represent each news using local impression modeling, which explicitly captures multi-modal visual cues within the visual impression and accordingly enhances the semantic understanding of news title, and global impression modeling, that models the visual impression as whole by further taking the arrangement of diî€erent î€›elds and relative position of title words into consideration. and visual impression of news, we devise the impression-aware recommendation framework, denoted as IMRec. As depicted in Figure 2, NRMS with IMRec framework (denoted asNRMS-IM) incorporates the local details of visual impression into the semantic understanding of news titles, which is inspired by the usersâ€™ browsing processes that users not only read the meaning of titles but meanwhile receive many impression details such as the visual appearance of words and regions in the news cover image. We denote such a process aslocal impression modeling. Moreover, once users have captured all the details, they might attempt to construct a holistic recognition of the news, in which we incorporate the fused representation of all modalities to enhance the details further. We denote the holistic impression modeling asglobal impression modeling, which introduces more structural information from a global view, such as the alignment of diî€erent î€›elds and the relative position of words. In the following sections, we will formally describe these two processes based on the sequence model NRMS. 3.2.1 Local Impression Modeling. Local impression modeling aims to simultaneously capture the local impression details while understanding the semantic meaning of news titles. Towards this end, we encapsulate an impression decomposition process that explicitly extracts meaningful cues from the impression image beforehand, and an impression-semantic reasoning module that bridges the modality gap and structural gap between impression and semantics. Impression Decomposition.To ease the modeling of local details in the impression image, we propose to extract meaningful cues in a pre-processing manner, which obtains analogous gains observed in many other domains [2,16]. However, diî€erent from previous works that commonly employ an object detector [21], which is designed to process natural scenes, we propose to î€›rst divide the impression image into several salient parts and extract cues from the corresponding feature maps. Since the impression image is well structured, we obtain the news title part, the news cover image part, and the news category part with simple edge detection techniques. For the newstitlepart, we view each word regionğ‘¤as an individual cue that users can potentially attend when understanding the semantic meaning of titles. We denote the vectorial representation of word regionğ‘¤asw. For the news cover imagepart, we view each region vectorvin the feature mapV = {v}extracted by a pre-trained CNN as a cue. For the newscategorypart, we directly view the whole region ğ‘with its vectorial representationaas a cue. The details of the pre-processing and used pre-trained architectures can be found in Section 5.1. To ease modeling, we group all the pre-extracted cues together to construct a impression cue memoryO = {o}. Since the representations of diî€erent cues are obtained using the same feature extractor, they naturally belong to the same embedding hypersphere and we treat them equally in the following modeling. Impression-Semantic Reasoning.Given one news titleğ‘’, comprised of a sequence of words{ğ‘¤}, we î€›rst embedding the sequence into a low-dimensional representationE = {w}. To capture the correlations between impression and semantics, we view the impression cues as the external knowledge and follow the memory network schema [22]: where ğ‘(Â·), ğ‘˜(Â·), and ğ‘£(Â·) denote linear transformations with bias terms.ğ›¼denotes the extent to which the user will attend to impression cueğ‘œwhen reading the wordğ‘¤. Summing all attended cues results to the linearly transformed word embeddingğ‘£ (w)in an impression-aware word representationË†w. To further reason on the impression-semantic joint representations, we next leverage the semantic dependencies implied by the self-attended weights: whereğ‘(Â·)andğ‘˜ (Â·)are linear transformations andğ›¼denotes the extent to which the model attends to the impression-semantic representation of theğ‘—th word to enhance the î€›nal representation of theğ‘–th word. The holistic representation of one news is obtained by summing all words with additive attention weights [4]: whereğ‘˜transformswinto a hidden space andğ‘computes the attention weights for aggregation. 3.2.2 Global Impression Modeling. Local impression modeling captures impression cues separately, which means we disregard the correlations and interactions between diî€erent impression cues. A straightforward way is to model them using traditional multimodal fusion techniques. However, directly employing oî€-the-shelf techniques might lead to the loss of structural information, such as the location arrangement of diî€erent î€›elds and the spatial position of diî€erent words. Therefore, instead of fusing diî€erent cues separately, we propose to encode the impression image as a whole with pre-trained extractors. Given the global impression embeddingo, we have: whereğ‘”(Â·)is a linear transformation,ğœdenotes the sigmoid function, andğ‘serves as a gate to control how much information we should let through fromeando, by taking their information into consideration. Such a gate is reasonable in the sense that users might not be equally interested in the impression and the textual semantics, and ğ‘ indicates a tradeoî€ between these two factors. For the user encoder, we employ the oî€-the-shelf sequence modeling tool, i.e., the self-attention mechanisms, to capture the correlations between diî€erent news historically clicked by the user. This can be formally formulated as: whereğ‘,ğ‘˜, andğ‘£denote linear transformations. In practice, we take multi-head self-attention for better performance and concatenate the outputs of multiple heads. Similarly, We obtain the î€›nal user representationğ‘¢by aggregating all enhanced item representations with additive attention weights: Given one candidate newsğ‘which we should predict how likely an userğ‘¢will click it, we î€›rst transform them into dense vectors canduusing IMRec, and treatË†ğ‘¦= ğœ (cu)as the indicator and ğ‘¦as the expected output. Motivated by Wu et al., [34] and Wang et al., [25], we use negative sampling techniques and cross entropy loss for model training: whereğ‘ƒis the number of positive training samples,ğ¾is the number of negative training samples for each positive sample, andğ‘ means theğ‘˜-th negative sample in the same group with theğ‘–-th positive sample. Noteworthy, the local impression modeling and global impression modeling modules can bemodel-agnosticand applied to any other CTR prediction model with ease. In the experiment , we extend another SOTA method, i.e., FIM [25], a non-sequence model that employs dilated CNN and computes the matching between each historically interacted news and the target news in a î€›ne-grained level, to the impression-aware version, i.e.,FIM-IM. Thereby, we can demonstrate the plug-and-play capability of the proposed modules. Speciî€›cally, in the FIM-IM model, only the memory network schema in the local impression module is applied to the initial word embeddings due to the high computation cost of FIM. For global impression modeling, we linearly transform global impression features into low-dimensional representations, based on which we directly compute the matching scores of each historical interacted news and the candidate news. Such matching scores are concatenated with the last layer output before the prediction. Given the integrated matching vectorsof a userğ‘¢and candidate newsğ‘ pair, and the corresponding global impression representationoof the news ğ‘, we can calculate the î€›nal click probability as follows: whereğ‘„,ğ‘„,ğ‘andğ‘are learnable parameters,âŠ•means concat operation. The loss function is consistent with the NRMS-IM model. Figure 3: Sampled cases of news visual impression. To the best of our knowledge, there is no news recommendation dataset suitable for impression-aware news recommendation. Therefore, we construct two benchmark datasetsbased on the MINDNews dataset [36] automatically, following the styles, sizes, and spatial arrangement of diî€erent î€›elds according to the visual impression presented in [36] and the HTML code of the Microsoft news platform. To extract news impressions, we have crawled the cover image from the given news URL and then combined news images with texts (title and category) to generate news visual impressions, as shown in î€›gure 3. The size of our news cards is 615*195px with white background. All news images were resized to 200*165px and were pasted at the location (15, 15) to (215, 180). News title starts at the location (215, 180) of background with 10.5px line spacing. The title lines of each news are no more than 3. Words in each line are no more than 27 characters. Font and size of title words areseguisband 27, respectively. The news category starts at the location (227.725, 142.5) of background. Font and size of the category are segoeui and 24, respectively. Especially if the news does not have an image or its URL is unavailable, the image area would be empty. Since the MIND-news dataset contains large and small versions, we generate visual impressions and construct the IM-MIND-Large and IM-MIND-Small accordingly. The detailed statistics of the IM-MIND-Small and IM-MIND-Large dataset are summarized in table 1. We useNewsto denote the Table 1: Statistics of IM-MIND-Small and IM-MIND-Large. news that contains the cover image. The whole datatset contains 876956 users and 130379 news articles. There are 54421 available news images among all the news. The number of avg./max./min./med. clicked news are 17.35, 801, 0 and 10. The number of avg./max./min./med. clicked news with the image are 6.54, 356, 0, and 4. The avg./max. lines of title are 2.76 and 3. The avg./max./min. words per title line are 4.12, 15, and 0 (punctuation only). We analyze IMRec framework and demonstrate its eî€ectiveness by anwering the following research questions: â€¢ RQ1: How does IMRec perform compared with existing state-ofthe-art news recommender systems? â€¢ RQ2: Does global/local impression modeling all contribute to the eî€ectiveness of base models in a model-agnostic manner? â€¢ RQ3: How does IMRec perform in practical news recommendation scenarios (e.g., cold-start setting, unseen users)? â€¢ RQ4: How does IMRec improve the performance internally? Implementation Details.The word embeddings are 100dimensional and initialized using pre-trained Glove embedding vectors [19]. We use pretrained resnet101[6] to extract local and global visual impression features. Speciî€›cally, for the visual word impression, we remove the last two layers of resnet101 and obtain the feature map with size (512, 28, 28) with solely the title region as input. We vertically divide the feature map by the lines of titles and further horizontally divide the vertically divided feature maps to have the word feature map by the length of words. We meanpool the word feature map to have the impression feature of each word with dimension 512. For the cover image impression, we use the same pipeline except that we equally divide the feature map into 9 regions and mean-pool each region feature map to have a region feature. Forglobal impression, we remove the last layer of resnet101 and obtain a 2048 dimensional tensor representing the global impression of the whole news card. The negative sampling ratio K is set to 4. Adam [10] is used as the optimizer, the batch size is 32, and the initial learning rate is 1e-4. These hyper-parameters are applied for both NRMS-IM and FIM-IM. â€¢ NRMS-IM.Self-attention networks have 3 heads, and the output dimension of each head is 50. The dimension of the additive attention query vectors is 200. The maximum length of the tokenized Table 3: Ablation studies by selectively discarding the local impression modeling module (- L_IM) and global impression modeling module (- G_IM). We study both NRMS-IM and FIM-IM to reveal the modal-agnostic capability of the proposed modules. word sequence of news title is set to 15. At most 60 browsed news are kept to construct the userâ€™s recently reading behaviors. â€¢ FIM-IM.The maximum length of the tokenized word sequence of news title is set to 30, and at most 50 browsed news are kept for representing the userâ€™s recently reading behaviors. Other hyper-parameter settings are following the original paper [25]. Evaluation Criteria.Following [31], we employ three widely used metrics for evaluation, i.e., AUC (Area Under the ROC Curve), NDCG (Normalized Discounted Cumulative Gain), and MRR (Mean Reciprocal Rank). Comparison Baseline Methods.For a comprehensive comparison to NRMS-IM and FIM-IM, we incorporate state-of-the-art baseline methods concerning both manual feature-based approaches and neural recommendation ones: â€¢ DeepFM[37]. DeepFM parallelly combines deep neural network and factorization machine. We implement it using the same feature as the LibFM. â€¢ DKN[26]. DKN leverages entity embeddings from knowledge graphs as external knowledge for news recommendation. â€¢ NPA[31]. NPA uses user ID embeddings to weight each word/news and thus captures important features. â€¢ LSTUR[1]. LSTUR takes the topic/subtopic as input of news encoder and uses GRU to fuse interacted news and the user embedding. â€¢ NRMS[30]. NRMS uses the multi-head self-attention to encode both news and users. â€¢ FIM[25]. FIM employs dilated CNN and computes the matching between each historically interacted news and the target news in a î€›ne-grained level. Table 2 lists the comparison results of NRMS-IM and FIM-IM with state-of-the-art neural recommendation methods on the MINDSmall and MIND-Large datasets. From the results, we can î€›nd that: â€¢Overall, the results across multiple evaluation metrics consistently indicate that NRMS-IM and FIM-IMbothachieve better results against various SOTA designs. We note that these improvements are signiî€›cant and comparative to the improvement of recent SOTAs (e.g., FIM over NRMS). â€¢Surprisingly, DeepFM achieves competitive performance on the MIND-Small dataset and outperforms many advanced designs like LSTUR with GRU and NPA with the attention mechanism. However, it achieves signiî€›cantly inferior results on the largescale MIND-Large dataset. The reason might be that FM based methods might fail to handle highly sparse and complex correlations. In contrast, NRMS-IM and FIM-IM achieve consistently convincing results on two datasets. â€¢Compared to DKN that also exploits additional information (i.e., entities in a knowledge graph) to enhance news representation learning. FIM-IM shows a clear advantage over it on two datasets. Noteworthy, FIM-IM improves DKN by AUC +0.0371 (relatively 5.9%), NDCG@5 +0.0427 (relatively 13.7%), NDCG@10 +0.0405 (relatively 10.8%) and MRR +0.0362 (relatively 12.8%) on the MIND-Small dataset. These results show that, compared to further enhancing the semantic understanding itself like DKN, it might be more promising to introduce visual impressions that explicitly get close to the userâ€™s click decision process. â€¢Compared to other attention-based approaches, i.e., NPA and NRMS, NRMS-IM also exhibits better performance, especially on the large-scale MIND-Large dataset. These results basically indicate that modeling the semantic-impression correlations (memory attending) can help improve the semantic-semantic correlation modeling. â€¢FIM-IM considers a strong baseline FIM that uses CNN as building blocks and further equips it with impression modeling modules. FIM-IM achieves state-of-the-art results with substantial Table 4: Analysis on diî€erent user/news groups. IMRe c framework shows consistent improvement across various scenarios. All NewsNRMS-IM 0.6866 0.3688 0.4317 0.3305 0.6901 0.3684 0.4317 0.3298 0.6630 0.3712 0.4317 0.3353 NewsNRMS-IM 0.6984 0.4273 0.4869 0.3812 0.7010 0.4246 0.4880 0.3816 0.6794 0.4175 0.4785 0.3785 NewsNRMS-IM 0.6724 0.4819 0.5379 0.4208 0.6764 0.4814 0.5380 0.4203 0.6443 0.4859 0.5374 0.4246 Table 5: Performance on NRMS-IM by varying the percentage of visual impression used in training. Percentage AUC NDCG@5 NDCG@10 MRR improvement, demonstrating that the proposed local/global impression modeling can improve a ranking baseline with arbitrary architectures in a plug-and-play manner. 5.3.1 Analysis on key building blocks (Ablation Study). Local impression modeling and global impression modeling are two key components of IMRec framework. We conduct the ablation study on them to reveal the eî€œcacy of the architectures and the beneî€›ts of incorporating local/global impression information. Speciî€›cally, we selectively discard the local impression modeling module and the global impression modeling module from NRMS-IM to generate ablation architectures, i.e., - L_IM, and - G_IM, respectively. We also conduct another ablation study on the FIM-IM model to show the model-agnostic capability of these two modules. The results are shown in Table 3. We can observe that: â€¢Removing either L_IM or G_IM leads to performance degradation, and removing both modules (i.e., the base model) leads to the worst performance. These results demonstrate the eî€ectiveness of the proposed two modules as well as the beneî€›ts of introducing visual impressions for news recommendation. We attribute this superiority to the fact that we can explicitly get close to the click decision-making process by modeling the interactions of visual impression and semantic understanding of news titles. â€¢Removing G_IM leads to more performance drops than removing L_IM. This means that directly modeling the visual cues in the impression image without capturing these cuesâ€™ spatial arrangement might be inferior to visual impression modeling. â€¢The results are consistent across diî€erent baselines, which demonstrates that the proposed two modules can easily boost a recommendation model in a plug-and-play and model-agnostic manner. 5.3.2 Analysis on diî€›erent user/news groups. In real-world news recommendation platforms, there are always unseen users beyond the training set and news without cover images. To reveal the eî€ectiveness of impression modeling on diî€erent recommendation scenarios, we take an in-depth analysis of these two factors. The results are shown in Table 4. For brevity, we useNewsto denote the news that contains the cover image andNewsto denote the news without the cover image. We can see that: â€¢We observe a consistent improvement of NRMS-IM/FIM-IM over the base model across various scenarios, which further demonstrates the eî€ectiveness of IMRec framework and especially the generalization capability across diî€erent settings. â€¢The results of the two models on unseen users are worse than those on seen users, which is a reasonable result. However, we notice that the improvement of NRMS-IM over the NRMS model onunseenusers is consistently more signiî€›cant than the other user group. For example, NRMS-IM achieves 0.0155 (relatively 4.36%) NDCG@5 improvement on unseen users (All News setting) and 0.0107 (relatively 2.99%) NDCG@5 improvement on seen user (All News setting). Since incorporating multi-modal content is essential for cold-start setting (unseen users), these results show that the introduction of visual impression is a promising direction for news recommendation. â€¢Both two models yield better results on the newsthan on the news. We attribute this phenomenon to the fact that users might click the newsand eventually î€›nd that the news turns out to be less attractive. In other words, click behaviors on the newsare less noisy than the news, and usersâ€™ interests in the newsare more consistent and easy to capture. Surprisingly, the improvement of impression modeling on the newsis more signiî€›cant than the others. Considering that the interests in the newsare generally harder to capture by only using title texts, the results indicate the advantage of impression modeling in dealing with such news (e.g., the visual words might help attract usersâ€™ attention). Overall, impression modeling yields improvement on all news. 5.3.3 Analysis on the percentage of visual impressions used in training. For this experiment, we disregard the visual impressions of randomly sampled 25%, 50%, and 75% news in training. In other words, news with visual impressions being masked will be represented solely by the semantic meaning of its title. We conduct the experiment on the IM-MIND-Large dataset with NRMS-IM. As shown in Table 5, the metrics grow monotonically as the percentage of news with visual impression increases, which suggests that IMRec framework boosts the performance of the base model by eî€ectively modeling the visual impression. The above analysis quantitatively shows the eî€ectiveness of impressionaware news recommendation. We take a further step to reveal how IMRec framework internally improves the performance of semanticonly news recommendation systems. As shown in Figure 4, we plot the memory attending weights of each impression word on each textual semantic word in the local impression module, which explicitly indicates the semantic-impression correlations in the î€›ne-grained level. We note that the cases are sampled from the IM-MIND-Large dataset and are unseen by NRMS-IM during training. Since news with the cover image will intuitively enhance the semantic representation by providing an additional modality, we disregard them here and are more interested in the impression words, which are harder to leverage. Based on the visualization, we can î€›nd that: â€¢Impression words that are at the left beginning of each line (e.g., reviewin the second case,lockin the third case) obtain more attention weights than the others typically. This î€›nding is intuitive, as users read the impression title in a left-to-right manner. IMRec framework automatically captures such a visual correlation pattern and accordingly enhances the semantic word representation. â€¢Semantic words are more likely to attend the impression words that are spatially closer to the corresponding impression words. For example, in the third case, semantic wordsRichardsonand takeboth attend to the impression wordholidays, which are all at the beginning of lines. This result further demonstrates that IMRec framework captures the impression cues besides the sequential dependencies in semantics. â€¢A few impression words obtain the most attention, showing that IMRec framework succeeds in capturing the critical points in the impression rather than roughly attending to all impression cues. In this work, we investigate usersâ€™ decision-making process when browsing and clicking news, and propose the visual impressionaware modeling framework, i.e., IMRec, for multi-modal news recommendation. IMRec explicitly gets close to the usersâ€™ news reading process and simultaneously attends to local details within the impression when understanding the news title. Furthermore, IMRec fuses the multi-modal local details by considering the global arrangement of them on the impression. We contribute visual images of news impressions to MIND dataset to promote this line of research. Extensive experiments demonstrate the eî€œcacy of IMRec in that both NRMS-IM and FIM-IM achieve better results against various state-of-the-art designs. To the best of our knowledge, the work is one of the initiatives to incorporate visual impressions for news recommendation. By modeling the visual impressions, we might attempt to safely disregard unnecessary modalities that are absent before users clicking the news and design application-speciî€›c modules for usersâ€™ interest mining. We believe that this idea can be inspirational for other researchers and will open up a promising direction for recommendation. We disregard more complex impression modeling designs in this paper to fairly show that introducing visual impression itself will bring many beneî€›ts. Incorporating more advanced techniques to boost performance can be a promising future work. Moreover, in many other recommendation domains, few works investigate usersâ€™ click decision-making process, hence we plan to extend our idea in these domains in the future. This work was supported in part by the National Key R&D Program of China under Grant No.2020YFC0832505, National Natural Science Foundation of China under Grant No.61836002, No.62072397 and Zhejiang Natural Science Foundation under Grant No.LR19F020006.