Sequential recommendation methods play an important role in real-world recommender systems. These systems are able to catch user preferences by taking advantage of historical records and then performing recommendations. Contrastive learning(CL) is a cuttingedge technology that can assist us in obtaining informative user representations, but these CL-based models need subtle negative sampling strategies, tedious data augmentation methods, and heavy hyper-parameters tuning work. In this paper, we introduce another way to generate better user representations and recommend more attractive items to users. Particularly, we put forward an eî€ective ConsistencyConstraint for sequentialRecommendation(C-Rec) in which only two extra training objectives are used without any structural modiî€›cations and data augmentation strategies. Substantial experiments have been conducted on three benchmark datasets and one real industrial dataset, which proves that our proposed method outperforms SOTA models substantially. Furthermore, our method needs much less training time than those CL-based models. Online AB-test on real-world recommendation systems also achieves 10.141% improvement on the click-through rate and 10.541% increase on the average click number per capita. The code is available at https://github.com/zhengrongqin/C2-Rec. â€¢ Information systems â†’ Recommender systems. Recommender Systems, Sequential Recommendation, Consistency Training ACM Reference Format: Chong Liu, Xiaoyang Liuand Rongqin Zheng, Lixin Zhang, Xiaobo Liang, Juntao Li, Lijun Wu, Min Zhang, Leyu Lin. 2022. C-Rec: An Eî€ective Consistency Constraint for Sequential Recommendation. In Lyon â€™22: The ACM Web Conference, April 25â€“29, 2022, Lyon, France. ACM, New York, NY, USA, 12 pages. https://doi.org/10.1145/1122445.1122456 Recommendation systems have been extensively applied in online platforms nowadays, e.g., Amazon [30], Google [2,5,20,23], Alibaba [53] and Tencent [11]. Due to the dynamic interactions between users and items, it is essential to capture evolving user interests from usersâ€™ historical records. To accurately represent user interests and make an appropriate recommendation, many eî€orts have been paid to study sequential recommendation methods [16,25,45,50]. Generally, the sequential recommendation task aims to characterize user representation from usersâ€™ historical behaviors and predict the expected item accordingly. In viewing the great success of deep learning for sequential dependency modeling and representation learning, many methods based on deep neural networks He et al. [14]have been introduced and proposed to solve this task, covering RNN-Based frameworks [16,32,44], diî€erent CNNs blocks and structures [45,46,58], Graph Neural Networks (GNNs) [38,50,51], and also model variants [25,28,42] relied on the powerful multi-head self-attention [47]. Though performing well, these methods might suî€er from the data sparsity problem [25,41,65] for sequential recommendation, especially for models built on the multi-head self-attention mechanism, where only one single item prediction loss is used to optimize the full model parameters for capturing all possible correlations in input interaction sequences. To address the above challenge, various self-supervised learning strategies are introduced [55,64]. Among these, the recently introduced contrastive learning (CL) objective [53] achieves very promising results. Through combing with eî€ective data augmentation strategies and cooperating with the vanilla sequential prediction objective, the CL-based method can learn better sequence-level user representations and enhance the performance of sequential recommendations. However, the eî€ectiveness of CL-based approaches is subject to the correlated data augmentation methods, pretext tasks, eî€œcient negative sampling, and hyper-parameters selection (e.g., the temperature in NCE and InfoNCE losses) [21]. To mitigate the above drawbacks, many eî€orts have been made to simplify contrastive learning. Gao et al. [9]propose a very simple yet effective contrastive learning scheme that utilizes dropout noise as data augmentation to construct high-quality positive samples for sequence-level representation learning. Although such a scheme is eî€ective for unsupervised representation learning, adapting it into the sequential recommendation task will still encounter the dilemma in which a higher proportion of CL objective in model training will lead to better representations that can easily distinguish positive and negative samples but might result in worse item prediction performance and vice versa. In other words, there is a noticeable gap between the discrimination of positive and negative samples and the task objective for the sequential recommendation. Thus, the key to obtaining better user representations mainly for the item prediction task is designing more training strategies that can address the inherent issues of sequential recommendation models. Inspired by the recent observation on the multi-head attention model that a very simple regularization strategy imposed on the output space of supervised tasks yields striking performance improvement [29] (achieving SOTA on many challenging tasks), we propose to thoroughly explore the eî€ect of consistency training for the sequential recommendation task. We î€›rst introduce the simple bidirectional KL divergence regularization into the output space to constrain the inconsistency between two forward passes with diî€erent dropouts. Unlike previous machine translation, summarization, and natural language understanding tasks, we argue that the introduced consistency regularization merely in the output space is not enough for the data sparsity setting of sequential recommendation. We then design a novel and simple regularization objective in the representation space. Unlike previous studies that utilize cosine [9] and L2 [36,66] distance to regularize the representation space, we propose to regularize the distributed probability of each user representation over others. Thus, we can extend and leverage the eî€ective bidirectional KL loss to regularize the representation inconsistency. Experiments on three public benchmarks and one newly collected large-scale corpus indicate that our proposed simple consistency training for sequential recommendation (CT4Rec) outperforms state-of-the-art methods based on contrastive learning by a large margin. Extensive experiments further prove that our proposed consistency training can be easily extended to the data side (except for model dropout) and can leverage ubiquitous consistency signals for the sequential recommendation tasks, e.g., the clicked items by the same user should share some attributes. The Online A/B test also conî€›rms the eî€ectiveness of our method. In a nutshell, we mainly have the following contributions: â€¢We propose a simple (with only two bidirectional KL losses) yet very eî€ective consistency training method for sequential recommendation systems. To the best of our knowledge, this is the î€›rst work to thoroughly study the eî€ect of consistency training from diî€erent perspectives and with a uniî€›ed training objective for the sequential recommendation task. â€¢Our proposed consistency training method can be easily extended to other inconsistency scenarios, e.g., data augmentation. It can also leverage more task-speciî€›c consistency. â€¢Extensive experiments on four oî€Ÿine datasets show the eî€ectiveness of our proposed CT4Rec over SOTA models based on contrastive learning with much better performance and faster convergence time. The Online A/B test also shows signiî€›cant improvement over the strong ensemble model. Early sequential recommendation (SR) methods are usually based on Markov Chain (MC), including adopting î€›rst-order MC [40] and high-order MCs [12,13]. As for current deep learning approaches, they can be generally divided into four categories, i.e., RNN-based methods. Concretely, GRU4Rec [16] applies RNN to SR and many variants have been proposed based on GRU4Rec by adding data augmentation GRU4Rec+ [44], hierarchical RNN [39], attention module [3,27,34], ranking losses [15], user-based gated units [7] and contextual information [1]. However, RNN-based methods usually exhibit worse performance than CNN-based and attention-based methods for the data sparsity setting. Caser [45] proposes a convolutional sequence embedding recommendation model, and Tuan and Phuong[46]use 3-dimensional convolutional neural networks to achieve character-level encoding of input data. NextItNet [58] uses dilated convolutions to model long-range item dependence and outperforms GRU4Rec. Meanwhile, attention mechanisms [47] is used to model user behavior sequences and have achieved outstanding performance [8,24,25,28,33,42]. Besides, many researches [38,50,51] combine attention mechanisms and GNNs to solve the SR task. Memory networks [4,19], disentangled representation learning [35], data augmentation Guo et al. [10], Yao et al. [54], transfer learning Zhang et al. [61]and session-based Hu et al. [18] models are also utilized to improve the performance of SR. Recently, the combination of contrastive learning and attention mechanisms [53,64] is widely used in SR and achieves great success. CauseRec [60] performs contrastive learning by contrasting the counterfactual with the observational. StackRec [49] utilizes stacking and î€›ne-tuning to develop a deep but easy-to-train SR model. ICAI-SR [59] focuses on the complex relations between items and categorical attributes in SR. SINE [43] can model multiple interest embeddings for the next-item prediction task. Unlike previous researches, we only introduce two consistency training objectives in the representation and output spaces without any structure modiî€›cations, extra data, and heuristic patterns from tasks. Though extremely simple, our method outperforms recent SOTA models by a large margin and is more eî€œcient for model training. There are only a few related researches in the î€›elds of natural language process [9] and machine learning [29,36,66]. Speciî€›cally, Gao et al. [9]introduce dropout as the alternative of data augmentation into contrasting learning for sequence representation learning while we focus on the inconsistency introduced by dropout in the data sparsity SR task. Our proposed method is also extremely simple compared with the paradigm of combining contrastive learning with the traditional item prediction objective. Ma et al. [36], Zolna et al. [66]mainly focus on the gap between training and testing and utilize L2 for regularizing the representation space, which is less eî€ective in the data sparsity setting, represented by the marginal to none performance improvements in Section 5. Diî€erent from introducing a regularization objective in the output space to constrain the randomness of sub-models brought by dropout [29], we focus on the consistency training of the data sparsity SR task from both the representation and output space. We also propose a simple yet eî€ective regularization strategy in the representation space to compensate and align the output space consistency loss. The overall structure of our model is illustrated in Figure 1. Before elaborating our propose CT4Rec, we î€›rst present some necessary notations to describe the sequential item prediction task. LetU = (ğ‘¢, ğ‘¢, ..., ğ‘¢)denotes a set of users, andV = (ğ‘£, ğ‘£, ..., ğ‘£) denotes a set of items. The sequence for userğ‘¢ âˆˆ Uis denoted as Figure 1: Model structure of CT4Rec. It takes user click sequences as input and outputs user representations for item retrieval in the matching stage of recommendation. The input sequences are transforme d into vector representations via the embedding layer and then encoded by N transformers with diî€erent hidden dropout masks. In addition, Distributed Regularization Loss and Regularized Dropout Loss are introduced to restrain these representations generated by diî€erent dropout masks. ğ‘ = (ğ‘£, ğ‘£, ..., ğ‘£, ..., ğ‘£), whereğ‘£âˆˆ Vis the item that userğ‘¢interacts at time stepğ‘¡and|ğ‘ |is the length of sequence ğ‘ . Given the historical sequenceğ‘ , the task of sequential recommendation is to predict the probability of all alternative items to be interacted by user ğ‘¢ at time step |ğ‘ | + 1, which is formulated as: Since our proposed consistency training method does not involve structural modiî€›cation and extra data utilization, we apply it to the widely used SASRec model [25]. Following the original setting in SASRec, the transformer encoder contains three parts, i.e., an embedding layer, the stacked multi-head self-attention blocks, and a prediction layer. Then, we can obtain user representationğ’”= ğ‘“ (ğ‘ ), where ğ‘“ (Â·) indicates the transformer encoder. To learn the relation between users and items in sequential recommendation, a similarity function, e.g., inner product, is applied to measure distances between user representation and item representation. Thus, for user representation ğ’”of user ğ‘¢ at time step ğ‘¡, we can get a similarity distributionP(ğ’”) = P(ğ‘£|ğ’”)to predict the item that userğ‘¢will interact at time stepğ‘¡ +1. Then, the basic loss function with positive item ğ’—and randomly sampled negative items ğ’—âˆˆ Vis denoted as: P(ğ’”; ğœ”) =ğ‘’ğ‘¥ğ‘ (ğ’”ğ’—) +Ãğ‘’ğ‘¥ğ‘ (ğ’”ğ’—)(2) where ğœ” refers to all trainable parameters of the model. Inspired by recent studies on dropout [29], we enhance the user representation from the perspective of reducing the model inconsistency and gap between training and testing. Concretely, we forward twice with diî€erent dropouts and learn the consistency between these two representations for each user, i.e., each user interaction sequencespassing the forward network twice and obtain two representations ğ’”and ğ’”. Regularized Dropout Loss (RD).Considering two representationsğ’”andğ’”for user u, as mentioned in Function (2), we can get two similarity distributionsP(ğ’”;ğœ”)andP(ğ’”;ğœ”). Following R-Dropout[29], we introduce a bidirectional KL-divergence loss to regularize the above two distributions, written by: L(ğ’”; ğœ”) =12(D(P (ğ’”; ğœ”)||P(ğ’”; ğœ”)) Distributed Regularization Loss (DR).To better regularize the representation space, we propose a distributed regularization method in which each user is represented by its correlations with other users rather than directly utilizing user representations for consistency regularization. In this paper, we compare users in each mini-batch, e.g.,ğ‘›users(ğ‘¢, ğ‘¢, ..., ğ‘¢)and two representations for each user generated by dropout denoted as(ğ’”, ğ’”, ..., ğ’”)and (ğ’”, ğ’”, . . . , ğ’”). As shown in Figure 2, for userğ‘¢, we calculate the similarities betweenğ’”and all the other user representation ğ’”, deî€›ned asğ‘ ğ‘–ğ‘š(ğ’”, ğ’”), and obtain the similarity distribution P(ğ’”; ğœ”) = ğ‘ ğ‘œ ğ‘“ ğ‘¡ğ‘šğ‘ğ‘¥ (ğ‘ ğ‘–ğ‘š(ğ’”, ğ’”), ..., ğ‘ ğ‘–ğ‘š(ğ’”, ğ’”))(5) Figure 2: Illustration of the DR loss. The click sequences of users î€›rst pass the representation model twice to obtain two diî€erent representation sets for users ğ’”and ğ’”. We then calculate the similarity between a certain user and others in the same set. We further use a bidirectional KL to constrain the similarity distributions between diî€erent sets. Then, a bidirectional KL-divergence loss is applied to regularize the two distributions P(ğ’”; ğœ”) and P(ğ’”; ğœ”), deî€›ned as: Final Objective.We combine the above two objectives together with the task-speciî€›c loss in the backbone model for model training. The task-speciî€›c loss and î€›nal training objective are as: L(ğ’”; ğœ”) = âˆ’12(ğ‘™ğ‘œğ‘”P(ğ’”; ğœ”) + ğ‘™ğ‘œğ‘”P (ğ’”; ğœ”))(7) The whole training process of CT4Rec is presented in Algorithm 1. As shown in Line 3-5, we obtain two user representationsğ’”and ğ’”by going forward the model twice for each user sequenceğ‘ . Line 6-7 calculate the Laccording to the loss function 8, and update the model parameters. The training process will continue until convergence. We conduct extensive experiments on three public benchmark datasets that are widely used in recent literature [53] and a new large-scale dataset collected from a real-world recommendation scenario, i.e., PC-WeChat Top Stories. These datasets are very different in domains, platforms, and data scale, where their detailed statistics are presented in Table 1. Table 1: Dataset statistics of four public benchmarks and an oî€line corpus from real-world application, where avg. refers to the average actions per user. â€¢ Amazon:a series of datasets comprise product reviews, which are crawled from one of the largest E-Commerce platforms, i.e., Amazon.com. As introduced in SASREC [25,37], these datasets are separated by top-level product categories. We follow one of the most recent researches [53] to utilize the Beauty and Sports categories for comparison. â€¢ Yelp:it is a widely acknowledged dataset for the business recommendation, which is collected from the Yelp platform. Following Zhou et al. [65], we leverage the data after January 1st, 2019, and treat business categories as attributes. â€¢ WeChat:this dataset is constructed from WeChat platform for PC Top Stories recommendation (denoted asWeChatto distinguish datasets from other platforms), which consists of interaction records from 7th to 13rd, June 2021. Each interaction provides positive feedback (i.e., click) of an item from a user. Concretely, we collect 9.5 million interactions from 0.74 million users on 0.21 million items and regard data from the î€›rst few days as the train set and the rest for testing. To verify the eî€ectiveness of our proposed method, we introduce four representative baselines and three very recent methods. â€¢ GRU4Rec [16].It utilizes GRU modules to model user action sequences for the session-based recommendation. We consider each userâ€™s click sequence as a session. â€¢ SASRec [25].It applies the multi-head self-attention mechanism to solve the sequential recommendation task, which is commonly treated as one of the state-of-the-art baselines. In this paper, we utilize SASRec as the backbone of our CT4Rec. â€¢ TiSASRec [28].Based on SASRec, it further introduces time interval aware self-attention mechanism to encode the userâ€™s interaction sequence, where the positions and time interval between any two items are considered. â€¢ BERT4Rec [42].It uses the deep bidirectional self-attention mechanism to model user interaction history in the sequential recommendation and trains the model like BERT [6]. â€¢ CL4SRec [53].It generates diî€erent views of the same user interaction sequence by using data augmentation methods and adds contrastive learning objective to the original objective of SASRec for sequential recommendation tasks. â€¢ CLRec.Zhou et al. [64]design a queue-based contrastive learning method named CLRec to de-bias deep candidate generation in the recommendation system and further propose Multi-CLRec for multi-intention aware bias reduction. In this work, we only compare the CLRec for fairness. â€¢ StackRec [49].It î€›rst uses a stacking operation on the pretrained layers/blocks to transfer knowledge from a shallow model to a deep model and then utilizes iterative stacking to obtain a deeper but easier-to-train recommendation model. We use multiple oî€Ÿine metrics with varying granularity and launch an online A/B test to evaluate the performance of all methods. Oî€line Metrics.Following many previous studies [14,25,53, 65], we employ the leave-one-out strategy to evaluate model performance. Speciî€›cally, for each user, we take the last interacted item for test. Similar to Kang and McAuley[25], Zhou et al. [65], we randomly sample 500items from the whole dataset for each positive item, and rank them by similarity scores. The model performances are evaluated by top-k Normalizedrand Discounted Cumulative Gain (NDCG@ğ‘˜) and top-k Hit Ratio (HR@ğ‘˜), which are both commonly used in top-k recommendation systems. Speciî€›cally, we report HR@ğ‘˜ and NDCG@ğ‘˜ with ğ‘˜ ={5, 10, 20}for all datasets. Online A/B Test.We further perform an online A/B test that resembles Hao et al. [11]to evaluate our CT4Rec in a real-world system. Concretely, we have deployed C4TRec for PC Wechat Top Stories recommendation. To calibrate the eî€ect of CT4Rec for the online system, CT4Rec is implemented as an additional channel in the current matching module with the rest of the system unchanged, where the existing matching module refers to an ensemble model that combines multiple methods, e.g., rule-based, reinforcementbased, sequence-based, DSSM, self-distillation. In the online A/B test, we utilize two metrics, i.e., click-through rate (CTR) and average click number per capita (ACN), to evaluate model performance. The online test lasts for 5 days and involves nearly 2 million users. All models are implemented based on TensorFlow. For baselines with oî€œcial codes, we utilize the implementations provided by authors. As for models without open-accessible codes from the original paper, we prefer the well-tested version from the opensource community. Speciî€›cally, we use code from https://github. com/Songweiping/GRU4Rec_TensorFlow as the implementation of GRU4Rec [16]. We implement CL4SRec and CLRec based on the model descriptions and experimental settings of the correlated papers since there is no oî€œcial code or popular implementation from the open-source community. For fair comparisons, the embedding dimension size is set to 50, and all models are optimized by Adam. Recall that our proposed CT4Rec method does not modify the model architecture and increases the model scale of the backbone model. Instead, it only involves two eî€ective consistency regularization strategies. Thus, we follow the backbone method, i.e., SASRec [25], to implement our CT4Rec. We use two self-attention layers and set the head number to 2. The maximum sequence length is 50 for all datasets. We optimize the parameters with the learning rate of 0.001 and the batch size as 128. The dropout rate of turning oî€ neurons is set to 0.5. To verify the eî€ect of each component and their combination, we î€›x the structure and other hyper-parameters of the model and only adjust the values ofğ›¼andğ›½, whereğ›¼andğ›½ ducing our experiments can be found in our anonymous code. We report the overall performance of all models on four oî€Ÿine datasets to compare their eî€ectiveness in top-K recommendation and further present the results of the online A/B test to learn the applicability of our CT4Rec on real-world recommendation system. Overall Oî€line Performance.As shown in Table 3, our proposed CT4Rec outperforms all other baseline methods, including multiple representative models and state-of-the-art sequential recommendation solutions, on three benchmark datasets and one large industrial corpus. Compared with other strong methods that utilize data augmentation or/and contrastive learning to obtain better user representations and mitigate the incompatibility between the single item prediction task and the vast amounts of parameters in data sparsity scenarios [25], our CT4Rec is simpler and more eî€ective without requiring any augmented data or delicate training strategies in which only two extra objectives are introduced. The performance advance of CT4Rec over SOTA models based on contrastive learning is conî€›rmed by the signiî€›cant improvements of HR@ğ‘˜, and NDCG@ğ‘˜scores over CLRec and CL4SRec. We also observe that the training objective in recent baselines performs much well than the original binary cross-entropy in SASRec, which is demonstrated by the results that SASRec* increase oî€Ÿine scores by a large margin over SASRec. Notice that our CT4Rec is implemented by introducing two training objectives into SASRec*. We compare CT4Re c with SASRec* to calibrate the eî€ect of our consistency training method. The universal enhancement of CT4Rec over SASRec* on four datasets and all HR@ğ‘˜and NDCG@ğ‘˜scores (relative improvements ranging from 5.60% to 16.50%) verify the superiority of our proposed simple method. Table 2: Performance improvement of online A/B test. Performance improvement of online A/B test.The performance improvement of CT4Rec on real-world recommendation Table 3: Model performance of baselines and our proposed CT4Rec on four oî€line datasets, where â€˜*â€™ refers to modifying the original binary cross-entropy loss in SASRec with the training objective in recent baselines, e.g., CLRec, CL4SRec, StackRec. Our CT4Rec is implemented on SASRec*. Improv. and Improv.* refer to the relative improvement of CT4Rec over SASRec and SASRec*, respectively. The performance improvement over baselines is statistically signiî€›cant with ğ‘ < present the experimental results of extra two runs with diî€erent random seeds in the Appendix. WeChat service is reported in Table 2, which can be seen that only implementing CT4Rec as an additional channel in the matching module with the rest of the system unchanged can yield very impressive performance improvement over the original ensemble model. Our method increases CTR and ACN metrics by 10.141% and 10.541%, respectively. This indicates that our proposed simple method is not only eî€ective for oî€Ÿine benchmarks and evaluation metrics but also generalizes well to the real-world online system. We have demonstrated the impressive performance of our proposed simple CT4Rec on both oî€Ÿine benchmarks and the online A/B test. In this section, we launch extensive experiments to understand and analyze CT4Rec from diî€erent perspectives. For convenience, these studies are performed on the Beauty dataset. More speciî€›cally, we mainly focus on:1)the eî€ect of each introduced objective (Ablation Study),2)the inî€uence of several important hyper-parameters (Hyper-Parameter Analysis),3)the exploration of more consistency regularization for sequential recommendation (More Consistency Training),4)the extension of CT4Rec to data augmentation (Extension to Data Augmentation),5)the change of training process and cost resulted by CT4Rec (Training and Cost Analysis). We perform an ablation study to explore the eî€ect of two objectives in Section 3, i.e., regularized dropout (RD) and distributed regularization (DR), where the results are illustrated in Figure 3. RD Objective.As shown in the left sub-î€›gure (a), the introduced RD objective achieves signiî€›cant performance improvement over the backbone SASRec* model. Compared with DR objective and its variants, RD loss contributes most to the performance increase, which concludes that launching consistency regularization in the model output space is the most beneî€›cial to SASRec*, which is similar to the observations in other tasks [29]. The possible reason might be that RD consistency regularization aî€ects directly on the same space of the model output probability distribution. DR Objective.We can see that the unsupervised DR loss can also yield substantial performance gains, which points out that unsupervised consistency regularization in user representation space for data sparsity sequential recommendation task is also essential. We also adapt two typical unsupervised strategies in previous studies [9,36,66] to regularize user representations, including cosine similarity and L2 distance. As shown in the right two sub-î€›gures in Figure 3, these two methods have not brought meaningful improvement upon the backbone method, which further proves that our designed DR objective is more preferable for consistency regularization in the representation space. Figure 3: Evaluation results for ablation study and analysis of ğ›¼ and ğ›½. (a) CT4Rec with L and L. (c) and (d) replace Lwith cosine and L2 loss, respectively, where the orange lines are the performance of SASRec*. Figure 4: The impact of diî€erent dropout rates on the performance of CT4Rec and SASRec* on the Beauty dataset. We mainly consider several critical hyper-parameters in this part, including ğ›¼ and ğ›½ in Equation 8 and the dropout rate. The Eî€ect of ğ›¼.Figure 3 also gives the results of diî€erentğ›¼ values. Considering that there are many feasible combinations of the (ğ›¼,ğ›½) grid, we temporarily remove the DR objective and only examine the inî€uence ofğ›¼for RD. It can be observed that a small value of ğ›¼ can bring meaningful performance improvement. With the increase ofğ›¼, the performance improvement further increases in which the best result is achieved whenğ›¼ =2.0. These results further conî€›rm that the consistency regularization directly performed on the output space is very eî€ective even when it only takes a small proportion for the î€›nal training objective. With a further increase ofğ›¼, the model will pay more attention to the consistency of model outputs, which will dilute the original item prediction objective, resulting in worse performance (e.g., ğ›¼ = 2.0 v.s.,ğ›¼ = 2.0). The Inî€uence of ğ›½.Similar to the analysis ofğ›¼, we only study the impact ofğ›½for each single unsupervised regularization loss (i.e., DR, cosine, and L2). Diî€erent fromğ›¼, the DR loss has no inî€uence on the overall performance when the value ofğ›½<0.3. This is probably because the consistency regularization on the user representations is overwhelmed and adapted when passing to the output space. With the increase ofğ›½, DR loss gradually produces a more important role and consistently improves model performance untilğ›½>2.0. And also, a largeğ›½value will force the model to focus on the consistency of user representation rather than perform the item prediction task. For the cosine objective, diî€erentğ›½values have a limited inî€uence on evaluation results. As for the L2 loss, a large ğ›½ value will cause inferior performance. Analysis on Dropout Rates.Besides the above analysis, we also study how the dropout rate aî€ects the eî€ectiveness of our CT4Rec since diî€erent dropout rates will lead to varying degrees of inconsistency. As demonstrated in Figure 4, dropout rates have a signiî€›cant inî€uence on the performance of the backbone model, and our proposed consistency training is applicable and eî€ective when the dropout rate<0.7. However, our proposed CT4Rec will lead to a negative eî€ect when the dropout rate>0.8. We speculate that the reason behind this might be the data sparsity issue and irreconcilable inconsistency. The two consistency regularization objectives introduced above are designed to address the inconsistency in both representation and output space for data sparsity sequential recommendation tasks. In this part, we explore to utilize more helpful consistency signals based on task-speciî€›c prior in training instead of solving the inconsistency problem of the model. Concretely, we investigate the eî€ect of a very simple and straightforward consistency constrain, i.e., the clicked items by the same user might share some traits. To utilize such inherent signals, we apply consistency regularization on two positive item embeddings of each user, which can also regularize the item representation space to some extent. Specially, we randomly sample two positive items from historical sequences of each user and calculate the similarity (i.e., cosine similarity) of these two positive items as an additional consistency objective to CT4Rec denoted as CT4Rec. As shown in Table 5, CT4Reccontinuously improves the model performance based on CT4Rec, which indicates that consistency training on ubiquitous task-related information can further enhance the sequential recommendation performance. Table 4: Performance comparison of diî€erent consistency regularization strategies in the data augmentation setting. Table 5: Exp erimental results of more consistency training. The above studies prove the eî€ectiveness of each component of our CT4Rec and its capability of leveraging more consistency regularization signals. We then study its generalization ability to other inconsistency scenarios. More concretely, we utilize our consistency training method to regularize the inconsistency brought by data other than the above-mentioned model-side inconsistency, i.e., we replace dropout with data augmentation methods to create two different user representations, and the corresponded task outputs for each user so as to conduct consistency regularization. To align with recent sequential recommendation methods based on constrastive learning, we leverage two easily implemented augmentation strategies from CL4SRec [25], i.e., Reorder and Mask. In doing so, we can directly compare the eî€ects of diî€erent training objectives (besides the item prediction one for the sequential recommendation) on the augmented data, including unsupervised methods (i.e., CL, cosine, L2, DR), the supervised regularization in the output space (RD), and the combination of DR and RD (CT4Rec). Table 4 presents the experimental results for the data augmentation setting. We can observe that: 1) Our introduced DR objective is the most eî€ective compared with other single methods, i.e., the consistency regularization on the representation space is the most preferable for than data augmentation scenario, which is in contrast with the observation for the dropout setting. We speculate that the labelinvariant data augmentation methods can lead to permuted and perturbed representation variants, which need more consistency regularization, while the label-invariant strategy does not deteriorate the inconsistency in the output space. 2) The combination of the consistency regularization in the representation space and the output space (CT4Rec) still performs the best with consistent and signiî€›cant performance over other training objectives. Figure 5: NDCG@10 and HR@10 curves on the valid set along with training epoch and time on the Beauty dataset. Since our CT4Rec does not modify the model structure of the backbone model or introduce extra augmented data, we mainly analyze the changes in the training process. We plot the curves of HR@10and NDCG@10 scores on the valid set along the training epoch number and time (seconds) for SASRec*, CL4SRec and our CT4Rec models, shown in Figure 5. At the early training stage, the backbone SASRec* model converges quickly with the same training epochs (around 15 epochs) and model performance as CT4Rec, but our CT4Rec can continuously improve the performance on the welltrained SASRec* model. It concludes that our consistency training objectives do not lead to more training epochs on the backbone SASRec*. But for CL4SRec that combines contrastive learning objective with SASRec*, it converges with much more training epochs and only moderate performance improvement. As for convergence time (seconds), our model indeed is slower than the backbone model since our consistency training objectives need an extra forward process (dropout) in each training step but achieves a much superior performance. Compared with CL4SRec, our CT4Rec is much more eî€œcient and eî€ective with a much better î€›nal optimum and less convergence time even when we havenâ€™t calculated the time cost of data augmentation and negative sampling for CL4SRec. Through the analysis, we can conclude that: 1) CT4Rec is much eî€œcient and eî€ective than the method based on contrastive learning even without counting its time cost of data augmentation and negative sampling; 2) CT4Rec indeed introduces extra training time for the backbone model, which can be mitigated by early stop insomuch as our CT4Rec can quickly surpass the backbone model in the early stage and with a much better î€›nal convergence performance. In this paper, we proposed a simple yet very eî€ective consistency training method for the sequential recommendation task, namely CT4Rec, which only involves two bidirectional KL losses. We î€›rst introduce a top-performed regularization in the output space by minimizing the bidirectional KL loss of two diî€erent outputs. We then design a novel consistency training term in the representation space by minimizing the distributed probability of two user representations. Extensive experiments and analysis demonstrate its eî€ectiveness, eî€œciency, generalization ability, and compatibility. Juntao Li is the corresponding author.