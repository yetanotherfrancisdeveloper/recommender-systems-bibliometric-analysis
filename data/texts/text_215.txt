In recommender systems (RecSys) and real-time bidding (RTB) for online advertisements, we often try to optimize se quential decision making using bandit and reinforcement learning (RL) techniques. In these applications, oî€Ÿine reinforcement learning (oî€Ÿine RL) and oî€-policy evaluation (OPE) are beneî€›cial because they enable safe policy optimization using only logged data without any risky online interaction. In this position paper, we explore the potential of using simulation to accelerate practical research of oî€Ÿine RL and OPE, particularly in RecSys and RTB. Speciî€›cally, we discuss how simulation can help us conduct empirical research of oî€Ÿine RL and OPE. We take a position to argue that we should eî€ectively use simulations in the empirical research of oî€Ÿine RL and OPE. To refute the counterclaim that experiments using only real-world data are preferable, we î€›rst point out the underlying risks and reproducibility issue in real-world experiments. Then, we describe how these issues can be addressed by using simulations. Moreover, we show how to incorporate the beneî€›ts of both real-world and simulation-based experiments to defend our position. Finally, we also present an open challenge to further facilitate practical research of oî€Ÿine RL and OPE in RecSys and RTB, with respect to public simulation platforms. As a possible solution for the issue, we show our ongoing open source project and its potential use case. We believe that building and utilizing simulation-based evaluation platforms for oî€Ÿine RL and OPE will be of great interest and relevance for the RecSys and RTB community. ACM Reference Format: Haruka Kiyohara, Kosuke Kawakami, and Yuta Saito. 2021. Accelerating Oî€Ÿine Reinforcement Learning Application in Real-Time Bidding and Recommendation: Potential Use of Simulation . In RecSys 2021 Workshop on Simulation Methods for Recommender Systems, October 2, 2021, Amsterdam. ACM, New York, NY, USA, 8 pages. https://doi.org/xxx In recommender systems (RecSys) and real-time bidding (RTB) for online advertisements, we often use sequential decision making algorithms to increase sales or to enhance user satisfaction. For this purpose, interactive bandit and reinforcement learning (RL) are considered powerful tools. The RecSys/RTB research communities have studied many applications of bandit and RL and demonstrated their eî€ectiveness in a wide variety of settings [ 55]. However, deploying RL policies in real-world systems is often diî€œcult due to the need for risky online interactions. Speciî€›cally, when we use an adaptive policy and learn it in the real environment, numerous numbers of exploration is needed before acquiring near-optimal decision makings [ damage sales or user satisfaction [ in the real environment. However, it involves high stakes because the unseen new policy may perform poorly on the system [12]. Therefore, online deployment of RL policies is often limited due to risk concerns, despite their potential beneî€›ts after the successful deployment. Emerging paradigms such as oî€Ÿine reinforcement learning (oî€Ÿine RL) and oî€-policy evaluation (OPE) try to tackle these issues in a data-driven manner [24]. In oî€Ÿine RL and OPE, we aim to learn and evaluate a new policy using only previously logged data, without any risky online interaction. The major beneî€›t of oî€Ÿine RL and OPE is that we can obtain a new policy that is likely to perform well in a completely safe manner, by 1) learning a new policy using only the logged data (oî€Ÿine RL), and 2) estimating the policy performance using the logged data to guarantee the safety in deployment (OPE). The potential to reduce the risks in deploying RL policies is gaining researchersâ€™ interest. There are their applicability in RecSys practice [5, 12, 13, 25, 27, 32, 34, 35, 43]. Discussion topic.In this paper, we discuss how simulation studies can help accelerate oî€Ÿine RL/OPE research, especially in RecSys/RTB. In particular, we focus on the roles of simulations in the evaluation of oî€Ÿine RL/OPE because empirical research is essential for researchers to compare oî€Ÿine RL/OPE methods and analyze their failure cases, leading to a new challenging research direction [9,33,39]. Moreover, validating the performance of the oî€Ÿine RL policies and the accuracy of OPE estimators is crucial to ensure their applicability in real-life situations [9]. Our position.We take a position thatwe should eî€ectively use simulations for the evaluation of oî€line RL and OPE.Against the position to argue that only the real-world data should be used in the experiments, we î€›rst show the diî€œculties of comprehensive and reproducible experiments incurred in real-world experiments. Then, we demonstrate the advantages of simulation-based experiments and how both real-world and simulation-based experiments are important from diî€erent perspectives. Finally, by presenting our ongoing open source project and its expected use case, we show how a simulation platform can assist future oî€Ÿine RL/OPE research in RecSys/RTB. 2 PRELIMINARIES In (general) RL, we have totalğ‘‡timesteps to optimize our decision making (the specialğ‘‡ =1 case is called the contextual bandit problem). At every timestepğ‘¡, the decision maker î€›rst observes stateğ‘ âˆˆ Sand decide which actionğ‘âˆˆ Ato take according to the policyğœ‹ (ğ‘| ğ‘ ). Then, the decision maker receives a rewardğ‘Ÿâˆ¼ ğ‘ƒ(ğ‘Ÿ| ğ‘ , ğ‘)and observes the state transitionğ‘ âˆ¼ T (ğ‘ | ğ‘ , ğ‘), whereğ‘ƒğ‘Ÿ (Â·)andT (Â·)are the unknown probability distributions. For example, in a RecSys setting,(ğ‘ , ğ‘, ğ‘Ÿ)can be user features, an item that the system recommends to the user, and the userâ€™s click indicator, respectively. Here, the objective of RL is to obtain a policy that maximizes the following policyî€‚Ãî€ƒ performance (i.e., expected total rewards)ğ‘‰ (ğœ‹):= Eğ›¾ğ‘Ÿ, whereğ›¾ âˆˆ (0,1]is a discount factor andE[Â·]isÃ the expectation over the trajectory distribution ğ‘(ğœ) âˆ¼ ğ‘(ğ‘ )ğœ‹ (ğ‘| ğ‘ )ğ‘ƒğ‘Ÿ (ğ‘Ÿ| ğ‘ , ğ‘)T (ğ‘ | ğ‘ , ğ‘). Let us suppose there is a logged dataset Dcollected by a behavior policy ğœ‹as follows. where the dataset consists ofğ‘›trajectories. In oî€Ÿine RL, we aim to learn a new policyğœ‹that maximizes the policy performanceğ‘‰ (ğœ‹)using onlyD. In OPE, the goal is to evaluate, or estimate, the policy performance of a new (evaluation) policyğœ‹using an OPE estimatorË†ğ‘‰andDasË†ğ‘‰ (ğœ‹;D) â‰ˆ ğ‘‰ (ğœ‹). To succeed in oî€Ÿine RL/OPE, it is essential to address the distribution shift between the new policyğœ‹and the behavior policyğœ‹. Therefore, various algorithms and estimators have been proposed for that purpose [ To evaluate and compare these methods in empirical research, we need to access both the logged dataset ground-truth policy performance of evaluation policy Dand ğ‘‰ (ğœ‹ 3 IS THE USE OF REAL-WORLD DATA SUFFICIENT TO FACILITATE OFFLINE RL? In this section, we discuss the advantages and drawbacks of the counterclaim: only the real-world data should be used in experiments of oî€Ÿine RL and OPE. We can implement a real-world oî€Ÿine RL/OPE experiment by running (at least) two diî€erent policies in the real-world environment. First, behavior policy RL/OPE, we need to approximateÃÃ real-world experiments compared to simulation is that it is informative in the sense that the experimental results are expected to generalize in real-world applications [33]. However, there are two critical drawbacks in empirical studies using only real-world data. The î€›rst issue is the risky data collection process and resulting limited experimental settings in the comprehensive experiments. The real-world experiments always necessitate the high-cost data collection process because the online interactions can be harmful until the performance of data collection policies ( variety of policies due to this risk concern, and the available empirical î€›ndings in real-world experiments are often limited. For example, when evaluating oî€Ÿine RL algorithms, we often want to know how well the algorithms learn from diî€erent logged data, such as the one collected by a sub-optimal policy [ behavior policy is often demanding because it may damage sales or user satisfaction [ of OPE estimators, researchers are often curious about how the divergence between behavior and evaluation policies aî€ects the accuracy of the performance estimation [ policies is challenging, as there is huge uncertainty in their performance [24]. The second issue is the lack of reproducibility. Due to conî€›dentiality and data collection costs in RecSys/RTB practice, there is only one public real-world dataset for OPE research (Open Bandit Dataset [ a real-world dataset for oî€Ÿine RL because the evaluation of a new policy requires access to the environment [ Therefore, conducting a reliable and comprehensive experiment is extremely diî€œcult using only real-world data, which we argue is a bottleneck of the current oî€Ÿine RL/OPE research in RecSys/RTB practice. 4 HOW CAN SIMULATIONS ACCELERATE OFFLINE RL RESEARCH? In this section, we describe how simulations can help evaluate oî€Ÿine RL/OPE methods together with real-world data. An alternative way to conduct experiments is to build a simulation platform and use it as a substitute for the real environment. Speciî€›cally, we can î€›rst deploy behavior policy synthetic dataset estimation the whole experimental procedure does not require any risky online interaction in the real environment. Since the policy deployment in the simulation platform is always safe, we can gain abundant î€›ndings from simulation research [ which is often diî€œcult in real-world experiments [ ) to conduct experiments of oî€Ÿine RL/OPE using both real-world and simulation-based synthetic data. ğ›¾ğ‘Ÿ(â‰ˆ ğ‘‰ (ğœ‹)), by deploying evaluation policyğœ‹to an online environment. The advantage of D. Then, we can calculate the ground-truth policy performanceğ‘‰(ğœ‹)or approximate it by on-policy ğ‘‰(ğœ‹)when the ground-truth calculation is computationally intensive. The important point here is that 9,10,39,42]. For example, in the evaluation of oî€Ÿine RL, we can easily deploy a sub-optimal behavior policy, RL by deploying a new policy several times in diî€erent training checkpoints, which is challenging due to risks and deployment costs in the real-world experiments [26]. In addition, we can test how well an OPE estimator identiî€›es evaluation policies that perform poorly, which is crucial to avoid failures in practical scenarios [28]. Furthermore, we can also tackle the reproducibility issue in real-world experiments by publicizing simulation platforms. Using an open-access simulation platform, researchers can easily reproduce the experimental results, which leads to a reliable comparison of the existing works [8,15]. Therefore, simulation-based experiments are beneî€›cial in enabling reproducible comprehensive studies of oî€Ÿine RL/OPE. Although the simulation-based empirical research overcomes the drawbacks of real-world experiments, it should also be noted that simulation-based experiments have a simulation gap issue [49]. Speciî€›cally, to model the real environment, we need function approximations for the probability distributions (i.e.,ğ‘ƒğ‘Ÿ (ğ‘Ÿ| ğ‘ , ğ‘)andğ‘‡ (ğ‘ | ğ‘ , ğ‘)). Unfortunately, there must be an inevitable modeling bias which may lead to less informative results. However, since both real-world and simulation-based experiments have diî€erent advantages, we can leverage both for diî€erent purposes, as shown in Table 1. Speciî€›cally, we can î€›rst conduct simulation-based comprehensive experiments to see how the conî€›guration changes aî€ect the performance of oî€Ÿine RL/OPE methods to discuss both the advantages and limitations of the methods in a reproducible manner. We can also verify if oî€Ÿine RL policies and OPE estimators work in a real-life scenario using real-world experiments with limited online interactions. Here, by performing preliminary experiments on a simulation platform and removing policies that are likely to perform poorly in advance, we can implement real-world experiments in a less risky manner. Thus, we argue that we should eî€ectively use simulations in the empirical research of oî€Ÿine RL and OPE. 5 TOWARDS PRACTICAL RESEARCH OF OFFLINE RL IN RECSYS AND RTB In this section, we discuss how we can further accelerate oî€Ÿine RL/OPE research in RecSys/RTB practice. The beneî€›ts of the simulation-based experiments have indeed pushed forward the oî€Ÿine RL/OPE research. Specifically, many research papers [2,6,9,14,20,22,29,44â€“46] have been published using a variety of simulated control tasks and their standardized synthetic datasets collected by diverse policies [8,15]. Moreover, the simulation-based benchmark experiments play important roles for researchers to discuss both advantages and limitations of the existing oî€Ÿine RL and OPE methods [9, 10, 39, 42]. Practical applications, however, are still limited, especially for oî€Ÿine RL (such as [5,25,27,31,43,47]). We attribute this to the lack of application-speciî€›c simulation environments that provide useful insights for speciî€›c research questions. For example, RecSys/RTB are unique regarding their huge action space and highly stochastic and delayed rewards [4,25,55]. Therefore, we need to build a simulation platform imitating such speciî€›c characteristics to better understand the empirical performance of oî€Ÿine RL/OPE methods in these particular situations. In the RecSys setting, there are two dominant simulation platforms well-designed for oî€Ÿine RL/OPE research, OpenBanditPipeline (OBP) [33] and RecoGym [32]. They are both beneî€›cial in enabling simulation-based experiments in a fully oî€Ÿine manner. Moreover, OBP is helpful in practice because it provides streamlined implementations of the experimental procedure and the modules to preprocess real-world data. However, their limitation is that they are unable to handle RL policies. OPE methods of RL policies relevant to the real-world sequential decision makings [ simulation platforms in RTB. There is a need to build a simulation-based evaluation platform for oî€Ÿine RL and OPE in RecSys/RTB settings. Motivated by the above necessity, we are developing an open-source simulation platform in the RTB setting. Our design principle is to provide an easy-to-use platform to the users. Below, we present an expected use case and describe how to utilize our platform in oî€Ÿine RL/OPE empirical research. We aim to conduct both simulation-based and real-world experiments, as described in Section 4. The solid arrows in Figure 1 show the workî€ow of simulation-based comprehensive experiments based on our platform. The key feature of the platform is that there are design choices for researchers, such as what behavior and evaluation policies to use and what oî€Ÿine RL and OPE methods to test. Moreover, researchers can also customize the environmental conî€›gurations in the simulation platform, such as action space performance of oî€Ÿine RL/OPE. After the detailed investigation in simulation-based experiments, we can also verify if the oî€Ÿine RL/OPE methods work in real-life scenarios with a limited number of online interactions. Our platform also provides streamlined implementation and data processing modules for assisting real-world experiments, as shown with the dotted arrows in Figure 1. The platform also allows researchers to identify a safe policy in advance using our semi-synthetic simulation, which replicates the real environment based on the original real-world dataset. The results of such a semi-synthetic simulation may help reduce the risks in real-world experiments. Finally, since we plan to publicize the platform, the research community can engage in our project to make the simulation platform to be a more diverse benchmark and more practically relevant. Moreover, we plan to extend our platform to the RecSys setting. These additional eî€orts will allow researchers to easily involve in the empirical research of oî€Ÿine RL/OPE in RecSys/RTB.