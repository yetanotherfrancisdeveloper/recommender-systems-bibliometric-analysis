<title>Masked Deep Q-Recommender for Eî€›ective î€estion Scheduling</title> <title>KEUNHYUNG CHUNG , DAEHAN KIM , SANGHEON LEE, and GUIK JUNG,</title> TmaxEdu Inc., Many online learning platforms like Coursera and Udacity are getting more and more attention in this era of distant learning. Unlike face-to-face learning environments, online learning has limited clues to grasp student knowledge status, since video lectures, question sheets, or textbooks are one-way material where interactions are scarce. To facilitate eî€ective online learning, two main questions need to be solved: â€¢ How can we quantitatively measure student performance on certain questions or concepts? How can we design a personal learning schedule in a way that maximizes his/her performance within a given period? <title>arXiv:2112.10125v1  [cs.AI]  19 Dec 2021</title> recommender is then reinforced by reward which consists of a change in student performance. Here, the student performance can be deî€›ned as the average correct probability over the studentâ€™s test range. Experimental results show that the proposed method improves 11.2% more knowledge level increase over expert-selected recommendation policy in average test range when evaluated in a math question dataset collected AIHUBmath. RL is a branch of machine learning that is used to optimize a continuous decision problem. In RL, the agent, the acting subject, repeatedly interacts with the environment. At each time-step , the agent observes the state of the environment, which is described as , and the agent determines action according to the state . The mapping function that receives the state as input and decides the action is called a policy , and it can be described as = ğœ‹ (ğ‘  . After the agent selects action , the environment takes the state-action pair (ğ‘  , ğ‘ and transits to the next state . The environment also emits a reward for each instantaneous transition (ğ‘  , ğ‘ , ğ‘  . In the next type-step ğ‘¡ + 1, the agent receives the next state and decides the next action by the policy , and this process continues until the end of an episode. The goal of RL is to î€›nd a policy that maximizes the discounted sum of future rewards received from the environment. This discounted sum of future rewards are called as return and return at time is deî€›ned as , where is a discount factor and is the time-step at which the episode ends. Q-learning [ 18 ] is one of the popular algorithms for î€›nding a policy that maximizes return. Q-learning uses the optimal action-value function (ğ‘ , ğ‘) which is the expected return achievable by following the best policy, after seeing some state and then taking some action (ğ‘ , ğ‘) = max E[ğ‘… |ğ‘  = ğ‘ , ğ‘ = ğ‘, ğœ‹] . This algorithm î€›nd (ğ‘ , ğ‘) by updating an action-value function (which is also called as Q-function) repeatedly using an important theory know as the Bellman equation, Using iteration (ğ‘ , ğ‘) â† ğ‘Ÿ +ğ›¾ max (ğ‘  , ğ‘ , a Q-function can converges to the optimal Q-function ( (ğ‘ , ğ‘) â†’ (ğ‘ , ğ‘), âˆ€(ğ‘ , ğ‘) as ğ‘– â†’ âˆ)[17] and ğ‘Ÿ + ğ›¾ max (ğ‘  , ğ‘ ) is called the target value. In practice, we approximate Q-function because it is impossible to represent all possible ğ‘„ (ğ‘ , ğ‘) separately if there are a huge number of state-action pairs (ğ‘ , ğ‘) . There have been many follow-up studies that approximate Q-function, and DQN [ 10 11 ] is the î€›rst to use convolutional neural networks to approximate Q-function. DQN trains an agent to play Atari 2600 games [ ] and use Q-function (ğ‘ , ğ‘) where is a set of raw pixel images from Atari 2600 games and are parameters of neural networks. This algorithm use replay memory [ ] which saves the transition (ğ‘  , ğ‘ , ğ‘Ÿ , ğ‘  when an agent plays with the environment and randomly samples previous transitions when the agent is trained. Another characteristic of DQN is that it uses another Q-network called target network for the stability of training. The target network is used to calculate the target value ğ‘Ÿ + ğ›¾ max (ğ‘  , ğ‘ where are parameters of the target network and are only updated with original Q-network parameters for every some intervals. In this paper, we used a variant of DQN algorithm to schedule the questions eî€œciently. KT is a task that evaluates a studentâ€™s knowledge level based on log data accumulated while learning. Student log data has various types of features such as timestamp, student action type, and time spent on action. Among them, question information and correct answers are mainly used for the KT task. The KT model infers the studentâ€™s mastery levels one each knowledge concept based on the studentâ€™s past question-solving log data. The output of the model is the probability that the student will correct the question corresponding to each concept, which can be interpreted as the studentâ€™s mastery of the corresponding knowledge concept. For the concept of the question provided to the student at time , the correct answer predicted by the model as , and the feedback result from the student as ğ‘“ , the model is trained through the following binary cross-entropy loss. The î€›rst proposed deep learning-based KT technique is the Deep Knowledge Tracing (DKT) [ 14 ]. They used a recurrent neural network (RNN) [ 20 ] that mainly deals with time-series data, focusing on the fact that the log data of the studentâ€™s question solving is sequential data. For the input vectors , ..., ğ‘¥ of the model, the studentâ€™s knowledge concept mastery probability ğ‘¦ , ...,ğ‘¦ is calculated as follows. where is the hidden state vector of the RNN, which is information that implies the studentâ€™s question-solving results up to time t. The parameters of the model are composed of an input weight matrix , a hidden state weight matrix , an output weight matrix , and biases, which are updated through training. After the DKT model was proposed, more advanced deep learning models than RNN dealing with sequential data were applied to the KT task. In this paper, we used the Bi-LSTM-based (NPA) model with self-attention [ ] as a student simulator. Unlike the original NPA model, we use the diî€œculty of question along with the knowledge concept of the question and the results of student feedback. The diî€œculty of the question aî€ects the change in the studentâ€™s knowledge level [ ]. Therefore, the KT model using this information can expect more precise knowledge level prediction. Also, using the studentâ€™s knowledge level based on the diî€œculty of the question, our recommender can schedule questions by specifying the diî€œculty of the question. where is a knowledge level on (ğ‘, ğ‘‘) measured by the KT model. We also assumed that the studentâ€™s knowledge level changes once a day for the simplicity of the scheduling simulation. To apply RL framework to the question scheduling simulation, we model the scheduling scenario as a Markov Decision Process (Figure 1). 3.1.1 State. The state is the studentâ€™s question solving history and it can be composed of knowledge concept of the question , a studentâ€™s feedback result (i.e., correct or wrong), and question diî€œculty . We denote (ğ‘, ğ‘“ , ğ‘‘) as a question-feedback and state at time ğ‘¡ is a sequence of question-feedbacks, i.e., the diî€œculty ğ‘‘ has three levels which are easy, medium, hard. 3.1.2 Action. At each time , MDQR can select which knowledge concept of the question and at what diî€œculty level of question to assign to the student, i.e., 3.1.3 State Transition. After the student solve the question which is recommended by action = (ğ‘ , ğ‘‘ the studentâ€™s feedback result is sampled by probability distribution ğ‘ƒ (ğ‘“ |ğ‘  , ğ‘ . The probability ğ‘ƒ (ğ‘“ |ğ‘  , ğ‘ is a prediction of whether the feedback result will be correct when the current state is and action (a recommended question) is ğ‘ . We modeled The function ğ‘ƒ by KT (Figure 2). Algorithm 1 Masked Deep Q-Recommender to ğ‘’ do âˆˆ ğ¶ with probability ğœ– â† argmax (ğ‘  , ğ‘) and observe reward ğ‘Ÿ and next state ğ‘  from the student simulator , ğ‘ , ğ‘Ÿ , ğ‘  ) in replay memory ğ· to ğ‘’ do = (ğ‘  , ğ‘ , ğ‘Ÿ , ğ‘  ) from ğ· , ğ‘„ , ğ‘„ , ğ¶ ): 3.1.4 Reward. The purpose of our recommendation model is to raise the studentâ€™s knowledge level above the initial state. We assumed that the probability of correcting a question is the studentâ€™s level of knowledge. Therefore, the reward at time ğ‘¡ is calculated using the correctness probability function ğ‘ƒ as follows: Where (ğ‘ƒ (ğ‘“ = 1|ğ‘  , ğ‘) âˆ’ ğ‘ƒ (ğ‘“ = 1|ğ‘  , ğ‘)), (9) and Ïˆ, if ğ‘ (10) 0, otherwise represents the increments in the knowledge level at state compared to state is a set of questions in test range e. is a penalty term that prevents duplicate recommendations for the same question. We give a duplicate penalty Ïˆ if the recommended question was included in the last 20 questions. We use the AIHUBmath dataset from National Information society Agency. AIHUBmath is a log of solving math questions for grades 7-9. There are 707,450 question-solving interactions from 4,673 students. The average number of interactions per student is 153.84. Table 1 shows examples of knowledge concepts contained within each test range in AIHUBmath. We simulate the following three recommended methods and compare their performance: random, expert, MDQR. Random is a method in which the concept and diî€œculty of questions are randomly asked within a speciî€›c test range. Expert ask questions in the order of the curriculum within a speciî€›c test range. The diî€œculty level of the question is adaptively adjusted according to the studentâ€™s previous question-solving record. If the previous question is correct, the next question is more diî€œcult than the previous question (until it reaches the highest diî€œculty), and if it is wrong, it is more easy than the previous question diî€œculty (until it reaches the lowest diî€œculty). MDQR recommends a question ğ‘ which maximizes ğ‘„ (ğ‘ , ğ‘) within a speciî€›c test range. Assuming that the question is recommended according to the current grade of the student, the scheduling simulation was also conducted so that the question was asked within a speciî€›c range. In addition, we conducted the scheduling simulation with 20 questions a day for 2 weeks. As shown in Table 2, in most test ranges, the MDQR increased the studentâ€™s knowledge level the most during a given period. Expert scheduler generally performed better than the random scheduler but showed worse results in some test ranges. When looking at the average performance in each test range, MDQR showed 21.3% increase of knowledge level while Expert showed a 10% increase. Figure 3 shows the change in the studentâ€™s knowledge level over two weeks when the simulation was conducted with each question scheduling method. MDQR and the random scheduler show a gradual rise in knowledge level, whereas the expert scheduler shows unstable changes in knowledge level. MDQR and random scheduler always recommend concepts of question in a diî€erent order, whereas expert scheduler always recommends concepts of question in the same order according to the curriculum, with only diî€erent question diî€œculty. This shows that certain concepts cause a studentâ€™s knowledge level to increase on average when solving questions on this concept while some concepts lower the knowledge level. There have been several attempts by many researchers to apply AI techniques to various î€›elds in the educational domain [ ]. Among them, the AI technique was mainly applied to the KT to analyze the knowledge level of the learner and the learning content scheduling technique for the eî€œcient learning of the learner. In this section, we describe research in which AI technology is applied to KT and learning content scheduling. KT is a technique to analyze and derive studentsâ€™ knowledge levels based on sequential learning log data. Corbett et al. [ ] proposed Bayesian Knowledge Tracing (BKT), which analyzes the knowledge level using the Hidden Markov Model [ 15 ] based on the correlation between the studentâ€™s question-solving result and the knowledge level. Piech et al. [ 14 ] proposed Deep Knowledge Tracing (DKT), which analyzes knowledge level based on LSTM, which is known to be proî€›cient in dealing with sequential data. After DKT was proposed, deep learning-based KT techniques such as key-value memory network-based model (DKVMN) [ 21 ] and attention-based model (SAKT) 12 ] were proposed and showed high performance. In this paper, we utilized a deep learning-based KT model for a student question-solving simulator. In addition to the KT technique, approaches to schedule appropriate learning content for students to learn eî€œciently have been widely studied. The key point of the learning content scheduling technique is to schedule the learning contents in the optimal order to maximize the evaluation measure such as the studentâ€™s knowledge level or test score. Ai et al. [ ] proposed a reinforcement learning-based approach to schedule exercises in an online learning system. Their method recommends the next question based on the current studentâ€™s knowledge level. Speciî€›cally, they utilized reinforcement learning to derive an exercise that maximizes the reward calculated as the average of the studentâ€™s knowledge level. They modeled the learning content scheduling process as a Partially Observable Markov Decision Process (POMDP) [ 19 ] and applied reinforcement learning based on the Trust Region Policy Optimization (TRPO) [ 16 ] algorithm. In contrast, we applied a deep learning-based reinforcement learning model that autonomously learns content scheduling policies. Also, we solved the problem of recommending the same question repeatedly by introducing a scheduling penalty. Loh et al. [ ] suggested a question recommendation algorithm that can eî€ectively raise a studentâ€™s test score. They explained the phenomenon that recommendation algorithm based on the knowledge level of the student simply recommends only questions that a student is most likely to get right. To overcome this problem, they also considered the expected test scores assuming the student got the recommended questions right. They implemented a linear approximation-based student correct rate prediction model and a Bi-LSTM-based test score prediction model to predict studentsâ€™ test scores. Similarly, we introduce the concept of a penalty to solve the problem where the same question is recommended as duplicate. However, there are three diî€erences from this study: First, we used the studentâ€™s average knowledge level as a scheduling outcome measure instead of predictive test scores. Second, their method î€›nds the next question with a greedy search based on the test score prediction model, whereas we î€›nd the optimal question sequence through a deep learning-based reinforcement learning model. Third, our study has the additional constraint that it should be scheduled among the questions that exist within the speciî€›c test range. In addition, various course scheduling techniques to which artiî€›cial intelligence models are applied have been proposed. For example, a technique using reinforcement learning to determine the optimal order and number of activities provided within a 90-minute online class [ ], and an approach to recommend a university course using catalog description and previous course enrollment history [ 13 ], etc. However, the purpose of our study is to schedule questions within a speciî€›c range, which is diî€erent from the corresponding studies. This paper proposed MDQR, the î€›rst scheduling model that can adjust the recommendation range according to the studentâ€™s test range. MDQR is a model that improved the DQN algorithm of reinforcement learning to î€›t our scheduling scenario. The purpose of the scheduling model is to maximize the learnerâ€™s knowledge level within a limited period of time. In order to examine the performance of MDQR, we conducted a comparative evaluation between a random scheduler and an expert scheduler. Experimental results showed that MDQR outperforms in terms of learnerâ€™s knowledge gain than other baselines in most of the test ranges.