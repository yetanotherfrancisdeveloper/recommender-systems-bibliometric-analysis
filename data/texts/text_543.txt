Digital advertising is a critical part of many e-commerce platforms such as Taobao and Amazon. While in recent years a lot of attention has been drawn to the consumer side including canonical problems like ctr/cvr prediction, the advertiser side, which directly serves advertisers by providing them with marketing tools, is now playing a more and more important role. When speaking of sponsored search, bid keyword recommendation is the fundamental service. This paper addresses the problem of keyword matching, the primary step of keyword recommendation. Existing methods for keyword matching merely consider modeling relevance based on a single type of relation among ads and keywords, such as query clicks or text similarity, which neglects rich heterogeneous interactions hidden behind them. To î€›ll this gap, the keyword matching problem faces several challenges including: 1) how to learn enriched and robust embeddings from complex interactions among various types of objects; 2) how to conduct high-quality matching for new ads that usually lack suî€œcient data. To address these challenges, we develop a heterogeneous-graphneural-network-based model for keyword matching named HetMatch, which has been deployed both online and oî€Ÿine at the core sponsored search platform of Alibaba Group. To extract enriched and robust embeddings among rich relations, we design a hierarchical structure to fuse and enhance the relevant neighborhood patterns both on the micro and the macro level. Moreover, by proposing a multi-view framework, the model is able to involve more positive samples for cold-start ads. Experimental results on a large-scale industrial dataset as well as online AB tests exhibit the eî€ectiveness of HetMatch. â€¢ Information systems â†’ Retrieval models and ranking;Computational advertising;â€¢ Computing methodologies â†’Neural networks. online advertisement, keyword recommendation, graph neural networks ACM Reference Format: Zongtao Liu, Bin Ma, Quan Liu, Jian Xu, Bo Zheng. 2021. Heterogeneous Graph Neural Networks for Large-Scale Bid Keyword Matching. In Proceedings of the 30th ACM International Conference on Information and Knowledge Management (CIKM â€™21), November 1â€“5, 2021, Virtual Event, QLD, Australia. ACM, New York, NY, USA, 10 pages. https://doi.org/10.1145/3459637.3481926 Figure 1: A real-world integrated keyword recommender system. Sponsored search is one of the main fashions of online advertising where advertisers acquire their desired ad impressions or clicks via bidding proper keywords. At the sponsored search platform of Alibaba, millions of advertisers in total manually add tens of millions of keywords every day, which reî€ects advertisersâ€™ strong bidding willingness to make their products obtain more potential consumers. Compared with such strong willingness, many advertisers lack expert knowledge to choose proper keywords thus fail to get expected ad impressions or clicks for their products. As illustrated in the previous empirical study [30], many advertisers tend to bid on a small number of popular keywords in the advertising platform, which makes those advertisers with low bid prices harder to obtain impressions. This challenge is also common at the sponsored search platforms in Alibaba, where only less than 10% of handcrafted selected keyword can obtain ad impressions on the next day. To improve marketing eî€ectiveness and lighten the burden of advertisers, many sponsored search platforms are now oî€ering diî€erent products that perform keyword recommendation, either in a white-box or a black-box fashion. White-box products oî€er many keyword candidates where advertisers can manually choose from, while black-box tools provide automatic keyword hosting services. Like many other industrial recommender systems (RSs), an advertising keyword recommender system can be achieved via two phases: 1) keyword matching/retrieval which collects a subset of tokens among sea of words for a given ad; 2) eî€œcient ranking which puts an order on individual subset based on relevance and estimated eî€ect (C.f. Figure 1). In this work, we address the problem of keyword matching, which is a fundamental step of keyword Figure 2: HIN schema for e-commerce bid keyword matching problem. An ad refers to an ad-group created in the advertising platform, a key word can either denote a bidding keyword or an ordinary query searched by users, and an item denotes an ordinary goods in the e-commerce platform. Besides, each ad is corresponding to a unique item. recommendation and also plays a determining role in the quality of the ranking stage. Regarding the keyword matching or retrieval problem, various approaches have been explored over years, including text similarity, collaborative î€›ltering, and topic-based ones [2,13]. However, there are several limitations of existing methods: 1) these approaches neglect rich heterogeneous information hidden behind the simple <ad, keyword>pair (Figure 2 illustrates a toy example of HIN schema in which ads, keywords, and items are denoted by diî€erent types of nodes), which helps understanding the relevance between them ; 2) This also causes the cold-start problem to be worse for new ad units, especially those of new advertisers. To utilize the rich information hidden behind ads and keywords, we resort to heterogeneous graph neural networks (HGNNs), which have various successful applications in modeling complex interactions between diî€erent types of objects [4,6], and then integrate HGNNs in an embedding-based framework which is proven eî€ective in multiple information retrieval tasks [19]. Embedding-based methods have been extensively studied in recommendation scenarios to improve the quality of matching. These methods aim to represent each node with diî€erent types of features in heterogeneous information network as a low-dimensional embedding vector, expecting that similar source objects (ads) and target objects (keywords) have similar embeddings. Among them, GNN-based approaches beneî€›t from their strong ability to fuse relevant information from neighbors at diî€erent distances in the network, thus achieving state-of-the-art performance in matching task [26]. More recent studies have extended GNN to heterogeneous information networks to capture the complex interactions among various types of nodes[19]. Most of these studies achieve this goals by aggregating neighborsâ€™ information by sampling subgraphs via diî€erent metapaths [16,32], where a metapath denotes a sequence of meta relations (C.f. Section 3). For heterogeneous recommendation such as keyword recommendation, the selected metapaths vary a lot between diî€erent sides of a two-tower structure [16,32], where a tower refer to a subnetwork to compute a nodeâ€™s î€›nal embedding, resulting that the sampled attributed subgraph in each tower is very diî€erent. More speciî€›cally, the types of nodes, edges and their corresponding features in the same position of each subgraph vary a lot. In this way, it might be hard to guarantee that the î€›nal embeddings of two towers fall into adjacent feature space. Besides, it is also a challenging problem to î€›lter irrelevant information when involving heterogeneous relations and attributes. This problem is especially severe in industrial web-scale scenes such as in Alibabaâ€™s e-commerce sponsored search platform. How to learn comprehensive and robust embeddings in modeling complex interaction among nodes is an intractable problem. Another factor aî€ecting the quality of keyword matching is the cold start of new ads. In Taobao, a leading e-commerce platform in China, millions of new ads are continuously created by advertisers every day. However, there are no user behaviors for these new ads throughout the platform. How to process these â€œcold start" ads is another challenging problem. Motivated by the concerns mentioned above, this paper proposes aHeterogeneous graph neural network for keywordMatching (HetMatch) for e-commerce sponsored search. To enhance the robustness of the extracted embeddings with various semenatics, on the macro level, we develop a Siamese network architecture by aggregating each towerâ€™s î€›nal embeddings and the averaged neighborhood embedding to conduct matching. In this way, the original heterogeneous matching problem between ads and keywords is transformed into a homogeneous matching between two <ad, keyword>pairs. On the micro level, we apply autoencoder in graph convolution as a fundamental component of our model to reduce the inî€uence of noisy signals and the computational cost. Moreover, to improve the capability to match cold-start ads, we leverage a multi-view framework to use multiple types of relations besides click data of ads as our objectives. Accordingly, our contributions are as follows: â€¢We propose a novel HGNN-based keyword matching framework that leverages the complex relational data behind ads and keywords. To extract enriched and robust embeddings among diî€erent relations, on the micro level, we apply an autoencoder in graph convolution to mitigate the irrelevant patterns in neighborhood; while on the macro level, we develop a Siamese neighbor matching layer on the top of HGNNs to involve more relevant neighborhood information. â€¢To introduce more supervised signals for alleviating the coldstart problem, we use a multi-view framework to learn the posterior probability of various objectives. â€¢Experiment results exhibit our modelâ€™s advantages over other methods both on oî€Ÿine and online evaluation. â€¢To the best of our knowledge, it is the î€›rst work to apply a HGNN-based method to keyword recommendation, which consolidates the foundation of sponsored search. Keyword Recommendation and Suggestion.Early methods of bid keyword recommendation in sponsored search mainly focus on retrieving keywords from the perspective of relevance. These methods can be broadly categorized into the following types: collaborative methods [2], proximity-based methods [1,13], and topicbased methods [3]. Some collaborative methods like [2] aim to mine phrases that co-occur with the seed queries, which is applied in Googleâ€™s Adword Tools. Proximity-based approaches mainly focus on designing diî€erent distance metrics to evaluate the similarity between queries, which can be further divided into network-based methods [13] and kernel-based methods [1]. Topic-based methods use a hierarchical or unsupervised approach to cluster queries to topic [3]. These methods mainly address the relevance of retrieved corpus and ignore modeling the eî€ect of selected keywords; thus may not perform well in practice. Another line of keyword recommendation focuses on retrieving the keywords from click-logs and estimate the user-click or impression brought by selected keywords [7, 15, 24]. For instance, Fuxman et al. [7]propose algorithm within Markov Random Field model to estimate user clicks. However, these methods are computationally expensive and lack the generalization ability for new ads; thus are usually applied to ranking retrieved keywords rather than performing matching tasks. Representation Learning on Heterogeneous Networks.Network representation learning (NRL) aims to automatically encode node information and network structure to î€›xed-sized latent representation, which can be used in downstream graph mining tasks. Over the years, quite a few NRL methods have been proposed. These methods can be divided into random-walk-based methods [4], factorization-based methods [17] and GNN-based methods [18,25,29]. Earlier works mostly focused on studying the network representation learning method on homogeneous graphs. Recently, inspired by the past homogeneous NRL, studies have attempted to extend homogeneous NRL methods on heterogeneous networks. And the core problem is to fuse diverse node attributes and different types of relations into a latent vector; thus, the extracted embeddings can preserve both semantic and structural properties among nodes. Based on previous random-walk-based homogeneous NRL, Dong et al. [4]and Fu et al. [6]introduce metapath-based random walk strategies, in which the walker is conî€›ned to transit based on given metapaths. On the other line of works, applying GNNs on heterogeneous networks has also been proven a promising direction [18,25,29]. For instance, Wang et al. [25]maintains diî€erent weights for diî€erent metapath-deî€›ned edges when applying aggregation in the graph attention networks. However, current HGNN-based methods do not explicitly address the importance of î€›ltering irrelevant patterns propagated through complex interactions, which is achieved by our extended version of graph convolution. Heterogeneous-Information-Network based Recommendation.More recently, leveraging heterogeneous information networks (HIN) becomes an emerging direction in recommender systems due to its capability of characterizing complex objects and rich relations [19]. For instance, Feng and Wang[5]proposed to alleviate the cold start issue with heterogeneous information network contained in the social tagged system. Metapath-based methods were introduced into hybrid recommender system in [28]. Recently, Hu et al. [9]leveraged metapath-based context in top-N recommendation. However, none of them addresses the problem of the heterogeneity of metapaths between two towers, which might lead to diî€œculties for learning a robust embedding for matching with very diî€erent computational graphs. Recently, several researches have explored how to utilize the rich relations and complex objects of heterogeneous information network (HIN) in recommendation systems, which is an auspicious direction. Keyword recommendation also deals with various types of objects and rich relations (C.f. Figure 2), thus it would be promising to leverage the HIN-based recommendation paradigm. Here we î€›rst give the necessary concepts about HIN. Deî€›nition 3.1.Heterogeneous Information Network[20]. A heterogeneous information network (HIN) can be denoted asG = (V, E, ğœ™,ğœ“ ), consisting of an object setVand a relation setE. Each nodeğ‘£owns a typeğœ™ (ğ‘£), and each edgeğ‘’has a typeğœ“ (ğ‘’).TandR denote the sets of predeî€›ned object types and relation types, where |T | + |R| >2.Vcan be written asV = Vâˆª ... âˆª Vâˆª ... âˆª V, whereğ‘¡âˆˆ TandVrepresents the node set with typeğ‘¡. Besides, each nodeğ‘£âˆˆ Vis augmented with node attributexâˆˆ R, whereğ‘‘is the attribute dimension of the nodes typedğ‘¡. Similarly, Ecan be written asEâˆª ... âˆª Eâˆª ... âˆª E, whereğ‘Ÿâˆˆ Rand Erepresents the edge set with type ğ‘Ÿ. More speciî€›cally, in the scenario of keyword recommendation, we denote the whole node setV = Vâˆª Vâˆª V, whereV, VandVrepresents the ad set, keyword set, and item set, respectively. More speciî€›cally, an ad refers to an ad-group created in the advertising platform, a keyword can either denote a bidding keyword or an ordinary query searched by users, and an item denotes an ordinary goods in the e-commerce platform. Among these nodes, there exists various types of relations which capture diî€erent semantic connections, such as the bid or click relations between ads and keywords. In order to capture the semantic and structural relation between diî€erent objects, the metapath is proposed by [21] as a relation sequence connecting two objects, and is widely applied in HIN model research. Deî€›nition 3.2.Metapath and Metapath-guided Neighbors [21]. For a relationğ‘’ = (ğ‘ , ğ‘¡)linking from nodeğ‘ toğ‘¡, its meta relation is deî€›ned as< ğœ™ (ğ‘ ),ğœ“ (ğ‘’), ğœ™ (ğ‘¡) >. A meta-pathğ‘is then deî€›ned as a sequence of meta relationsğ‘¡âˆ’âˆ’â†’ ğ‘¡âˆ’âˆ’â†’ Â· Â· Â·âˆ’â†’ ğ‘¡, which describes a composite relationğ‘… = ğ‘Ÿâ—¦ ğ‘ŸÂ· Â· Â· â—¦ Â· Â· Â· ğ‘Ÿbetween nodesğ‘andğ‘, whereâ—¦denotes the composition operator on relations. Furthermore, the metapath-guided neighborhood is deî€›ned as the set of all visited nodes when a given nodeğ‘£walks along the given metapath ğ‘. Finally we present the problem deî€›nition of HIN-based keyword matching. Deî€›nition 3.3.HIN-based Ad-Keyword Matching Problem. Given a HING = (V, E), letA = {ğ‘, ğ‘, ..., ğ‘} âŠ‚ Vbe a set of selected ads to match keywords, andE, EâŠ‚ A Ã— Vbe the candidate relation set and target relation set respective, where â€˜Ã—â€™ represents the Cartesian product operator between two set. More speciî€›cally, each adğ‘corresponds to a target keyword set QâŠ‚ Vand a candidate keyword set QâŠ‚ V. The ad-keyword matching problem can be formulated as a recall optimization task [11]. Given a set of adsA = {ğ‘}with theirÃ corresponding candidate relation setsE=(ğ‘Ã— Q)and theÃ target relation setsE=(ğ‘Ã— Q), the problem is to pick aÃ retrieved relation setE=(ğ‘Ã— O), whereOrepresents the retrieved keywords for adğ‘sized no more thanğ¾, the object is to maximize the total recall ratio: Usually, the candidate set of a matching problem is set as a group of target nodes which is related to the source nodes under a certain criterion. In our task, we deî€›ne the candidate setğ‘„of an adğ‘ as the keywords that share the same category withğ‘, which is adjusted by a trained category classiî€›er. The target relation set Econtains the node pair of<ad, keyword>which would bring user clicks in the future. In other words, we aim to optimize our matching model in that the matched keywords can cover as many eî€ective bidding keywords as possible. In recent years, attention in the î€›eld of RS is increasingly shifting towards HIN-based recommendation. We extend this paradigm of RS for bid keyword matching problem and design a hierarchical network architecture to model the enriched interactions behind ads and keywords in a robust way. On the micro level, we leverage metapath-based graph convolution to aggregate neighborhood context from diî€erent perspectives and utilize an autoencoder module to î€›lter irrelevant neighborhood patterns. On the macro level, we design a Siamese network structure to enhance the complementary patterns in the neighborhood. To further alleviate the cold start problem of newly created ads, we extend our framework as a multi-view matching problem among diî€erent relation targets between ads and keywords. In this way, more supervising information are introduced and the associated tasks can improve each otherâ€™s performance by sharing information. In this section, we describe the details of our proposed model,Heterogeneous Graph Neural Network for keywordMatching (HetMatch). Figure 3 shows the architecture of our proposed model. Before integrating data from metapath neighbors for each node, we î€›rst extract heterogeneous feature vectorxâˆˆ RfromXfor each nodeğ‘£âˆˆ Vand transform it as a î€›xed-length embedding. We list the details of features in Table 1. These features can be categorized into two types, i.e., id features and numeric features. We transform each numeric feature as a discrete value, which represents the quantile of its feature distribution. After discretization, we can acquire the embedding of each feature via its corresponding lookup table. To lighten the computation burden, the same features from diî€erent node types will share one look-up table. For instance, the feature terms of title appears both in adâ€™s and itemâ€™s feature list, and thus the embedding of this feature shall be acquired from the same look-up table. We then concatenate these embeddings and feed the concatenation into a type-speciî€›c neural networkğ‘“to get the node-level embedding h. Metapath.After fusing the node-level information, the next step is to involve the rich context information around each node. Under the paradigm of metapath-based heterogeneous GNNs, multiple metapaths are sampled which play diî€erent roles in capturing the structural and semantic context. We thus introduce the metapaths used in our model, which can be categorized into the following two groups: (1) Bid-based group: The subgraphs based on bid relations around a given ad or keyword can directly characterize the bidding environment around it, which involves the competitive ads that bid on the shared keywords and a surge of keywords that the competitive ads are interested in. We use the following four metapaths constructed by user clicks and advertiser bids to capture such environment: â€¢ ğ‘âˆ’âˆ’âˆ’âˆ’â†’ ğ‘âˆ’âˆ’âˆ’âˆ’â†’q, ğ‘âˆ’âˆ’âˆ’âˆ’â†’ ğ‘âˆ’âˆ’âˆ’â†’q â€¢ ğ‘âˆ’âˆ’âˆ’âˆ’â†’ ğ‘âˆ’âˆ’âˆ’âˆ’â†’a, ğ‘âˆ’âˆ’âˆ’â†’ ğ‘âˆ’âˆ’âˆ’âˆ’â†’a where the user-click relations can stand for the bids that can directly bring clicks, and the ordinary bid relations can make appropriate supplements to the cold-start ads. (2)Item-based group: Sometimes a user might pay attention to an ad and an ordinary item in the same page view. These items can build bridges between ads and related keywords that are not directed connected and provide extra useful semantics that helps capture richer context information. Besides, those co-clicked items can also oî€er similar textual and behavioral patterns that can characterize how the connected nodes are like. Graph Convolution Layer with AutoEncoder.After introducing the metapaths used in our model, we then present how we aggregate the node-level embeddings robustly and eî€œciently given a certain metapath. The core idea of graph neural networks is to iteratively aggregate feature information in the neighborhood by a local î€›lter. However, the neighborhood information aggregated by diî€erent heterogeneous relations might contain irrelevant information. Without loss of generality, we apply the autoencoder, which is successfully used in previous tasks for learning the compressed representation for denoising [23], to î€›lter irrelevant neighborhood patterns and meanwhile improve the eî€œciency of aggregating. The Node Type Features adad id, terms of title/description, category path, brand, propitemitem id, terms of title/description, category path, brand, keywordkeyword id, terms of keyword, predicted category, aver- Figure 3: Architecture of our proposed model. The left part is the two-tower structure of our proposed method in which we use a Siamese neighborhood matching layer to involve more homogeneous node patterns and utilize a view-transformation module to involve more supervised signals. The right part is the metapath-based heterogeneous GNN in which we use an autoencoder to reduce the inî€uence of noisy signals and the computational cost. process of graph convolution can be formulated as below: â„= AGGREGATE(â„, âˆ€ğ‘¢ âˆˆ N whereâ„denotes the hidden state of nodeğ‘£after theğ‘˜-th convolution layer, andAGGREGATEis a pooling operator (mean, sum, and etc.) andâ„is the neighborhood vector ofğ‘£that incorporates the surrounding information ofğ‘£given a meta relationğ‘Ÿ. Without loss of generality, we instantiate the pooling function as a sum operator. In(3),ğ‘“ (Â·)andğ‘”(Â·)are two distinctive data-driven functions that mapsâ„andâ„to a new feature space respectively. HereN(ğ‘£)contains the neighbors ofğ‘£with the top-m largest edge weights. The motivation we select two distinctive functions is due to the heterogeneity of these two hidden representations which correspond to diî€erent node types. For the hidden features of the node itself in(4), we directly use a linear projection with a weight matrix Wâˆˆ R. For the neighborhood vector in(5), we î€›rst compress it to a low-dimension vector sizedğ‘™by a linear transformation with Vâˆˆ R, whereğ‘™ < ğ‘‘, followed by a activation functionğœ. For term convenience, we nameğ‘™as latent feature size. We then apply another linear combination withUâˆˆ Rto map the intermediate hidden features to the original dimension sizedğ‘‘. We select such autoencoder-based method for the neighborhood vector because it can be viewed as a noise î€›lter from the neighborhood context which usually contains some irrelevant information. Besides, it can also reduce the computational complexity compared with the direct assignment of a weight matrix sizedğ‘‘ Ã— ğ‘‘on the hidden features fromO(ğ‘‘)toO(ğ‘‘ Â·ğ‘™). Although the graph attention network (GAT) [22,27] also addresses the problem to extract the most inî€uential information from neighbors by the attention mechanism, it has been experimentally veriî€›ed that GAT performed the same as or worse than GCN in noisy graphs [33]. This is because GAT introduces too many parameters when learning attention coeî€œcients, which leads to overî€›tting to noise. Besides, compared with our design, GAT is more computation expensive. After feeding the node information toğ¾such layers, which are deî€›ned by metapathğ‘, we can eventually obtain the node semantic embeddingâ„. To reduce the computational cost, the network parameters of the graph convolution are shared across diî€erent nodes at the same layer. Semantic Attention Layer.We next introduce the semantic attention layer, which aims to fuse multiple embeddings extracted based on various metapaths. The general idea is as follows: diî€erent metapaths reveal diî€erent aspects of node context; thus, there is a necessity to aggregate the semantics revealed from diî€erent metapaths. To address this issue, we follow the idea proposed in [?], which uses a self-attention mechanism to capture the diverse semantics revealed from diî€erent metapaths. The mathematical expressions of the semantic fusion are as follows: whereğ‘¤is the learned importance weight of metapathğ‘, which is computed by a scaled self-attention mechanism from(6), and ğ‘Šâˆˆ Ris the weight matrix for mapping the original hidden representationâ„to a scalar which will be scaled to the importance weight of the corresponding metapath ğ‘. In the past two sections, we have presented the methodology to fuse node-level and subgraph-level information. In an ordinary matching model, the next phase is usually to feed the source and target embeddings into a contrastive loss function, like in [12]. Before calculating the loss function, in this section, we design a Siamese neighbor matching layer to transform the original matching problem between heterogeneous nodes to a matching problem between two pairs of<ad, keyword>, which is illustrated in the left part of Figure 3. Each pair contains the source or target node and its most inî€uential neighbors. More speciî€›cally, the<ad, keyword> pair in the tower of ads contains the embedding of source adğ‘£ and the averaged embedding ofğ‘£â€™sğœ…most inî€uential neighbors with type of keyword, while the<ad, keyword>pair in the tower of keywords contains the embedding of target keywordğ‘£and the averaged embedding of itsğœ…most inî€uential neighbors with type of ad. We î€›nally combine the averaged semantic embedding of a nodeâ€™s most inî€uential neighbors with the node itself. The motivation here is that although in a heterogeneous matching problem, the embeddings of both the source node (ad) and the target node (keyword) are expected to be close in feature space by optimization, there is no explicit guarantee for such expectations due to their diî€erences in the network structures, parameters, and metapaths to compute the î€›nal embeddings. Besides, by introducing so much side-information brought by the heterogeneous graph neural networks, it would become more challenging to satisfy such assumption. In contrast, compared with only utilizing the source semantic embedding itself, introducing the averaged semantic embedding of its inî€uential neighbors (named neighborhood embedding) would help match with target embedding. This is because they share the same raw feature types, network structures, metapaths, and parameters. Furthermore, the inî€uential neighbors can also directly characterize what the source nodes are like in an ecommerce scenario. The mathematical expressions are formulated as below: whereN(ğ‘£)denotesğœ…most inî€uential keywords bid by node ğ‘£; similarly,N(ğ‘£)denotesğœ…most inî€uential ads bidding on keywordğ‘£. In this way, the whole structures to computeğ‘§and ğ‘§are symmetric, which is also the reason we name this network Siamese neighbor matching layer. Itâ€™s important to note that Siamese neighborhood matching mechanism diî€ers from the dual matching scheme used in [14,16,31]. These studies design symmetric network structures for heterogeneous objects in two-tower; however, their neighborhood embedding is computed based on diî€erent metapaths and network parameters thus tend to be hard for heterogeneous matching. For improving the eî€ectiveness of matching with cold-start ads, we introduce diî€erent types of relations between ads and keywords as our objectives. More speciî€›cally, we select click relations between ads and keywords, bid relations between ads and keywords, and click relations between keywords and items (each ad corresponds to an item (Figure 2)) as our objectives. As diî€erent type of objectives might have diî€erent feature space to match, we use a view transformation functionğ‘“(Â·)to map the original embeddingsğ‘§to a view-speciî€›c embeddingğ‘§, whereğ‘“(Â·)is a view-speciî€›c multi-layer perceptron and ğ‘Ÿ represent the view type. We train our HetMatch in a supervised manner using a contrastive loss used in [12]. The basic idea is that we aim to maximize the inner product of positive pairs, i.e., the transformed embedding of the source ad and the corresponding related keyword. Meanwhile, we also want the inner product of negative examples, the source ad and its unrelated keyword, to be smaller than that of the positive sample. To achieve this, we compute the posterior probability of a keyword given an ad from the semantic relevance score between them through a softmax function: whereğ›¾is a smoothing factor in the softmax function.Qdenotes a subset of candidate keywords to match. For each positive pair, denoted by (ğ‘£,ğ‘£) whereğ‘£is the relevant keyword,Qincludes ğ‘£and î€›ve randomly selected keywords in the candidate setQ. The candidate setQcontains the keywords that are discriminated by a category classiî€›er as the same category as ad ğ‘£. In the training phase, we optimize the whole networksâ€™ parameter set by maximizing the likelihood of the posterior probability of all keywords across the training set. Equivalently, we acquire to optimize the following loss function:îƒ• whereÎ˜represents the parameter set of the whole networks which is updated iteratively by an Adam optimizer. Pipeline of Keyword Recommendation.Keyword recommendation is a multi-stage system, consisting of matching, î€›ltering and ranking phases. In the keyword matching phase, we î€›rst handle network construction from user behavioral records and advertiser bidding records oî€Ÿine by MapReduce downstream in the Alibaba Cloud platform. During the training and inference, we leverage the distributed deep learning framework XDLand graph search engine, Euler, to conduct run-time subgraph extraction and parameter update in a distributed fashion. After inference, ad embeddings and keyword embeddings are stored in a database for online services, and we use an approximate nearest neighbor (ANN) search engine to retrieve the most relevant keywords for each ad. In the î€›ltering phase, we use a term-match based relevance model to î€›lter the unrelated keywords for each ad. Lastly, in the ranking phase, we use an MLP-based model with enriched features to estimate the potential number of clicks brought by each remained keyword and rank those keywords based on the estimated values. In this way, we can recommend advertisers with the keywords that can bring as many user clicks as possible. Acceleration.HetMatch can be implemented by sampling subgraphs separately for each node and computing each nodeâ€™s corresponding î€›nal embedding via neural networks, which is computation consuming. As the extracted subgraphs based on the top-k sampling strategy of a particular node and its inî€uential neighbors usually share plenty of relation-paths, we lighten the computation burden by only calculating the intermediate embedding once for each distinctive relation-path. Negative Sampling.As illustrated in Section 4.5, for each positive pairs, we need to sample multiple negative keyword nodes for training. The whole procedure for negative sampling can be divided into two phases: 1) we calculate the weights (square root of the searched count) of keywords in each leaf category oî€Ÿine. 2) At run time of training, for each positive<ad, keyword>, we pick î€›ve negative keywords in the same leaf category by weighted random selection with the help of the graph index engine Euler. Dataset Description.We evaluate our proposed method based on a real-world dataset collected from the search logs in Taobao platform and the advertiser behavioral records in Alibabaâ€™s e-commerce sponsored search platform. The dataset covers seven consecutive days of these records, spanning from August 19th, 2020 to August 26th, 2020. We construct our heterogeneous network from these logs based on the network schema illustrated in Figure 2, where the edge weight (the importance of an edge) is deî€›ned as the appearing times of a particular relation in the records. For the training set, we sample 10M ad-keyword pairs in each view. The target relation set contains 50M click relations between ads and keywords in the constructed network. To prevent information leakage, we drop the relations in the target relation set from the original network. The more speciî€›c data statistical information is exhibited in Table 2. Baselines.To demonstrate our proposed methodâ€™s eî€ectiveness, we compare our method with multiple baseline methods and their variants. To ensure the fairness of the comparison, the results we reported are all under the multi-view learning framework mentioned in Section 4.4 unless otherwise stated. â€¢ Term-Match: This is a keyword retrieval method that extracts related keywords by calculating the term similarity between adsâ€™ titles and keywords, which is one of the keyword matching models currently deployed in Alibabaâ€™s ecommerce sponsored search platform. â€¢ DSSM: This is a two-tower matching model that projects the features of objects to a low dimensional space via multi-layer perceptrons (MLPs) [12]. â€¢ HAN: We replace the MLPs in DSSM by heterogeneous attention networks (HANs) proposed in [25]. The metapaths to aggregate neighbors are mentioned in Section 4.2. â€¢ IntentGC: This is a dual-HGCN-based recommendation model that captures heterogeneous relations between nodes with the same node types [31]. â€¢ HetMatch: This is our proposed model. We set the embedding lengthğ‘‘as 64 and the latent sizeğ‘™as 16. The model is optimized using an Adam optimizer with a learning rate of 0.03, and the batch size is 512. â€¢ DSSM(s), HAN(s), IntentGC(s): Based on DSSM, HAN or IntentGC, we add the Siamese matching layer between the output of multi-layer perceptrons and the multi-view transformation layerâ€™s input. â€¢ HetMatch: This model drops the Siamese neighbor matching layer in HetMatch. The remaining settings for this variant are the same as for our proposed methods. â€¢ HetMatch: This model only utilizes click data between ads and keywords as the labeled positive samples. â€¢ HetMatch: This variant replaces the graph convolution layer with autoencoder of HetMatch by GraphSage [8]. â€¢ HetMatch, HetMatch: Each of these variants only use the bid/item relations to aggregate neighborsâ€™ information. HetMatch18.82% 29.60% 48.34% 63.31% HetMatch19.32% 30.28% 49.77% 65.63% HetMatch17.99% 23.91% 26.66% 47.43% HetMatch15.97% 26.74% 46.33% 64.81% HetMatch15.78% 26.62% 47.48% 66.11% As our task focuses on retrieving hundreds of keyword candidates, we use the recall rate as our evaluation metrics as illustrated in Section 3, which is also adopted in [11]. As our model is under a multi-view framework (the number of view is 3), we use Recall@3K instead of Recall@K as our metrics. For the approaches under the multi-view framework, we retrieve each adâ€™s the top-K most related keywords in each view, and aggregate these relations as the î€›nal retrieved setOto compute Recall@3K. To ensure fair comparison, for those methods that are not under a multi-view framework (TermMatch and HetMatch), we directly retrieve the top-3K most related keywords for each ad to compute Recall@3K. Table 3 reports the performance of our proposed method compared to other competing methods or some variants from them. It can be seen from the table that our model can consistently outperform all comparative approaches or variants by achieving the highest recall rate in most cases. Firstly, we compare the relevancebased model deployed in our platform with other embedding-based retrieval methods. We observe that our relevance-based model (Term-Match) cannot achieve a good performance, especially when K is large. This î€›nding demonstrates that the retrieval model that only relies on text relevance might not have good practical performance. Then we compare HetMatch with DSSM that does not utilize neighbor information. We note that our method achieves much better performance, as HetMatch can capture auxiliary information from heterogeneous relationships. Besides, we compare HetMatch with GNN-based methods (HAN, IntentGC); here, HAN and IntentGC also performs the aggregation operations on neighbors, but it does not explicitly consider the misleading inî€uence brought by noisy links and features, and the attention mechanism used in HANâ€™s aggregator might lead to overî€›tting. To further demonstrate how our Siamese neighbor matching layer helps matching task, we conduct several ablation studies on DSSM(s), HAN(s), IntentGC(s) and HetMatchğ‘ . The comparison between DSSM with DSSM(s), HAN with HAN(s), IntentGC with IntentGC(s) and HetMatch with HetMatchpresents the eî€ectiveness of introducing Siamese neighbor matching in a DNNbased matching task. Interestingly, we observed that adding our Siamese neighbor matching layer to a simple DNN-based matching approach (DSSM) can signiî€›cantly improve performance, achieving very close performance to other GNN-based methods. This again shows the importance of conducting Siamese neighbor matching. We can also note that HetMatch surpasses the performance of HetMatch. One reason for this is that the autoencoder based graph convolution layer prevents misleading information from propagating from neighbors and thus enhances the quality of neighbor embeddings. Besides, by eliminating the multi-view framework, we observe a drastic drop in performance, which demonstrates the need to involve diî€erent types of objectives in learning. We also explore how our proposed model performs with diî€erent groups of metapaths. Compared with sampling neighbors based on both bid-based and item-based metapaths, sampling neighbors with only one group of metapath harms the performance. This is because the bid-based and item-based relations can provide useful information from diî€erent aspects, which can complement each other. It would also be interesting to note that the performance of HetMatch is better than the performance of HetMatchwhen K is no more than 200, and vice versa when K is more than 200. This is because the item-based metapaths brings more diverse neighbors compared with those sampled based on the bid-based metapaths (since this group is not directly related to the object task), which might hamper the quality of matched elements whose matching score is relatively high, but improve the total recall rate when K is relatively large. This also shows the importance of selecting proper types of metapaths in the task of keyword matching. This section aims to compare the performance of diî€erent approaches for each view objectives (Table 4). Our model achieves state-of-the-art performance in each view, and its improvement on the performance over other baseline methods is signiî€›cant. This î€›nding demonstrates our model can achieve a good performance not only in total but also in each channel. Another interesting î€›nding is that we note that the performance in the channel of bidding relations is worse than that in the channels of item-clicking and ad-clicking in most cases. This is because the labels of bidding relations are extracted by advertisersâ€™ manual selection of keywords, which might introduce some non-professional bidding behaviors and achieve performance. Besides, we also note that HetMatch achieves better performance over HetMatchin the view of adclick, which demonstrates that learning with auxiliary labels in other views can improve the performance in the each view. Conducting cold-start recommendation is a critical problem for a recommender system, which is not an exception for keyword recommendation. In this section, we present the experimental results for the cold-start ads. More speciî€›cally, Table 5 reports the performance of diî€erent methods on the ads which are newly created on August 26, 2020. Unlike the target set we used in Section 5.2, we select the candidate set of a new ad as the keywords that bring user clicks to the ad in the next 14 days. The performance of these methods is consistent with that in Section 5.2. HetMatch performs the best across all the competitive approaches, which demonstrates the HetMatch works well on the whole dataset and has an excellent capability to retrieve useful keywords for new ads. More speciî€›cally, we î€›nd that learning with various groups of metapaths and using autoencoder module in graph convolution and Siamese neighbor matching layer can improve the performance on cold-start ads. Table 5: Performance comparison of cold-start ads. HetMatch17.05% 28.13% 48.93% 67.01% HetMatch19.32% 30.28% 49.77% 65.63% HetMatch12.95% 22.91% 31.78% 41.50% HetMatch16.35% 27.58% 49.35% 68.77% HetMatch16.22% 27.33% 48.99% 68.31% To evaluate the eî€ectiveness of our proposed method on the realworld products, we deployed our HetMatch both on the keyword suggestion tool and the automatic keyword hosting tool of Alibabaâ€™s e-commerce sponsored search platform. More details about these tools can be referred to in the part of Appendix. Table 6 presents the results of online AB tests on the keyword suggestion tool and the automatic keyword hosting tool in Alibabaâ€™s e-commerce sponsored search platform. For the keyword suggestion tool, our proposed method achieves an improvement of +4.19% in terms of the adopting rate and +5.35% in terms of the number of clicks over the online deployed term-match based model. For the automatic keyword hosting tool, our proposed method obtains an improvement of +10.89% in terms of the number of clicks over the online deployed GraphSage-based matching model. Table 6: Online performance of compared methods in diî€erent scenes. Keyword suggestiona term-match-basedadopting rate + 4.19%, toolmodel#click +5.35% Keyword hosting toola two-layer Graph-#click +10.89% In this paper, we proposed HetMatch for the keyword matching problem based on a metapath-based heterogeneous GNN, which consists of a hierarchical structure to capture complex structures and rich semantics behind heterogeneous information networks in a robust way. The proposed model leverages node-level fusion, subgraph-level fusion, and Siamese neighbor matching to adaptively aggregate relevant neighborhood patterns. By introducing the autoencoder-based graph convolution layer and the Siamese matching layer, HetMatch can mitigate the negative eî€ect from noisy patterns and enhance relevant information. In addition, we use a multi-view framework to learn the posterior probability of various objectives, which helps introduce more supervised signals. Experimental results show that our model consistently outperforms competitive approaches. Extra online AB tests also demonstrate the superiority over existing methods deployed online. For future work, weâ€™d like to continue exploring into other sophisticated relations hidden in the heterogeneous graph, such as text similarity, user proî€›les, etc. Furthermore, HetMatch still relies on human-crafted metapaths while recently there are a few transformer-based models that could potentially learn all aggregation strategies by themselves [10], which is an auspicious direction. Last, considering Bert-like pre-trained models have pushed state of the art in many areas of NLP, combining Bert with GNN to enhance the quality of keyword retrieval may also be promising. That being said, both transformer-based HGNN and Bert-like models are usually of very large model size, which would be a great challenge for training and deployment in industrial systems. Further eî€orts like model compression might also be required. For the implementations of all comparable methods, we set the same hyperparameters. Keyword Suggestion Tool.This is a suggestion tool for advertisers when they manually add keywords. More than 30% of new keywords with impressions are brought by the suggestion tools each day. For each ad, the suggestion tool will present hundreds of keyword candidates; the advertisers can adopt the candidates from the suggestion list. The control group contains a relevance-based keyword matching model. In the treatment group, we combine keywords retrieved by HetMatch and the relevance-based model. The adopting rate of the suggested keywords and the total clicks brought by the suggested keywords are the main metrics that we are concerned about. Keyword Hosting Tool.In Alibabaâ€™s e-commerce sponsored search platform, advertisers are provided with many black-box marketing tools such as the automatic keyword hosting tool. By merely setting budgets and bid prices, advertisers can handle online marketing for sponsored search without explicitly choosing any keywords. The automatic hosting procedure hidden behind the product can be divided into two parts. The î€›rst part is the addition of keywords based on the retrieved keywords with estimated ad clicks, while another part is to delete existing keywords that cannot bring impressions or have very low ctr/cvr. In the experiment, the control group contains a two-layer GraphSage based matching model, which is proven eî€ective in the previous AB test. In the treatment group, we use the keywords retrieved by HetMatch. We use the averaged ad clicks brought by daily keyword addition to evaluate diî€erent methodsâ€™ performance.