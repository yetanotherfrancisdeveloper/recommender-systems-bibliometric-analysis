Recommendation Systems (RS) are a type of choice advisor to overcome the explosive growth of information on the web. These systems facilitate users with personalized items (products or services), which they are more likely to be interested in. RS have been employed to a wide variety of ï¬elds: movies (Wei et al., 2016; Moreno et al., 2016), music (Mao et al., 2016; Horsburgh et al., 2015), news (Shi et al., 2016; Wang and Shang, 2015), books, e-commerce, tourism, etc. An eï¬ƒcient RS may dramatically increase the number of sales of customers to boost business (Jannach et al., 2010; Ricci et al., 2015). In common, recommendations are generated based on user preferences, item features, user-item interactions, and some other information such as temporal and spatial data. RS methods are mainly categorized into Collaborative Filtering (CF), Content-Based Filtering (CBF), and hybrid recommender system based on the input data (Adomavicius and Tuzhilin, 2005). CF models (Salah et al., 2016; Polatidis and Georgiadis, 2016; Koren and Bell, 2015) aim to exploit information about the rating history of users for items to provide a personalized recommendation. In this case, if someone rated a few items, CF relies on estimating the ratings he would have given to thousands of other items by using all the other usersâ€™ ratings. On the other side, CBF uses the useritem side information to estimate a new rating. For instance, user information can be age, gender, or occupation. Item information can be the movie genre(s), director(s), or the tags. CF is more applied than CBF because it only aims at the usersâ€™ ratings, while CBF requires advanced processing on items to perform well (Lops et al., 2011). Nevertheless, the CF model is preferred; it has some limitations. One of CFâ€™s limitations is known as the cold-start problem: how to recommend an item when any rating does not exist for either the user or the item? One idea to overcome this issue is to build a hybrid model by combining CF and CBF, where side information can be utilized in the training process to compensate the lack of ratings through it. Some successful approaches extend the Probabilistic Matrix Factorization (Adams and Murray, 2010; Salakhutdinov and Mnih, 2008) to integrate side information. However, some algorithms outperform them in the general case. The past few decades have tremendous achievements of the deep learning (DL) in many application domains such as computer vision (Ding and Tao, 2015; Tian et al., 2016; Byeon et al., 2016; Huang and Sun, 2016) and speech tasks (Graves et al., 2013; Xue et al., 2016). Deep learning models have already been studied in a wider range of applications due to its capability in solving many complex tasks. Recently, DL has been inspiring the recommendation frameworks and brought us many performance improvements to the recommender. Deep learning can capture the non-linear user-item relationships and catches the complicated relationships within the data itself from diï¬€erent data sources such as visual, textual, and contextual. In recent years, the DL-based recommendation models achieve state-of-the-art recommendation tasks, and many companies apply deep learning for enhanced quality of their recommendation (Covington et al., 2016; Sh. Okura et al., 2017). For example, Salakhutdinov tackled the Netï¬‚ix challenge using Restricted Boltzmann Machines (RBM-CF) (Georgiev and Nakov, 2013). AutoRec is an Autoencoder for collaborative ï¬ltering (Sedhain et al., 2015), which uses Autoencoder to predict missing ratings. Uencoders are stacked, denoising Autoencoders, and Sparse Inputs for collaborative ï¬ltering (Strub and Mary, 2015). Covington et al. (2016) proposed a DNN-based recommendation algorithm for video recommendation on YouTube, Cheng et al. (2016) presented an application recommender system for Google Play, and Sh. Okura et al. (2017) presented an RNN-based recommender system for Yahoo News. All of these models have shown signiï¬cant improvement over traditional models. However, the existing deep learning models have not regarded the side information about the users or items, which is highly correlative to the usersâ€™ rating. Indeed, combining deep learning and side information may help us to discover a surpass solution for the considered challenges. In this paper, we introduce a hybrid approach using Autoencoder, which tackles both challenges: learning a nonlinear representation of users-items and dominating the cold start problem by integrating side information. Compared to previous models in that direction (Sedhain et al., 2015; Strub and Mary, 2015; Wu et al., 2016), our framework integrates the usersâ€™ preferences, similarities, and side information in a unique matrix. This conjunction leads to improved results in CF. The outline of the paper is organized as follows. First, section 2 discusses related works in both Autoencoder-based and hybrid recommendation models. Then, our proposed model is described in section 3. Finally, experimental results are given and discussed in section 4 and followed by a conclusion section. This section introduces the categories of DL-based recommendation models and then focuses on advanced research to identify the most outstanding and promising progress in recent years. Deep learning is a research ï¬eld of machine learning. It learns multiple levels of representations and abstractions from data and it can solve both supervised and unsupervised learning tasks. We can categorize the existing recommendation models based on the types of employed deep learning approaches into the following two classes (Zhang et al., 2019) â€¢ The recommendation with Neural Building Blocks; In this category, the deep learning technique determines the recommendation modelâ€™s applicability. For example, MLP can simply model the non-linear interactions between users and items; CNNs can extract local and global representations from heterogeneous data sources like text and image; recommender system can model the temporal dynamics and sequential evolution of content information using RNNs. Figure 1: Schematic structure of an Autoencoder with Three fully connected hidden layers. The code (z) is the most internal layer. DL-based recommendation models utilize more than one deep learning technique. Deep neural networksâ€™ ï¬‚exibility makes it possible to combine several neural building blocks to complement one another and form a more powerful hybrid model. There are many possible combinations of these deep learning techniques, but not all have been exploited. Additionally, we review and summarize some publications which utilize Autoencoder, and they will be discussed in the following sub-sections. Autoencoder is an unsupervised model attempting to reconstruct its input data in the output layer. In general, the bottleneck layer (the middle-most layer) is used as a salient feature representation of the input data (Zhang et al., 2019). The schematic of basic Autoencoder is illustrated in Figure 1, which output ğ‘‹should become closer to the input ğ‘‹ and the bottleneck layer is shown by ğ‘§. The main variants of Autoencoders can be considered as denoising Autoencoder, marginalized denoising Autoencoder, sparse Autoencoder, contractive Autoencoder and variational Autoencoder (Goodfellow et al., 2016). There are two general ways to apply Autoencoder to a recommender system (Zhang et al., 2019): 1. Using Autoencoder to learn lower-dimensional feature representations at the bottleneck layer; or 2. Filling the blanks of the interaction matrix directly in the reconstruction layer. Almost all the Autoencoder variants such as denoising Autoencoder, variational Autoencoder, contractive Autoencoder, and marginalized Autoencoder can be applied to the recommendation task. In this paper, we employed the ï¬rst technique to extract new low-dimension features. Figure 1 illustrates the structure of diï¬€erent recommendation models based on Autoencoder (Zhang et al., 2019). One of the successful applications is to consider collaborative ï¬ltering from the Autoencoder perspective. AutoRec (Sedhain et al., 2015) took user partial vectors ğ‘Ÿor item partial vectors ğ‘Ÿas input and attempted to reconstruct them in the output layer. Indeed, it has two variants: item-based AutoRec (I-AutoRec) and user-based AutoRec (U-AutoRec), corresponding to the two types of inputs. There are essential points about AutoRec that worth noticing (Zhang et al., 2019). First, I-AutoRec performs better than U-AutoRec, which may be due to the higher variance of user partially observed vectors. Second, a diï¬€erent combination of activation functions will inï¬‚uence the performance signiï¬cantly. Third, moderately increasing the hidden unit size will improve the result as expanding the hidden layer dimensionality gives AutoRec more capacity to model the input characteristics. Furthermore, adding more layers to formulate a deep network can lead to slight improvement. CFN (Strub et al., 2016; Strub and Mary, 2015) is a continuation of AutoRec, and posses the following two improvements: 1. Deploying the denoising techniques makes CFN more robust. 2. Incorporating the side information such as user proï¬les and item descriptions mitigates the sparsity and cold start inï¬‚uence. The CFN input is also partially observed vectors, so it also has two variants: I-CFN and U-CFN, taking ğ‘Ÿand ğ‘Ÿ as input, respectively. Masking noise is imposed as a great regularizer to better deal with missing elements (with zero value). Further extension of CFN also incorporates side information. However, instead of just integrating side information in the ï¬rst layer, CFN injects side information in every layer (Zhang et al., 2019). Collaborative Denoising Autoencoder (CDAE). The three models reviewed earlier are mainly designed for rating prediction, while CDAE (Wu et al., 2016) is principally used for ranking prediction. The input of CDAE is user partially observed implicit feedbacks. If the user likes the movie, the entry value is one, otherwise zero. It can also be considered as a preference vector that reï¬‚ects the userâ€™s interests in items (Zhang et al., 2019). Figure 1b illustrates the structure of CDAE. This model uses a unique weight matrix for each user and has a notable impact on model performance. Parameters of CDAE are also learned by minimizing the reconstruction error. CDAE initially updates its parameters using SGD overall feedbacks. However, it is impractical to consider all ratings in real-world applications. A negative sampling technique has been proposed to sample a small subset from the negative set (items with which the user has not interacted), which reduces the time complexity substantially without degrading the ranking quality (Zhang et al., 2019). Figure 2: Illustration of: (a) Item based AutoRec; (b) Collaborative denoising Autoencoder; (c) Deep collaborative ï¬ltering framework (Zhang et al., 2019) Muli-VAE and Multi-DAE (Liang et al., 2018) proposed a variant of variational Autoencoder for recommendation with implicit data, showing better performance than CDAE. These methods introduced a principled Bayesian inference approach for parameter estimation and showed agreeable results than generally used likelihood functions. Based on a survey by Zhang et al. (2019), Autoencoderbased Collaborative Filtering (ACF) (Ouyang et al., 2014) is the ï¬rst Autoencoder based collaborative recommendation model. Instead of using the original partial observed vectors, it decomposes them by integer ratings. Like AutoRec and CFN, ACF aims at reducing the mean squared error as the cost function. But, there are two demerits of ACF; it loses to deal with non-integer ratings, and the decomposition of partially observed vectors increases the sparseness of input data and drives to worse prediction accuracy. Autoencoder is a dominant feature representation learning approach, and it can be used in recommender systems to learn feature representations from users-items content features. In the following, we will summarize some of the related methods. Collaborative Deep Learning (CDL) (Wang et al., 2015) is a hierarchical Bayesian model that integrates stacked denoising Autoencoder (SDAE) into probabilistic matrix factorization. The method proposed a general Bayesian deep learning framework (Wang and Yeung, 2016) to combine the deep learning and recommendation model. The framework consists of two tightly coupled parts: the perception component (deep neural network) and task-speciï¬c component. Mainly, CDLâ€™s perception component is a probabilistic representation of ordinal SDAE, and PMF (Probability Mass Function) works as the task-speciï¬c component. This tight combination enables CDL to balance the impacts of side information and interaction records. Collaborative Deep Ranking (CDR). CDR (Ying et al., 2016) is devised speciï¬cally in a pairwise framework for topn recommendation. Some studies have demonstrated that the pairwise model is more suitable for ranking lists generation. Experimental results also show that CDR outperforms CDL in terms of ranking prediction (Zhang et al., 2019). Deep Collaborative Filtering Framework is a general framework for unifying deep learning approaches with a collaborative ï¬ltering model (Li et al., 2015). This framework makes it easy to utilize deep feature learning techniques to build hybrid collaborative models (Zhang et al., 2019). There is a marginalized denoising Autoencoder-based collaborative ï¬ltering model (mDA-CF) on top of this framework. In comparison to CDL, mDA-CF explores more computationally eï¬ƒcient variants of the Autoencoder. The method saves the computational costs of searching suï¬ƒcient corrupted input by marginalizing the corrupted input, and it makes the mDACF more scalable than CDL. Plus, mDA-CF embeds content information of items and users, while CDL only regards item featuresâ€™ eï¬€ects. AutoSVD++ (Zhang et al., 2017) uses contractive Autoencoder (Rifai et al., 2011) to learn item feature representations, then integrates them into the classic recommendation model, SVD++. The model posses the following advantages (Zhang et al., 2019) Autoencoder captures the inï¬nitesimal input variations. 2. It models the implicit feedback to enhance the accuracy further. 3. An eï¬ƒcient training algorithm is designed to reduce training time. HRCD (Wei et al., 2017) is a hybrid collaborative model based on Autoencoder and timeSVD++. It is a time-aware model that uses SDAE to learn item representations from raw features and solve the cold item problem (Zhang et al., 2019). In this section, we focus on our proposed method which can be categorized as a hybrid recommendation system. First we ï¬rst deï¬ne the basic notations used throughout the paper. Next, we describe the proposed model in an architectural view and algorithmic steps. Then, graph-based features will be declared separately. Finally, we will explain about the clustering method and how we ï¬nd the optimum number of clusters. We ï¬rst deï¬ne the basic notations used throughout this paper. Given the set of n users, ğ‘ˆ = {ğ‘¢, ..., ğ‘¢} , and the set of m item, ğ¼ = {ğ‘–, ..., ğ‘–}. All user-item pairs can be denoted by an n-by-m matrix ğ‘… = ğ‘ˆ Ã— ğ¼ , where the entry ğ‘Ÿindicates the assigned value of implicit feedback of user ğ‘¢ to item ğ‘–. If ğ‘Ÿhas been observed (or known), it is represented by a speciï¬ed rating associated in a speciï¬c range and interval; otherwise, a global default rating is zero. We used this matrix to ï¬nd similarity between usersâ€™ preferences. After generating the similarity graph which represents users as nodes and the relations as edges, we extract the features from this graph, ğ¹= {ğ‘“, ..., ğ‘“}, and preserve them in the nby-g matrix. We collect the some usersâ€™ features of dataset which are called side information, ğ¹= {ğ‘“, ..., ğ‘“}, some itemsâ€™ side information ğ¹= {ğ‘“, ..., ğ‘“} and obtain the combined feature matrix which is n-by-g+s. Without loss of generality, we categorized all the features of ğ‘¢as binary which enlarged ï¬nal feature vector for each user. The overall structure of our aggregated recommender system (GHRS) is presented in Figure 3. The Graph-based Hybrid Recommender System comprises the following six steps. At ï¬rst, we build a graph with the number of usersâ€™ nodes that connected two users based on their similarities, representing an edge. The edge connects a pair of users who have more than ğ›¼ percent of items with similar ratings. In the second step, a set of information will be extracted from the similarity graph for each user. For instance, we compute PageRank of the nodes, degree centrality, closeness centrality, the shortest-path betweenness centrality, load centrality, and the average degree of each nodeâ€™s neighborhood in the graph. As a result, this matrix relies on the diï¬€erent data processing magnitude using a preference-based collaborative approach. We combine side information such as gender and age with graph-based features in the third step to retrieve the most relevant movies for users. Therefore, we have one combined matrix from diï¬€erent types of features, which is then used as the Autoencoder stage input. During the Autoencoder stage, its capabilities for new feature extraction and a dimensional reduction will be deployed. Afterward, we utilize these new features for user clustering, using the K-means algorithm to create a small number of peer groups. Finally, the recommendation will be listed using the estimation of new rates for each user according to the average rating of its cluster, or the new user will be recommended to some movies. Algorithm 1 declared the total workï¬‚ow in details. Figure 3: The framework of the proposed recommendation system. The method encodes the combined features with autoencoder and create the model by clustering the users using the encoded features (upper part). At last, a preference-based ranking model is used to retrieve the predicted movie rank for the target user (lower part) This section reviews the intuition of some graph features that represent similarities and their general computational process. â€¢ Page Rank: Page Rank is an algorithm that measures the transitive inï¬‚uence or connectivity of nodes. It was initially designed as an algorithm to rank web pages (Xing and Ghorbani, 2004). We can compute the Page Rank by either iteratively distributing one nodeâ€™s rank (based on the degree) over the neighbors or randomly traversing the graph and counting the frequency of hitting each node during these paths. In this paper, we used the ï¬rst method. â€¢ Degree Centrality: Degree centrality measures the number of incoming and outgoing relationships from a node. The Degree Centrality algorithm can be used to ï¬nd the popularity of individual nodes (Freeman, 1979). The degree centrality values are normalized by dividing by the maximum possible degree in a simple graph n-1, where n is the number of nodes. â€¢ Closeness Centrality: Closeness centrality is a way to detect nodes that can spread information eï¬ƒciently through a graph (Freeman, 1979). The closeness centrality of a node u measures its average farness (inverse distance) to all n-1 other nodes. Since the sum of distances depends on the number of graph nodes, closeness is normalized by the sum of the minimum Input: ğ‘ˆ, ğ¼, ğ‘…, ğ¹, ğ¹ Output: Estimated rates for user-item tween two users of ğ›¼ (the percentage of items which two users rated them similarly) as nodes : Extracted graph-based features for users (nodes) : Preprocessed and categorized usersâ€™ side information (demographic informations) the Autoencoder on ğ¹and train the model with the best settings dimensional feature vector ğ¹ ğ¹and ï¬nd clusters ğ¶ then ter ğ‘) in the cluster ğ‘ then cluster ğ‘) possible distances ğ‘› âˆ’ 1. where ğ‘‘(ğ‘£, ğ‘¢) is the shortest-path distance between ğ‘£ and ğ‘¢, and ğ‘› is the number of nodes in the graph. Nodes with a high closeness score have the shortest distances to all other nodes. â€¢ Betweenness Centrality: Betweenness centrality is a factor which we use to detect the amount of inï¬‚uence a node has over the ï¬‚ow of information in a graph. It is often used to ï¬nd nodes that serve as a bridge from one part of a graph to another (Moore, 1959). The Betweenness Centrality algorithm calculates the shortest (weighted) path between every pair of nodes in a connected graph, using the breadth-ï¬rst search algorithm (Moore, 1959). Each node receives a score, based on the number of these shortest paths that pass through the node. Nodes that most frequently lie on these shortest paths will have a higher betweenness centrality score. where ğ‘‰ is the set of nodes, ğœ(ğ‘ , ğ‘¡) is the number of shortest path between (ğ‘ , ğ‘¡), and ğœ(ğ‘ , ğ‘¡îƒ°ğ‘¢) is the number of those paths passing through some node ğ‘¢ other than ğ‘  and ğ‘¡. If ğ‘  = ğ‘¡ â†’ ğœ(ğ‘ , ğ‘¡) = 1, and if ğ‘£ âˆˆ {ğ‘ , ğ‘¡} â†’ ğœ(ğ‘ , ğ‘¡îƒ°ğ‘¢) = 0 (Brandes, 2008). â€¢ Load Centrality. The load centrality of a node is the fraction of all shortest paths that pass through that node (Newman, 2001). â€¢ Average Neighbor Degree. Returns the average degree of the neighborhood of each node. The average degree of a node i is: where ğ‘(ğ‘¢) are the neighbors of node ğ‘¢ and ğ‘˜is the degree of node ğ‘£ which belongs to ğ‘ (ğ‘¢). For weighted graphs, an analogous measure can be deï¬ned (Barrat et al., 2004), where ğ‘ is the weighted degree of node ğ‘¢, ğ‘¤is the weight of the edge that links ğ‘¢ and ğ‘£, and ğ‘(ğ‘¢) are the neighbors of node ğ‘¢. As we mentioned before in section ğ‘‹, each user belongs to a speciï¬c cluster and the cluster rate for an item will be considered as the estimated rating for the user-item pair. In the proposed method we use K-Mean algorithm to cluster the users based on extracted features by Autoencoder. One important issue in using such algorithms is to ï¬nd the proper number of clusters regarding performance factors. We use two methods to choose the number of clusters; Elbow method and Average Silhouette algorithm. In this section we will explain the summary of K-Mean algorithms and how we tackle and solve the number of clusters issue with both mentioned methods. The K-means algorithm is a simple iterative clustering algorithm. Using the distance as the metric and given the K classes in the data set, calculate the distance mean, giving the initial centroid, with each class described by the centroid (Yuan and Yang, 2019; Awad and Khanna, 2015). For a given data set X with n data samples and the number of category K, the Euclidean distance is the measure of the similarity index, and the clustering method aims minimize the sum of the squares of the various types. It means that it minimizes (Wang et al., 2012) where ğ‘˜ represents ğ¾ cluster centers, ğ‘¢represents the ğ‘˜center, and ğ‘¥represents the ğ‘–point in the data set. The fundamental idea of the method is to (Yuan and Yang, 2019): 1. Randomly extract K sample points from the sample set as the center of the initial cluster 2. Divide each sample point into the cluster represented by the nearest center point 3. Update the center point of the cluster with the center point of all sample points in each cluster 4. Repeat the above steps until the center point of the cluster is unchanged or reaches the set number of iterations. The algorithm results change with the center pointâ€™s choice, resulting in an instability of the results. The central pointâ€™s determination depends on the choice of the K value, which is the focus of the algorithm (Yuan and Yang, 2019); it directly aï¬€ects the clustering results, such as the local optimality or global optimality (Rathod and Garg, 2017). The basic idea behind cluster partitioning methods, such as k-means clustering, is to deï¬ne clusters such that the total intra-cluster variation (known as a total within-cluster variation or total within-cluster sum of squares) is minimized. It measures the compactness of the clusters, and it should be as small as possible. Thus, we can use the following algorithm to deï¬ne the optimal clusters (Kaufman and Rousseeuw, 2009) 1. Compute clustering algorithm (e.g., k-means clustering) for diï¬€erent values of k, for instance, by varying k from 1 to 10 clusters. 2. For each k, calculate the total within-cluster sum of squares. 3. Plot the curve of the within-cluster sum of squares according to the number of clusters k. 4. The elbowâ€™s location in the plot is generally considered an indicator of the appropriate number of clusters. Brieï¬‚y, the average silhouette approach measures the quality of a clustering. It means that it determines how well each object occupies within its cluster. A high average silhouette width intimates a valuable clustering. The average silhouette method computes the average silhouette of observations for diï¬€erent values of k. The optimal number of clusters k is the one that maximizes the average silhouette over a range of possible values for ğ‘˜ (Kaufman and Rousseeuw, 2009). The algorithm is similar to the Elbow method declared in the previous section: 1. Compute clustering algorithm (in our method K-Means) for diï¬€erent values of ğ‘˜. For instance, by varying ğ‘˜ from 1 to 30 clusters. 2. For each ğ‘˜, calculate the average silhouette of observations. 3. Plot the curve of average silhouette according to the number of clusters ğ‘˜. 4. The location of the maximum is considered as the appropriate number of clusters. In this section, the performance of the proposed model is evaluated, analyzed, and enumerated in this section. The dataset is processed and described in detail, followed by the requisite experimental setup. Due to the variation of steps and processes in the proposed method, we elaborate on the practical results in-depth. Finally, we compared the method with basics and modern methods, which we discussed most of them in related works. We have utilized two benchmark datasets (MovieLens 100K and MovieLens 1M) of the real-world in recommender systems to implement the model practically (Harper and Konstan, 2015). MovieLens 100K contains 100,000 ratings ğ‘… âˆˆ {1, 2, 3, 4, 5}, 1,682 movies (items) rated by 943 users. MovieLens 1M comprises of 1,000,209 ratings ğ‘… âˆˆ {1, 2, 3, 4, 5} of approximately 3,900 movies made by 6,040 users. As discussed in Section 3, the proposed method uses usersâ€™ demographic data to solve the new usersâ€™ cold-start issue. Hence, due to the lack of usersâ€™ demographic data in larger datasets like MovieLens 10M, it would not be possible to evaluate the model more on larger datasets. We used the MovieLens 100K dataset for analyzing the proposed methodâ€™s steps. The ï¬nal evaluations and comparisons have been done on the MovieLense 1M dataset. Table 1 shows the details of the mentioned datasets. As declared in Section 3, we use two types of features in the proposed method: side information (usersâ€™ demographic data) and features extracted from the similarity graph between users. We transformed the demographic data into a categorical format, concatenated both types of features, and Figure 4: Visualization of Similarity Graph for ğ›¼ = {0.005, 0.01, 0.02, 0.03} for MovieLens 100K with 943 users. made the raw feature set before dimension reduction with an Autoencoder. In this section, we discuss a little about the statistics of the raw features. We have declared before the only parameter we have used for generating the graph is ğ›¼, the value of a threshold for connecting two users having at least several same movies in their ratings. This threshold is represented as a percentage of total movies in the dataset. Hence, we have an exploration of a very sparse graph to near a full-mesh graph. Figure 4 illustrates the similarity graph visualization for ğ›¼ = {0.005, 0.01, 0.02, 0.03} for 943 users in MovieLens 100K. Figure 5 shows the normalized graph-based featuresâ€™ distributions against each other for MovieLens 100K and MovieLens 1M with ğ›¼ = 0.015. We can see correlations between these types of features in some cases. As all the demographic features are transformed into a categorical format, the demographic features vector is onehot encoded and has a speciï¬c sparsity level for each dataset. On the other hand, we declared that the graph-based featuresâ€™ value is related to the similarity graph size, and the graph size is directly related to the factor ğ›¼. In Figure 6, we can see that the feature setâ€™s sparsity rises when the value of the ğ›¼ increases. We use 10-fold cross-validation on MovieLens 1M dataset and 5-fold cross-validation on MovieLens 100K dataset to partition the datasets into training and testing to measure the performance of the GHRS. The ï¬nal prediction metrics are the average of the iterations of training and testing base on the number of folds in each dataset. The training set comprises the User-Item list with given ratings, userâ€™s demographic information, and itemâ€™s side information. We consider the Root Mean Squared Error (RMSE) as the metric for evaluation. RMSE (eq) is generally related to the cost function of conventional rating prediction models: ğ‘…ğ‘€ğ‘†ğ¸ =ğ‘ğ‘¢ğ‘šğ‘ğ‘’ğ‘Ÿğ‘œğ‘“ ğ‘…ğ‘ğ‘¡ğ‘–ğ‘›ğ‘”ğ‘ (6) Besides those mentioned above, we also use Precision and Recall (the most popular metrics for evaluating information retrieval systems) as an evaluation metric to measure the proposed modelâ€™s accuracy. Precision measures the ratio of correct recommendations to the total recommendations, and Recall shows the ratio of correct recommendations to total correct information. Consequently, we have to separate the items into two classes with a threshold while considering their actual ratings, i.e., non-relevant and relevant to measure Precision and Recall. In this regard, items rated between [1 âˆ’3] were considered non-relevant and rated with [4 âˆ’ 5] as relevant. Additionally, the items in datasets were divided as selected and not selected on their predicted ratings. Therefore, Precision and Recall of the model can be deï¬ned as: ğ‘ƒ ğ‘Ÿğ‘’ğ‘ğ‘–ğ‘ ğ‘–ğ‘œğ‘› =ğ‘‡ ğ‘ƒğ‘‡ ğ‘ƒ + ğ‘‡ ğ¹(7) ğ‘…ğ‘’ğ‘ğ‘ğ‘™ğ‘™ =ğ‘‡ ğ‘ƒğ‘‡ ğ‘ƒ + ğ¹ ğ‘(8) Where ğ‘‡ ğ‘ƒ stands for True Positive (Item is correctly selected as it is relevant), ğ¹ ğ‘ƒ stands for False Positive (Item is incorrectly selected as it is not relevant), and ğ¹ ğ‘ stands for False Negative (Relevant item is not selected) Root Mean Square Error (RMSE) for the proposed model gives a lower error value for the testing dataset. Observed results produced by iterations of cross-validation (10-fold for MovieLens 1M and 5-fold for MovieLens 100K) show almost similar validation errors. Figure 5: Graph-based features for (a) MovieLens 100K and (b) MovieLens 1M both for ğ›¼ = 0.015. Abbreviation used in the ï¬gure: PR (page rank), CD (degree centrality), CC (closeness centrality), CB (betweenness centrality), AND (average neighbor degree), and LC (load centrality). In this experiment, we check the impact of graph size on rating accuracy. As we have use graph features for every node (users) in the similarity graph, itâ€™s important to produce a similarity graph in a state that represents the similarity between nodes as optimized as it can be. For this purpose, we experimented with searching in parameter space, which impacts the size and the shape of the similarity graph. Figure 7 shows the RMSE vs. ğ›¼ in both dataset we used for the evaluation. As it can be seen in the ï¬gure, there is no direct relation between the result of the method. But, the minimum value of RMSE achieved on a speciï¬c value of alpha in the middle of the experiment range. The main reason for this result is that when the alphaâ€™s value is very small, all users can be connected due to this Figure 6: Sparsity of combined featuresâ€™ dataset vs. ğ›¼ before dimension reduction Figure 7: RMSE vs. ğ›¼. There is an optimum point for ğ›¼ near the alpha = 0.01 for dataset MovieLens 100K and near the alpha = 0.005 for MovieLens 1M. value because we consider just a very little common items in their ratings to connect them to each other in the similarity graph. Hence, most of the users are similar to each other in this condition and the diï¬€erence will be missed in some cases. On the other hand, when the alphaâ€™s value raises the similarity graph become more sparse (As it is shown in Figure 6). So, we consider the most of users not related to each other when the ğ›¼ value increases to very large values. There is an optimum point for the size of the similarity graph near the alpha = 0.01 for dataset MovieLens 100K and near the alpha = 0.005 for MovieLens 1M. We declared that we use Autoencoder to simultaneously extract new features and reduce the raw feature set dimension before clustering. In this experiment, we examine the learning algorithms for Autoencoder and check each algorithmâ€™s ability to minimize the loss function on our input raw feature set. In all experiments, we have used a 5-layer Autoencoder with the structure shown in Figure 8. The activation function Figure 8: Structure of Autoencoder used for dimension reduction. Both input and output size are 35 for MovieLens 100K and 36 for MovieLens 1M Figure 9: The value of loss function on validation data over 100 epochs of training with seven target optimizers in the experiment of all layers is ReLU (Nair and Hinton, 2010). Both input and output size (raw feature vectorâ€™s sizes) are 35 for MovieLens 100K and 36 for MovieLens 1M. We experiment with a set of optimizers to train the Autoencoder. Optimizers include Adagrad (Duchi et al., 2011), Adadelta (Zeiler, 2012), RMSProp (Hinton et al., 2012), Adam (Kingma and Ba, 2014), AdaMax (Kingma and Ba, 2014), Nadam (Dozat, 2016) and AMSGrad (Reddi et al., 2019) with loss function of Mean Squared Error. Figure 9 shows the result of the experiment and compares the optimizers in minimizing the loss function on validation data over 100 epochs of training. We have randomly selected 10% of users in MovieLens 1M and 20% of users in MovieLens 100K as validation data and exclude them from the training set. We can see the best results for Adam, Adadelta, and RMSProp optimizers. As a discussion about the result, Figure 10: Final Features Scatter (Dimension is reduced to 4 using Autoencoder with Adam optimizer). RMSprop can be considered as an extension of Adagrad that deals with its radically diminishing learning rates. It is identical to Adadelta, except that Adadelta uses the RMS of parameter updates in the nominator update rule. Adam, ï¬nally, adds bias-correction and momentum to RMSprop (Ruder, 2016). RMSprop, Adadelta, and Adam are very similar algorithms that do well in similar conditions. Kingma and Ba (2014) showed that the bias-correction helps Adam slightly exceed RMSprop towards the end of optimization when the gradients become sparser. Hence, Adam seems to be the best option for the optimizer. Recently, many researchers use vanilla SGD without momentum and a simple learning rate annealing schedule (Ruder, 2016). Nevertheless, In our experiment, SGD approaches to achieves a minimum, but it may take longer than other methods. We selected Adam as the optimizer in Autoencoder to encode the raw feature set. The output of the encoding process shows a diverse distribution with a low correlation between the encoded features. Figure 11 shows the encoded features for both MovieLens 100K and MovieLens 1M, which will be used for clustering the users. This section examines the mentioned method in section 3.3 to ï¬nd the correct number of clusters for both datasets MovieLens 1M and MovieLens 100K. As listed before, we applied two methods for this reason; the Elbow method and the Average Silhouette method. The input of both methods is the encoded feature sets from the previous state. For both methods, we consider the range of K in [1 âˆ’ 30]. Figure 11 and Figure 12 show the algorithmsâ€™ iteration for the Elbow method and Average Silhouette method, respectively. The best value of K has been founded, as shown in Table 2. Performance of the proposed model has been evaluated on the datasets mentioned in section 4.1. Table ?? shows the result of the proposed model based on the best setting derived from the experiments conducted to ï¬nd the best values for parameters ğ›¼ (section 4.4) and K (section 4.6), and best optimizer for Autoencoder (section 4.5). Figure 11: Finding the elbow point as the optimum number of clusters of users. (a) K-Mean Algorithm. (b) MiniBatchKMean Algorithm Table 3 Performance metrics value for the proposed method on target dataset The proposed method (GHRS) has additionally been compared with some primary methods and state of the art methods. It is evident that the proposed method shows an improvement in the best result of RMSE on MovieLens 1M and has the best performance as same as AutoRec after the Autoencoder COFILS. Full comparison is shown in Table ??. We have proposed a method for the recommendation in user-item systems in this paper. The method can be used for every user-item system that provides side information for both users and items. The proposed methodâ€™s main idea is ï¬nding the relation between users based on their similarities as nodes in a similarity graph and combining them with Figure 12: Finding the elbow point as the optimum number of clusters of users. (a) K-Mean Algorithm. (b) MiniBatchKMean Algorithm the usersâ€™ side information to solve the cold-start issue. Plus, we applied Autoencoder to extract new low dimensional features with low correlation and more information. This made the ï¬nal clustering step more accurate and highly performed in time consumption. Final experiments and comparison with other methods showed the competitive results for the selected datasets and improved the best result on MovieLens 1M dataset. There are several lines of research arising from this work that should be pursued. Future research might apply for the work on the item properties like user side information to detect similarity between items precisely. Admittedly, it will be like considering similarities between two users who similarly rate the same items, and their rates and properties in the similarity graph are close to each other. Indeed, in this case, items will be considered similar if identical or similar users (based on similarity deï¬nition between users in this research) rate them with the same patterns. Thus, we will also have the approach using graph features and deep-learning for users, for items. On the other hand, it is great to devote future research to developing and extracting more other features from the similarity graph, which we did not mention in the current study. Besides, the structure of the Autoencoder might be an impor- Table 4 Comparison with other methods tant area for future research. The diï¬€erent structures should be examined regarding the Autoencoder structure aï¬€ecting feature extraction, training duration, and the modelâ€™s ï¬nal performance. In this article, we used a predeï¬ned structure for Autoencoder using the heuristic method and manual tuning. This assumption might be addressed in future studies. As discussed in section 4.1, few datasets have side information for users and items(e.g., demographic data for users). It will be desirable to assess the proposed method with other future datasets that include this information.