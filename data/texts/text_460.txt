Due to the growing privacy concerns, decentralization emerges rapidly in personalized services, especially recommendation. Also, recent studies have shown that centralized models are vulnerable to poisoning attacks, compromising their integrity. In the context of recommender systems, a typical goal of such poisoning attacks is to promote the adversaryâ€™s target items by interfering with the training dataset and/or process. Hence, a common practice is to subsume recommender systems under the decentralized federated learning paradigm, which enables all user devices to collaboratively learn a global recommender while retaining all the sensitive data locally. Without exposing the full knowledge of the recommender and entire dataset to end-users, such federated recommendation is widely regarded â€˜safeâ€™ towards poisoning attacks. In this paper, we present a systematic approach to backdooring federated recommender systems for targeted item promotion. The core tactic is to take advantage of the inherent popularity bias that commonly exists in data-driven recommenders. As popular items are more likely to appear in the recommendation list, our innovatively designed attack model enables the target item to have the characteristics of popular items in the embedding space. Then, by uploading carefully crafted gradients via a small number of malicious users during the model update, we can eî€ectively increase the exposure rate of a target (unpopular) item in the resulted federated recommender. Evaluations on two real-world datasets show that 1) our attack model signiî€›cantly boosts the exposure rate of the target item in a stealthy way, without harming the accuracy of the poisoned recommender; and 2) existing defenses are not eî€ective enough, highlighting the need for new defenses against our local model poisoning attacks to federated recommender systems. â€¢ Information systems â†’ Collaborative î€›ltering. Federated Recommender System; Poisoning Attacks; Deep Learning ACM Reference Format: Shijie Zhang, Hongzhi Yin, Tong Chen, Zi Huang, Quoc Viet Hung Nguyen, and Lizhen Cui. 2021. PipAttack: Poisoning Federated Recommender Systems for Manipulating Item Promotion. In Proceedings of ACM Conference (Conferenceâ€™17). ACM, New York, NY, USA, 9 pages. https://doi.org/10.1145/ nnnnnnn.nnnnnnn The demand for recommender systems has increased more than ever before. Over the years, various recommendation algorithms have been proposed and proven successful in various applications. Collaborative î€›ltering (CF), which infers usersâ€™ potential interests from usersâ€™ historical behavior data, lies at the core of modern recommender systems. Recently, enhancing CF with deep neural networks has been a widely adopted means for modelling the complex user-item interactions [20] and demonstrated state-of-the-art performance in a wide range of recommendation tasks. The conventional recommender systems centrally store usersâ€™ personal data to facilitate the centralized model training, which are, however, increasing the privacy risks [40]. Apart from privacy issues, another major risk emerges w.r.t. the correctness of the learned recommender in the presence of malicious users, where the trained model can be induced to deliver altered results as the adversary desires, e.g., promoting a target productsuch that it gets more exposures in the item lists recommended to users. This is termedpoisoning aî€Ÿack[12,25], which is usually driven by î€›nancial incentives but incurs strong unfairness in a trained recommender. In light of both privacy and security issues, there has been a recent surge in decentralizing recommender systems, where federated learning [28,33,34] appears to be one of the most representative solutions. Speciî€›cally, a federated recommender allows usersâ€™ personal devices to locally host their data for training, and the global recommendation model is trained in a multi-round fashion by submitting a subset of locally updated on-device models to the central server and performing aggregation (e.g., model averaging). Subsuming a recommender under the federated learning paradigm naturally protects user privacy as the data is no longer uploaded to a central server or cloud. Moreover, it also prevents a recommender from being poisoned. The rationale is, in recommendation, the predominant poisoning method for manipulating item Figure 1: Visualization on the popularity bias of a CF-based federated recommender. (a), (b) and (c) are the t-SNE projection of item embeddings at diî€erent training stages, where more details on popularity labels can be found in Section 4.1. (d) shows the convergence curve of a popularity classiî€›er using ğ¹ 1 score. promotion is data poisoning, where the adversary manipulates malicious users to generate fake yet plausible user-item interactions (e.g., ratings), making the trained recommender biased towards the target item [12,13,25,36]. However, these aforementioned data poisoning attacks operate under the assumption that the adversary has prior knowledge about the entire training dataset. Apparently, such assumption is voided in federated recommendation as the user data is distributively held, thus restricting the adversaryâ€™s access to the entire dataset. Though the adversary can still conduct data poisoning by increasing the amount of fake interactions and/or malicious user accounts, it inevitably makes the attack more identiî€›able from either the abnormal user behavioral footprints [23,38] or heavily impaired recommendation accuracy [25]. Thus, we aim to answer this challenging question: can we backdoor federated recommender systems via poisoning attacks? In this paper, we provide a positive answer to this question with a novel attack model that manipulates the target itemâ€™s exposure rate in this nontrivial decentralized setting. Meanwhile, our study may also shed some light on the security risks of federated recommenders. Unlike data poisoning, our attack approach is built upon the model poisoning scheme, which compromises the integrity of the model learning process by manipulating the gradients [4,5] of local models submitted by several malicious users. Given a federated recommender, we assume an adversary controls several malicious users/devices, each of which has the authority to alter the local model gradients used for updating the global model. Since the uploaded parameters directly aî€ect the global model, it is more cost-eî€ective for an adversary to poison the federated recommender from the model level, where a small group of malicious users will suî€œce. Despite the eî€œcacy of model poisoning in many federated learning applications, the majority of these attacks are only focused on perturbing the results in classiî€›cation tasks (e.g., image classiî€›cation and word prediction) [3,11]. However, designed for personalized ranking tasks, federated recommenders are optimized towards completely diî€erent learning objectives, leaving poisoning attacks for item promotion largely unexplored. In the meantime, the federated environment signiî€›cantly limits our adversaryâ€™s prior knowledge to only partial local resources (i.e., malicious usersâ€™ data and models) and the global model. Moreover, the poisoned global model should maintain high recommendation accuracy, so as to promote the target item stealthily while providing high-quality recommendations to benign users. To address these challenges, we take advantage of a common characteristics of recommendation models, namely the popularity bias. As pointed out by previous studies [1,39,41,42], data-driven recommenders, especially CF-based methods are prone to amplify the popularity by over-recommending popular items that are frequently visited. Intuitively, because such inherent bias still exists in federated recommenders, if we can disguise our target item as the popular items by uploading carefully crafted local gradients via malicious users, we can eî€ectively trick the federated recommender to become biased towards our target item, thus boosting its exposure. Speciî€›cally, we leverage the learnable item embeddings as an interface to facilitate model poisoning. To provide a proof-ofconcept, in Figure 1, we use t-SNE to visualize the item embeddings learned by a generic federated recommender (see Section 3.1 for model conî€›guration). As the model progresses from its initial state towards î€›nal convergence (Figure 1(a)-(c)), items from three diî€erent popularity groups gradually forms distinct clusters. To further reî€ect the strong popularity bias, we train a simplistic popularity classiî€›er (see Section 3.4) that predicts an itemâ€™s popularity group given its learned embeddings, and show its testing performance in Figure 1(d). In short, the high prediction accuracy (ğ¹1>0.8) again veriî€›es that the item embeddings in a well-trained federated recommender are highly discriminative w.r.t. their popularity. To this end, we proposepoisoningattackforitempromotion (PipAttack), a novel poisoning attack model targeted on recommender systems in the decentralized, federated setting. Apart from optimizing PipAttack under the explicit promotion constraint that straightforwardly pushes the recommender to raise the ranking score of the target item, we innovatively design two learning objectives, namely the popularity obfuscation constraint and distance constraint so as to achieve our attack goal via fewer model updates without imposing dramatic accuracy drops. On one hand, popularity obfuscation confuses the federated recommender by aligning the target item with popular ones in the embedding space, thus greatly beneî€›ting the target itemâ€™s exposure rate via the popularity bias. On the other hand, to avoid harming the usability of the federated recommender and being detected, our attack model bears a distance constraint to prevent the manipulated gradients uploaded by malicious users from deviating too far from the original ones. We summarize our main contributions as follows: â€¢To the best of our knowledge, we present the î€›rst systematic approach to poisoning federated recommender systems, where an adversary only has limited prior knowledge compared with existing centralized scenarios. Our study reveals the existence of recommendersâ€™ security backdoors even in a decentralized environment. â€¢We propose PipAttack, a novel attack model that induces the federated recommender to promote the target item by uploading carefully crafted local gradients through several malicious users. PipAttack assumes no access to the full training dataset and other benign usersâ€™ local information, and innovatively takes advantage of the inherent popularity bias to eî€ectively boost the exposure rate of the target item. â€¢Experiments on two real-world datasets demonstrate the advantageous performance of PipAttack even when defensive strategies are in place. Furthermore, compared with all baselines, PipAttack is more cost-eî€ective as it can reach the attack goal with fewer model updates and malicious users. In this section, we î€›rst revisit the fundamental settings of federated recommendation and then formally deî€›ne our research problem. LetVandUdenote the sets ofğ‘items andğ‘€users/devices, respectively. Each userğ‘¢âˆˆ Uowns a local training datasetD consisting of implicit feedback tuples(ğ‘¢, ğ‘£, ğ‘Ÿ), whereğ‘Ÿ=1 if ğ‘¢has visited itemğ‘£âˆˆ V(i.e., a positive instance), andğ‘Ÿ=0 if there is no interaction between them (i.e., a negative instance). Note that the negative instances are downsampled using a positiveto-negative ratio of 1 :ğ‘for eachğ‘¢due to the large amount of unobserved user-item interactions. Then, for every user, a federated recommender (FedRec) is trained to estimate the feedbackË†ğ‘Ÿâˆˆ [0,1]betweenğ‘¢and all items, whereË†ğ‘Ÿis also interpreted as the ranking/similarity score for a user-item pair. With the ranking score computed, FedRec then recommends an item list for each userğ‘¢ by selectingğ¾top-ranked items w.r.t.Ë†ğ‘Ÿ. It can be represented as: ğ¹ğ‘’ğ‘‘ğ‘…ğ‘’ğ‘ (ğ‘¢|Î˜) = {ğ‘£|Ë†ğ‘Ÿis top-ğ¾ in {Ë†ğ‘Ÿ} whereÂ¯I(ğ‘–)denotes unrated items ofğ‘¢andÎ˜denotes all the trainable parameters in FedRec. For notation simplicity, we directly use Î˜ to represent the recommendation model. Federated Learning Protocol.In FedRec, a central server coordinates individual user devices, of which each keepsDand a copy of the recommendation model locally. To train FedRec, the local model on theğ‘–-th user device is optimized locally by minimizing the following loss function: L= âˆ’Ãğ‘ŸlogË†ğ‘Ÿ+ (1 âˆ’ ğ‘Ÿ) log(1 âˆ’Ë†ğ‘Ÿ), (2) where we treat the estimatedË†ğ‘Ÿâˆˆ [0,1]as the probability of observing the interaction betweenğ‘¢andğ‘£. Then, the above cross-entropy loss quantiî€›es the diî€erence betweenË†ğ‘Ÿand the binary ground truthğ‘Ÿ. It is worth mentioning that, other popular distance-based loss functions (e.g., hinge loss and Bayesian personalized ranking loss [31]) are also applicable and behave similarly in FedRec. With the user-speciî€›c lossLcomputed, we can derive the gradients ofğ‘¢â€™s local modelÎ˜, denoted byâˆ‡Î˜. At iterationğ‘¡, a subset of usersUare randomly drawn. Eachğ‘¢âˆˆ Uthen downloads the latest global modelÎ˜and updates its local gradients w.r.t.D, denoted byâˆ‡Î˜. After the central server receives all local gradients submitted by|U|users, it aggregates the collected gradients to facilitate global model update. Speciî€›cally, FedRec follows the commonly used gradient averaging [27] to obtain the updated model Î˜with learning rate ğœ‚: The training proceeds iteratively until convergence. Unlike centralized recommenders, FedRec collects only each userâ€™s local model gradients instead of her/his own dataD, making existing poisoning attacks [9,25,36] fail due to their ill-posed assumptions on the access to the entire dataset. Furthermore, diî€erent local gradients are not shared across users, which further reduces the amount of prior knowledge that a malicious party can acquire. Following [11], we assume an adversary can compromise a small proportion of users in FedRec, which we term malicious users. The attack is scheduled to start at epochğ‘¡, and all malicious users participate in the training of FedRec normally before that. From epoch ğ‘¡, the malicious users start poisoning the global model by replacing the real local gradientsâˆ‡Î˜with purposefully crafted gradients gâˆ‡Î˜, so as to gradually guide the global model to recommend the target item more frequently. The task is formally deî€›ned as follows: Problem 1.Poisoning FedRec for Item Promotion.Given a federated recommender parameterized byÎ˜, our adversary aims to promote a target itemğ‘£âˆˆ Vby altering the local gradients submitted by every compromised malicious user deviceğ‘¢âˆˆ U at each update iterationğ‘¡, i.e.,ğ‘“:âˆ‡Î˜â†¦â†’gâˆ‡Î˜. Note that each malicious user will produce its unique deceptive gradientsgâˆ‡Î˜. Hence, the deceptive gradients{gâˆ‡Î˜}for all malicious users are parameters to be learned during the attack process, such that for each benign userğ‘¢âˆˆ U\U, the probability thatğ‘£appears in ğ¹ğ‘’ğ‘‘ğ‘…ğ‘’ğ‘ (ğ‘¢|Î˜) is maximized. In this section, we introduce our proposed PipAttack in details. Federated learning is compatible with the majority of latent factor models. Without loss of generality, we adopt neural collaborative î€›ltering (NCF) [20], a performant and widely adopted latent factor model, as the base recommender of FedRec. In short, NCF extends CF by leveraging anğ¿-layer feedforward networkğ¹ ğ¹ ğ‘ (Â·)to model the complex user-item interactions and estimateË†ğ‘Ÿ: whereu, vâˆˆ Rare respectively user and item embeddings,âŠ•is the vector concatenation,h âˆˆ Rdenotes the projection weights that corresponds to theğ‘‘-dimensional output of the last network layer, andğœis the sigmoid function that rectiî€›es the output to fall between 0 and 1. Meanwhile, it is worth noting that our attack method is generic and applicable to many other recommenders like feature-based [32] and graph-based [19] ones. In summary, the parametersÎ˜to be learned in FedRec are: the projection vectorh, all weights and biases in theğ¿-layer FFN, as well as embeddingsuandvfor all users and items. Meanwhile, an important note is that, as user embeddings are regarded highly sensitive as they directly reî€ect usersâ€™ personal interests, they are commonly disallowed to be shared across users in federated recommenders [8,30] for privacy reasons. In FedRec, this is achieved by withholdinguand its gradients on every user device, and the user embeddings are updated locally. Hence, the gradients ofuare excluded from bothâˆ‡Î˜andgâˆ‡Î˜that will be respectively submitted by benign and malicious users during the update of FedRec. To perform poisoning attacks on FedRec, traditional attack approaches designed for centralized recommenders are inapplicable. This is due mainly to the prior knowledge that needs to be accessible to the adversary, e.g., all user-item interactions [13,36] and even other benign usersâ€™ embeddings [12,25], which becomes an ill-posed assumption for federated settings as most of such information is retained at the user side. In short, FedRec substantially narrows down the prior knowledge and capability of an attack model, which is restricted to the following: I.The adversary can access the global modelÎ˜at any iterationğ‘¡, excluding all benign usersâ€™ embeddings, i.e., {u}. II.The adversary can access and alter all malicious usersâ€™ local models and their gradients. III.The adversary knows the whole item set (not interactions) which is commonly available on any e-commerce platform, as well as side information that reî€ects each itemâ€™s popularity. We will expand on this in Section 3.4. In what follows, we present the key components of PipAttack as shown in Fig 2 for learning all deceptive gradients{gâˆ‡Î˜}, which induces FedRec to promote the target item ğ‘£. Like many studies on poisoning attacks on recommender systems, our adversaryâ€™s goal is to manipulate the learned global model such that the generated recommendation results meet the adversaryâ€™s demand. Essentially, to give the target itemğ‘£more exposures (i.e., to be recommended to more users), we need to raise the ranking scoreË†ğ‘Ÿwhenever a userğ‘¢is paired withğ‘£. As a minimum requirement, we need to ensure all malicious users can receiveğ‘£ in their top-ğ¾recommendations. With the adversaryâ€™s control over a group of malicious usersU, we can explicitly boost the ranking score ofğ‘£for everyğ‘¢âˆˆ Uvia the following objective function: which encourages a large similarity score between every malicious user and the target item. Theoretically, this mimics the eî€ect of inserting fake interaction records (i.e., data poisoning) with the target item via malicious users, which can gradually drive the CFbased global recommender to predict positive feedback onğ‘£for other benign users who are similar to ğ‘¢âˆˆ U. Unfortunately, Eq.(5) requires the adversary to manipulate a relatively large number of malicious users in order to successfully and eî€œciently poison FedRec. Otherwise, after aggregating the local gradients from sampled users, the large user base of a recommender system can easily dilute the impact of deceptive gradients uploaded by a small group ofU. In this regard, on top of explicit promotion, we propose a more cost-eî€ective tactic in PipAttack from the popularity perspective. As discussed in Section 1, it is well acknowledged that CF-based recommenders are intrinsically biased towards popular items that are frequently visited [2], and FedRec is no exception. Compared with long-tail/unpopular items, popular items are more widely and frequently trained in a recommender, thus amplifying their likelihood of receiving a larger ranking score and gaining advantages in exposure rates. Hence, we make full use of the inherent popularity bias rooted in recommender systems, where we aim to learn deceptive gradients that can trick FedRec to â€˜mistakeâ€™ğ‘£as a popular item. In a latent factor model like FedRec, this can be achieved by poisoning the item embeddings in the global recommender, so that embeddingvis semantically similar to popular items inV. Intuitively, if a popular item andğ‘£are close to each other in the latent space, so will their ranking scores for the same user. Given the existence of popularity bias, vwill be promoted to more usersâ€™ recommendation lists. Availability of Popularity Information.Our popularity obfuscation strategy needs prior knowledge about itemsâ€™ popularity. However, directly obtaining such information via user-item interaction frequencies is infeasible as it requires access to the entire dataset. Fortunately, though the user behavioral data is protected, the prosperity of online service platforms has brought a wide range of visible clues on the popularity of items. For example, hot tracks are always displayed on music applications (e.g., Spotify) without revealing identities of their listeners, and e-commerce sites (e.g., eBay) records the sales volume of each product while keeping the purchasers anonymized. Furthermore, as PipAttack is aware of the item set, item popularity can be easily looked up on a fully public platform (e.g., Yelp). Thus, popularity information can be fetched from various public sources to assist our poisoning attack, and the prerequisites of FedRec remain intact. Popularity Estimator.The most straightforward way to facilitate such popularity obfuscation is to incorporate a distance metric to penalizes anygâˆ‡Î˜that enlarges the distance betweenğ‘£and a randomly sampled popular itemğ‘£. However, being a personalized model, whether an item can be recommended toğ‘¢is not only determined by its popularity, but also the user-item similarity. Therefore, such constraints will work the best only if the selectedğ‘£accounts for each userâ€™s personal preference, which is infeasible due to the fact that all benign users withinU\Uare intransparent to the adversary. In PipAttack, we propose a novel popularity estimatorbased method that collectively engages all the popular items in the item set. Speciî€›cally, we can assign each item a discrete label w.r.t. its popularity level obtained via public information. Then, suppose there areğ¶popularity classes, the popularity estimator ğ‘“(Â·)is a deep neural network (DNN) that inputs a learned item embeddingvat iteration stepğ‘¡, and computes ağ¶-dimensional probability distribution vectorË†yvia the î€›nal softmax layer. Each elementË†y[ğ‘] âˆˆ Ë†y(ğ‘ =1,2, ...,ğ¶) represents the probability thatğ‘£ belongs to classğ‘. We trainğ‘“(Â·)with cross-entropy loss on all (v, y) pairs for all ğ‘£ â‰  ğ‘£, where y = {0, 1}is the one-hot label: Boosting Item Popularity.Once we obtain a well-trainedğ‘“(Â·), it is highly reî€ective of the popularity characteristics encoded in each item embedding, where items at the same popularity level will have high semantic similarity in the embedding space. So, at theğ‘¡-th training iteration of FedRec, we aim to boost the predicted popularity ofğ‘£by making targeted updates on embeddingvâˆˆ Î˜with crafted deceptive gradientsgâˆ‡Î˜. This is achieved by minimizing the negative log-likelihood of classğ‘(i.e., the highest popularity level) in the output of ğ‘“(v): Note thatğ‘“(Â·)stays î€›xed and will no longer be updated after the popularity obfuscation starts. Essentially, by optimizingL, PipAttack generates gradientsgâˆ‡Î˜that enforcesğ‘£to approximate the characteristics of all items from the top popularity group in the embedding space, resulting in a boosted exposure rate. Generally, aggressively fabricated deceptive gradientsgâˆ‡Î˜may help the adversary achieve the attack goal with fewer training iterations, but will also incur strong discrepancies with the real gradientsâˆ‡Î˜, leading to a higher chance to be detected by the central server [5] and harm the performance of the global model. Hence, the crafted gradients should maintain a certain level of similarity with the genuine ones. As such, we place a constraint on the distance between eachgâˆ‡Î˜forğ‘¢âˆˆ Uand all malicious usersâ€™ original local gradient âˆ‡Î˜: where|| Â· ||is p-norm distance and we adopt|| Â· ||in PipAttack. In this subsection, we deî€›ne the loss function of PipAttack for model training. Instead of training each component separately, we combine their losses and use joint learning to optimize the following objective function: whereğ›¼andğ›¾are non-negative coeî€œcients to scale and balance the eî€ect of each part. In this section, we î€›rst outline the evaluation protocols for our PipAttacks and then conduct experiments on two real-world datasets to evaluate the performance of PipAttack. Particularly, we aim to answer the following research questions (RQs) via experiments: RQ1:Can PipAttack perform poisoning attacks eî€ectively on federated recommender systems? RQ2:Does PipAttack harm the recommendation performance of the federated recommender signiî€›cantly? RQ3: How does PipAttack beneî€›t from each key component? RQ4: What is the impact of hyperparameters to PipAttack? RQ5:Can PipAttack bypass defensive strategies deployed at the server side? We adopt two popular public datasets for evaluation, namely MovieL ens-1M (ML) [16] and Amazon (AZ) [17]. ML contains 1 million ratings involving 6,039 users and 3,705 movies, while AZ contains 103,593 ratings involving 13,174 users and 5,970 cellphone-related products. Following [18,20,21], we have binarized the user feedback, where all ratings are converted toğ‘Ÿ=1, and negative instances are sampledğ‘ =4 times the amount of positive ones. PipAttack needs to obtain the popularity labels of items. As described in Section 3.4, such information can be easily obtained in real-life scenarios (e.g., page views) without sacriî€›cing user identity in FedRec. However, as our datasets are mainly collected for pure recommendation research, there is no such side information available. Hence, we segment itemsâ€™ popularity into three levels by sorting all items by the amount of interactions they receive, with intervals of the top 10% (high popularity), top 10% to 45% (medium popularity), and the last 55% (low popularity). Note that we only use the full dataset once to compensate for the unavailable side information about item popularity, and the interaction records of each user are locally stored throughout the training of FedRec and poisoning attack. Following the common setting for attacking federated models [5], we î€›rst train FedRec without attacks (malicious users behave normally in that period) for several epochs, then PipAttack starts poisoning FedRec by activating all malicious users. To ensure fairness in evaluation, all tested methods are asked to attack the same pretrained FedRec model. We introduce our evaluation metrics below. Poisoning Attack Eî€ectiveness.We use exposure rate at rank ğ¾(ğ¸ğ‘…@ğ¾) as our evaluation metric. Suppose each user receives ğ¾recommended items, then for the target item to be promoted, ğ¸ğ‘…@ğ¾is the fraction of users whoseğ¾recommended items include the target item. Correspondingly, largerğ¸ğ‘…@ğ¾represents stronger poisoning eî€ectiveness. Notably, from both datasets, we select the least popular item as the target item in our experiments as this is the hardest possible item to promote. For each user, all items excluding positive training examples are used for ranking. Recommendation Eî€ectiveness.It is important that the poisoning attack does not harm the accuracy of FedRec. To evaluate the recommendation accuracy, we adopt the leave-one-out approach [21] to hold out ground truth items for evaluation. An item is held out for each user to build a validation set. Following [20], we employ hit ratio at rankğ¾(ğ»ğ‘…@ğ¾) to quantify the fraction of observing the ground truth item in the top-ğ¾recommendation lists. We compare PipAttack with î€›ve poisoning attack methods, where the î€›st three are model poisoning and the last two are data poisoning methods. Notably, many recent models are only designed to attack centralized recommenders, thus requiring prior knowledge Figure 3: Attack ((a)-(h)) and recommendation ((i)-(p)) results on ML and AZ. Note that the curves start from the î€›rst ep och after the attack starts. that cannot be obtained in the federated setting. Hence, we choose the following baselines that do not hold assumptions on those inaccessible knowledge:P1[5]: This work aims to poison federated learning models by directing the model to misclassify the target input.P2[4]: This is a general approach for attacking distributed models and evading defense mechanisms.Explicit Boosting (EB): The adversary directly optimizes the explicit item promotion objectiveL.Popular Attacks (PA)[15]: It injects fake interactions with both target and popular items via manipulated malicious users to promote the target item.Random Attacks (RA)[15]. It poisons a recommender in a similar way to PA, but uses fake interactions with the target item and randomly selected items. In FedRec, we set the latent dimensionğ‘‘, learning rate, local batch size to 64, 0.01 and 64, respectively. Each user is considered as an individual device, where 10% and 5% of the users (including both benign and malicious users) are randomly selected on ML and AZ at every iteration. Model parameters in FedRec are randomly initialized using Gaussian distribution (ğœ‡ =0, ğœ =1). The popularity estimatorğ‘“(Â·)is formulated as a 4-layer deep neural network with 32, 16, 8, and 3 hidden dimensions from bottom to top. In Pipattack, the local epoch and learning rate are respectively 30 and 0.01 on both datasets. We also examine the eî€ect of diî€erent fractionsğœof malicious users withğœ âˆˆ {10%,20%,30%,40%}on ML andğœ âˆˆ {5%,10%,20%,30%}on AZ. For the coeî€œcients in loss functionL, we setğ›¼ =60 whenğœ =10% andğ›¼ =20 when ğœ âˆˆ {20%,30%,40%}on ML, andğ›¼ =10 on AZ across all malicious user fractions. We apply ğ›¾ = 0.0005 on both datasets. Figure 3 shows the performance ofğ¸ğ‘…@5 w.r.t. to diî€erent malicious user fractions. It is clear that PipAttack is successful at inducing the recommender to recommend the target item. In fact, the recommender becomes highly conî€›dent in its recommendation results and recommend the target item to all the users after the 40th epoch, with just a small number of malicious users (i.e., 10% on ML and 5% on AZ). By increasing the fraction of fraudsters, the attack performance improves signiî€›cantly, as it is easier for the adversary to manipulate the parameters of the global model (a) Before Poisoning Attack (b) After Poisoning Attack (ğ¸ğ‘…@5 = 1) Figure 5: Visualization of item embeddings before and after being attacked by PipAttack. (a) With Distance Constraint (b) Without Distance Constraint. Figure 6: Comparison of gradient distributions between benign and malicious users. via more malicious users. Additionally, PipAttack outperforms all baselines consistently in terms ofğ¸ğ‘…@5, showing strong capability to promote the target items to all users. Furthermore, as the attack proceeds, our model can consistently reach 100% exposure rate in most scenarios, while there are obvious î€uctuations in the performance of other baseline methods. Finally, we observe that model poisoning methods (i.e., P1, P2 and Explicit Boosting) generally perform better than data poisoning methods (i.e., Random and Popular attack), indicating the superiority of model poisoning paradigm since the global recommender can be manipulated arbitrarily for mean even with limited number of compromised users. Recommendation accuracy plays a signiî€›cant role in the success of adversarial attacks. On one hand, it is an important property that the server can use to detect anomalous updates. On the other hand, only a recommender that is highly accurate will have a large user base for promoting the target item. Figure 3 shows the overall performance curve of FedRec after the attack starts. The î€›rst observation is that, PipAttack, which is the most powerful for item promotion, still achieves competitive recommendation results on both datasets. It proves that PipAttack can avoid harming the usefulness of FedRec. Another observation is that there is an overall upward trend for all baseline models except random attack. Finally, compared with model poisoning attacks, data poisoning attacks, especially random attack, cause a larger decrease on recommendation accuracy, which further conî€›rms the eî€ectiveness of model poisoning methods. We conduct ablation analysis to better understand the performance gain from the major components proposed in PipAttack. We set ğœ =20% throughout this set of experiments, and we discuss the key components below. 4.7.1 Explicit Promotion Constraint. We î€›rst remove explicit promotion constraintLand plot the new results in Figure 4. Apparently, it leads to severe performance drop on two datasets. As the main adversarial objective is to increase the ranking score for the target item, removingLdirectly limits PipAttackâ€™s capability. In addition, the degraded PipAttack is more eî€ective on the AZ than that on ML. A possible reason is that AZ is sparser and thus is easier to attack, despite the removal of explicit promotion constraint. 4.7.2 Popularity Obfuscation Constraint. We validate our hypothesis of leveraging the popularity bias in FedRec for poisoning attack. In Figure 4, without the popularity obfuscation constraintL, PipAttack suî€ers from the obviously inferior performance on both two datasets. For instance,ğ¸ğ‘…@5=100% is respectively achieved at 17th and 14th epochs on ML and AZ, which are far behind the full version of PipAttack. It conî€›rms that by taking advantage of the popularity bias, PipAttack can eî€ectively accomplish the attack goal with less attack attempts. To further verify the eî€œcacy of popularity obfuscation in PipAttack, we visualize the item embeddings via t-SNE in Figure 5. Obviously, after the poisoning attack, the target item embedding moves from the cluster of unpopular items to the cluster of the most popular items. Hence, the use of popularity bias is proved beneî€›cial to promoting the target item. 4.7.3 Distance Constraint. As introduced in [5], the server can use weight update statistics to detect anomalous updates and even refuse the corresponding updates. To verify the contribution of the distance constraint in Eq.(9), we derive one variant of PipAttack by removing the distance constraintL. In Figure 6, on the ML dataset, the averaged gradientsâ€™ distributions for all benign updates and malicious updates are plotted. It is clear that the deceptive gradientsâ€™ distribution under the distance constraint is similar to that of benign users. In contrast, the gradients are much sparser and diverges far from benign usersâ€™ gradients. Furthermore, we calculate the KL-divergence to measure the diî€erence between two distributions in each î€›gure. Specially, the result achieved with distance constraint (0.005) is much lower than the result achieved without it (0.177). Hence, PipAttack is stealthier, and is harder for the server to î€ag abnormal gradients. We answer RQ4 by investigating the performance î€uctuations of PipAttack with varied hyperparameters on both dataset. Specifically, we examine the values ofğ›¼in{1,10,20,40,60,80}on ML and{1,5,10,15,20,30}on AZ, respectively (ğœ =20%). Note that we omit the eî€ect ofğ›¾as it brings much less variation to the model performance. Figure 7 presents the results with variedğ›¼. As can be inferred from the î€›gure, all the best attack results are achieved withğ›¼ =20 on ML andğ›¼ =10 on AZ. Meanwhile, altering this coeî€œcient on the attack objective has less impact on the recommendation accuracy. Overall, settingğ›¼ =20 on ML andğ›¼ =10 on AZ is suî€œcient for promoting target item, while ensuring the high-quality recommendation of FedRec. In general, the mean aggregation rule used in FedRec assumes that the central server works in a secure environment. To answer RQ5, we investigate how our diî€erent attack models perform in the presence of attack-resistant aggregation rules, namely Bulyan [14] and Trimmed Mean [35]. Note that though Krum [6] is also a widely used defensive method, it is not considered in our work due to the severe accuracy loss it causes. We benchmark all methodsâ€™ performance on the ML dataset, and Figure 8 shows theğ¸ğ‘…@5 results with ğœ =20%. The î€›rst observation we can draw from the results is that, PipAttack consistently outperforms all baselines and maintains ğ¸ğ‘…@5=1, which further conî€›rms the superiority of PipAttack. The main goal of those resilient aggregation methods is to ensure convergence of the global model under attacks, while our modelâ€™s distance constraint eî€ectively ensures this. Our experiments reveal the limited robustness of existing defenses in federated recommendation, thus emphasising the need for more advanced defensive strategies on such poisoning attacks. A growing number of online platforms are deploying centralized recommender systems to increase user interaction and enrich shopping potential [29,37,40]. However, many studies show that attacking recommender systems can aî€ect usersâ€™ decisions in order to have target products recommended more often than before [9,22,24]. In [10], a reinforcement learning (RL)-based model is designed to generate fake proî€›les by copying the benign usersâ€™ proî€›les in the source domain, while [36] î€›rstly constructed a simulator recommender based on the full user-item interaction network, and then the results of the simulator system is used to design rewards function for policy training. [9,25] generate fake users through Generative Adversarial Networks (GAN) to achieve adversarial intend. [13] and [12] formulate the attack as an optimization problem by maximizing the hit ratio of the target item. However, many of these attack approaches fundamentally rely on the white-box model, in which the attacker requires the adversary to have full knowledge of the target model and dataset. Another variant of federated recommenders are devised to inhibit the availability of dataset [26,28,30,34]. For federated recommender systems, expecting complete access to the dataset and model is not realistic. [7,15] study the inî€uence of low-knowledge attack approaches to promote an item (e.g., random, popular attack), but the performance is unsatisfactory. Despite previous attack methods success with various adversarial objectives under federated setting such as misclassiî€›cation [4,5] and increased error rate [11], the study of promoting an item for federated recommender system still remains largely unexplored. Therefore, we propose a novel framework that does not have full knowledge of the target model to attack under the federated setting to î€›ll this gap. In this paper, we present a novel model poisoning attack framework for manipulating item promotion named PipAttack. We also prove that our attack model can work on the presence of defensive protocols. With three innovatively designed attack objectives, the attack model is able to perform eî€œcient attacks against federated recommender systems and maintain high-quality recommendation results generated by the poisoned recommender. Furthermore, the distance constraint plays an essential role in disguising malicious users as benign users to avoid detection. The experimental results based on two real-world datasets demonstrate the superiority and practicality of PipAttack over peer methods even in the presence of defensive strategies deployed at the server side. This work is supported by Australian Research Council Future Fellowship (Grant No. FT210100624), Discovery Project (Grant No. DP190101985) and Discovery Early Career Research Award (Grant No. DE200101465).